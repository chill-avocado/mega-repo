# Merged file for agent_frameworks/utils
# This file contains code merged from multiple repositories

import argparse
import logging
import tempfile
from typing import List
from typing import Tuple
from pygments import highlight
from pygments.formatters import TerminalFormatter
from pygments.lexers import PythonLexer
from sphinx.util.console import darkgreen
from sphinx.util.console import darkred
from sphinx.util.console import faint
from sphinx.util.console import red
from sphinx.util.console import teal
import subprocess

# From python/check_md_code_blocks.py
def extract_python_code_blocks(markdown_file_path: str) -> List[Tuple[str, int]]:
    """Extract Python code blocks from a Markdown file."""
    with open(markdown_file_path, "r", encoding="utf-8") as file:
        lines = file.readlines()

    code_blocks: List[Tuple[str, int]] = []
    in_code_block = False
    current_block: List[str] = []

    for i, line in enumerate(lines):
        if line.strip().startswith("```python"):
            in_code_block = True
            current_block = []
        elif line.strip().startswith("```"):
            in_code_block = False
            code_blocks.append(("\n".join(current_block), i - len(current_block) + 1))
        elif in_code_block:
            current_block.append(line)

    return code_blocks

# From python/check_md_code_blocks.py
def check_code_blocks(markdown_file_paths: List[str]) -> None:
    """Check Python code blocks in a Markdown file for syntax errors."""
    files_with_errors = []

    for markdown_file_path in markdown_file_paths:
        code_blocks = extract_python_code_blocks(markdown_file_path)
        had_errors = False
        for code_block, line_no in code_blocks:
            markdown_file_path_with_line_no = f"{markdown_file_path}:{line_no}"
            logger.info("Checking a code block in %s...", markdown_file_path_with_line_no)

            # Skip blocks that don't import autogen_agentchat, autogen_core, or autogen_ext
            if all(all(import_code not in code_block for import_code in [f"import {module}", f"from {module}"]) for module in ["autogen_agentchat", "autogen_core", "autogen_ext"]):
                logger.info(" " + darkgreen("OK[ignored]"))
                continue

            with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as temp_file:
                temp_file.write(code_block.encode("utf-8"))
                temp_file.flush()

                # Run pyright on the temporary file using subprocess.run
                import subprocess

                result = subprocess.run(["pyright", temp_file.name], capture_output=True, text=True)
                if result.returncode != 0:
                    logger.info(" " + darkred("FAIL"))
                    highlighted_code = highlight(code_block, PythonLexer(), TerminalFormatter())  # type: ignore
                    output = f"{faint('========================================================')}\n{red('Error')}: Pyright found issues in {teal(markdown_file_path_with_line_no)}:\n{faint('--------------------------------------------------------')}\n{highlighted_code}\n{faint('--------------------------------------------------------')}\n\n{teal('pyright output:')}\n{red(result.stdout)}{faint('========================================================')}\n"
                    logger.info(output)
                    had_errors = True
                else:
                    logger.info(" " + darkgreen("OK"))

        if had_errors:
            files_with_errors.append(markdown_file_path)

    if files_with_errors:
        raise RuntimeError("Syntax errors found in the following files:\n" + "\n".join(files_with_errors))

import glob
import sys
from pathlib import Path
import tomli
from poethepoet.app import PoeThePoet
from rich import print

# From python/run_task_in_pkgs_if_exist.py
def discover_projects(workspace_pyproject_file: Path) -> List[Path]:
    with workspace_pyproject_file.open("rb") as f:
        data = tomli.load(f)

    projects = data["tool"]["uv"]["workspace"]["members"]
    exclude = data["tool"]["uv"]["workspace"].get("exclude", [])

    all_projects: List[Path] = []
    for project in projects:
        if "*" in project:
            globbed = glob.glob(str(project), root_dir=workspace_pyproject_file.parent)
            globbed_paths = [Path(p) for p in globbed]
            all_projects.extend(globbed_paths)
        else:
            all_projects.append(Path(project))

    for project in exclude:
        if "*" in project:
            globbed = glob.glob(str(project), root_dir=workspace_pyproject_file.parent)
            globbed_paths = [Path(p) for p in globbed]
            all_projects = [p for p in all_projects if p not in globbed_paths]
        else:
            all_projects = [p for p in all_projects if p != Path(project)]

    return all_projects

# From python/run_task_in_pkgs_if_exist.py
def extract_poe_tasks(file: Path) -> set[str]:
    with file.open("rb") as f:
        data = tomli.load(f)

    tasks = set(data.get("tool", {}).get("poe", {}).get("tasks", {}).keys())

    # Check if there is an include too
    include: str | None = data.get("tool", {}).get("poe", {}).get("include", None)
    if include:
        include_file = file.parent / include
        if include_file.exists():
            tasks = tasks.union(extract_poe_tasks(include_file))

    return tasks

# From python/run_task_in_pkgs_if_exist.py
def main() -> None:
    pyproject_file = Path(__file__).parent / "pyproject.toml"
    projects = discover_projects(pyproject_file)

    if len(sys.argv) < 2:
        print("Please provide a task name")
        sys.exit(1)

    task_name = sys.argv[1]
    for project in projects:
        tasks = extract_poe_tasks(project / "pyproject.toml")
        if task_name in tasks:
            print(f"Running task {task_name} in {project}")
            app = PoeThePoet(cwd=project)
            result = app(cli_args=sys.argv[1:])
            if result:
                sys.exit(result)
        else:
            print(f"Task {task_name} not found in {project}")

from typing import Dict

import asyncio
from typing import Union
from autogen_core import DefaultTopicId
from autogen_core import MessageContext
from autogen_core import RoutedAgent
from autogen_core import message_handler
from protos.agent_events_pb2 import ConversationClosed
from protos.agent_events_pb2 import Input
from protos.agent_events_pb2 import NewMessageReceived
from protos.agent_events_pb2 import Output

# From core_xlang_hello_python_agent/user_input.py
class UserProxy(RoutedAgent):
    """An agent that allows the user to play the role of an agent in the conversation via input."""

    DEFAULT_DESCRIPTION = "A human user."

    def __init__(
        self,
        description: str = DEFAULT_DESCRIPTION,
    ) -> None:
        super().__init__(description)

    @message_handler
    async def handle_user_chat_input(self, message: input_types, ctx: MessageContext) -> None:
        logger = logging.getLogger("autogen_core")

        if isinstance(message, Input):
            response = await self.ainput("User input ('exit' to quit): ")
            response = response.strip()
            logger.info(response)

            await self.publish_message(NewMessageReceived(message=response), topic_id=DefaultTopicId())
        elif isinstance(message, Output):
            logger.info(message.message)
        else:
            pass

    async def ainput(self, prompt: str) -> str:
        return await asyncio.to_thread(input, f"{prompt} ")

from setuptools import setup


import pytest

# From autogen-ext/conftest.py
def pytest_addoption(parser):
    parser.addoption(
        "--grpc", action="store_true", default=False, help="run grpc tests"
    )

# From autogen-ext/conftest.py
def pytest_collection_modifyitems(config, items):
    grpc_option_passed = config.getoption("--grpc")
    skip_grpc = pytest.mark.skip(reason="Need --grpc option to run")
    skip_non_grpc = pytest.mark.skip(reason="Skipped since --grpc passed")

    for item in items:
        if "grpc" in item.keywords and not grpc_option_passed:
            item.add_marker(skip_grpc)
        elif "grpc" not in item.keywords and grpc_option_passed:
            item.add_marker(skip_non_grpc)


from typing import Sequence
from opentelemetry.sdk.trace import ReadableSpan
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry.sdk.trace.export import SpanExporter
from opentelemetry.sdk.trace.export import SpanExportResult

# From autogen_test_utils/telemetry_test_utils.py
class MyTestExporter(SpanExporter):
    def __init__(self) -> None:
        self.exported_spans: List[ReadableSpan] = []

    def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        self.exported_spans.extend(spans)
        return SpanExportResult.SUCCESS

    def shutdown(self) -> None:
        pass

    def clear(self) -> None:
        """Clears the list of exported spans."""
        self.exported_spans.clear()

    def get_exported_spans(self) -> List[ReadableSpan]:
        """Returns the list of exported spans."""
        return self.exported_spans

# From autogen_test_utils/telemetry_test_utils.py
def get_test_tracer_provider(exporter: MyTestExporter) -> TracerProvider:
    tracer_provider = TracerProvider()
    tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))
    return tracer_provider

# From autogen_test_utils/telemetry_test_utils.py
def export(self, spans: Sequence[ReadableSpan]) -> SpanExportResult:
        self.exported_spans.extend(spans)
        return SpanExportResult.SUCCESS

# From autogen_test_utils/telemetry_test_utils.py
def shutdown(self) -> None:
        pass

# From autogen_test_utils/telemetry_test_utils.py
def clear(self) -> None:
        """Clears the list of exported spans."""
        self.exported_spans.clear()

# From autogen_test_utils/telemetry_test_utils.py
def get_exported_spans(self) -> List[ReadableSpan]:
        """Returns the list of exported spans."""
        return self.exported_spans

import os
import warnings
from typing import Optional
import typer
import uvicorn
from typing_extensions import Annotated
from version import VERSION
from autogenstudio.lite import LiteStudio

# From autogenstudio/cli.py
def get_env_file_path():
    app_dir = os.path.join(os.path.expanduser("~"), ".autogenstudio")
    if not os.path.exists(app_dir):
        os.makedirs(app_dir, exist_ok=True)
    return os.path.join(app_dir, "temp_env_vars.env")

# From autogenstudio/cli.py
def ui(
    host: str = "127.0.0.1",
    port: int = 8081,
    workers: int = 1,
    reload: Annotated[bool, typer.Option("--reload")] = False,
    docs: bool = True,
    appdir: str | None = None,
    database_uri: Optional[str] = None,
    auth_config: Optional[str] = None,
    upgrade_database: bool = False,
):
    """
    Run the AutoGen Studio UI.

    Args:
        host (str, optional): Host to run the UI on. Defaults to 127.0.0.1 (localhost).
        port (int, optional): Port to run the UI on. Defaults to 8081.
        workers (int, optional): Number of workers to run the UI with. Defaults to 1.
        reload (bool, optional): Whether to reload the UI on code changes. Defaults to False.
        docs (bool, optional): Whether to generate API docs. Defaults to False.
        appdir (str, optional): Path to the AutoGen Studio app directory. Defaults to None.
        database_uri (str, optional): Database URI to connect to. Defaults to None.
        auth_config (str, optional): Path to authentication configuration YAML. Defaults to None.
        upgrade_database (bool, optional): Whether to upgrade the database. Defaults to False.
    """
    # Write configuration
    env_vars = {
        "AUTOGENSTUDIO_HOST": host,
        "AUTOGENSTUDIO_PORT": port,
        "AUTOGENSTUDIO_API_DOCS": str(docs),
    }

    if appdir:
        env_vars["AUTOGENSTUDIO_APPDIR"] = appdir
    if database_uri:
        env_vars["AUTOGENSTUDIO_DATABASE_URI"] = database_uri
    if auth_config:
        if not os.path.exists(auth_config):
            typer.echo(f"Error: Auth config file not found: {auth_config}", err=True)
            raise typer.Exit(1)
        env_vars["AUTOGENSTUDIO_AUTH_CONFIG"] = auth_config
    if upgrade_database:
        env_vars["AUTOGENSTUDIO_UPGRADE_DATABASE"] = "1"

    # Create temporary env file to share configuration with uvicorn workers
    env_file_path = get_env_file_path()
    with open(env_file_path, "w") as temp_env:
        for key, value in env_vars.items():
            temp_env.write(f"{key}={value}\n")

    uvicorn.run(
        "autogenstudio.web.app:app",
        host=host,
        port=port,
        workers=workers,
        reload=reload,
        reload_excludes=["**/alembic/*", "**/alembic.ini", "**/versions/*"] if reload else None,
        env_file=env_file_path,
    )

# From autogenstudio/cli.py
def serve(
    team: str = "",
    host: str = "127.0.0.1",
    port: int = 8084,
    workers: int = 1,
    reload: Annotated[bool, typer.Option("--reload")] = False,
    docs: bool = False,
):
    """
    Serve an API Endpoint based on an AutoGen Studio workflow json file.

    Args:
        team (str): Path to the team json file.
        host (str, optional): Host to run the UI on. Defaults to 127.0.0.1 (localhost).
        port (int, optional): Port to run the UI on. Defaults to 8084
        workers (int, optional): Number of workers to run the UI with. Defaults to 1.
        reload (bool, optional): Whether to reload the UI on code changes. Defaults to False.
        docs (bool, optional): Whether to generate API docs. Defaults to False.

    """

    os.environ["AUTOGENSTUDIO_API_DOCS"] = str(docs)
    os.environ["AUTOGENSTUDIO_TEAM_FILE"] = team

    # validate the team file
    if not os.path.exists(team):
        raise ValueError(f"Team file not found: {team}")

    uvicorn.run(
        "autogenstudio.web.serve:app",
        host=host,
        port=port,
        workers=workers,
        reload=reload,
    )

# From autogenstudio/cli.py
def version():
    """
    Print the version of the AutoGen Studio UI CLI.
    """

    typer.echo(f"AutoGen Studio  CLI version: {VERSION}")

# From autogenstudio/cli.py
def lite(
    team: Optional[str] = None,
    host: str = "127.0.0.1",
    port: int = 8080,
    auto_open: bool = True,
    session_name: str = "Lite Session",
):
    """
    Launch AutoGen Studio in lightweight mode for quick experimentation.

    Args:
        team (str, optional): Path to team JSON/YAML file. If not provided, uses a default team.
        host (str): Host to run on. Defaults to 127.0.0.1.
        port (int): Port to run on. Defaults to 8080.
        auto_open (bool): Auto-open browser. Defaults to True.
        session_name (str): Name for the auto-created session.
    """
    from autogenstudio.lite import LiteStudio

    # Create and start studio instance
    studio = LiteStudio(team=team, host=host, port=port, auto_open=auto_open, session_name=session_name)

    try:
        studio.start()  # Blocking call for CLI
    except KeyboardInterrupt:
        studio.stop()

# From autogenstudio/cli.py
def run():
    app()


from abc import ABC
from abc import abstractmethod
from datetime import datetime
from typing import Any
from typing import Type
from autogen_agentchat.base import TaskResult
from autogen_agentchat.base import Team
from autogen_agentchat.messages import ChatMessage
from autogen_agentchat.messages import MultiModalMessage
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_core import Component
from autogen_core import ComponentBase
from autogen_core import ComponentModel
from autogen_core import Image
from autogen_core.models import ChatCompletionClient
from autogen_core.models import UserMessage
from pydantic import BaseModel
from typing_extensions import Self
from datamodel.eval import EvalRunResult
from datamodel.eval import EvalTask

# From eval/runners.py
class BaseEvalRunnerConfig(BaseModel):
    """Base configuration for evaluation runners."""

    name: str
    description: str = ""
    metadata: Dict[str, Any] = {}

# From eval/runners.py
class BaseEvalRunner(ABC, ComponentBase[BaseEvalRunnerConfig]):
    """Base class for evaluation runners that defines the interface for running evaluations.

    This class provides the core interface that all evaluation runners must implement.
    Subclasses should implement the run method to define how a specific evaluation is executed.
    """

    component_type = "eval_runner"

    def __init__(self, name: str, description: str = "", metadata: Optional[Dict[str, Any]] = None):
        self.name = name
        self.description = description
        self.metadata = metadata or {}

    @abstractmethod
    async def run(self, task: EvalTask, cancellation_token: Optional[CancellationToken] = None) -> EvalRunResult:
        """Run the evaluation on the provided task and return a result.

        Args:
            task: The task to evaluate
            cancellation_token: Optional token to cancel the evaluation

        Returns:
            EvaluationResult: The result of the evaluation
        """
        pass

    def _to_config(self) -> BaseEvalRunnerConfig:
        """Convert the runner configuration to a configuration object for serialization."""
        return BaseEvalRunnerConfig(name=self.name, description=self.description, metadata=self.metadata)

# From eval/runners.py
class ModelEvalRunnerConfig(BaseEvalRunnerConfig):
    """Configuration for ModelEvalRunner."""

    model_client: ComponentModel

# From eval/runners.py
class ModelEvalRunner(BaseEvalRunner, Component[ModelEvalRunnerConfig]):
    """Evaluation runner that uses a single LLM to process tasks.

    This runner sends the task directly to a model client and returns the response.
    """

    component_config_schema = ModelEvalRunnerConfig
    component_type = "eval_runner"
    component_provider_override = "autogenstudio.eval.runners.ModelEvalRunner"

    def __init__(
        self,
        model_client: ChatCompletionClient,
        name: str = "Model Runner",
        description: str = "Evaluates tasks using a single LLM",
        metadata: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(name, description, metadata)
        self.model_client = model_client

    async def run(self, task: EvalTask, cancellation_token: Optional[CancellationToken] = None) -> EvalRunResult:
        """Run the task with the model client and return the result."""
        # Create initial result object
        result = EvalRunResult()

        try:
            model_input = []
            if isinstance(task.input, str):
                text_message = UserMessage(content=task.input, source="user")
                model_input.append(text_message)
            elif isinstance(task.input, list):
                message_content = [x for x in task.input]
                model_input.append(UserMessage(content=message_content, source="user"))
            # Run with the model
            model_result = await self.model_client.create(messages=model_input, cancellation_token=cancellation_token)

            model_response = model_result.content if isinstance(model_result, str) else model_result.model_dump()

            task_result = TaskResult(
                messages=[TextMessage(content=str(model_response), source="model")],
            )
            result = EvalRunResult(result=task_result, status=True, start_time=datetime.now(), end_time=datetime.now())

        except Exception as e:
            result = EvalRunResult(status=False, error=str(e), end_time=datetime.now())

        return result

    def _to_config(self) -> ModelEvalRunnerConfig:
        """Convert to configuration object including model client configuration."""
        base_config = super()._to_config()
        return ModelEvalRunnerConfig(
            name=base_config.name,
            description=base_config.description,
            metadata=base_config.metadata,
            model_client=self.model_client.dump_component(),
        )

    @classmethod
    def _from_config(cls, config: ModelEvalRunnerConfig) -> Self:
        """Create from configuration object with serialized model client."""
        model_client = ChatCompletionClient.load_component(config.model_client)
        return cls(
            name=config.name,
            description=config.description,
            metadata=config.metadata,
            model_client=model_client,
        )

# From eval/runners.py
class TeamEvalRunnerConfig(BaseEvalRunnerConfig):
    """Configuration for TeamEvalRunner."""

    team: ComponentModel

# From eval/runners.py
class TeamEvalRunner(BaseEvalRunner, Component[TeamEvalRunnerConfig]):
    """Evaluation runner that uses a team of agents to process tasks.

    This runner creates and runs a team based on a team configuration.
    """

    component_config_schema = TeamEvalRunnerConfig
    component_type = "eval_runner"
    component_provider_override = "autogenstudio.eval.runners.TeamEvalRunner"

    def __init__(
        self,
        team: Union[Team, ComponentModel],
        name: str = "Team Runner",
        description: str = "Evaluates tasks using a team of agents",
        metadata: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(name, description, metadata)
        self._team = team if isinstance(team, Team) else Team.load_component(team)

    async def run(self, task: EvalTask, cancellation_token: Optional[CancellationToken] = None) -> EvalRunResult:
        """Run the task with the team and return the result."""
        # Create initial result object
        result = EvalRunResult()

        try:
            team_task: Sequence[ChatMessage] = []
            if isinstance(task.input, str):
                team_task.append(TextMessage(content=task.input, source="user"))
            if isinstance(task.input, list):
                for message in task.input:
                    if isinstance(message, str):
                        team_task.append(TextMessage(content=message, source="user"))
                    elif isinstance(message, Image):
                        team_task.append(MultiModalMessage(source="user", content=[message]))

            # Run task with team
            team_result = await self._team.run(task=team_task, cancellation_token=cancellation_token)

            result = EvalRunResult(result=team_result, status=True, start_time=datetime.now(), end_time=datetime.now())

        except Exception as e:
            result = EvalRunResult(status=False, error=str(e), end_time=datetime.now())

        return result

    def _to_config(self) -> TeamEvalRunnerConfig:
        """Convert to configuration object including team configuration."""
        base_config = super()._to_config()
        return TeamEvalRunnerConfig(
            name=base_config.name,
            description=base_config.description,
            metadata=base_config.metadata,
            team=self._team.dump_component(),
        )

    @classmethod
    def _from_config(cls, config: TeamEvalRunnerConfig) -> Self:
        """Create from configuration object with serialized team configuration."""
        return cls(
            team=Team.load_component(config.team),
            name=config.name,
            description=config.description,
            metadata=config.metadata,
        )

from loguru import logger
from datamodel.eval import EvalDimensionScore
from datamodel.eval import EvalJudgeCriteria
from datamodel.eval import EvalScore

# From eval/judges.py
class BaseEvalJudgeConfig(BaseModel):
    """Base configuration for evaluation judges."""

    name: str = "Base Judge"
    description: str = ""
    metadata: Dict[str, Any] = {}

# From eval/judges.py
class BaseEvalJudge(ABC, ComponentBase[BaseEvalJudgeConfig]):
    """Abstract base class for evaluation judges."""

    component_type = "eval_judge"

    def __init__(self, name: str = "Base Judge", description: str = "", metadata: Optional[Dict[str, Any]] = None):
        self.name = name
        self.description = description
        self.metadata = metadata or {}

    @abstractmethod
    async def judge(
        self,
        task: EvalTask,
        result: EvalRunResult,
        criteria: List[EvalJudgeCriteria],
        cancellation_token: Optional[CancellationToken] = None,
    ) -> EvalScore:
        """Judge the result of an evaluation run."""
        pass

    def _to_config(self) -> BaseEvalJudgeConfig:
        """Convert the judge configuration to a configuration object for serialization."""
        return BaseEvalJudgeConfig(name=self.name, description=self.description, metadata=self.metadata)

# From eval/judges.py
class LLMEvalJudgeConfig(BaseEvalJudgeConfig):
    """Configuration for LLMEvalJudge."""

    model_client: Any

# From eval/judges.py
class LLMEvalJudge(BaseEvalJudge, Component[LLMEvalJudgeConfig]):
    """Judge that uses an LLM to evaluate results."""

    component_config_schema = LLMEvalJudgeConfig
    component_type = "eval_judge"
    component_provider_override = "autogenstudio.eval.judges.LLMEvalJudge"

    def __init__(
        self,
        model_client: ChatCompletionClient,
        name: str = "LLM Judge",
        description: str = "Evaluates results using an LLM",
        metadata: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(name, description, metadata)
        self.model_client = model_client

    async def judge(
        self,
        task: EvalTask,
        result: EvalRunResult,
        criteria: List[EvalJudgeCriteria],
        cancellation_token: Optional[CancellationToken] = None,
    ) -> EvalScore:
        """Judge the result using an LLM."""
        # Create a score object
        score = EvalScore(max_value=10.0)

        # Judge each dimension in parallel
        dimension_score_tasks = []
        for criterion in criteria:
            dimension_score_tasks.append(self._judge_dimension(task, result, criterion, cancellation_token))

        dimension_scores = await asyncio.gather(*dimension_score_tasks)
        score.dimension_scores = dimension_scores

        # Calculate overall score (average of dimension scores)
        valid_scores = [ds.score for ds in dimension_scores if ds.score is not None]
        if valid_scores:
            score.overall_score = sum(valid_scores) / len(valid_scores)

        return score

    async def _judge_dimension(
        self,
        task: EvalTask,
        result: EvalRunResult,
        criterion: EvalJudgeCriteria,
        cancellation_token: Optional[CancellationToken] = None,
    ) -> EvalDimensionScore:
        """Judge a specific dimension."""
        # Format task and result for the LLM
        task_description = self._format_task(task)
        result_description = result.model_dump()

        # Create the prompt
        prompt = f"""
        You are evaluating the quality of a system response to a task.
        Task: {task_description}Response: {result_description}
        Evaluation criteria: {criterion.dimension}
        {criterion.prompt}
        Score the response on a scale from {criterion.min_value} to {criterion.max_value}.
        First, provide a detailed explanation of your evaluation.
        Then, give your final score as a single number between 0 and {criterion.max_value}.
        Format your answer should be a json for the EvalDimensionScore class:
        {{
            "dimension": "{criterion.dimension}",
            "reason": "<explanation>",
            "score": <score>
        }}
        Please ensure the score is a number between {criterion.min_value} and {criterion.max_value}.
        If you cannot evaluate the response, please return a score of null.
        If the response is not relevant, please return a score of 0.
        If the response is perfect, please return a score of {criterion.max_value}.
        If the response is not relevant, please return a score of 0.
        If the response is perfect, please return a score of {criterion.max_value}.
        """

        # Get judgment from LLM
        model_input = []
        text_message = UserMessage(content=prompt, source="user")
        model_input.append(text_message)

        # Run with the model client in the same format as used in runners
        model_result = await self.model_client.create(
            messages=model_input,
            cancellation_token=cancellation_token,
            json_output=EvalDimensionScore,
        )

        # Extract content from the response
        model_response = model_result.content if isinstance(model_result.content, str) else str(model_result.content)

        try:
            # validate response string as EvalDimensionScore
            model_response = EvalDimensionScore.model_validate_json(model_response)
            return model_response
        except Exception as e:
            logger.warning(f"Failed to parse LLM response: {e}", model_result.content)
            return EvalDimensionScore(
                dimension=criterion.dimension,
                reason="Failed to parse response",
                score=0.0,
                max_value=criterion.max_value,
                min_value=criterion.min_value,
            )

    def _format_task(self, task: EvalTask) -> str:
        """Format the task for the LLM."""
        task_parts = []

        if task.description:
            task_parts.append(task.description)
        if isinstance(task.input, str):
            task_parts.append(task.input)
        elif isinstance(task.input, list):
            task_parts.append("\n".join(str(x) for x in task.input if isinstance(x, str)))

        return "\n".join(task_parts)

    def _parse_judgment(self, judgment_text: str, max_value: float) -> Tuple[str, Optional[float]]:
        """Parse judgment text to extract explanation and score."""
        explanation = ""
        score = None

        # Simple parsing - could be improved with regex
        lines = judgment_text.split("\n")
        for line in lines:
            if line.strip().lower().startswith("explanation:"):
                explanation = line.split(":", 1)[1].strip()
            elif line.strip().lower().startswith("score:"):
                try:
                    score_str = line.split(":", 1)[1].strip()
                    score = float(score_str)
                    # Ensure score is within bounds
                    score = min(max(score, 0), max_value)
                except (ValueError, IndexError):
                    pass

        return explanation, score

    def _to_config(self) -> LLMEvalJudgeConfig:
        """Convert to configuration object including model client configuration."""
        base_config = super()._to_config()
        return LLMEvalJudgeConfig(
            name=base_config.name,
            description=base_config.description,
            metadata=base_config.metadata,
            model_client=self.model_client.dump_component(),
        )

    @classmethod
    def _from_config(cls, config: LLMEvalJudgeConfig) -> Self:
        """Create from configuration object with serialized model client."""
        model_client = ChatCompletionClient.load_component(config.model_client)
        return cls(
            model_client=model_client, name=config.name, description=config.description, metadata=config.metadata
        )

import uuid
from pdb import run
from typing import TypedDict
from database.db_manager import DatabaseManager
from datamodel.db import EvalCriteriaDB
from datamodel.db import EvalRunDB
from datamodel.db import EvalTaskDB
from datamodel.eval import EvalRunStatus
from judges import BaseEvalJudge
from runners import BaseEvalRunner

# From eval/orchestrator.py
class DimensionScore(TypedDict):
    score: Optional[float]
    reason: Optional[str]

# From eval/orchestrator.py
class RunEntry(TypedDict):
    id: str
    name: str
    task_name: str
    runner_type: str
    overall_score: Optional[float]
    scores: List[Optional[float]]
    reasons: Optional[List[Optional[str]]]

# From eval/orchestrator.py
class TabulatedResults(TypedDict):
    dimensions: List[str]
    runs: List[RunEntry]

# From eval/orchestrator.py
class EvalOrchestrator:
    """
    Orchestrator for evaluation runs.

    This class manages the lifecycle of evaluation tasks, criteria, and runs.
    It can operate with or without a database manager for persistence.
    """

    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        """
        Initialize the orchestrator.

        Args:
            db_manager: Optional database manager for persistence.
                        If None, data is stored in memory only.
        """
        self._db_manager = db_manager

        # In-memory storage (used when db_manager is None)
        self._tasks: Dict[str, EvalTask] = {}
        self._criteria: Dict[str, EvalJudgeCriteria] = {}
        self._runs: Dict[str, Dict[str, Any]] = {}

        # Active runs tracking
        self._active_runs: Dict[str, asyncio.Task] = {}

    # ----- Task Management -----

    async def create_task(self, task: EvalTask) -> str:
        """
        Create a new evaluation task.

        Args:
            task: The evaluation task to create

        Returns:
            Task ID
        """
        if not task.task_id:
            task.task_id = str(uuid.uuid4())

        if self._db_manager:
            # Store in database
            task_db = EvalTaskDB(name=task.name, description=task.description, config=task)
            response = self._db_manager.upsert(task_db)
            if not response.status:
                logger.error(f"Failed to store task: {response.message}")
                raise RuntimeError(f"Failed to store task: {response.message}")
            task_id = str(response.data.get("id")) if response.data else str(task.task_id)
        else:
            # Store in memory
            task_id = str(task.task_id)
            self._tasks[task_id] = task

        return task_id

    async def get_task(self, task_id: str) -> Optional[EvalTask]:
        """
        Retrieve an evaluation task by ID.

        Args:
            task_id: The ID of the task to retrieve

        Returns:
            The task if found, None otherwise
        """
        if self._db_manager:
            # Retrieve from database
            response = self._db_manager.get(EvalTaskDB, filters={"id": int(task_id) if task_id.isdigit() else task_id})

            if response.status and response.data and len(response.data) > 0:
                task_data = response.data[0]
                return (
                    task_data.get("config")
                    if isinstance(task_data.get("config"), EvalTask)
                    else EvalTask.model_validate(task_data.get("config"))
                )
        else:
            # Retrieve from memory
            return self._tasks.get(task_id)

        return None

    async def list_tasks(self) -> List[EvalTask]:
        """
        List all available evaluation tasks.

        Returns:
            List of evaluation tasks
        """
        if self._db_manager:
            # Retrieve from database
            response = self._db_manager.get(EvalTaskDB)

            tasks = []
            if response.status and response.data:
                for task_data in response.data:
                    config = task_data.get("config")
                    if config:
                        if isinstance(config, EvalTask):
                            tasks.append(config)
                        else:
                            tasks.append(EvalTask.model_validate(config))
            return tasks
        else:
            # Retrieve from memory
            return list(self._tasks.values())

    # ----- Criteria Management -----

    async def create_criteria(self, criteria: EvalJudgeCriteria) -> str:
        """
        Create new evaluation criteria.

        Args:
            criteria: The evaluation criteria to create

        Returns:
            Criteria ID
        """
        criteria_id = str(uuid.uuid4())

        if self._db_manager:
            # Store in database
            criteria_db = EvalCriteriaDB(name=criteria.dimension, description=criteria.prompt, config=criteria)
            response = self._db_manager.upsert(criteria_db)
            if not response.status:
                logger.error(f"Failed to store criteria: {response.message}")
                raise RuntimeError(f"Failed to store criteria: {response.message}")
            criteria_id = str(response.data.get("id")) if response.data else criteria_id
        else:
            # Store in memory
            self._criteria[criteria_id] = criteria

        return criteria_id

    async def get_criteria(self, criteria_id: str) -> Optional[EvalJudgeCriteria]:
        """
        Retrieve evaluation criteria by ID.

        Args:
            criteria_id: The ID of the criteria to retrieve

        Returns:
            The criteria if found, None otherwise
        """
        if self._db_manager:
            # Retrieve from database
            response = self._db_manager.get(
                EvalCriteriaDB, filters={"id": int(criteria_id) if criteria_id.isdigit() else criteria_id}
            )

            if response.status and response.data and len(response.data) > 0:
                criteria_data = response.data[0]
                return (
                    criteria_data.get("config")
                    if isinstance(criteria_data.get("config"), EvalJudgeCriteria)
                    else EvalJudgeCriteria.model_validate(criteria_data.get("config"))
                )
        else:
            # Retrieve from memory
            return self._criteria.get(criteria_id)

        return None

    async def list_criteria(self) -> List[EvalJudgeCriteria]:
        """
        List all available evaluation criteria.

        Returns:
            List of evaluation criteria
        """
        if self._db_manager:
            # Retrieve from database
            response = self._db_manager.get(EvalCriteriaDB)

            criteria_list = []
            if response.status and response.data:
                for criteria_data in response.data:
                    config = criteria_data.get("config")
                    if config:
                        if isinstance(config, EvalJudgeCriteria):
                            criteria_list.append(config)
                        else:
                            criteria_list.append(EvalJudgeCriteria.model_validate(config))
            return criteria_list
        else:
            # Retrieve from memory
            return list(self._criteria.values())

    # ----- Run Management -----

    async def create_run(
        self,
        task: Union[str, EvalTask],
        runner: BaseEvalRunner,
        judge: BaseEvalJudge,
        criteria: List[Union[str, EvalJudgeCriteria]],
        name: str = "",
        description: str = "",
    ) -> str:
        """
        Create a new evaluation run configuration.

        Args:
            task: The task to evaluate (ID or task object)
            runner: The runner to use for evaluation
            judge: The judge to use for evaluation
            criteria: List of criteria to use for evaluation (IDs or criteria objects)
            name: Name for the run
            description: Description for the run

        Returns:
            Run ID
        """
        # Resolve task
        task_obj = None
        if isinstance(task, str):
            task_obj = await self.get_task(task)
            if not task_obj:
                raise ValueError(f"Task not found: {task}")
        else:
            task_obj = task

        # Resolve criteria
        criteria_objs = []
        for criterion in criteria:
            if isinstance(criterion, str):
                criterion_obj = await self.get_criteria(criterion)
                if not criterion_obj:
                    raise ValueError(f"Criteria not found: {criterion}")
                criteria_objs.append(criterion_obj)
            else:
                criteria_objs.append(criterion)

        # Generate run ID
        run_id = str(uuid.uuid4())

        # Create run configuration
        runner_config = runner.dump_component() if hasattr(runner, "dump_component") else runner._to_config()
        judge_config = judge.dump_component() if hasattr(judge, "dump_component") else judge._to_config()

        if self._db_manager:
            # Store in database
            run_db = EvalRunDB(
                name=name or f"Run {run_id}",
                description=description,
                task_id=int(task) if isinstance(task, str) and task.isdigit() else None,
                runner_config=runner_config.model_dump(),
                judge_config=judge_config.model_dump(),
                criteria_configs=criteria_objs,
                status=EvalRunStatus.PENDING,
            )
            response = self._db_manager.upsert(run_db)
            if not response.status:
                logger.error(f"Failed to store run: {response.message}")
                raise RuntimeError(f"Failed to store run: {response.message}")
            run_id = str(response.data.get("id")) if response.data else run_id
        else:
            # Store in memory
            self._runs[run_id] = {
                "task": task_obj,
                "runner_config": runner_config,
                "judge_config": judge_config,
                "criteria_configs": [c.model_dump() for c in criteria_objs],
                "status": EvalRunStatus.PENDING,
                "created_at": datetime.now(),
                "run_result": None,
                "score_result": None,
                "name": name or f"Run {run_id}",
                "description": description,
            }

        return run_id

    async def start_run(self, run_id: str) -> None:
        """
        Start an evaluation run.

        Args:
            run_id: The ID of the run to start
        """
        # Check if run is already active
        if run_id in self._active_runs:
            logger.warning(f"Run {run_id} is already active")
            return

        # Start the run asynchronously
        run_task = asyncio.create_task(self._execute_run(run_id))
        self._active_runs[run_id] = run_task

        # Update run status
        await self._update_run_status(run_id, EvalRunStatus.RUNNING)

    async def _execute_run(self, run_id: str) -> None:
        """
        Execute an evaluation run.

        Args:
            run_id: The ID of the run to execute
        """
        try:
            # Get run configuration
            run_config = await self._get_run_config(run_id)
            if not run_config:
                raise ValueError(f"Run not found: {run_id}")

            # Get task
            task = run_config.get("task")
            if not task:
                raise ValueError(f"Task not found for run: {run_id}")

            # Initialize runner
            runner_config = run_config.get("runner_config")
            runner = BaseEvalRunner.load_component(runner_config) if runner_config else None

            # Initialize judge
            judge_config = run_config.get("judge_config")
            judge = BaseEvalJudge.load_component(judge_config) if judge_config else None

            if not runner or not judge:
                raise ValueError(f"Runner or judge not found for run: {run_id}")

            # Initialize criteria
            criteria_configs = run_config.get("criteria_configs")
            criteria = []
            if criteria_configs:
                criteria = [
                    EvalJudgeCriteria.model_validate(c) if not isinstance(c, EvalJudgeCriteria) else c
                    for c in criteria_configs
                ]

            # Execute runner
            logger.info(f"Starting runner for run {run_id}")
            start_time = datetime.now()
            run_result = await runner.run(task)

            # Update run result
            await self._update_run_result(run_id, run_result)

            if not run_result.status:
                logger.error(f"Runner failed for run {run_id}: {run_result.error}")
                await self._update_run_status(run_id, EvalRunStatus.FAILED)
                return

            # Execute judge
            logger.info(f"Starting judge for run {run_id}")
            score_result = await judge.judge(task, run_result, criteria)

            # Update score result
            await self._update_score_result(run_id, score_result)

            # Update run status
            end_time = datetime.now()
            await self._update_run_completed(run_id, start_time, end_time)

            logger.info(f"Run {run_id} completed successfully")

        except Exception as e:
            logger.exception(f"Error executing run {run_id}: {str(e)}")
            await self._update_run_error(run_id, str(e))
        finally:
            # Remove from active runs
            if run_id in self._active_runs:
                del self._active_runs[run_id]

    async def get_run_status(self, run_id: str) -> Optional[EvalRunStatus]:
        """
        Get the status of an evaluation run.

        Args:
            run_id: The ID of the run

        Returns:
            The run status if found, None otherwise
        """
        run_config = await self._get_run_config(run_id)
        return run_config.get("status") if run_config else None

    async def get_run_result(self, run_id: str) -> Optional[EvalRunResult]:
        """
        Get the result of an evaluation run.

        Args:
            run_id: The ID of the run

        Returns:
            The run result if found, None otherwise
        """
        run_config = await self._get_run_config(run_id)
        if not run_config:
            return None

        run_result = run_config.get("run_result")
        if not run_result:
            return None

        return run_result if isinstance(run_result, EvalRunResult) else EvalRunResult.model_validate(run_result)

    async def get_run_score(self, run_id: str) -> Optional[EvalScore]:
        """
        Get the score of an evaluation run.

        Args:
            run_id: The ID of the run

        Returns:
            The run score if found, None otherwise
        """
        run_config = await self._get_run_config(run_id)
        if not run_config:
            return None

        score_result = run_config.get("score_result")
        if not score_result:
            return None

        return score_result if isinstance(score_result, EvalScore) else EvalScore.model_validate(score_result)

    async def list_runs(self) -> List[Dict[str, Any]]:
        """
        List all available evaluation runs.

        Returns:
            List of run configurations
        """
        if self._db_manager:
            # Retrieve from database
            response = self._db_manager.get(EvalRunDB)

            runs = []
            if response.status and response.data:
                for run_data in response.data:
                    runs.append(
                        {
                            "id": run_data.get("id"),
                            "name": run_data.get("name"),
                            "status": run_data.get("status"),
                            "created_at": run_data.get("created_at"),
                            "updated_at": run_data.get("updated_at"),
                        }
                    )
            return runs
        else:
            # Retrieve from memory
            return [
                {
                    "id": run_id,
                    "name": run_config.get("name"),
                    "status": run_config.get("status"),
                    "created_at": run_config.get("created_at"),
                    "updated_at": run_config.get("updated_at", run_config.get("created_at")),
                }
                for run_id, run_config in self._runs.items()
            ]

    async def cancel_run(self, run_id: str) -> bool:
        """
        Cancel an active evaluation run.

        Args:
            run_id: The ID of the run to cancel

        Returns:
            True if the run was cancelled, False otherwise
        """
        # Check if run is active
        if run_id not in self._active_runs:
            logger.warning(f"Run {run_id} is not active")
            return False

        # Cancel the run task
        try:
            self._active_runs[run_id].cancel()
            await self._update_run_status(run_id, EvalRunStatus.CANCELED)
            del self._active_runs[run_id]
            return True
        except Exception as e:
            logger.error(f"Failed to cancel run {run_id}: {str(e)}")
            return False

    # ----- Helper Methods -----

    async def _get_run_config(self, run_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the configuration of an evaluation run.

        Args:
            run_id: The ID of the run

        Returns:
            The run configuration if found, None otherwise
        """
        if self._db_manager:
            # Retrieve from database
            response = self._db_manager.get(EvalRunDB, filters={"id": int(run_id) if run_id.isdigit() else run_id})

            if response.status and response.data and len(response.data) > 0:
                run_data = response.data[0]

                # Get task
                task = None
                if run_data.get("task_id"):
                    task_response = self._db_manager.get(EvalTaskDB, filters={"id": run_data.get("task_id")})
                    if task_response.status and task_response.data and len(task_response.data) > 0:
                        task_data = task_response.data[0]
                        task = (
                            task_data.get("config")
                            if isinstance(task_data.get("config"), EvalTask)
                            else EvalTask.model_validate(task_data.get("config"))
                        )

                return {
                    "task": task,
                    "runner_config": run_data.get("runner_config"),
                    "judge_config": run_data.get("judge_config"),
                    "criteria_configs": run_data.get("criteria_configs"),
                    "status": run_data.get("status"),
                    "run_result": run_data.get("run_result"),
                    "score_result": run_data.get("score_result"),
                    "name": run_data.get("name"),
                    "description": run_data.get("description"),
                    "created_at": run_data.get("created_at"),
                    "updated_at": run_data.get("updated_at"),
                }
        else:
            # Retrieve from memory
            return self._runs.get(run_id)

        return None

    async def _update_run_status(self, run_id: str, status: EvalRunStatus) -> None:
        """
        Update the status of an evaluation run.

        Args:
            run_id: The ID of the run
            status: The new status
        """
        if self._db_manager:
            # Update in database
            response = self._db_manager.get(EvalRunDB, filters={"id": int(run_id) if run_id.isdigit() else run_id})

            if response.status and response.data and len(response.data) > 0:
                run_data = response.data[0]
                run_db = EvalRunDB.model_validate(run_data)
                run_db.status = status
                run_db.updated_at = datetime.now()
                self._db_manager.upsert(run_db)
        else:
            # Update in memory
            if run_id in self._runs:
                self._runs[run_id]["status"] = status
                self._runs[run_id]["updated_at"] = datetime.now()

    async def _update_run_result(self, run_id: str, run_result: EvalRunResult) -> None:
        """
        Update the result of an evaluation run.

        Args:
            run_id: The ID of the run
            run_result: The run result
        """
        if self._db_manager:
            # Update in database
            response = self._db_manager.get(EvalRunDB, filters={"id": int(run_id) if run_id.isdigit() else run_id})

            if response.status and response.data and len(response.data) > 0:
                run_data = response.data[0]
                run_db = EvalRunDB.model_validate(run_data)
                run_db.run_result = run_result
                run_db.updated_at = datetime.now()
                self._db_manager.upsert(run_db)
        else:
            # Update in memory
            if run_id in self._runs:
                self._runs[run_id]["run_result"] = run_result
                self._runs[run_id]["updated_at"] = datetime.now()

    async def _update_score_result(self, run_id: str, score_result: EvalScore) -> None:
        """
        Update the score of an evaluation run.

        Args:
            run_id: The ID of the run
            score_result: The score result
        """
        if self._db_manager:
            # Update in database
            response = self._db_manager.get(EvalRunDB, filters={"id": int(run_id) if run_id.isdigit() else run_id})

            if response.status and response.data and len(response.data) > 0:
                run_data = response.data[0]
                run_db = EvalRunDB.model_validate(run_data)
                run_db.score_result = score_result
                run_db.updated_at = datetime.now()
                self._db_manager.upsert(run_db)
        else:
            # Update in memory
            if run_id in self._runs:
                self._runs[run_id]["score_result"] = score_result
                self._runs[run_id]["updated_at"] = datetime.now()

    async def _update_run_completed(self, run_id: str, start_time: datetime, end_time: datetime) -> None:
        """
        Update a run as completed.

        Args:
            run_id: The ID of the run
            start_time: The start time
            end_time: The end time
        """
        if self._db_manager:
            # Update in database
            response = self._db_manager.get(EvalRunDB, filters={"id": int(run_id) if run_id.isdigit() else run_id})

            if response.status and response.data and len(response.data) > 0:
                run_data = response.data[0]
                run_db = EvalRunDB.model_validate(run_data)
                run_db.status = EvalRunStatus.COMPLETED
                run_db.start_time = start_time
                run_db.end_time = end_time
                run_db.updated_at = datetime.now()
                self._db_manager.upsert(run_db)
        else:
            # Update in memory
            if run_id in self._runs:
                self._runs[run_id]["status"] = EvalRunStatus.COMPLETED
                self._runs[run_id]["start_time"] = start_time
                self._runs[run_id]["end_time"] = end_time
                self._runs[run_id]["updated_at"] = datetime.now()

    async def _update_run_error(self, run_id: str, error_message: str) -> None:
        """
        Update a run with an error.

        Args:
            run_id: The ID of the run
            error_message: The error message
        """
        if self._db_manager:
            # Update in database
            response = self._db_manager.get(EvalRunDB, filters={"id": int(run_id) if run_id.isdigit() else run_id})

            if response.status and response.data and len(response.data) > 0:
                run_data = response.data[0]
                run_db = EvalRunDB.model_validate(run_data)
                run_db.status = EvalRunStatus.FAILED
                run_db.error_message = error_message
                run_db.end_time = datetime.now()
                run_db.updated_at = datetime.now()
                self._db_manager.upsert(run_db)
        else:
            # Update in memory
            if run_id in self._runs:
                self._runs[run_id]["status"] = EvalRunStatus.FAILED
                self._runs[run_id]["error_message"] = error_message
                self._runs[run_id]["end_time"] = datetime.now()
                self._runs[run_id]["updated_at"] = datetime.now()

    async def tabulate_results(self, run_ids: List[str], include_reasons: bool = False) -> TabulatedResults:
        """
        Generate a tabular representation of evaluation results across runs.

        This method collects scores across different runs and organizes them by
        dimension, making it easy to create visualizations like radar charts.

        Args:
            run_ids: List of run IDs to include in the tabulation
            include_reasons: Whether to include scoring reasons in the output

        Returns:
            A dictionary with structured data suitable for visualization
        """
        result: TabulatedResults = {"dimensions": [], "runs": []}

        # Parallelize fetching of run configs and scores
        fetch_tasks = []
        for run_id in run_ids:
            fetch_tasks.append(self._get_run_config(run_id))
            fetch_tasks.append(self.get_run_score(run_id))

        # Wait for all fetches to complete
        fetch_results = await asyncio.gather(*fetch_tasks)

        # Process fetched data
        dimensions_set = set()
        run_data = {}

        for i in range(0, len(fetch_results), 2):
            run_id = run_ids[i // 2]
            run_config = fetch_results[i]
            score = fetch_results[i + 1]

            # Store run data for later processing
            run_data[run_id] = (run_config, score)

            # Collect dimensions
            if score and score.dimension_scores:
                for dim_score in score.dimension_scores:
                    dimensions_set.add(dim_score.dimension)

        # Convert dimensions to sorted list
        result["dimensions"] = sorted(list(dimensions_set))

        # Process each run's data
        for run_id, (run_config, score) in run_data.items():
            if not run_config or not score:
                continue

            # Determine runner type
            runner_type = "unknown"
            if run_config.get("runner_config"):
                runner_config = run_config.get("runner_config")
                if runner_config is not None and "provider" in runner_config:
                    if "ModelEvalRunner" in runner_config["provider"]:
                        runner_type = "model"
                    elif "TeamEvalRunner" in runner_config["provider"]:
                        runner_type = "team"

            # Get task name
            task = run_config.get("task")
            task_name = task.name if task else "Unknown Task"

            # Create run entry
            run_entry: RunEntry = {
                "id": run_id,
                "name": run_config.get("name", f"Run {run_id}"),
                "task_name": task_name,
                "runner_type": runner_type,
                "overall_score": score.overall_score,
                "scores": [],
                "reasons": [] if include_reasons else None,
            }

            # Build dimension lookup map for O(1) access
            dim_map = {ds.dimension: ds for ds in score.dimension_scores}

            # Populate scores aligned with dimensions
            for dim in result["dimensions"]:
                dim_score = dim_map.get(dim)
                if dim_score:
                    run_entry["scores"].append(dim_score.score)
                    if include_reasons:
                        run_entry["reasons"].append(dim_score.reason)  # type: ignore
                else:
                    run_entry["scores"].append(None)
                    if include_reasons:
                        run_entry["reasons"].append(None)  # type: ignore

            result["runs"].append(run_entry)

        return result

from fastapi import WebSocketDisconnect
from pydantic.networks import AnyUrl

# From mcp/utils.py
class McpOperationError(Exception):
    """Raised when MCP operation fails"""

    pass

# From mcp/utils.py
def extract_real_error(e: Exception) -> str:
    """Extract the real error message from potentially wrapped exceptions"""
    error_parts: List[str] = []

    # Handle ExceptionGroup (Python 3.11+)
    if hasattr(e, "exceptions") and getattr(e, "exceptions", None):
        exceptions_list = getattr(e, "exceptions", [])
        for sub_exc in exceptions_list:
            error_parts.append(f"{type(sub_exc).__name__}: {str(sub_exc)}")

    # Handle chained exceptions
    elif hasattr(e, "__cause__") and e.__cause__:
        current = e
        while current:
            error_parts.append(f"{type(current).__name__}: {str(current)}")
            current = getattr(current, "__cause__", None)

    # Handle context exceptions
    elif hasattr(e, "__context__") and e.__context__:
        error_parts.append(f"Context: {type(e.__context__).__name__}: {str(e.__context__)}")
        error_parts.append(f"Error: {type(e).__name__}: {str(e)}")

    # Default case
    else:
        error_parts.append(f"{type(e).__name__}: {str(e)}")

    return " | ".join(error_parts)

# From mcp/utils.py
def serialize_for_json(obj: Any) -> Any:
    """Convert objects to JSON-serializable format"""
    if isinstance(obj, AnyUrl):
        return str(obj)
    elif isinstance(obj, dict):
        return {str(k): serialize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [serialize_for_json(item) for item in obj]
    elif hasattr(obj, "model_dump"):
        return serialize_for_json(obj.model_dump())
    else:
        return obj

# From mcp/utils.py
def is_websocket_disconnect(e: Exception) -> bool:
    """Check if an exception (potentially nested) is a WebSocket disconnect"""

    def check_exception(exc: BaseException) -> bool:
        if isinstance(exc, WebSocketDisconnect):
            return True

        exc_name = type(exc).__name__
        exc_str = str(exc)

        if "WebSocketDisconnect" in exc_name or "NO_STATUS_RCVD" in exc_str:
            return True

        # Recursively check ExceptionGroup
        if hasattr(exc, "exceptions") and getattr(exc, "exceptions", None):
            exceptions_list = getattr(exc, "exceptions", [])
            for sub_exc in exceptions_list:
                if check_exception(sub_exc):
                    return True

        # Check chained exceptions
        if hasattr(exc, "__cause__") and exc.__cause__:
            if check_exception(exc.__cause__):
                return True

        # Check context exceptions
        if hasattr(exc, "__context__") and exc.__context__:
            if check_exception(exc.__context__):
                return True

        return False

    return check_exception(e)

# From mcp/utils.py
def check_exception(exc: BaseException) -> bool:
        if isinstance(exc, WebSocketDisconnect):
            return True

        exc_name = type(exc).__name__
        exc_str = str(exc)

        if "WebSocketDisconnect" in exc_name or "NO_STATUS_RCVD" in exc_str:
            return True

        # Recursively check ExceptionGroup
        if hasattr(exc, "exceptions") and getattr(exc, "exceptions", None):
            exceptions_list = getattr(exc, "exceptions", [])
            for sub_exc in exceptions_list:
                if check_exception(sub_exc):
                    return True

        # Check chained exceptions
        if hasattr(exc, "__cause__") and exc.__cause__:
            if check_exception(exc.__cause__):
                return True

        # Check context exceptions
        if hasattr(exc, "__context__") and exc.__context__:
            if check_exception(exc.__context__):
                return True

        return False

from typing import Protocol
from mcp import ClientSession
from utils import McpOperationError
from utils import extract_real_error
from utils import serialize_for_json

# From mcp/client.py
class MCPEventHandler(Protocol):
    """Interface for handling MCP events"""

    async def on_initialized(self, session_id: str, capabilities: Any) -> None:
        """Called when MCP session is initialized"""
        ...

    async def on_operation_result(self, operation: str, data: Dict[str, Any]) -> None:
        """Called when an MCP operation completes successfully"""
        ...

    async def on_operation_error(self, operation: str, error: str) -> None:
        """Called when an MCP operation fails"""
        ...

    async def on_mcp_activity(self, activity_type: str, message: str, details: Dict[str, Any]) -> None:
        """Called for MCP protocol activity"""
        ...

    async def on_elicitation_request(self, request_id: str, message: str, requested_schema: Any) -> None:
        """Called when MCP requests user input"""
        ...

# From mcp/client.py
class MCPClient:
    """Handles MCP protocol operations independently of transport"""

    def __init__(self, session: ClientSession, session_id: str, event_handler: MCPEventHandler):
        self.session = session
        self.session_id = session_id
        self.event_handler = event_handler
        self._initialized = False
        self._capabilities = None

    async def initialize(self) -> None:
        """Initialize the MCP session"""
        try:
            initialize_result = await self.session.initialize()

            if initialize_result:
                self._capabilities = initialize_result.capabilities
            else:
                self._capabilities = None

            self._initialized = True

            # Notify handler
            await self.event_handler.on_initialized(
                self.session_id, serialize_for_json(self._capabilities.model_dump()) if self._capabilities else None
            )

        except Exception as e:
            await self.event_handler.on_operation_error("initialize", str(e))
            raise

    async def handle_operation(self, operation: Dict[str, Any]) -> None:
        """Handle an MCP operation - this preserves the exact behavior of handle_mcp_operation"""
        operation_type = operation.get("operation")

        try:
            if operation_type == "list_tools":
                result = await self.session.list_tools()
                tools_data = [serialize_for_json(tool.model_dump()) for tool in result.tools]
                await self.event_handler.on_operation_result("list_tools", {"tools": tools_data})

            elif operation_type == "call_tool":
                tool_name = operation.get("tool_name")
                arguments = operation.get("arguments", {})
                if not tool_name:
                    raise McpOperationError("Tool name is required")

                result = await self.session.call_tool(tool_name, arguments)
                await self.event_handler.on_operation_result(
                    "call_tool", {"tool_name": tool_name, "result": serialize_for_json(result.model_dump())}
                )

            elif operation_type == "list_resources":
                result = await self.session.list_resources()
                await self.event_handler.on_operation_result("list_resources", serialize_for_json(result.model_dump()))

            elif operation_type == "read_resource":
                uri = operation.get("uri")
                if not uri:
                    raise McpOperationError("Resource URI is required")

                result = await self.session.read_resource(uri)
                await self.event_handler.on_operation_result("read_resource", serialize_for_json(result.model_dump()))

            elif operation_type == "list_prompts":
                result = await self.session.list_prompts()
                prompts_data = [serialize_for_json(prompt.model_dump()) for prompt in result.prompts]
                await self.event_handler.on_operation_result("list_prompts", {"prompts": prompts_data})

            elif operation_type == "get_prompt":
                name = operation.get("name")
                arguments = operation.get("arguments")
                if not name:
                    raise McpOperationError("Prompt name is required")

                result = await self.session.get_prompt(name, arguments)
                await self.event_handler.on_operation_result("get_prompt", serialize_for_json(result.model_dump()))

            else:
                await self.event_handler.on_operation_error(
                    operation_type or "unknown", f"Unknown operation: {operation_type}"
                )

        except Exception as e:
            real_error = extract_real_error(e)
            await self.event_handler.on_operation_error(operation_type or "unknown", real_error)

    @property
    def capabilities(self):
        return self._capabilities

# From mcp/client.py
def capabilities(self):
        return self._capabilities

from datetime import timezone
from mcp.shared.context import RequestContext
from mcp.shared.session import RequestResponder
from mcp.types import ClientResult
from mcp.types import CreateMessageRequestParams
from mcp.types import CreateMessageResult
from mcp.types import ElicitRequestParams
from mcp.types import ElicitResult
from mcp.types import ErrorData
from mcp.types import ServerNotification
from mcp.types import ServerRequest
from mcp.types import TextContent
from wsbridge import MCPWebSocketBridge

# From mcp/callbacks.py
def create_message_handler(bridge: MCPWebSocketBridge):
    """Create a message handler callback that streams MCP protocol messages to the UI"""

    async def message_handler(
        message: RequestResponder[ServerRequest, ClientResult] | ServerNotification | Exception,
    ) -> None:
        try:
            if isinstance(message, Exception):
                await bridge.on_mcp_activity(
                    "error", f"Protocol error: {str(message)}", {"details": extract_real_error(message)}
                )

            elif hasattr(message, "method"):
                method = getattr(message, "method", "unknown")
                params = getattr(message, "params", None)

                await bridge.on_mcp_activity(
                    "protocol",
                    f"MCP {method}",
                    {
                        "method": method,
                        "params": serialize_for_json(params) if params else None,
                        "message_type": type(message).__name__,
                    },
                )

            else:
                await bridge.on_mcp_activity(
                    "protocol",
                    f"{type(message).__name__}",
                    {
                        "message_type": type(message).__name__,
                        "content": serialize_for_json(message) if hasattr(message, "model_dump") else str(message),
                    },
                )

        except Exception as e:
            logger.error(f"Error in message handler: {extract_real_error(e)}")

    return message_handler

# From mcp/callbacks.py
def create_sampling_callback(bridge: MCPWebSocketBridge):
    """Create a sampling callback that handles AI sampling requests from tools"""

    async def sampling_callback(
        context: RequestContext[Any, Any, Any],
        params: CreateMessageRequestParams,
    ) -> CreateMessageResult | ErrorData:
        try:
            request_id = str(uuid.uuid4())

            await bridge.on_mcp_activity(
                "sampling",
                f"Tool requested AI sampling for {len(params.messages)} message(s)",
                {
                    "request_id": request_id,
                    "params": serialize_for_json(params.model_dump()),
                    "context": "Tool is requesting AI to generate a response",
                },
            )

            dummy_response = CreateMessageResult(
                role="assistant",
                content=TextContent(
                    type="text",
                    text="[AutoGen Studio Default Sampling Response - This is a placeholder response for AI sampling requests. In a production setup, this would be handled by your configured LLM.]",
                ),
                model="autogen-studio-default",
            )

            await bridge.on_mcp_activity(
                "sampling",
                "Provided default sampling response to tool",
                {
                    "request_id": request_id,
                    "response": serialize_for_json(dummy_response.model_dump()),
                    "note": "This is a placeholder response - configure an LLM for real sampling",
                },
            )

            logger.info("Handled sampling request with default response")
            return dummy_response

        except Exception as e:
            error_msg = extract_real_error(e)
            logger.error(f"Error in sampling callback: {error_msg}")

            await bridge.on_mcp_activity("error", f"Sampling callback error: {error_msg}", {"error": error_msg})

            return ErrorData(code=-32603, message=f"Sampling failed: {error_msg}")

    return sampling_callback

# From mcp/callbacks.py
def create_elicitation_callback(
    bridge: MCPWebSocketBridge,
) -> Tuple[Any, Dict[str, asyncio.Future[ElicitResult | ErrorData]]]:
    """Create an elicitation callback that handles user input requests from tools"""

    async def elicitation_callback(
        context: RequestContext[Any, Any, Any],
        params: ElicitRequestParams,
    ) -> ElicitResult | ErrorData:
        try:
            request_id = str(uuid.uuid4())

            await bridge.on_mcp_activity(
                "elicitation",
                f"Tool requesting user input: {params.message}",
                {
                    "request_id": request_id,
                    "message": params.message,
                    "requestedSchema": serialize_for_json(params.requestedSchema) if params.requestedSchema else None,
                    "context": "Tool is requesting additional information from user",
                },
            )

            await bridge.on_elicitation_request(
                request_id,
                params.message,
                serialize_for_json(params.requestedSchema) if params.requestedSchema else None,
            )

            response_future: asyncio.Future[ElicitResult | ErrorData] = asyncio.Future()
            bridge.pending_elicitations[request_id] = response_future

            try:
                user_response = await asyncio.wait_for(response_future, timeout=60.0)

                await bridge.on_mcp_activity(
                    "elicitation",
                    "User responded to elicitation request",
                    {"request_id": request_id, "response": serialize_for_json(user_response), "status": "completed"},
                )

                return user_response

            except asyncio.TimeoutError:
                logger.warning(f"User did not respond to elicitation request {request_id} within 60 seconds")
                error_msg = "User did not respond to elicitation request within 60 seconds"

                await bridge.on_mcp_activity(
                    "error",
                    f"Elicitation timeout: {error_msg}",
                    {"request_id": request_id, "error": error_msg, "timeout": 60},
                )

                return ErrorData(code=-32603, message=error_msg)

            finally:
                bridge.pending_elicitations.pop(request_id, None)

        except Exception as e:
            error_msg = extract_real_error(e)
            logger.error(f"Error in elicitation callback: {error_msg}")

            await bridge.on_mcp_activity("error", f"Elicitation callback error: {error_msg}", {"error": error_msg})

            return ErrorData(code=-32603, message=f"Elicitation failed: {error_msg}")

    return elicitation_callback, bridge.pending_elicitations

import json
from fastapi import WebSocket
from client import MCPClient
from client import MCPEventHandler
from utils import is_websocket_disconnect
from web.routes.mcp import active_sessions
from fastapi.websockets import WebSocketState

# From mcp/wsbridge.py
class MCPWebSocketBridge(MCPEventHandler):
    """Bridges WebSocket connections to MCP operations"""

    def __init__(self, websocket: WebSocket, session_id: str):
        self.websocket = websocket
        self.session_id = session_id
        self.mcp_client: Optional[MCPClient] = None
        self.pending_elicitations: Dict[str, asyncio.Future[ElicitResult | ErrorData]] = {}
        self._running = True

    async def send_message(self, message: Dict[str, Any]) -> None:
        """Send a message through the WebSocket"""
        try:
            from fastapi.websockets import WebSocketState

            if self.websocket.client_state == WebSocketState.CONNECTED:
                serialized_message = serialize_for_json(message)
                await self.websocket.send_json(serialized_message)
        except Exception as e:
            real_error = extract_real_error(e)
            logger.error(f"Error sending WebSocket message: {real_error}")

    # Implement MCPEventHandler interface
    async def on_initialized(self, session_id: str, capabilities: Any) -> None:
        await self.send_message(
            {
                "type": "initialized",
                "session_id": session_id,
                "capabilities": capabilities,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        )

    async def on_operation_result(self, operation: str, data: Dict[str, Any]) -> None:
        await self.send_message(
            {
                "type": "operation_result",
                "operation": operation,
                "data": data,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        )

    async def on_operation_error(self, operation: str, error: str) -> None:
        await self.send_message(
            {
                "type": "operation_error",
                "operation": operation,
                "error": error,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        )

    async def on_mcp_activity(self, activity_type: str, message: str, details: Dict[str, Any]) -> None:
        await self.send_message(
            {
                "type": "mcp_activity",
                "activity_type": activity_type,
                "message": message,
                "details": details,
                "session_id": self.session_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        )

    async def on_elicitation_request(self, request_id: str, message: str, requested_schema: Any) -> None:
        await self.send_message(
            {
                "type": "elicitation_request",
                "request_id": request_id,
                "message": message,
                "requestedSchema": requested_schema,
                "session_id": self.session_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        )

    def set_mcp_client(self, mcp_client: MCPClient) -> None:
        """Set the MCP client after initialization"""
        self.mcp_client = mcp_client

    async def handle_websocket_message(self, message: Dict[str, Any]) -> None:
        """Handle incoming WebSocket messages"""
        message_type = message.get("type")

        # Update last activity for session tracking
        from datetime import datetime, timezone

        from ..web.routes.mcp import active_sessions

        if self.session_id in active_sessions:
            active_sessions[self.session_id]["last_activity"] = datetime.now(timezone.utc)

        if message_type == "operation":
            # CRITICAL: Run in background task to avoid blocking message loop
            # This preserves the exact behavior from the original code
            if self.mcp_client:
                asyncio.create_task(self.mcp_client.handle_operation(message))
            else:
                await self.send_message(
                    {
                        "type": "error",
                        "error": "MCP client not initialized",
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    }
                )

        elif message_type == "ping":
            await self.send_message({"type": "pong", "timestamp": datetime.now(timezone.utc).isoformat()})

        elif message_type == "elicitation_response":
            await self._handle_elicitation_response(message)

        else:
            await self.send_message(
                {
                    "type": "error",
                    "error": f"Unknown message type: {message_type}",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            )

    async def _handle_elicitation_response(self, message: Dict[str, Any]) -> None:
        """Handle user response to elicitation request"""
        request_id = message.get("request_id")

        if not request_id:
            await self.send_message(
                {
                    "type": "error",
                    "error": "Missing request_id in elicitation response",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            )
            return

        if request_id in self.pending_elicitations:
            try:
                action = message.get("action", "cancel")
                data = message.get("data", {})

                if action == "accept":
                    result = ElicitResult(action="accept", content=data)
                elif action == "decline":
                    result = ElicitResult(action="decline")
                else:
                    result = ElicitResult(action="cancel")

                future = self.pending_elicitations[request_id]
                if not future.done():
                    future.set_result(result)
                else:
                    logger.warning(f"Future for elicitation request {request_id} was already done")

            except Exception as e:
                error_msg = extract_real_error(e)
                logger.error(f"Error processing elicitation response: {error_msg}")

                future = self.pending_elicitations.get(request_id)
                if future and not future.done():
                    future.set_result(
                        ErrorData(code=-32603, message=f"Error processing elicitation response: {error_msg}")
                    )
        else:
            logger.warning(f"Unknown elicitation request_id: {request_id}")
            await self.send_message(
                {
                    "type": "operation_error",
                    "error": f"Unknown elicitation request_id: {request_id}",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            )

    async def run(self) -> None:
        """Main message loop"""
        try:
            while self._running:
                try:
                    raw_message = await self.websocket.receive_text()
                    message = json.loads(raw_message)
                    await self.handle_websocket_message(message)

                except json.JSONDecodeError:
                    logger.warning(f"Invalid JSON received from session {self.session_id}")
                    await self.send_message(
                        {
                            "type": "error",
                            "error": "Invalid message format",
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        }
                    )

        except Exception as e:
            if not is_websocket_disconnect(e):
                real_error = extract_real_error(e)
                logger.error(f"Error in message loop: {real_error}")
                raise

    def stop(self) -> None:
        """Stop the bridge"""
        self._running = False

# From mcp/wsbridge.py
def set_mcp_client(self, mcp_client: MCPClient) -> None:
        """Set the MCP client after initialization"""
        self.mcp_client = mcp_client

# From mcp/wsbridge.py
def stop(self) -> None:
        """Stop the bridge"""
        self._running = False

import threading
import time
import webbrowser
from autogenstudio.gallery.builder import create_default_lite_team

# From lite/studio.py
class LiteStudio:
    """
    Core class for managing AutoGen Studio lite mode instances.
    Supports both file-based and programmatic team configurations.
    """

    def __init__(
        self,
        team: Union[str, Path, Dict[str, Any], ComponentModel, None] = None,
        host: str = "127.0.0.1",
        port: int = 8080,
        session_name: str = "Lite Session",
        auto_open: bool = True,
    ):
        """
        Initialize LiteStudio instance.

        Args:
            team: Team configuration - can be:
                - str: Path to team JSON file
                - Path: Path object to team JSON file
                - Dict[str, Any]: Team configuration dictionary
                - ComponentModel: AutoGen ComponentModel instance
                - None: Creates default team
            host: Host to run server on
            port: Port to run server on
            session_name: Name for the auto-created session
            auto_open: Whether to auto-open browser
        """
        self.host = host
        self.port = port
        self.session_name = session_name
        self.auto_open = auto_open
        self.server_process = None
        self.server_thread = None  # Handle team loading
        self.team_file_path = self._load_team(team)

    def _load_team(self, team: Union[str, Path, Dict[str, Any], ComponentModel, None]) -> str:
        """
        Load team from file path, object, or create default.
        Returns the file path to the team JSON.
        Args:
            team: Can be file path (str/Path), dict, ComponentModel, or None
        """
        if team is None:
            # Create default team
            from autogenstudio.gallery.builder import create_default_lite_team

            return create_default_lite_team()

        elif isinstance(team, (str, Path)):
            # File path provided
            team_path = Path(team)
            if not team_path.exists():
                raise FileNotFoundError(f"Team file not found: {team_path}")
            return str(team_path.absolute())

        elif isinstance(team, dict):
            # Team dict provided - save to temp file
            temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False)
            try:
                json.dump(team, temp_file, indent=2)
                temp_file.flush()
                return temp_file.name
            finally:
                temp_file.close()

        elif isinstance(team, ComponentModel):
            # ComponentModel - use model_dump directly
            team_dict = team.model_dump()
            temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False)
            try:
                json.dump(team_dict, temp_file, indent=2)
                temp_file.flush()
                return temp_file.name
            finally:
                temp_file.close()

        else:
            # Try to serialize other team objects
            team_dict = None

            # Try dump_component() method (AutoGen teams)
            if hasattr(team, "dump_component"):
                component = team.dump_component()
                if hasattr(component, "model_dump"):
                    team_dict = component.model_dump()
                elif hasattr(component, "dict"):
                    team_dict = component.dict()
                else:
                    team_dict = dict(component)

            # Try model_dump() method (Pydantic v2)
            elif hasattr(team, "model_dump"):
                team_dict = team.model_dump()

            # Try dict() method (Pydantic v1)
            elif hasattr(team, "dict"):
                team_dict = team.dict()

            if team_dict is None:
                raise ValueError(
                    f"Cannot serialize team object of type {type(team)}. "
                    f"Expected: file path, dict, ComponentModel, or object with dump_component()/model_dump()/dict() method."
                )

            # Save serialized team to temp file
            temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False)
            try:
                json.dump(team_dict, temp_file, indent=2)
                temp_file.flush()
                return temp_file.name
            finally:
                temp_file.close()

    def _get_env_file_path(self) -> str:
        """Get path for environment variables file."""
        app_dir = os.path.join(os.path.expanduser("~"), ".autogenstudio")
        if not os.path.exists(app_dir):
            os.makedirs(app_dir, exist_ok=True)
        return os.path.join(app_dir, "temp_env_vars.env")

    def _setup_environment(self) -> str:
        """
        Setup environment variables for lite mode.
        Returns path to the environment file.
        """
        env_vars = {
            "AUTOGENSTUDIO_HOST": self.host,
            "AUTOGENSTUDIO_PORT": str(self.port),
            "AUTOGENSTUDIO_LITE_MODE": "true",
            "AUTOGENSTUDIO_API_DOCS": "false",
            "AUTOGENSTUDIO_AUTH_DISABLED": "true",
            "AUTOGENSTUDIO_LITE_SESSION_NAME": self.session_name,
            "AUTOGENSTUDIO_LITE_TEAM_FILE": self.team_file_path,
            "AUTOGENSTUDIO_DATABASE_URI": "sqlite:///:memory:",
        }

        env_file_path = self._get_env_file_path()
        with open(env_file_path, "w") as temp_env:
            for key, value in env_vars.items():
                temp_env.write(f"{key}={value}\n")

        return env_file_path

    def _setup_browser_opening(self):
        """Setup browser opening in a separate thread."""
        if self.auto_open:

            def open_browser():
                time.sleep(3)  # Wait for server startup
                url = f"http://{self.host}:{self.port}/lite"
                webbrowser.open(url)

            threading.Thread(target=open_browser, daemon=True).start()

    def start(self, background: bool = False):
        """
        Start the lite studio server.
        Args:
            background: If True, run server in background thread
        """
        # Check if already running
        if self.server_thread and self.server_thread.is_alive():
            raise RuntimeError("LiteStudio is already running")

        # Setup environment
        env_file_path = self._setup_environment()

        # Setup browser opening
        self._setup_browser_opening()

        if background:
            # Run server in background thread
            def run_server():
                uvicorn.run(
                    "autogenstudio.web.app:app",
                    host=self.host,
                    port=self.port,
                    workers=1,
                    env_file=env_file_path,
                )

            self.server_thread = threading.Thread(target=run_server, daemon=True)
            self.server_thread.start()
        else:
            # Run server in foreground (blocking)
            uvicorn.run(
                "autogenstudio.web.app:app",
                host=self.host,
                port=self.port,
                workers=1,
                env_file=env_file_path,
            )

    def stop(self):
        """Stop the lite studio server."""
        if self.server_thread and self.server_thread.is_alive():
            # For background threads, we can't easily stop uvicorn
            # This is a limitation - in production you'd want proper shutdown
            self.server_thread.join(timeout=5)
            self.server_thread = None

    def __enter__(self):
        """Context manager entry - start in background."""
        self.start(background=True)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):  # type: ignore
        """Context manager exit - stop server."""
        self.stop()

    @classmethod
    def shutdown_port(cls, port: int):
        """
        Utility to shutdown any process running on the specified port.
        Args:
            port: Port number to shutdown
        """
        try:
            # Try to find and kill process on port (Unix/Linux/Mac)
            result = subprocess.run(["lsof", "-ti", f":{port}"], capture_output=True, text=True)

            if result.returncode == 0 and result.stdout.strip():
                pids = result.stdout.strip().split("\n")
                for pid in pids:
                    subprocess.run(["kill", "-9", pid], check=False)

        except (subprocess.SubprocessError, FileNotFoundError):
            # lsof might not be available on all systems
            pass

# From lite/studio.py
def start(self, background: bool = False):
        """
        Start the lite studio server.
        Args:
            background: If True, run server in background thread
        """
        # Check if already running
        if self.server_thread and self.server_thread.is_alive():
            raise RuntimeError("LiteStudio is already running")

        # Setup environment
        env_file_path = self._setup_environment()

        # Setup browser opening
        self._setup_browser_opening()

        if background:
            # Run server in background thread
            def run_server():
                uvicorn.run(
                    "autogenstudio.web.app:app",
                    host=self.host,
                    port=self.port,
                    workers=1,
                    env_file=env_file_path,
                )

            self.server_thread = threading.Thread(target=run_server, daemon=True)
            self.server_thread.start()
        else:
            # Run server in foreground (blocking)
            uvicorn.run(
                "autogenstudio.web.app:app",
                host=self.host,
                port=self.port,
                workers=1,
                env_file=env_file_path,
            )

# From lite/studio.py
def shutdown_port(cls, port: int):
        """
        Utility to shutdown any process running on the specified port.
        Args:
            port: Port number to shutdown
        """
        try:
            # Try to find and kill process on port (Unix/Linux/Mac)
            result = subprocess.run(["lsof", "-ti", f":{port}"], capture_output=True, text=True)

            if result.returncode == 0 and result.stdout.strip():
                pids = result.stdout.strip().split("\n")
                for pid in pids:
                    subprocess.run(["kill", "-9", pid], check=False)

        except (subprocess.SubprocessError, FileNotFoundError):
            # lsof might not be available on all systems
            pass

# From lite/studio.py
def open_browser():
                time.sleep(3)  # Wait for server startup
                url = f"http://{self.host}:{self.port}/lite"
                webbrowser.open(url)

# From lite/studio.py
def run_server():
                uvicorn.run(
                    "autogenstudio.web.app:app",
                    host=self.host,
                    port=self.port,
                    workers=1,
                    env_file=env_file_path,
                )

from autogen_agentchat.agents import AssistantAgent

# From validation/component_test_service.py
class ComponentTestResult(BaseModel):
    status: bool
    message: str
    data: Optional[Any] = None
    logs: List[str] = []

# From validation/component_test_service.py
class ComponentTestRequest(BaseModel):
    component: ComponentModel
    model_client: Optional[Dict[str, Any]] = None
    timeout: Optional[int] = 30

# From validation/component_test_service.py
class ComponentTestService:
    @staticmethod
    async def test_agent(
        component: ComponentModel, model_client: Optional[ChatCompletionClient] = None
    ) -> ComponentTestResult:
        """Test an agent component with a simple message"""
        try:
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.messages import TextMessage
            from autogen_core import CancellationToken

            # Try to load the agent
            try:
                # Construct the agent with the model client if provided
                if model_client:
                    component.config["model_client"] = model_client

                agent = AssistantAgent.load_component(component)

                logs = ["Agent component loaded successfully"]
            except Exception as e:
                return ComponentTestResult(
                    status=False,
                    message=f"Failed to initialize agent: {str(e)}",
                    logs=[f"Agent initialization error: {str(e)}"],
                )

            # Test the agent with a simple message
            test_question = "What is 2+2? Keep it brief."
            try:
                response = await agent.on_messages(
                    [TextMessage(content=test_question, source="user")],
                    cancellation_token=CancellationToken(),
                )

                # Check if we got a valid response
                status = response and response.chat_message is not None

                if status:
                    logs.append(
                        f"Agent responded with: {response.chat_message.to_text()} to the question : {test_question}"
                    )
                else:
                    logs.append("Agent did not return a valid response")

                return ComponentTestResult(
                    status=status,
                    message="Agent test completed successfully" if status else "Agent test failed - no valid response",
                    data=response.chat_message.model_dump() if status else None,
                    logs=logs,
                )
            except Exception as e:
                return ComponentTestResult(
                    status=False,
                    message=f"Error during agent response: {str(e)}",
                    logs=logs + [f"Agent response error: {str(e)}"],
                )

        except Exception as e:
            return ComponentTestResult(
                status=False, message=f"Error testing agent component: {str(e)}", logs=[f"Exception: {str(e)}"]
            )

    @staticmethod
    async def test_model(
        component: ComponentModel, model_client: Optional[ChatCompletionClient] = None
    ) -> ComponentTestResult:
        """Test a model component with a simple prompt"""
        try:
            # Use the component itself as a model client
            model = ChatCompletionClient.load_component(component)

            # Prepare a simple test message
            test_question = "What is 2+2? Give me only the answer."
            messages = [UserMessage(content=test_question, source="user")]

            # Try to get a response
            response = await model.create(messages=messages)

            # Test passes if we got a response with content
            status = response and response.content is not None

            logs = ["Model component loaded successfully"]
            if status:
                logs.append(f"Model responded with: {response.content} (Query:{test_question})")
            else:
                logs.append("Model did not return a valid response")

            return ComponentTestResult(
                status=status,
                message="Model test completed successfully" if status else "Model test failed - no valid response",
                data=response.model_dump() if status else None,
                logs=logs,
            )
        except Exception as e:
            return ComponentTestResult(
                status=False, message=f"Error testing model component: {str(e)}", logs=[f"Exception: {str(e)}"]
            )

    @staticmethod
    async def test_tool(component: ComponentModel) -> ComponentTestResult:
        """Test a tool component with sample inputs"""
        # Placeholder for tool test logic
        return ComponentTestResult(
            status=True, message="Tool test not yet implemented", logs=["Tool component loaded successfully"]
        )

    @staticmethod
    async def test_team(
        component: ComponentModel, model_client: Optional[ChatCompletionClient] = None
    ) -> ComponentTestResult:
        """Test a team component with a simple task"""
        # Placeholder for team test logic
        return ComponentTestResult(
            status=True, message="Team test not yet implemented", logs=["Team component loaded successfully"]
        )

    @staticmethod
    async def test_termination(component: ComponentModel) -> ComponentTestResult:
        """Test a termination component with sample message history"""
        # Placeholder for termination test logic
        return ComponentTestResult(
            status=True,
            message="Termination test not yet implemented",
            logs=["Termination component loaded successfully"],
        )

    @classmethod
    async def test_component(
        cls, component: ComponentModel, timeout: int = 60, model_client: Optional[ChatCompletionClient] = None
    ) -> ComponentTestResult:
        """Test a component based on its type with appropriate test inputs"""
        try:
            # Get component type
            component_type = component.component_type

            # Select test method based on component type
            test_method = {
                "agent": cls.test_agent,
                "model": cls.test_model,
                "tool": cls.test_tool,
                "team": cls.test_team,
                "termination": cls.test_termination,
            }.get(component_type or "unknown")

            if not test_method:
                return ComponentTestResult(status=False, message=f"Unknown component type: {component_type}")

            # Determine if the test method accepts a model_client parameter
            accepts_model_client = component_type in ["agent", "model", "team"]

            # Run test with timeout
            try:
                if accepts_model_client:
                    result = await asyncio.wait_for(test_method(component, model_client), timeout=timeout)
                else:
                    result = await asyncio.wait_for(test_method(component), timeout=timeout)
                return result
            except asyncio.TimeoutError:
                return ComponentTestResult(status=False, message=f"Component test exceeded the {timeout}s timeout")

        except Exception as e:
            return ComponentTestResult(status=False, message=f"Error testing component: {str(e)}")

import importlib
from autogen_core import is_component_class

# From validation/validation_service.py
class ValidationRequest(BaseModel):
    component: ComponentModel

# From validation/validation_service.py
class ValidationError(BaseModel):
    field: str
    error: str
    suggestion: Optional[str] = None

# From validation/validation_service.py
class ValidationResponse(BaseModel):
    is_valid: bool
    errors: List[ValidationError] = []
    warnings: List[ValidationError] = []

# From validation/validation_service.py
class ValidationService:
    @staticmethod
    def validate_provider(provider: str) -> Optional[ValidationError]:
        """Validate that the provider exists and can be imported"""
        try:
            if provider in ["azure_openai_chat_completion_client", "AzureOpenAIChatCompletionClient"]:
                provider = "autogen_ext.models.openai.AzureOpenAIChatCompletionClient"
            elif provider in ["openai_chat_completion_client", "OpenAIChatCompletionClient"]:
                provider = "autogen_ext.models.openai.OpenAIChatCompletionClient"

            module_path, class_name = provider.rsplit(".", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)

            if not is_component_class(component_class):
                return ValidationError(
                    field="provider",
                    error=f"Class {provider} is not a valid component class",
                    suggestion="Ensure the class inherits from Component and implements required methods",
                )
            return None
        except ImportError:
            return ValidationError(
                field="provider",
                error=f"Could not import provider {provider}",
                suggestion="Check that the provider module is installed and the path is correct",
            )
        except Exception as e:
            return ValidationError(
                field="provider",
                error=f"Error validating provider: {str(e)}",
                suggestion="Check the provider string format and class implementation",
            )

    @staticmethod
    def validate_component_type(component: ComponentModel) -> Optional[ValidationError]:
        """Validate the component type"""
        if not component.component_type:
            return ValidationError(
                field="component_type",
                error="Component type is missing",
                suggestion="Add a component_type field to the component configuration",
            )

    @staticmethod
    def validate_config_schema(component: ComponentModel) -> List[ValidationError]:
        """Validate the component configuration against its schema"""
        errors: List[ValidationError] = []
        try:
            # Convert to ComponentModel for initial validation
            model = component.model_copy(deep=True)

            # Get the component class
            provider = model.provider
            module_path, class_name = provider.rsplit(".", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)

            # Validate against component's schema
            if hasattr(component_class, "component_config_schema"):
                try:
                    component_class.component_config_schema.model_validate(model.config)
                except Exception as e:
                    errors.append(
                        ValidationError(
                            field="config",
                            error=f"Config validation failed: {str(e)}",
                            suggestion="Check that the config matches the component's schema",
                        )
                    )
            else:
                errors.append(
                    ValidationError(
                        field="config",
                        error="Component class missing config schema",
                        suggestion="Implement component_config_schema in the component class",
                    )
                )
        except Exception as e:
            errors.append(
                ValidationError(
                    field="config",
                    error=f"Schema validation error: {str(e)}",
                    suggestion="Check the component configuration format",
                )
            )
        return errors

    @staticmethod
    def validate_instantiation(component: ComponentModel) -> Optional[ValidationError]:
        """Validate that the component can be instantiated"""
        try:
            model = component.model_copy(deep=True)
            # Attempt to load the component
            module_path, class_name = model.provider.rsplit(".", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)
            component_class.load_component(model)
            return None
        except Exception as e:
            error_str = str(e)

            # Check for version compatibility issues
            if "component_version" in error_str and "_from_config_past_version is not implemented" in error_str:
                # Extract component information for a better error message
                try:
                    # Get the current component version
                    module_path, class_name = component.provider.rsplit(".", maxsplit=1)
                    module = importlib.import_module(module_path)
                    component_class = getattr(module, class_name)
                    current_version = getattr(component_class, "component_version", None)
                    config_version = component.component_version or component.version or 1

                    return ValidationError(
                        field="component_version",
                        error=f"Component version mismatch: Your configuration uses version {config_version}, but the component requires version {current_version}",
                        suggestion=f"Update your component configuration to use version {current_version}. Set 'component_version: {current_version}' in your configuration.",
                    )
                except Exception:
                    # Fallback to a more general version error message
                    return ValidationError(
                        field="component_version",
                        error="Component version compatibility issue detected",
                        suggestion="Your component configuration version is outdated. Update the 'component_version' field to match the latest component requirements.",
                    )

            # Check for other common instantiation issues
            elif "Could not import provider" in error_str or "ImportError" in error_str:
                return ValidationError(
                    field="provider",
                    error=f"Provider import failed: {error_str}",
                    suggestion="Ensure the provider module is installed and the import path is correct",
                )
            elif "component_config_schema" in error_str:
                return ValidationError(
                    field="config",
                    error="Component configuration schema validation failed",
                    suggestion="Check that your configuration matches the component's expected schema",
                )
            else:
                return ValidationError(
                    field="instantiation",
                    error=f"Failed to instantiate component: {error_str}",
                    suggestion="Check that the component can be properly instantiated with the given config",
                )

    @classmethod
    def validate(cls, component: ComponentModel) -> ValidationResponse:
        """Validate a component configuration"""
        errors: List[ValidationError] = []
        warnings: List[ValidationError] = []

        # Check provider
        if provider_error := cls.validate_provider(component.provider):
            errors.append(provider_error)

        # Check component type
        if type_error := cls.validate_component_type(component):
            errors.append(type_error)

        # Validate schema
        schema_errors = cls.validate_config_schema(component)
        errors.extend(schema_errors)

        # Only attempt instantiation if no errors so far
        if not errors:
            if inst_error := cls.validate_instantiation(component):
                errors.append(inst_error)

        # Check for version warnings
        if not component.version:
            warnings.append(
                ValidationError(
                    field="version",
                    error="Component version not specified",
                    suggestion="Consider adding a version to ensure compatibility",
                )
            )

        return ValidationResponse(is_valid=len(errors) == 0, errors=errors, warnings=warnings)

# From validation/validation_service.py
def validate_provider(provider: str) -> Optional[ValidationError]:
        """Validate that the provider exists and can be imported"""
        try:
            if provider in ["azure_openai_chat_completion_client", "AzureOpenAIChatCompletionClient"]:
                provider = "autogen_ext.models.openai.AzureOpenAIChatCompletionClient"
            elif provider in ["openai_chat_completion_client", "OpenAIChatCompletionClient"]:
                provider = "autogen_ext.models.openai.OpenAIChatCompletionClient"

            module_path, class_name = provider.rsplit(".", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)

            if not is_component_class(component_class):
                return ValidationError(
                    field="provider",
                    error=f"Class {provider} is not a valid component class",
                    suggestion="Ensure the class inherits from Component and implements required methods",
                )
            return None
        except ImportError:
            return ValidationError(
                field="provider",
                error=f"Could not import provider {provider}",
                suggestion="Check that the provider module is installed and the path is correct",
            )
        except Exception as e:
            return ValidationError(
                field="provider",
                error=f"Error validating provider: {str(e)}",
                suggestion="Check the provider string format and class implementation",
            )

# From validation/validation_service.py
def validate_component_type(component: ComponentModel) -> Optional[ValidationError]:
        """Validate the component type"""
        if not component.component_type:
            return ValidationError(
                field="component_type",
                error="Component type is missing",
                suggestion="Add a component_type field to the component configuration",
            )

# From validation/validation_service.py
def validate_config_schema(component: ComponentModel) -> List[ValidationError]:
        """Validate the component configuration against its schema"""
        errors: List[ValidationError] = []
        try:
            # Convert to ComponentModel for initial validation
            model = component.model_copy(deep=True)

            # Get the component class
            provider = model.provider
            module_path, class_name = provider.rsplit(".", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)

            # Validate against component's schema
            if hasattr(component_class, "component_config_schema"):
                try:
                    component_class.component_config_schema.model_validate(model.config)
                except Exception as e:
                    errors.append(
                        ValidationError(
                            field="config",
                            error=f"Config validation failed: {str(e)}",
                            suggestion="Check that the config matches the component's schema",
                        )
                    )
            else:
                errors.append(
                    ValidationError(
                        field="config",
                        error="Component class missing config schema",
                        suggestion="Implement component_config_schema in the component class",
                    )
                )
        except Exception as e:
            errors.append(
                ValidationError(
                    field="config",
                    error=f"Schema validation error: {str(e)}",
                    suggestion="Check the component configuration format",
                )
            )
        return errors

# From validation/validation_service.py
def validate_instantiation(component: ComponentModel) -> Optional[ValidationError]:
        """Validate that the component can be instantiated"""
        try:
            model = component.model_copy(deep=True)
            # Attempt to load the component
            module_path, class_name = model.provider.rsplit(".", maxsplit=1)
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)
            component_class.load_component(model)
            return None
        except Exception as e:
            error_str = str(e)

            # Check for version compatibility issues
            if "component_version" in error_str and "_from_config_past_version is not implemented" in error_str:
                # Extract component information for a better error message
                try:
                    # Get the current component version
                    module_path, class_name = component.provider.rsplit(".", maxsplit=1)
                    module = importlib.import_module(module_path)
                    component_class = getattr(module, class_name)
                    current_version = getattr(component_class, "component_version", None)
                    config_version = component.component_version or component.version or 1

                    return ValidationError(
                        field="component_version",
                        error=f"Component version mismatch: Your configuration uses version {config_version}, but the component requires version {current_version}",
                        suggestion=f"Update your component configuration to use version {current_version}. Set 'component_version: {current_version}' in your configuration.",
                    )
                except Exception:
                    # Fallback to a more general version error message
                    return ValidationError(
                        field="component_version",
                        error="Component version compatibility issue detected",
                        suggestion="Your component configuration version is outdated. Update the 'component_version' field to match the latest component requirements.",
                    )

            # Check for other common instantiation issues
            elif "Could not import provider" in error_str or "ImportError" in error_str:
                return ValidationError(
                    field="provider",
                    error=f"Provider import failed: {error_str}",
                    suggestion="Ensure the provider module is installed and the import path is correct",
                )
            elif "component_config_schema" in error_str:
                return ValidationError(
                    field="config",
                    error="Component configuration schema validation failed",
                    suggestion="Check that your configuration matches the component's expected schema",
                )
            else:
                return ValidationError(
                    field="instantiation",
                    error=f"Failed to instantiate component: {error_str}",
                    suggestion="Check that the component can be properly instantiated with the given config",
                )

# From validation/validation_service.py
def validate(cls, component: ComponentModel) -> ValidationResponse:
        """Validate a component configuration"""
        errors: List[ValidationError] = []
        warnings: List[ValidationError] = []

        # Check provider
        if provider_error := cls.validate_provider(component.provider):
            errors.append(provider_error)

        # Check component type
        if type_error := cls.validate_component_type(component):
            errors.append(type_error)

        # Validate schema
        schema_errors = cls.validate_config_schema(component)
        errors.extend(schema_errors)

        # Only attempt instantiation if no errors so far
        if not errors:
            if inst_error := cls.validate_instantiation(component):
                errors.append(inst_error)

        # Check for version warnings
        if not component.version:
            warnings.append(
                ValidationError(
                    field="version",
                    error="Component version not specified",
                    suggestion="Consider adding a version to ensure compatibility",
                )
            )

        return ValidationResponse(is_valid=len(errors) == 0, errors=errors, warnings=warnings)

import base64

# From utils/utils.py
def construct_task(query: str, files: list[dict] | None = None) -> Sequence[ChatMessage]:
    """
    Construct a task from a query string and list of files.
    Returns a list of ChatMessage objects suitable for processing by the agent system.

    Args:
        query: The text query from the user
        files: List of file objects with properties name, content, and type

    Returns:
        List of BaseChatMessage objects (TextMessage, MultiModalMessage)
    """
    if files is None:
        files = []

    messages = []

    # Add the user's text query as a TextMessage
    if query:
        messages.append(TextMessage(source="user", content=query))

    # Process each file based on its type
    for file in files:
        try:
            if file.get("type", "").startswith("image/"):
                # Handle image file using from_base64 method
                # The content is already base64 encoded according to the convertFilesToBase64 function
                image = Image.from_base64(file["content"])
                messages.append(
                    MultiModalMessage(
                        source="user", content=[image], metadata={"filename": file.get("name", "unknown.img")}
                    )
                )
            elif file.get("type", "").startswith("text/"):
                # Handle text file as TextMessage
                text_content = base64.b64decode(file["content"]).decode("utf-8")
                messages.append(
                    TextMessage(
                        source="user", content=text_content, metadata={"filename": file.get("name", "unknown.txt")}
                    )
                )
            else:
                # Log unsupported file types but still try to process based on best guess
                logger.warning(f"Potentially unsupported file type: {file.get('type')} for file {file.get('name')}")
                if file.get("type", "").startswith("application/"):
                    # Try to treat as text if it's an application type (like JSON)
                    text_content = base64.b64decode(file["content"]).decode("utf-8")
                    messages.append(
                        TextMessage(
                            source="user",
                            content=text_content,
                            metadata={
                                "filename": file.get("name", "unknown.file"),
                                "filetype": file.get("type", "unknown"),
                            },
                        )
                    )
        except Exception as e:
            logger.error(f"Error processing file {file.get('name')}: {str(e)}")
            # Continue processing other files even if one fails

    return messages

from sqlalchemy import exc
from sqlalchemy import inspect
from sqlalchemy import text
from sqlmodel import Session
from sqlmodel import SQLModel
from sqlmodel import and_
from sqlmodel import create_engine
from sqlmodel import select
from datamodel import BaseDBModel
from datamodel import Response
from datamodel import Team
from teammanager import TeamManager
from schema_manager import SchemaManager
import enum

# From database/db_manager.py
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, o):
        if hasattr(o, "get_secret_value") and callable(o.get_secret_value):
            return o.get_secret_value()
        # Handle datetime objects
        if isinstance(o, datetime):
            return o.isoformat()
        # Handle Enum objects
        import enum

        if isinstance(o, enum.Enum):
            return o.value
        return super().default(o)

# From database/db_manager.py
class DatabaseManager:
    _init_lock = threading.Lock()

    def __init__(self, engine_uri: str, base_dir: Optional[Union[str, Path]] = None) -> None:
        """
        Initialize DatabaseManager with database connection settings.
        Does not perform any database operations.

        Args:
            engine_uri: Database connection URI (e.g. sqlite:///db.sqlite3)
            base_dir: Base directory for migration files. If None, uses current directory
        """
        connection_args = {"check_same_thread": True} if "sqlite" in engine_uri else {}

        if base_dir is not None and isinstance(base_dir, str):
            base_dir = Path(base_dir)

        self.engine = create_engine(
            engine_uri, connect_args=connection_args, json_serializer=lambda obj: json.dumps(obj, cls=CustomJSONEncoder)
        )
        self.schema_manager = SchemaManager(
            engine=self.engine,
            base_dir=base_dir,
        )

    def _should_auto_upgrade(self) -> bool:
        """
        Check if auto upgrade should run based on schema differences
        """
        needs_upgrade, _ = self.schema_manager.check_schema_status()
        return needs_upgrade

    def initialize_database(self, auto_upgrade: bool = False, force_init_alembic: bool = True) -> Response:
        """
        Initialize database and migrations in the correct order.

        Args:
            auto_upgrade: If True, automatically generate and apply migrations for schema changes
            force_init_alembic: If True, reinitialize alembic configuration even if it exists
        """
        if not self._init_lock.acquire(blocking=False):
            return Response(message="Database initialization already in progress", status=False)

        try:
            # Enable foreign key constraints for SQLite
            if "sqlite" in str(self.engine.url):
                with self.engine.connect() as conn:
                    conn.execute(text("PRAGMA foreign_keys=ON"))
            inspector = inspect(self.engine)
            tables_exist = inspector.get_table_names()
            if not tables_exist:
                logger.info("Creating database tables...")
                SQLModel.metadata.create_all(self.engine)

                if self.schema_manager.initialize_migrations(force=force_init_alembic):
                    return Response(message="Database initialized successfully", status=True)
                return Response(message="Failed to initialize migrations", status=False)

            # Handle existing database
            if auto_upgrade:
                logger.info("Checking database schema...")
                if self.schema_manager.ensure_schema_up_to_date():
                    return Response(message="Database schema is up to date", status=True)
                return Response(message="Database upgrade failed", status=False)
            elif self._should_auto_upgrade():
                logger.info("Schema changes detected, but auto_upgrade is disabled. Skipping migration check.")
                logger.info("To enable automatic migrations, set auto_upgrade=True")

            return Response(message="Database is ready", status=True)

        except Exception as e:
            error_msg = f"Database initialization failed: {str(e)}"
            logger.error(error_msg)
            return Response(message=error_msg, status=False)
        finally:
            self._init_lock.release()

    def reset_db(self, recreate_tables: bool = True) -> Response:
        """
        Reset the database by dropping all tables and optionally recreating them.

        Args:
            recreate_tables (bool): If True, recreates the tables after dropping them.
                                Set to False if you want to call create_db_and_tables() separately.
        """
        if not self._init_lock.acquire(blocking=False):
            logger.warning("Database reset already in progress")
            return Response(message="Database reset already in progress", status=False, data=None)

        try:
            # Dispose existing connections
            self.engine.dispose()
            with Session(self.engine) as session:
                try:
                    # Disable foreign key checks for SQLite
                    if "sqlite" in str(self.engine.url):
                        session.exec(text("PRAGMA foreign_keys=OFF"))  # type: ignore

                    # Drop all tables
                    SQLModel.metadata.drop_all(self.engine)
                    logger.info("All tables dropped successfully")

                    # Re-enable foreign key checks for SQLite
                    if "sqlite" in str(self.engine.url):
                        session.exec(text("PRAGMA foreign_keys=ON"))  # type: ignore

                    session.commit()

                except Exception as e:
                    session.rollback()
                    raise e
                finally:
                    session.close()
                    self._init_lock.release()

            if recreate_tables:
                logger.info("Recreating tables...")
                self.initialize_database(auto_upgrade=False, force_init_alembic=True)

            return Response(
                message="Database reset successfully" if recreate_tables else "Database tables dropped successfully",
                status=True,
                data=None,
            )

        except Exception as e:
            error_msg = f"Error while resetting database: {str(e)}"
            logger.error(error_msg)
            return Response(message=error_msg, status=False, data=None)
        finally:
            if self._init_lock.locked():
                self._init_lock.release()
                logger.info("Database reset lock released")

    def upsert(self, model: BaseDBModel, return_json: bool = True) -> Response:
        """Create or update an entity

        Args:
            model (SQLModel): The model instance to create or update
            return_json (bool, optional): If True, returns the model as a dictionary.
                If False, returns the SQLModel instance. Defaults to True.

        Returns:
            Response: Contains status, message and data (either dict or SQLModel based on return_json)
        """
        status = True
        model_class = type(model)
        existing_model = None

        with Session(self.engine) as session:
            try:
                existing_model = session.exec(select(model_class).where(model_class.id == model.id)).first()
                if existing_model:
                    model.updated_at = datetime.now()
                    for key, value in model.model_dump().items():
                        setattr(existing_model, key, value)
                    model = existing_model
                    session.add(model)
                else:
                    session.add(model)
                session.commit()
                session.refresh(model)
            except Exception as e:
                session.rollback()
                logger.error("Error while updating/creating " + str(model_class.__name__) + ": " + str(e))
                status = False

        return Response(
            message=(
                f"{model_class.__name__} Updated Successfully"
                if existing_model
                else f"{model_class.__name__} Created Successfully"
            ),
            status=status,
            data=model.model_dump() if return_json else model,
        )

    def _model_to_dict(self, model_obj):
        return {col.name: getattr(model_obj, col.name) for col in model_obj.__table__.columns}

    def get(
        self,
        model_class: type[BaseDBModel],
        filters: dict | None = None,
        return_json: bool = False,
        order: str = "desc",
    ):
        """List entities"""
        with Session(self.engine) as session:
            result = []
            status = True
            status_message = ""

            try:
                statement = select(model_class)  # type: ignore
                if filters:
                    conditions = [getattr(model_class, col) == value for col, value in filters.items()]
                    statement = statement.where(and_(*conditions))

                if hasattr(model_class, "created_at") and order:
                    order_by_clause = getattr(model_class.created_at, order)()  # Dynamically apply asc/desc
                    statement = statement.order_by(order_by_clause)

                items = session.exec(statement).all()
                result = [self._model_to_dict(item) if return_json else item for item in items]
                status_message = f"{model_class.__name__} Retrieved Successfully"
            except Exception as e:
                session.rollback()
                status = False
                status_message = f"Error while fetching {model_class.__name__}"
                logger.error("Error while getting items: " + str(model_class.__name__) + " " + str(e))

            return Response(message=status_message, status=status, data=result)

    def delete(self, model_class: type[BaseDBModel], filters: dict | None = None) -> Response:
        """Delete an entity"""
        status_message = ""
        status = True

        with Session(self.engine) as session:
            try:
                if "sqlite" in str(self.engine.url):
                    session.exec(text("PRAGMA foreign_keys=ON"))  # type: ignore
                statement = select(model_class)  # type: ignore
                if filters:
                    conditions = [getattr(model_class, col) == value for col, value in filters.items()]
                    statement = statement.where(and_(*conditions))

                rows = session.exec(statement).all()

                if rows:
                    for row in rows:
                        session.delete(row)
                    session.commit()
                    status_message = f"{model_class.__name__} Deleted Successfully"
                else:
                    status_message = "Row not found"
                    logger.info(f"Row with filters {filters} not found")

            except exc.IntegrityError as e:
                session.rollback()
                status = False
                status_message = f"Integrity error: The {model_class.__name__} is linked to another entity and cannot be deleted. {e}"
                # Log the specific integrity error
                logger.error(status_message)
            except Exception as e:
                session.rollback()
                status = False
                status_message = f"Error while deleting: {e}"
                logger.error(status_message)

        return Response(message=status_message, status=status, data=None)

    async def import_team(
        self, team_config: Union[str, Path, dict], user_id: str, check_exists: bool = False
    ) -> Response:
        try:
            # Load config if path provided
            if isinstance(team_config, (str, Path)):
                config = await TeamManager.load_from_file(team_config)
            else:
                config = team_config

            # Check existence if requested
            if check_exists:
                existing = await self._check_team_exists(config, user_id)
                if existing:
                    return Response(
                        message="Identical team configuration already exists", status=True, data={"id": existing.id}
                    )

            # Store in database
            team_db = Team(user_id=user_id, component=config)

            result = self.upsert(team_db)
            return result

        except Exception as e:
            logger.error(f"Failed to import team: {str(e)}")
            return Response(message=str(e), status=False)

    async def import_teams_from_directory(
        self, directory: Union[str, Path], user_id: str, check_exists: bool = False
    ) -> Response:
        """
        Import all team configurations from a directory.

        Args:
            directory: Path to directory containing team configs
            user_id: User ID to associate with imported teams
            check_exists: Whether to check for existing teams

        Returns:
            Response containing import results for all files
        """
        try:
            # Load all configs from directory
            configs = await TeamManager.load_from_directory(directory)

            results = []
            for config in configs:
                try:
                    result = await self.import_team(team_config=config, user_id=user_id, check_exists=check_exists)

                    # Add result info
                    results.append(
                        {
                            "status": result.status,
                            "message": result.message,
                            "id": result.data.get("id") if result.data and result.data is not None else None,
                        }
                    )

                except Exception as e:
                    logger.error(f"Failed to import team config: {str(e)}")
                    results.append({"status": False, "message": str(e), "id": None})

            return Response(message="Directory import complete", status=True, data=results)

        except Exception as e:
            logger.error(f"Failed to import directory: {str(e)}")
            return Response(message=str(e), status=False)

    async def _check_team_exists(self, config: dict, user_id: str) -> Optional[Team]:
        """Check if identical team config already exists"""
        response = self.get(Team, {"user_id": user_id})
        teams = response.data if response.status and response.data is not None else []

        for team in teams:
            if team.component == config:
                return team

        return None

    async def close(self):
        """Close database connections and cleanup resources"""
        logger.info("Closing database connections...")
        try:
            # Dispose of the SQLAlchemy engine
            self.engine.dispose()
            logger.info("Database connections closed successfully")
        except Exception as e:
            logger.error(f"Error closing database connections: {str(e)}")
            raise

# From database/db_manager.py
def default(self, o):
        if hasattr(o, "get_secret_value") and callable(o.get_secret_value):
            return o.get_secret_value()
        # Handle datetime objects
        if isinstance(o, datetime):
            return o.isoformat()
        # Handle Enum objects
        import enum

        if isinstance(o, enum.Enum):
            return o.value
        return super().default(o)

# From database/db_manager.py
def initialize_database(self, auto_upgrade: bool = False, force_init_alembic: bool = True) -> Response:
        """
        Initialize database and migrations in the correct order.

        Args:
            auto_upgrade: If True, automatically generate and apply migrations for schema changes
            force_init_alembic: If True, reinitialize alembic configuration even if it exists
        """
        if not self._init_lock.acquire(blocking=False):
            return Response(message="Database initialization already in progress", status=False)

        try:
            # Enable foreign key constraints for SQLite
            if "sqlite" in str(self.engine.url):
                with self.engine.connect() as conn:
                    conn.execute(text("PRAGMA foreign_keys=ON"))
            inspector = inspect(self.engine)
            tables_exist = inspector.get_table_names()
            if not tables_exist:
                logger.info("Creating database tables...")
                SQLModel.metadata.create_all(self.engine)

                if self.schema_manager.initialize_migrations(force=force_init_alembic):
                    return Response(message="Database initialized successfully", status=True)
                return Response(message="Failed to initialize migrations", status=False)

            # Handle existing database
            if auto_upgrade:
                logger.info("Checking database schema...")
                if self.schema_manager.ensure_schema_up_to_date():
                    return Response(message="Database schema is up to date", status=True)
                return Response(message="Database upgrade failed", status=False)
            elif self._should_auto_upgrade():
                logger.info("Schema changes detected, but auto_upgrade is disabled. Skipping migration check.")
                logger.info("To enable automatic migrations, set auto_upgrade=True")

            return Response(message="Database is ready", status=True)

        except Exception as e:
            error_msg = f"Database initialization failed: {str(e)}"
            logger.error(error_msg)
            return Response(message=error_msg, status=False)
        finally:
            self._init_lock.release()

# From database/db_manager.py
def reset_db(self, recreate_tables: bool = True) -> Response:
        """
        Reset the database by dropping all tables and optionally recreating them.

        Args:
            recreate_tables (bool): If True, recreates the tables after dropping them.
                                Set to False if you want to call create_db_and_tables() separately.
        """
        if not self._init_lock.acquire(blocking=False):
            logger.warning("Database reset already in progress")
            return Response(message="Database reset already in progress", status=False, data=None)

        try:
            # Dispose existing connections
            self.engine.dispose()
            with Session(self.engine) as session:
                try:
                    # Disable foreign key checks for SQLite
                    if "sqlite" in str(self.engine.url):
                        session.exec(text("PRAGMA foreign_keys=OFF"))  # type: ignore

                    # Drop all tables
                    SQLModel.metadata.drop_all(self.engine)
                    logger.info("All tables dropped successfully")

                    # Re-enable foreign key checks for SQLite
                    if "sqlite" in str(self.engine.url):
                        session.exec(text("PRAGMA foreign_keys=ON"))  # type: ignore

                    session.commit()

                except Exception as e:
                    session.rollback()
                    raise e
                finally:
                    session.close()
                    self._init_lock.release()

            if recreate_tables:
                logger.info("Recreating tables...")
                self.initialize_database(auto_upgrade=False, force_init_alembic=True)

            return Response(
                message="Database reset successfully" if recreate_tables else "Database tables dropped successfully",
                status=True,
                data=None,
            )

        except Exception as e:
            error_msg = f"Error while resetting database: {str(e)}"
            logger.error(error_msg)
            return Response(message=error_msg, status=False, data=None)
        finally:
            if self._init_lock.locked():
                self._init_lock.release()
                logger.info("Database reset lock released")

# From database/db_manager.py
def upsert(self, model: BaseDBModel, return_json: bool = True) -> Response:
        """Create or update an entity

        Args:
            model (SQLModel): The model instance to create or update
            return_json (bool, optional): If True, returns the model as a dictionary.
                If False, returns the SQLModel instance. Defaults to True.

        Returns:
            Response: Contains status, message and data (either dict or SQLModel based on return_json)
        """
        status = True
        model_class = type(model)
        existing_model = None

        with Session(self.engine) as session:
            try:
                existing_model = session.exec(select(model_class).where(model_class.id == model.id)).first()
                if existing_model:
                    model.updated_at = datetime.now()
                    for key, value in model.model_dump().items():
                        setattr(existing_model, key, value)
                    model = existing_model
                    session.add(model)
                else:
                    session.add(model)
                session.commit()
                session.refresh(model)
            except Exception as e:
                session.rollback()
                logger.error("Error while updating/creating " + str(model_class.__name__) + ": " + str(e))
                status = False

        return Response(
            message=(
                f"{model_class.__name__} Updated Successfully"
                if existing_model
                else f"{model_class.__name__} Created Successfully"
            ),
            status=status,
            data=model.model_dump() if return_json else model,
        )

# From database/db_manager.py
def get(
        self,
        model_class: type[BaseDBModel],
        filters: dict | None = None,
        return_json: bool = False,
        order: str = "desc",
    ):
        """List entities"""
        with Session(self.engine) as session:
            result = []
            status = True
            status_message = ""

            try:
                statement = select(model_class)  # type: ignore
                if filters:
                    conditions = [getattr(model_class, col) == value for col, value in filters.items()]
                    statement = statement.where(and_(*conditions))

                if hasattr(model_class, "created_at") and order:
                    order_by_clause = getattr(model_class.created_at, order)()  # Dynamically apply asc/desc
                    statement = statement.order_by(order_by_clause)

                items = session.exec(statement).all()
                result = [self._model_to_dict(item) if return_json else item for item in items]
                status_message = f"{model_class.__name__} Retrieved Successfully"
            except Exception as e:
                session.rollback()
                status = False
                status_message = f"Error while fetching {model_class.__name__}"
                logger.error("Error while getting items: " + str(model_class.__name__) + " " + str(e))

            return Response(message=status_message, status=status, data=result)

# From database/db_manager.py
def delete(self, model_class: type[BaseDBModel], filters: dict | None = None) -> Response:
        """Delete an entity"""
        status_message = ""
        status = True

        with Session(self.engine) as session:
            try:
                if "sqlite" in str(self.engine.url):
                    session.exec(text("PRAGMA foreign_keys=ON"))  # type: ignore
                statement = select(model_class)  # type: ignore
                if filters:
                    conditions = [getattr(model_class, col) == value for col, value in filters.items()]
                    statement = statement.where(and_(*conditions))

                rows = session.exec(statement).all()

                if rows:
                    for row in rows:
                        session.delete(row)
                    session.commit()
                    status_message = f"{model_class.__name__} Deleted Successfully"
                else:
                    status_message = "Row not found"
                    logger.info(f"Row with filters {filters} not found")

            except exc.IntegrityError as e:
                session.rollback()
                status = False
                status_message = f"Integrity error: The {model_class.__name__} is linked to another entity and cannot be deleted. {e}"
                # Log the specific integrity error
                logger.error(status_message)
            except Exception as e:
                session.rollback()
                status = False
                status_message = f"Error while deleting: {e}"
                logger.error(status_message)

        return Response(message=status_message, status=status, data=None)

import io
import shutil
from contextlib import redirect_stdout
import sqlmodel
from alembic import command
from alembic.autogenerate import compare_metadata
from alembic.config import Config
from alembic.runtime.migration import MigrationContext
from alembic.script import ScriptDirectory
from alembic.util.exc import CommandError
from sqlalchemy import Engine

# From database/schema_manager.py
class SchemaManager:
    """
    Manages database schema validation and migrations using Alembic.
    Operations are initiated explicitly by DatabaseManager.
    """

    def __init__(
        self,
        engine: Engine,
        base_dir: Optional[Path] = None,
    ):
        """
        Initialize configuration only - no filesystem or DB operations.

        Args:
            engine: SQLAlchemy engine instance
            base_dir: Base directory for Alembic files. If None, uses current working directory
        """
        # Convert string path to Path object if necessary
        if isinstance(base_dir, str):
            base_dir = Path(base_dir)

        self.engine = engine
        self.base_dir = base_dir or Path(__file__).parent
        self.alembic_dir = self.base_dir / "alembic"
        self.alembic_ini_path = self.base_dir / "alembic.ini"

    def initialize_migrations(self, force: bool = False) -> bool:
        try:
            if force:
                # logger.info("Force reinitialization of migrations...")
                self._cleanup_existing_alembic()
                if not self._initialize_alembic():
                    return False
            else:
                try:
                    self._validate_alembic_setup()
                    logger.info("Using existing Alembic configuration")
                    self._update_configuration()
                except FileNotFoundError:
                    logger.info("Initializing new Alembic configuration")
                    if not self._initialize_alembic():
                        return False

            # Only generate initial revision if alembic is properly initialized
            # logger.info("Creating initial migration...")
            return self.generate_revision("Initial schema") is not None

        except Exception as e:
            logger.error(f"Failed to initialize migrations: {e}")
            return False

    def _update_configuration(self) -> None:
        """Updates existing Alembic configuration with current settings."""
        logger.info("Updating existing Alembic configuration...")

        # Update alembic.ini
        config_content = self._generate_alembic_ini_content()
        with open(self.alembic_ini_path, "w") as f:
            f.write(config_content)

        # Update env.py
        env_path = self.alembic_dir / "env.py"
        if env_path.exists():
            self._update_env_py(env_path)
        else:
            self._create_minimal_env_py(env_path)

    def _cleanup_existing_alembic(self) -> None:
        """
        Completely remove existing Alembic configuration including versions.
        For fresh initialization, we don't need to preserve anything.
        """
        # logger.info("Cleaning up existing Alembic configuration...")

        # Remove entire alembic directory if it exists
        if self.alembic_dir.exists():
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f"Removed alembic directory: {self.alembic_dir}")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink()
            logger.info("Removed alembic.ini")

    def _initialize_alembic(self) -> bool:
        """Initialize alembic structure and configuration"""
        try:
            # Ensure parent directory exists
            self.alembic_dir.parent.mkdir(exist_ok=True)

            # Run alembic init to create fresh directory structure
            # logger.info("Initializing alembic directory structure...")

            # Create initial config file for alembic init
            config_content = self._generate_alembic_ini_content()
            with open(self.alembic_ini_path, "w") as f:
                f.write(config_content)

            # Use the config we just created
            config = Config(str(self.alembic_ini_path))

            with redirect_stdout(io.StringIO()):
                command.init(config, str(self.alembic_dir))

            # Update script template after initialization
            self.update_script_template()

            # Update env.py with our customizations
            self._update_env_py(self.alembic_dir / "env.py")

            logger.info("Alembic initialization complete")
            return True

        except Exception as e:
            # Explicitly convert error to string
            logger.error(f"Failed to initialize alembic: {str(e)}")
            return False

    def _create_minimal_env_py(self, env_path: Path) -> None:
        """Creates a minimal env.py file for Alembic."""
        content = """
from logging.config import fileConfig
from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context
from sqlmodel import SQLModel

config = context.config
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = SQLModel.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True
    )
    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    with connectable.connect() as connection:
        is_sqlite = connection.dialect.name == "sqlite"
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True
            render_as_batch=is_sqlite,
        )
        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()"""

        with open(env_path, "w") as f:
            f.write(content)

    def _generate_alembic_ini_content(self) -> str:
        """
        Generates content for alembic.ini file.
        """
        engine_url = str(self.engine.url).replace("%", "%%")
        return f"""
[alembic]
script_location = {self.alembic_dir}
sqlalchemy.url = {engine_url}

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
""".strip()

    def update_script_template(self):
        """Update the Alembic script template to include SQLModel."""
        template_path = self.alembic_dir / "script.py.mako"
        try:
            with open(template_path, "r") as f:
                content = f.read()

            # Add sqlmodel import to imports section
            import_section = "from alembic import op\nimport sqlalchemy as sa"
            new_imports = "from alembic import op\nimport sqlalchemy as sa\nimport sqlmodel"

            content = content.replace(import_section, new_imports)

            with open(template_path, "w") as f:
                f.write(content)

            return True

        except Exception as e:
            logger.error(f"Failed to update script template: {e}")
            return False

    def _update_env_py(self, env_path: Path) -> None:
        """
        Updates the env.py file to use SQLModel metadata.
        """
        if not env_path.exists():
            self._create_minimal_env_py(env_path)
            return
        try:
            with open(env_path, "r") as f:
                content = f.read()

            # Add SQLModel import if not present
            if "from sqlmodel import SQLModel" not in content:
                content = "from sqlmodel import SQLModel\n" + content

            # Replace target_metadata
            content = content.replace("target_metadata = None", "target_metadata = SQLModel.metadata")

            # Update both configure blocks properly
            content = content.replace(
                """context.configure(
            url=url,
            target_metadata=target_metadata,
            literal_binds=True,
            dialect_opts={"paramstyle": "named"},
        )""",
                """context.configure(
            url=url,
            target_metadata=target_metadata,
            literal_binds=True,
            dialect_opts={"paramstyle": "named"},
            compare_type=True,
        )""",
            )

            content = content.replace(
                """        context.configure(
                connection=connection, target_metadata=target_metadata
            )""",
                """        context.configure(
                connection=connection,
                target_metadata=target_metadata,
                compare_type=True,
            )""",
            )

            with open(env_path, "w") as f:
                f.write(content)
        except Exception as e:
            logger.error(f"Failed to update env.py: {e}")
            raise

    # Fixed: use keyword-only argument

    def _ensure_alembic_setup(self, *, force: bool = False) -> None:
        """
        Ensures Alembic is properly set up, initializing if necessary.

        Args:
            force: If True, removes existing configuration and reinitializes
        """
        try:
            self._validate_alembic_setup()
            if force:
                logger.info("Force initialization requested. Cleaning up existing configuration...")
                self._cleanup_existing_alembic()
                self._initialize_alembic()
        except FileNotFoundError:
            logger.info("Alembic configuration not found. Initializing...")
            if self.alembic_dir.exists():
                logger.warning("Found existing alembic directory but missing configuration")
                self._cleanup_existing_alembic()
            self._initialize_alembic()
            logger.info("Alembic initialization complete")

    def _validate_alembic_setup(self) -> None:
        """Validates that Alembic is properly configured."""
        required_files = [self.alembic_ini_path, self.alembic_dir / "env.py", self.alembic_dir / "versions"]

        missing = [f for f in required_files if not f.exists()]
        if missing:
            raise FileNotFoundError(f"Alembic configuration incomplete. Missing: {', '.join(str(f) for f in missing)}")

    def get_alembic_config(self) -> Config:
        """
        Gets Alembic configuration.

        Returns:
            Config: Alembic Config object

        Raises:
            FileNotFoundError: If alembic.ini cannot be found
        """
        if not self.alembic_ini_path.exists():
            raise FileNotFoundError("Could not find alembic.ini")

        return Config(str(self.alembic_ini_path))

    def get_current_revision(self) -> Optional[str]:
        """
        Gets the current database revision.

        Returns:
            str: Current revision string or None if no revision
        """
        with self.engine.connect() as conn:
            context = MigrationContext.configure(conn)
            return context.get_current_revision()

    def get_head_revision(self) -> Optional[str]:
        """
        Gets the latest available revision.

        Returns:
            Optional[str]: Head revision string or None if no revisions exist
        """
        config = self.get_alembic_config()
        script = ScriptDirectory.from_config(config)
        return script.get_current_head()

    def get_schema_differences(self) -> List[tuple]:
        """
        Detects differences between current database and models.

        Returns:
            List[tuple]: List of differences found
        """
        with self.engine.connect() as conn:
            context = MigrationContext.configure(conn)
            diff = compare_metadata(context, SQLModel.metadata)
            return list(diff)

    def check_schema_status(self) -> Tuple[bool, str]:
        """
        Checks if database schema matches current models and migrations.

        Returns:
            Tuple[bool, str]: (needs_upgrade, status_message)
        """
        try:
            current_rev = self.get_current_revision()
            head_rev = self.get_head_revision()

            if current_rev != head_rev:
                return True, f"Database needs upgrade: {current_rev} -> {head_rev}"

            differences = self.get_schema_differences()
            if differences:
                changes_desc = "\n".join(str(diff) for diff in differences)
                return True, f"Unmigrated changes detected:\n{changes_desc}"

            return False, "Database schema is up to date"

        except Exception as e:
            logger.error(f"Error checking schema status: {str(e)}")
            return True, f"Error checking schema: {str(e)}"

    def upgrade_schema(self, revision: str = "head") -> bool:
        """
        Upgrades database schema to specified revision.

        Args:
            revision: Target revision (default: "head")

        Returns:
            bool: True if upgrade successful
        """
        try:
            config = self.get_alembic_config()
            command.upgrade(config, revision)
            logger.info(f"Schema upgraded successfully to {revision}")
            return True

        except Exception as e:
            logger.error(f"Schema upgrade failed: {str(e)}")
            return False

    def check_and_upgrade(self) -> Tuple[bool, str]:
        """
        Checks schema status and upgrades if necessary.

        Returns:
            Tuple[bool, str]: (action_taken, status_message)
        """
        needs_upgrade, status = self.check_schema_status()

        if needs_upgrade:
            # Remove the auto_upgrade check since we explicitly called this method
            if self.upgrade_schema():
                return True, "Schema was automatically upgraded"
            else:
                return (
                    False,
                    "Automatic schema upgrade failed. You are seeing this message because there were differences in your current database schema and the most recent version of the Autogen Studio app database. You can ignore the error, or specifically, you can install AutoGen Studio in a new path `autogenstudio ui --appdir <new path>`.",
                )

        return False, status

    def generate_revision(self, message: str = "auto") -> Optional[str]:
        """
        Generates new migration revision for current schema changes.

        Args:
            message: Revision message

        Returns:
            str: Revision ID if successful, None otherwise
        """
        try:
            config = self.get_alembic_config()
            with redirect_stdout(io.StringIO()):
                command.revision(config, message=message, autogenerate=True)
                return self.get_head_revision()

        except Exception as e:
            logger.error(f"Failed to generate revision: {str(e)}")
            return None

    def get_pending_migrations(self) -> List[str]:
        """
        Gets list of pending migrations that need to be applied.

        Returns:
            List[str]: List of pending migration revision IDs
        """
        config = self.get_alembic_config()
        script = ScriptDirectory.from_config(config)

        current = self.get_current_revision()
        head = self.get_head_revision()

        if current == head:
            return []

        pending = []
        for rev in script.iterate_revisions(current, head):
            pending.append(rev.revision)

        return pending

    def print_status(self) -> None:
        """Prints current migration status information to logger."""
        current = self.get_current_revision()
        head = self.get_head_revision()
        differences = self.get_schema_differences()
        pending = self.get_pending_migrations()

        logger.info("=== Database Schema Status ===")
        logger.info(f"Current revision: {current}")
        logger.info(f"Head revision: {head}")
        logger.info(f"Pending migrations: {len(pending)}")
        for rev in pending:
            logger.info(f"  - {rev}")
        logger.info(f"Unmigrated changes: {len(differences)}")
        for diff in differences:
            logger.info(f"  - {diff}")

    def ensure_schema_up_to_date(self) -> bool:
        """
        Reset migrations and create fresh migration for current schema state.
        """
        try:
            logger.info("Resetting migrations and updating to current schema...")

            # 1. Clear the entire alembic directory
            if self.alembic_dir.exists():
                shutil.rmtree(self.alembic_dir)
                logger.info("Cleared alembic directory")

            # 2. Clear alembic_version table
            with self.engine.connect() as connection:
                connection.execute(text("DROP TABLE IF EXISTS alembic_version"))
                connection.commit()
                logger.info("Reset alembic version")

            # 3. Reinitialize alembic from scratch
            if not self._initialize_alembic():
                logger.error("Failed to reinitialize alembic")
                return False

            # 4. Generate fresh migration from current schema
            revision = self.generate_revision("current_schema")
            if not revision:
                logger.error("Failed to generate new migration")
                return False
            logger.info(f"Generated fresh migration: {revision}")

            # 5. Apply the migration
            if not self.upgrade_schema():
                logger.error("Failed to apply migration")
                return False
            logger.info("Successfully applied migration")

            return True

        except Exception as e:
            logger.error(f"Failed to ensure schema is up to date: {e}")
            return False

# From database/schema_manager.py
def initialize_migrations(self, force: bool = False) -> bool:
        try:
            if force:
                # logger.info("Force reinitialization of migrations...")
                self._cleanup_existing_alembic()
                if not self._initialize_alembic():
                    return False
            else:
                try:
                    self._validate_alembic_setup()
                    logger.info("Using existing Alembic configuration")
                    self._update_configuration()
                except FileNotFoundError:
                    logger.info("Initializing new Alembic configuration")
                    if not self._initialize_alembic():
                        return False

            # Only generate initial revision if alembic is properly initialized
            # logger.info("Creating initial migration...")
            return self.generate_revision("Initial schema") is not None

        except Exception as e:
            logger.error(f"Failed to initialize migrations: {e}")
            return False

# From database/schema_manager.py
def update_script_template(self):
        """Update the Alembic script template to include SQLModel."""
        template_path = self.alembic_dir / "script.py.mako"
        try:
            with open(template_path, "r") as f:
                content = f.read()

            # Add sqlmodel import to imports section
            import_section = "from alembic import op\nimport sqlalchemy as sa"
            new_imports = "from alembic import op\nimport sqlalchemy as sa\nimport sqlmodel"

            content = content.replace(import_section, new_imports)

            with open(template_path, "w") as f:
                f.write(content)

            return True

        except Exception as e:
            logger.error(f"Failed to update script template: {e}")
            return False

# From database/schema_manager.py
def get_alembic_config(self) -> Config:
        """
        Gets Alembic configuration.

        Returns:
            Config: Alembic Config object

        Raises:
            FileNotFoundError: If alembic.ini cannot be found
        """
        if not self.alembic_ini_path.exists():
            raise FileNotFoundError("Could not find alembic.ini")

        return Config(str(self.alembic_ini_path))

# From database/schema_manager.py
def get_current_revision(self) -> Optional[str]:
        """
        Gets the current database revision.

        Returns:
            str: Current revision string or None if no revision
        """
        with self.engine.connect() as conn:
            context = MigrationContext.configure(conn)
            return context.get_current_revision()

# From database/schema_manager.py
def get_head_revision(self) -> Optional[str]:
        """
        Gets the latest available revision.

        Returns:
            Optional[str]: Head revision string or None if no revisions exist
        """
        config = self.get_alembic_config()
        script = ScriptDirectory.from_config(config)
        return script.get_current_head()

# From database/schema_manager.py
def get_schema_differences(self) -> List[tuple]:
        """
        Detects differences between current database and models.

        Returns:
            List[tuple]: List of differences found
        """
        with self.engine.connect() as conn:
            context = MigrationContext.configure(conn)
            diff = compare_metadata(context, SQLModel.metadata)
            return list(diff)

# From database/schema_manager.py
def check_schema_status(self) -> Tuple[bool, str]:
        """
        Checks if database schema matches current models and migrations.

        Returns:
            Tuple[bool, str]: (needs_upgrade, status_message)
        """
        try:
            current_rev = self.get_current_revision()
            head_rev = self.get_head_revision()

            if current_rev != head_rev:
                return True, f"Database needs upgrade: {current_rev} -> {head_rev}"

            differences = self.get_schema_differences()
            if differences:
                changes_desc = "\n".join(str(diff) for diff in differences)
                return True, f"Unmigrated changes detected:\n{changes_desc}"

            return False, "Database schema is up to date"

        except Exception as e:
            logger.error(f"Error checking schema status: {str(e)}")
            return True, f"Error checking schema: {str(e)}"

# From database/schema_manager.py
def upgrade_schema(self, revision: str = "head") -> bool:
        """
        Upgrades database schema to specified revision.

        Args:
            revision: Target revision (default: "head")

        Returns:
            bool: True if upgrade successful
        """
        try:
            config = self.get_alembic_config()
            command.upgrade(config, revision)
            logger.info(f"Schema upgraded successfully to {revision}")
            return True

        except Exception as e:
            logger.error(f"Schema upgrade failed: {str(e)}")
            return False

# From database/schema_manager.py
def check_and_upgrade(self) -> Tuple[bool, str]:
        """
        Checks schema status and upgrades if necessary.

        Returns:
            Tuple[bool, str]: (action_taken, status_message)
        """
        needs_upgrade, status = self.check_schema_status()

        if needs_upgrade:
            # Remove the auto_upgrade check since we explicitly called this method
            if self.upgrade_schema():
                return True, "Schema was automatically upgraded"
            else:
                return (
                    False,
                    "Automatic schema upgrade failed. You are seeing this message because there were differences in your current database schema and the most recent version of the Autogen Studio app database. You can ignore the error, or specifically, you can install AutoGen Studio in a new path `autogenstudio ui --appdir <new path>`.",
                )

        return False, status

# From database/schema_manager.py
def generate_revision(self, message: str = "auto") -> Optional[str]:
        """
        Generates new migration revision for current schema changes.

        Args:
            message: Revision message

        Returns:
            str: Revision ID if successful, None otherwise
        """
        try:
            config = self.get_alembic_config()
            with redirect_stdout(io.StringIO()):
                command.revision(config, message=message, autogenerate=True)
                return self.get_head_revision()

        except Exception as e:
            logger.error(f"Failed to generate revision: {str(e)}")
            return None

# From database/schema_manager.py
def get_pending_migrations(self) -> List[str]:
        """
        Gets list of pending migrations that need to be applied.

        Returns:
            List[str]: List of pending migration revision IDs
        """
        config = self.get_alembic_config()
        script = ScriptDirectory.from_config(config)

        current = self.get_current_revision()
        head = self.get_head_revision()

        if current == head:
            return []

        pending = []
        for rev in script.iterate_revisions(current, head):
            pending.append(rev.revision)

        return pending

# From database/schema_manager.py
def print_status(self) -> None:
        """Prints current migration status information to logger."""
        current = self.get_current_revision()
        head = self.get_head_revision()
        differences = self.get_schema_differences()
        pending = self.get_pending_migrations()

        logger.info("=== Database Schema Status ===")
        logger.info(f"Current revision: {current}")
        logger.info(f"Head revision: {head}")
        logger.info(f"Pending migrations: {len(pending)}")
        for rev in pending:
            logger.info(f"  - {rev}")
        logger.info(f"Unmigrated changes: {len(differences)}")
        for diff in differences:
            logger.info(f"  - {diff}")

# From database/schema_manager.py
def ensure_schema_up_to_date(self) -> bool:
        """
        Reset migrations and create fresh migration for current schema state.
        """
        try:
            logger.info("Resetting migrations and updating to current schema...")

            # 1. Clear the entire alembic directory
            if self.alembic_dir.exists():
                shutil.rmtree(self.alembic_dir)
                logger.info("Cleared alembic directory")

            # 2. Clear alembic_version table
            with self.engine.connect() as connection:
                connection.execute(text("DROP TABLE IF EXISTS alembic_version"))
                connection.commit()
                logger.info("Reset alembic version")

            # 3. Reinitialize alembic from scratch
            if not self._initialize_alembic():
                logger.error("Failed to reinitialize alembic")
                return False

            # 4. Generate fresh migration from current schema
            revision = self.generate_revision("current_schema")
            if not revision:
                logger.error("Failed to generate new migration")
                return False
            logger.info(f"Generated fresh migration: {revision}")

            # 5. Apply the migration
            if not self.upgrade_schema():
                logger.error("Failed to apply migration")
                return False
            logger.info("Successfully applied migration")

            return True

        except Exception as e:
            logger.error(f"Failed to ensure schema is up to date: {e}")
            return False

from contextlib import asynccontextmanager
from typing import AsyncGenerator
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from auth import authroutes
from auth.middleware import AuthMiddleware
from config import settings
from deps import cleanup_managers
from deps import init_auth_manager
from deps import init_managers
from deps import register_auth_dependencies
from initialization import AppInitializer
from routes import gallery
from routes import mcp
from routes import runs
from routes import sessions
from routes import settingsroute
from routes import teams
from routes import validation
from routes import ws

# From web/app.py
def create_app() -> FastAPI:
    """
    Factory function to create and configure the FastAPI application.
    Useful for testing and different deployment scenarios.
    """
    return app

from contextlib import contextmanager
from fastapi import Depends
from fastapi import HTTPException
from fastapi import Request
from fastapi import status
from database import DatabaseManager
from auth import AuthConfig
from auth import AuthManager
from auth import AuthMiddleware
from auth.dependencies import get_auth_manager
from managers.connection import WebSocketManager
from datamodel.db import Session

# From web/deps.py
class ManagerOperationError(Exception):
    """Custom exception for manager operation errors"""

    def __init__(self, manager_name: str, operation: str, detail: str):
        self.manager_name = manager_name
        self.operation = operation
        self.detail = detail
        super().__init__(f"{manager_name} failed during {operation}: {detail}")

# From web/deps.py
def is_lite_mode() -> bool:
    """Check if lite mode is enabled via environment variable"""
    return os.getenv("AUTOGENSTUDIO_LITE_MODE", "").lower() in ["true", "1"]

# From web/deps.py
def get_db_context():
    """Provide a transactional scope around a series of operations."""
    if not _db_manager:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Database manager not initialized"
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f"Database operation failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Database operation failed"
        ) from e

# From web/deps.py
def init_auth_manager(config_dir: Path) -> AuthManager:
    """Initialize authentication manager"""
    # Check if auth is explicitly disabled via environment variable
    if os.getenv("AUTOGENSTUDIO_AUTH_DISABLED", "").lower() in ["true", "1"]:
        config = AuthConfig(type="none")
        auth_manager = AuthManager(config)
        logger.info("Authentication disabled via environment variable")
        return auth_manager

    auth_config_path = os.environ.get("AUTOGENSTUDIO_AUTH_CONFIG")

    if auth_config_path and os.path.exists(auth_config_path):
        try:
            auth_manager = AuthManager.from_yaml(auth_config_path)
            logger.info(f"Authentication initialized with provider: {auth_manager.config.type}")
            return auth_manager
        except Exception as e:
            logger.error(f"Failed to initialize authentication from config file: {str(e)}")
            logger.warning("Falling back to no authentication")

    # Default or fallback
    config = AuthConfig(type="none")
    auth_manager = AuthManager(config)
    logger.info("Authentication disabled (no config provided)")
    return auth_manager

# From web/deps.py
def get_manager_status() -> dict:
    """Get the initialization status of all managers"""
    return {
        "database_manager": _db_manager is not None,
        "websocket_manager": _websocket_manager is not None,
        "team_manager": _team_manager is not None,
        "auth_manager": _auth_manager is not None,
    }

# From web/deps.py
def require_managers(*manager_names: str):
    """Decorator to require specific managers for a route"""

    async def dependency():
        manager_status = get_manager_status()  # Different name
        missing = [name for name in manager_names if not manager_status.get(f"{name}_manager")]
        if missing:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,  # Now this refers to the imported module
                detail=f"Required managers not available: {', '.join(missing)}",
            )
        return True

    return Depends(dependency)

from dotenv import load_dotenv
from config import Settings

# From web/initialization.py
class _AppPaths(BaseModel):
    """Internal model representing all application paths"""

    app_root: Path
    static_root: Path
    user_files: Path
    ui_root: Path
    config_dir: Path
    database_uri: str

# From web/initialization.py
class AppInitializer:
    """Handles application initialization including paths and environment setup"""

    def __init__(self, settings: Settings, app_path: str):
        """
        Initialize the application structure.

        Args:
            settings: Application settings
            app_path: Path to the application code directory
        """
        self.settings = settings
        self._app_path = Path(app_path)
        self._paths = self._init_paths()
        self._create_directories()
        self._load_environment()
        logger.info(f"Initializing application data folder: {self.app_root} ")

    def _get_app_root(self) -> Path:
        """Determine application root directory"""
        if app_dir := os.getenv("AUTOGENSTUDIO_APPDIR"):
            return Path(app_dir)
        return Path.home() / ".autogenstudio"

    def _get_database_uri(self, app_root: Path) -> str:
        """Generate database URI based on settings or environment"""
        if db_uri := os.getenv("AUTOGENSTUDIO_DATABASE_URI"):
            return db_uri
        return self.settings.DATABASE_URI.replace("./", str(app_root) + "/")

    def _init_paths(self) -> _AppPaths:
        """Initialize and return AppPaths instance"""
        app_root = self._get_app_root()
        return _AppPaths(
            app_root=app_root,
            static_root=app_root / "files",
            user_files=app_root / "files" / "user",
            ui_root=self._app_path / "ui",
            config_dir=app_root / self.settings.CONFIG_DIR,
            database_uri=self._get_database_uri(app_root),
        )

    def _create_directories(self) -> None:
        """Create all required directories"""
        self.app_root.mkdir(parents=True, exist_ok=True)
        dirs = [self.static_root, self.user_files, self.ui_root, self.config_dir]
        for path in dirs:
            path.mkdir(parents=True, exist_ok=True)

    def _load_environment(self) -> None:
        """Load environment variables from .env file if it exists"""
        env_file = self.app_root / ".env"
        if env_file.exists():
            # logger.info(f"Loading environment variables from {env_file}")
            load_dotenv(str(env_file))

    # Properties for accessing paths
    @property
    def app_root(self) -> Path:
        """Root directory for the application"""
        return self._paths.app_root

    @property
    def static_root(self) -> Path:
        """Directory for static files"""
        return self._paths.static_root

    @property
    def user_files(self) -> Path:
        """Directory for user files"""
        return self._paths.user_files

    @property
    def ui_root(self) -> Path:
        """Directory for UI files"""
        return self._paths.ui_root

    @property
    def config_dir(self) -> Path:
        """Directory for configuration files"""
        return self._paths.config_dir

    @property
    def database_uri(self) -> str:
        """Database connection URI"""
        return self._paths.database_uri

# From web/initialization.py
def app_root(self) -> Path:
        """Root directory for the application"""
        return self._paths.app_root

# From web/initialization.py
def static_root(self) -> Path:
        """Directory for static files"""
        return self._paths.static_root

# From web/initialization.py
def user_files(self) -> Path:
        """Directory for user files"""
        return self._paths.user_files

# From web/initialization.py
def ui_root(self) -> Path:
        """Directory for UI files"""
        return self._paths.ui_root

# From web/initialization.py
def config_dir(self) -> Path:
        """Directory for configuration files"""
        return self._paths.config_dir

# From web/initialization.py
def database_uri(self) -> str:
        """Database connection URI"""
        return self._paths.database_uri


from pydantic_settings import BaseSettings

# From web/config.py
class Settings(BaseSettings):
    DATABASE_URI: str = "sqlite:///./autogen04203.db"
    API_DOCS: bool = False
    CLEANUP_INTERVAL: int = 300  # 5 minutes
    SESSION_TIMEOUT: int = 3600  # 1 hour
    CONFIG_DIR: str = "configs"  # Default config directory relative to app_root
    DEFAULT_USER_ID: str = "guestuser@gmail.com"
    UPGRADE_DATABASE: bool = False

    # Lite mode settings
    LITE_MODE: bool = False
    LITE_TEAM_FILE: str = ""
    LITE_SESSION_NAME: str = ""

    model_config = {"env_prefix": "AUTOGENSTUDIO_"}

from typing import Awaitable
from typing import Callable
import aiofiles
import yaml
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import BaseAgentEvent
from autogen_agentchat.messages import BaseChatMessage
from autogen_agentchat.teams import BaseGroupChat
from autogen_core import EVENT_LOGGER_NAME
from autogen_core.logging import LLMCallEvent
from datamodel.types import EnvironmentVariable
from datamodel.types import LLMCallEventMessage
from datamodel.types import TeamResult
from web.managers.run_context import RunContext

# From teammanager/teammanager.py
class RunEventLogger(logging.Handler):
    """Event logger that queues LLMCallEvents for streaming"""

    def __init__(self):
        super().__init__()
        self.events: asyncio.Queue[LLMCallEventMessage] = asyncio.Queue()

    def emit(self, record: logging.LogRecord):
        if isinstance(record.msg, LLMCallEvent):
            self.events.put_nowait(LLMCallEventMessage(content=str(record.msg)))

# From teammanager/teammanager.py
class TeamManager:
    """Manages team operations including loading configs and running teams"""

    def __init__(self):
        self._team: Optional[BaseGroupChat] = None
        self._run_context = RunContext()

    @staticmethod
    async def load_from_file(path: Union[str, Path]) -> Any:
        """Load team configuration from JSON/YAML file"""
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"Config file not found: {path}")

        async with aiofiles.open(path) as f:
            content = await f.read()
            if path.suffix == ".json":
                return json.loads(content)
            elif path.suffix in (".yml", ".yaml"):
                return yaml.safe_load(content)
            raise ValueError(f"Unsupported file format: {path.suffix}")

    @staticmethod
    async def load_from_directory(directory: Union[str, Path]) -> List[Any]:
        """Load all team configurations from a directory"""
        directory = Path(directory)
        configs: List[Any] = []
        valid_extensions = {".json", ".yaml", ".yml"}

        for path in directory.iterdir():
            if path.is_file() and path.suffix.lower() in valid_extensions:
                try:
                    config = await TeamManager.load_from_file(path)
                    configs.append(config)
                except Exception as e:
                    logger.error(f"Failed to load {path}: {e}")

        return configs

    async def _create_team(
        self,
        team_config: Union[str, Path, Dict[str, Any], ComponentModel],
        input_func: Optional[InputFuncType] = None,
        env_vars: Optional[List[EnvironmentVariable]] = None,
    ) -> BaseGroupChat:
        """Create team instance from config"""
        if isinstance(team_config, (str, Path)):
            config = await self.load_from_file(team_config)
        elif isinstance(team_config, dict):
            config = team_config
        elif isinstance(team_config, ComponentModel):
            config = team_config.model_dump()
        else:
            raise ValueError(f"Unsupported team_config type: {type(team_config)}")

        # Load env vars into environment if provided
        if env_vars:
            logger.info("Loading environment variables")
            for var in env_vars:
                os.environ[var.name] = var.value

        self._team = BaseGroupChat.load_component(config)

        for agent in self._team._participants:  # type: ignore
            if hasattr(agent, "input_func") and isinstance(agent, UserProxyAgent) and input_func:
                agent.input_func = input_func

        return self._team

    async def run_stream(
        self,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None,
        team_config: Union[str, Path, Dict[str, Any], ComponentModel],
        input_func: Optional[InputFuncType] = None,
        cancellation_token: Optional[CancellationToken] = None,
        env_vars: Optional[List[EnvironmentVariable]] = None,
    ) -> AsyncGenerator[Union[BaseAgentEvent | BaseChatMessage | LLMCallEvent, BaseChatMessage, TeamResult], None]:
        """Stream team execution results"""
        start_time = time.time()
        team = None

        # Setup logger correctly
        logger = logging.getLogger(EVENT_LOGGER_NAME)
        logger.setLevel(logging.INFO)
        llm_event_logger = RunEventLogger()
        logger.handlers = [llm_event_logger]  # Replace all handlers

        try:
            team = await self._create_team(team_config, input_func, env_vars)

            async for message in team.run_stream(task=task, cancellation_token=cancellation_token):
                if cancellation_token and cancellation_token.is_cancelled():
                    break

                if isinstance(message, TaskResult):
                    yield TeamResult(task_result=message, usage="", duration=time.time() - start_time)
                else:
                    yield message

                # Check for any LLM events
                while not llm_event_logger.events.empty():
                    event = await llm_event_logger.events.get()
                    yield event
        finally:
            # Cleanup - remove our handler
            if llm_event_logger in logger.handlers:
                logger.handlers.remove(llm_event_logger)

            # Ensure cleanup happens
            if team and hasattr(team, "_participants"):
                for agent in team._participants:  # type: ignore
                    if hasattr(agent, "close"):
                        await agent.close()

    async def run(
        self,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None,
        team_config: Union[str, Path, Dict[str, Any], ComponentModel],
        input_func: Optional[InputFuncType] = None,
        cancellation_token: Optional[CancellationToken] = None,
        env_vars: Optional[List[EnvironmentVariable]] = None,
    ) -> TeamResult:
        """Run team synchronously"""
        start_time = time.time()
        team = None

        try:
            team = await self._create_team(team_config, input_func, env_vars)
            result = await team.run(task=task, cancellation_token=cancellation_token)

            return TeamResult(task_result=result, usage="", duration=time.time() - start_time)

        finally:
            if team and hasattr(team, "_participants"):
                for agent in team._participants:  # type: ignore
                    if hasattr(agent, "close"):
                        await agent.close()

# From teammanager/teammanager.py
def emit(self, record: logging.LogRecord):
        if isinstance(record.msg, LLMCallEvent):
            self.events.put_nowait(LLMCallEventMessage(content=str(record.msg)))

from enum import Enum
from uuid import UUID
from uuid import uuid4
from sqlmodel import Field

# From datamodel/eval.py
class EvalTask(BaseModel):
    """Definition of a task to be evaluated."""

    task_id: UUID | str = Field(default_factory=uuid4)
    input: str | Sequence[str | Image]
    name: str = ""
    description: str = ""
    expected_outputs: Optional[List[Any]] = None
    metadata: Dict[str, Any] = {}

# From datamodel/eval.py
class EvalRunResult(BaseModel):
    """Result of an evaluation run."""

    result: TaskResult | None = None
    status: bool = False
    start_time: Optional[datetime] = Field(default=datetime.now())
    end_time: Optional[datetime] = None
    error: Optional[str] = None

# From datamodel/eval.py
class EvalDimensionScore(BaseModel):
    """Score for a single evaluation dimension."""

    dimension: str
    score: float
    reason: str
    max_value: float
    min_value: float

# From datamodel/eval.py
class EvalScore(BaseModel):
    """Composite score from evaluation."""

    overall_score: Optional[float] = None
    dimension_scores: List[EvalDimensionScore] = []
    reason: Optional[str] = None
    max_value: float = 10.0
    min_value: float = 0.0
    metadata: Dict[str, Any] = {}

# From datamodel/eval.py
class EvalJudgeCriteria(BaseModel):
    """Criteria for judging evaluation results."""

    dimension: str
    prompt: str
    max_value: float = 10.0
    min_value: float = 0.0
    metadata: Dict[str, Any] = {}

# From datamodel/eval.py
class EvalRunStatus(str, Enum):
    """Status of an evaluation run."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELED = "canceled"

# From datamodel/eval.py
class EvalResult(BaseModel):
    """Result of an evaluation run."""

    task_id: UUID | str
    # runner_id: UUID | str
    status: EvalRunStatus = EvalRunStatus.PENDING
    start_time: Optional[datetime] = Field(default=datetime.now())
    end_time: Optional[datetime] = None

from typing import Literal
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import ConfigDict
from pydantic import Field
from pydantic import SecretStr

# From datamodel/types.py
class MessageConfig(BaseModel):
    source: str
    content: str | ChatMessage | Sequence[ChatMessage] | None
    message_type: Optional[str] = "text"

# From datamodel/types.py
class TeamResult(BaseModel):
    task_result: TaskResult
    usage: str
    duration: float

# From datamodel/types.py
class LLMCallEventMessage(TextMessage):
    source: str = "llm_call_event"

    def to_text(self) -> str:
        return self.content

    def to_model_text(self) -> str:
        return self.content

    def to_model_message(self) -> UserMessage:
        raise NotImplementedError("This message type is not supported.")

# From datamodel/types.py
class MessageMeta(BaseModel):
    task: Optional[str] = None
    task_result: Optional[TaskResult] = None
    summary_method: Optional[str] = "last"
    files: Optional[List[dict]] = None
    time: Optional[datetime] = None
    log: Optional[List[dict]] = None
    usage: Optional[List[dict]] = None

# From datamodel/types.py
class GalleryMetadata(BaseModel):
    author: str
    # created_at: datetime = Field(default_factory=datetime.now)
    # updated_at: datetime = Field(default_factory=datetime.now)
    version: str
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    license: Optional[str] = None
    homepage: Optional[str] = None
    category: Optional[str] = None
    last_synced: Optional[datetime] = None

    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat(),
        }
    )

# From datamodel/types.py
class GalleryComponents(BaseModel):
    agents: List[ComponentModel]
    models: List[ComponentModel]
    tools: List[ComponentModel]
    terminations: List[ComponentModel]
    teams: List[ComponentModel]
    workbenches: List[ComponentModel]

# From datamodel/types.py
class GalleryConfig(BaseModel):
    id: str
    name: str
    url: Optional[str] = None
    metadata: GalleryMetadata
    components: GalleryComponents

    model_config = ConfigDict(
        json_encoders={datetime: lambda v: v.isoformat(), SecretStr: lambda v: v.get_secret_value()}
    )

# From datamodel/types.py
class EnvironmentVariable(BaseModel):
    name: str
    value: str
    type: Literal["string", "number", "boolean", "secret"] = "string"
    description: Optional[str] = None
    required: bool = False

# From datamodel/types.py
class UISettings(BaseModel):
    show_llm_call_events: bool = False
    expanded_messages_by_default: bool = True
    show_agent_flow_by_default: bool = True
    human_input_timeout_minutes: int = Field(
        default=3, ge=1, le=30, description="Human input timeout in minutes (1-30)"
    )

# From datamodel/types.py
class SettingsConfig(BaseModel):
    environment: List[EnvironmentVariable] = []
    default_model_client: Optional[ComponentModel] = OpenAIChatCompletionClient(
        model="gpt-4o-mini", api_key="your-api-key"
    ).dump_component()
    ui: UISettings = UISettings()

# From datamodel/types.py
class Response(BaseModel):
    message: str
    status: bool
    data: Optional[Any] = None

# From datamodel/types.py
class SocketMessage(BaseModel):
    connection_id: str
    data: Dict[str, Any]
    type: str

# From datamodel/types.py
def to_text(self) -> str:
        return self.content

# From datamodel/types.py
def to_model_text(self) -> str:
        return self.content

# From datamodel/types.py
def to_model_message(self) -> UserMessage:
        raise NotImplementedError("This message type is not supported.")

from pydantic import field_validator
from sqlalchemy import ForeignKey
from sqlalchemy import Integer
from sqlmodel import JSON
from sqlmodel import Column
from sqlmodel import DateTime
from sqlmodel import func
from eval import EvalJudgeCriteria
from eval import EvalRunResult
from eval import EvalRunStatus
from eval import EvalScore
from eval import EvalTask
from types import GalleryComponents
from types import GalleryConfig
from types import GalleryMetadata
from types import MessageConfig
from types import MessageMeta
from types import SettingsConfig
from types import TeamResult

# From datamodel/db.py
class BaseDBModel(SQLModel, table=False):
    """
    Base model with common fields for all database tables.
    Not a table itself - meant to be inherited by concrete model classes.
    """

    __abstract__ = True

    # Common fields present in all database tables
    id: Optional[int] = Field(default=None, primary_key=True)

    created_at: datetime = Field(
        default_factory=datetime.now,
        sa_type=DateTime(timezone=True),  # type: ignore[assignment]
        sa_column_kwargs={"server_default": func.now(), "nullable": True},
    )

    updated_at: datetime = Field(
        default_factory=datetime.now,
        sa_type=DateTime(timezone=True),  # type: ignore[assignment]
        sa_column_kwargs={"onupdate": func.now(), "nullable": True},
    )

    user_id: Optional[str] = None
    version: Optional[str] = "0.0.1"

# From datamodel/db.py
class Team(BaseDBModel, table=True):
    __table_args__ = {"sqlite_autoincrement": True}
    component: Union[ComponentModel, dict] = Field(sa_column=Column(JSON))

# From datamodel/db.py
class Message(BaseDBModel, table=True):
    __table_args__ = {"sqlite_autoincrement": True}

    config: Union[MessageConfig, dict] = Field(
        default_factory=lambda: MessageConfig(source="", content=""), sa_column=Column(JSON)
    )
    session_id: Optional[int] = Field(
        default=None, sa_column=Column(Integer, ForeignKey("session.id", ondelete="NO ACTION"))
    )
    run_id: Optional[int] = Field(default=None, sa_column=Column(Integer, ForeignKey("run.id", ondelete="CASCADE")))

    message_meta: Optional[Union[MessageMeta, dict]] = Field(default={}, sa_column=Column(JSON))

# From datamodel/db.py
class Session(BaseDBModel, table=True):
    __table_args__ = {"sqlite_autoincrement": True}
    team_id: Optional[int] = Field(default=None, sa_column=Column(Integer, ForeignKey("team.id", ondelete="CASCADE")))
    name: Optional[str] = None

    @field_validator("created_at", "updated_at", mode="before")
    @classmethod
    def parse_datetime(cls, value: Union[str, datetime]) -> datetime:
        if isinstance(value, str):
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        return value

# From datamodel/db.py
class RunStatus(str, Enum):
    CREATED = "created"
    ACTIVE = "active"
    COMPLETE = "complete"
    ERROR = "error"
    STOPPED = "stopped"

# From datamodel/db.py
class Run(BaseDBModel, table=True):
    """Represents a single execution run within a session"""

    __table_args__ = {"sqlite_autoincrement": True}

    session_id: int = Field(sa_column=Column(Integer, ForeignKey("session.id", ondelete="CASCADE"), nullable=False))
    status: RunStatus = Field(default=RunStatus.CREATED)

    # Store the original user task
    task: Union[MessageConfig, dict] = Field(
        default_factory=lambda: MessageConfig(source="", content=""), sa_column=Column(JSON)
    )

    # Store TeamResult which contains TaskResult
    team_result: Union[TeamResult, dict] = Field(default=None, sa_column=Column(JSON))

    error_message: Optional[str] = None
    messages: Union[List[Message], List[dict]] = Field(default_factory=list, sa_column=Column(JSON))

    model_config = ConfigDict(json_encoders={datetime: lambda v: v.isoformat()})

# From datamodel/db.py
class Gallery(BaseDBModel, table=True):
    __table_args__ = {"sqlite_autoincrement": True}

    config: Union[GalleryConfig, dict] = Field(
        default_factory=lambda: GalleryConfig(
            id="",
            name="",
            metadata=GalleryMetadata(author="", version=""),
            components=GalleryComponents(agents=[], models=[], tools=[], terminations=[], teams=[], workbenches=[]),
        ),
        sa_column=Column(JSON),
    )

    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat(),
            SecretStr: lambda v: v.get_secret_value(),  # Add this line
        }
    )

# From datamodel/db.py
class EvalTaskDB(BaseDBModel, table=True):
    """Database model for storing evaluation tasks."""

    __table_args__ = {"sqlite_autoincrement": True}

    name: str = "Unnamed Task"
    description: str = ""
    config: Union[EvalTask, dict] = Field(sa_column=Column(JSON))

# From datamodel/db.py
class EvalCriteriaDB(BaseDBModel, table=True):
    """Database model for storing evaluation criteria."""

    __table_args__ = {"sqlite_autoincrement": True}

    name: str = "Unnamed Criteria"
    description: str = ""
    config: Union[EvalJudgeCriteria, dict] = Field(sa_column=Column(JSON))

# From datamodel/db.py
class EvalRunDB(BaseDBModel, table=True):
    """Database model for tracking evaluation runs."""

    __table_args__ = {"sqlite_autoincrement": True}

    name: str = "Unnamed Evaluation Run"
    description: str = ""

    # References to related components
    task_id: Optional[int] = Field(
        default=None, sa_column=Column(Integer, ForeignKey("evaltaskdb.id", ondelete="SET NULL"))
    )

    # Serialized configurations for runner and judge
    runner_config: Union[ComponentModel, dict] = Field(sa_column=Column(JSON))
    judge_config: Union[ComponentModel, dict] = Field(sa_column=Column(JSON))

    # List of criteria IDs or embedded criteria configs
    criteria_configs: List[Union[EvalJudgeCriteria, dict]] = Field(default_factory=list, sa_column=Column(JSON))

    # Run status and timing information
    status: EvalRunStatus = Field(default=EvalRunStatus.PENDING)
    start_time: Optional[datetime] = Field(default=None)
    end_time: Optional[datetime] = Field(default=None)

    # Results (updated as they become available)
    run_result: Union[EvalRunResult, dict] = Field(default=None, sa_column=Column(JSON))

    score_result: Union[EvalScore, dict] = Field(default=None, sa_column=Column(JSON))

    # Additional metadata
    error_message: Optional[str] = None

# From datamodel/db.py
def parse_datetime(cls, value: Union[str, datetime]) -> datetime:
        if isinstance(value, str):
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        return value

from urllib.parse import urljoin
import html2text
import httpx
from autogen_core.code_executor import ImportFromModule
from autogen_core.tools import FunctionTool
from bs4 import BeautifulSoup



# From tools/calculator.py
def calculator(a: float, b: float, operator: str) -> str:
    try:
        if operator == "+":
            return str(a + b)
        elif operator == "-":
            return str(a - b)
        elif operator == "*":
            return str(a * b)
        elif operator == "/":
            if b == 0:
                return "Error: Division by zero"
            return str(a / b)
        else:
            return "Error: Invalid operator. Please use +, -, *, or /"
    except Exception as e:
        return f"Error: {str(e)}"

from openai import OpenAI
from PIL import Image


import re
from fastapi import Response
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.middleware.base import RequestResponseEndpoint
from starlette.status import HTTP_401_UNAUTHORIZED
from starlette.types import ASGIApp
from exceptions import AuthException
from manager import AuthManager

# From auth/middleware.py
class AuthMiddleware(BaseHTTPMiddleware):
    """
    Middleware for handling authentication for all routes.
    """

    def __init__(self, app: ASGIApp, auth_manager: AuthManager) -> None:
        super().__init__(app)
        self.auth_manager = auth_manager

    async def dispatch(self, request: Request, call_next: RequestResponseEndpoint) -> Response:
        """Process each request, authenticating as needed."""
        # Skip auth for OPTIONS requests (CORS preflight)
        if request.method == "OPTIONS":
            return await call_next(request)

        path = request.url.path

        if (
            path == "/"
            or path == "/login"
            or path == "/callback"
            or path == "/images"
            or path.startswith("/page-data/")
            or path in self.auth_manager.config.exclude_paths
            or re.match(r"/[^/]+\.(js|css|png|ico|svg|jpg|webmanifest|json)$", path)
            or re.match(r".*\.(js\.map|svg)$", path)
        ):
            return await call_next(request)

        # Skip auth if disabled
        if self.auth_manager.config.type == "none":
            request.state.user = await self.auth_manager.authenticate_request(request)
            return await call_next(request)

        # WebSocket handling (special case)
        if request.url.path.startswith("/api/ws") or request.url.path.startswith("/api/maker"):
            # For WebSockets, we'll add auth in the WebSocket accept handler
            # Just pass through here
            return await call_next(request)

        # Handle authentication for all other requests
        try:
            user = await self.auth_manager.authenticate_request(request)
            # Add user to request state for use in route handlers
            request.state.user = user
            return await call_next(request)

        except AuthException as e:
            # Handle authentication errors
            return Response(
                status_code=HTTP_401_UNAUTHORIZED,
                content=json.dumps({"status": False, "detail": e.detail}),
                media_type="application/json",
                headers=e.headers or {},
            )
        except Exception as e:
            # Log unexpected errors
            logger.error(f"Unexpected error in auth middleware: {str(e)}")
            return Response(
                status_code=HTTP_401_UNAUTHORIZED,
                content=json.dumps({"status": False, "detail": "Authentication failed"}),
                media_type="application/json",
            )

# From auth/middleware.py
class WebSocketAuthMiddleware:
    """
    Helper for authenticating WebSocket connections.
    Not a middleware in the traditional sense - used in WebSocket endpoint.
    """

    def __init__(self, auth_manager: AuthManager) -> None:
        self.auth_manager = auth_manager

    async def authenticate(self, websocket: WebSocket) -> bool:
        """
        Authenticate a WebSocket connection.
        Returns True if authenticated, False otherwise.
        """
        if self.auth_manager.config.type == "none":
            return True

        try:
            # Extract token from query params or cookies
            token = None
            if "token" in websocket.query_params:
                token = websocket.query_params["token"]
            elif "authorization" in websocket.headers:
                auth_header = websocket.headers["authorization"]
                if auth_header.startswith("Bearer "):
                    token = auth_header.replace("Bearer ", "")

            if not token:
                logger.warning("No token found for WebSocket connection")
                return False

            # Validate token
            valid = self.auth_manager.is_valid_token(token)
            if not valid:
                logger.warning("Invalid token for WebSocket connection")
                return False

            return True

        except Exception as e:
            logger.error(f"WebSocket auth error: {str(e)}")
            return False

from datetime import timedelta
import jwt
from exceptions import ConfigurationException
from exceptions import InvalidTokenException
from exceptions import MissingTokenException
from models import AuthConfig
from models import User
from providers import AuthProvider
from providers import FirebaseAuthProvider
from providers import GithubAuthProvider
from providers import MSALAuthProvider
from providers import NoAuthProvider

# From auth/manager.py
class AuthManager:
    """
    Manages authentication for the application.
    Handles token creation, validation, and provider selection.
    """

    def __init__(self, config: AuthConfig):
        """Initialize the auth manager with configuration."""
        self.config = config
        self.provider = self._create_provider()
        logger.info(f"Initialized auth manager with provider: {config.type}")

    def _create_provider(self) -> AuthProvider:
        """Create the appropriate auth provider based on config."""
        try:
            if self.config.type == "github":
                return GithubAuthProvider(self.config)
            elif self.config.type == "msal":
                return MSALAuthProvider(self.config)
            elif self.config.type == "firebase":
                return FirebaseAuthProvider(self.config)
            else:
                return NoAuthProvider()
        except Exception as e:
            logger.error(f"Failed to create auth provider: {str(e)}")
            # Fall back to no auth if provider creation fails
            return NoAuthProvider()

    def create_token(self, user: User) -> str:
        """Create a JWT token for authenticated user."""
        if not self.config.jwt_secret:
            logger.warning("JWT secret not configured, using insecure token")
            return "dummy_token_" + user.id

        expiry = datetime.now(timezone.utc) + timedelta(minutes=self.config.token_expiry_minutes)
        payload = {
            "sub": user.id,
            "name": user.name,
            "email": user.email,
            "provider": user.provider,
            "roles": user.roles,
            "exp": expiry,
        }
        return jwt.encode(payload, self.config.jwt_secret, algorithm="HS256")

    async def authenticate_request(self, request: Request) -> User:
        """Authenticate a request and return user information."""
        # Check if path should be excluded from auth
        # print("************ authenticating request ************", request.url.path, self.config.type )
        if request.url.path in self.config.exclude_paths:
            return User(id="guestuser@gmail.com", name="Default User", provider="none")

        if self.config.type == "none":
            # No auth mode - return default user
            return User(id="guestuser@gmail.com", name="Default User", provider="none")

        # Extract token from Authorization header
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            raise MissingTokenException()

        token = auth_header.replace("Bearer ", "")

        try:
            if not self.config.jwt_secret:
                # For development with no JWT secret
                logger.warning("JWT secret not configured, accepting all tokens")
                return User(id="guestuser@gmail.com", name="Default User", provider="none")

            # Decode and validate JWT
            payload = jwt.decode(token, self.config.jwt_secret, algorithms=["HS256"])

            # Create User object from token payload
            return User(
                id=payload.get("sub"),
                name=payload.get("name", "Unknown User"),
                email=payload.get("email"),
                provider=payload.get("provider", "jwt"),
                roles=payload.get("roles", ["user"]),
            )

        except jwt.ExpiredSignatureError as e:
            logger.warning(f"Expired token received: {token[:10]}...")
            raise InvalidTokenException() from e
        except jwt.InvalidTokenError as e:
            logger.warning(f"Invalid token received: {token[:10]}...")
            raise InvalidTokenException() from e

    def is_valid_token(self, token: str) -> bool:
        """Check if a JWT token is valid."""
        if not self.config.jwt_secret:
            return True  # No validation in dev mode

        try:
            jwt.decode(token, self.config.jwt_secret, algorithms=["HS256"])
            return True
        except jwt.ExpiredSignatureError:
            logger.warning("Token has expired")
            return False

    @classmethod
    def from_yaml(cls, yaml_path: str) -> Self:
        """Create AuthManager from YAML config file."""
        try:
            with open(yaml_path, "r") as f:
                config_data = yaml.safe_load(f)
            config = AuthConfig(**config_data)
            return cls(config)
        except Exception as e:
            logger.error(f"Failed to load auth config from {yaml_path}: {str(e)}")
            raise ConfigurationException(f"Failed to load auth config: {str(e)}") from e

    @classmethod
    def from_env(cls) -> Self:
        """Create AuthManager from environment variables."""
        auth_type = os.environ.get("AUTOGENSTUDIO_AUTH_TYPE", "none")

        config_dict: Dict[str, Any] = {
            "type": auth_type,
            "jwt_secret": os.environ.get("AUTOGENSTUDIO_JWT_SECRET"),
            "token_expiry_minutes": int(os.environ.get("AUTOGENSTUDIO_TOKEN_EXPIRY", "60")),
        }

        # Add provider-specific config based on the auth type
        if auth_type == "github":
            config_dict["github"] = {
                "client_id": os.environ.get("AUTOGENSTUDIO_GITHUB_CLIENT_ID", ""),
                "client_secret": os.environ.get("AUTOGENSTUDIO_GITHUB_CLIENT_SECRET", ""),
                "callback_url": os.environ.get("AUTOGENSTUDIO_GITHUB_CALLBACK_URL", ""),
                "scopes": os.environ.get("AUTOGENSTUDIO_GITHUB_SCOPES", "user:email").split(","),
            }
        # Add other provider config parsing here

        config = AuthConfig(**config_dict)
        return cls(config)

# From auth/manager.py
def create_token(self, user: User) -> str:
        """Create a JWT token for authenticated user."""
        if not self.config.jwt_secret:
            logger.warning("JWT secret not configured, using insecure token")
            return "dummy_token_" + user.id

        expiry = datetime.now(timezone.utc) + timedelta(minutes=self.config.token_expiry_minutes)
        payload = {
            "sub": user.id,
            "name": user.name,
            "email": user.email,
            "provider": user.provider,
            "roles": user.roles,
            "exp": expiry,
        }
        return jwt.encode(payload, self.config.jwt_secret, algorithm="HS256")

# From auth/manager.py
def is_valid_token(self, token: str) -> bool:
        """Check if a JWT token is valid."""
        if not self.config.jwt_secret:
            return True  # No validation in dev mode

        try:
            jwt.decode(token, self.config.jwt_secret, algorithms=["HS256"])
            return True
        except jwt.ExpiredSignatureError:
            logger.warning("Token has expired")
            return False

# From auth/manager.py
def from_yaml(cls, yaml_path: str) -> Self:
        """Create AuthManager from YAML config file."""
        try:
            with open(yaml_path, "r") as f:
                config_data = yaml.safe_load(f)
            config = AuthConfig(**config_data)
            return cls(config)
        except Exception as e:
            logger.error(f"Failed to load auth config from {yaml_path}: {str(e)}")
            raise ConfigurationException(f"Failed to load auth config: {str(e)}") from e

# From auth/manager.py
def from_env(cls) -> Self:
        """Create AuthManager from environment variables."""
        auth_type = os.environ.get("AUTOGENSTUDIO_AUTH_TYPE", "none")

        config_dict: Dict[str, Any] = {
            "type": auth_type,
            "jwt_secret": os.environ.get("AUTOGENSTUDIO_JWT_SECRET"),
            "token_expiry_minutes": int(os.environ.get("AUTOGENSTUDIO_TOKEN_EXPIRY", "60")),
        }

        # Add provider-specific config based on the auth type
        if auth_type == "github":
            config_dict["github"] = {
                "client_id": os.environ.get("AUTOGENSTUDIO_GITHUB_CLIENT_ID", ""),
                "client_secret": os.environ.get("AUTOGENSTUDIO_GITHUB_CLIENT_SECRET", ""),
                "callback_url": os.environ.get("AUTOGENSTUDIO_GITHUB_CALLBACK_URL", ""),
                "scopes": os.environ.get("AUTOGENSTUDIO_GITHUB_SCOPES", "user:email").split(","),
            }
        # Add other provider config parsing here

        config = AuthConfig(**config_dict)
        return cls(config)

import html
from fastapi import APIRouter
from fastapi import Cookie
from fastapi.responses import JSONResponse
from exceptions import ProviderAuthException

# From auth/authroutes.py
def get_auth_manager(request: Request) -> AuthManager:
    """Get the auth manager from app state."""
    if not hasattr(request.app.state, "auth_manager"):
        raise HTTPException(status_code=500, detail="Authentication system not initialized")
    return request.app.state.auth_manager

# From auth/authroutes.py
def get_current_user(request: Request) -> User:
    """Get the current authenticated user."""
    if hasattr(request.state, "user"):
        return request.state.user

    # This shouldn't normally happen as middleware should set user
    logger.warning("User not found in request state")
    return User(id="anonymous", name="Anonymous User")

import secrets
from urllib.parse import urlencode
from models import GithubAuthConfig

# From auth/providers.py
class AuthProvider(ABC):
    """Base authentication provider interface."""

    @abstractmethod
    async def get_login_url(self) -> str:
        """Return the URL for initiating login."""
        pass

    @abstractmethod
    async def process_callback(self, code: str, state: str | None = None) -> User:
        """Process the OAuth callback code and return user data."""
        pass

    @abstractmethod
    async def validate_token(self, token: str) -> bool:
        """Validate a provider token and return boolean indicating validity."""
        pass

# From auth/providers.py
class NoAuthProvider(AuthProvider):
    """Default provider that always authenticates (for development)."""

    def __init__(self):
        self.default_user = User(
            id="guestuser@gmail.com", name="Default User", email="guestuser@gmail.com", provider="none"
        )

    async def get_login_url(self) -> str:
        """Return the URL for initiating login."""
        return "/api/auth/callback?automatic=true"

    async def process_callback(self, code: str | None = None, state: str | None = None) -> User:
        """Process the OAuth callback code and return user data."""
        return self.default_user

    async def validate_token(self, token: str) -> bool:
        """Validate a provider token and return boolean indicating validity."""
        return True

# From auth/providers.py
class GithubAuthProvider(AuthProvider):
    """GitHub OAuth authentication provider."""

    def __init__(self, config: AuthConfig):
        if not config.github:
            raise ConfigurationException("GitHub auth configuration is missing")

        self.config = config.github
        self.client_id = self.config.client_id
        self.client_secret = self.config.client_secret
        self.callback_url = self.config.callback_url
        self.scopes = self.config.scopes

    async def get_login_url(self) -> str:
        """Return the GitHub OAuth login URL."""
        state = secrets.token_urlsafe(32)  # Generate a secure random state
        params = {
            "client_id": self.client_id,
            "redirect_uri": self.callback_url,
            "scope": " ".join(self.scopes),
            "state": state,
            "allow_signup": "true",
        }
        return f"https://github.com/login/oauth/authorize?{urlencode(params)}"

    async def process_callback(self, code: str, state: str | None = None) -> User:
        """Exchange code for access token and get user info."""
        if not code:
            raise ProviderAuthException("github", "Authorization code is missing")

        # Exchange code for access token
        token_url = "https://github.com/login/oauth/access_token"
        token_data = {
            "client_id": self.client_id,
            "client_secret": self.client_secret,
            "code": code,
            "redirect_uri": self.callback_url,
        }

        async with httpx.AsyncClient() as client:
            token_response = await client.post(token_url, data=token_data, headers={"Accept": "application/json"})

            if token_response.status_code != 200:
                logger.error(f"GitHub token exchange failed: {token_response.text}")
                raise ProviderAuthException("github", "Failed to exchange code for access token")

            token_json = token_response.json()
            access_token = token_json.get("access_token")

            if not access_token:
                logger.error(f"No access token in GitHub response: {token_json}")
                raise ProviderAuthException("github", "No access token received")

            # Get user info with the access token
            user_response = await client.get(
                "https://api.github.com/user",
                headers={"Authorization": f"token {access_token}", "Accept": "application/json"},
            )

            if user_response.status_code != 200:
                logger.error(f"GitHub user info fetch failed: {user_response.text}")
                raise ProviderAuthException("github", "Failed to fetch user information")

            user_data = user_response.json()

            # Get user emails if scope includes email
            email = None
            if "user:email" in self.scopes:
                email_response = await client.get(
                    "https://api.github.com/user/emails",
                    headers={"Authorization": f"token {access_token}", "Accept": "application/json"},
                )

                if email_response.status_code == 200:
                    emails = email_response.json()
                    primary_emails = [e for e in emails if e.get("primary") is True]
                    if primary_emails:
                        email = primary_emails[0].get("email")

            # Create User object
            return User(
                id=str(user_data.get("id")),
                name=user_data.get("name") or user_data.get("login"),
                email=email,
                avatar_url=user_data.get("avatar_url"),
                provider="github",
                metadata={
                    "login": user_data.get("login"),
                    "github_id": user_data.get("id"),
                    "access_token": access_token,
                },
            )

    async def validate_token(self, token: str) -> bool:
        """Validate a GitHub access token."""
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "https://api.github.com/user", headers={"Authorization": f"token {token}", "Accept": "application/json"}
            )
            return response.status_code == 200

# From auth/providers.py
class MSALAuthProvider(AuthProvider):
    """Microsoft Authentication Library (MSAL) provider."""

    def __init__(self, config: AuthConfig):
        if not config.msal:
            raise ConfigurationException("MSAL auth configuration is missing")

        self.config = config.msal
        # MSAL provider implementation would go here
        # This is a placeholder - full implementation would use msal library

    async def get_login_url(self) -> str:
        """Return the MSAL OAuth login URL."""
        # Placeholder - would use MSAL library to generate auth URL
        return "https://login.microsoftonline.com/placeholder"

    async def process_callback(self, code: str, state: str | None = None) -> User:
        """Process the MSAL callback."""
        # Placeholder - would use MSAL library to process code and get token/user info
        return User(id="msal_user_id", name="MSAL User", provider="msal")

    async def validate_token(self, token: str) -> bool:
        """Validate an MSAL token."""
        # Placeholder - would validate token with MSAL library
        return False

# From auth/providers.py
class FirebaseAuthProvider(AuthProvider):
    """Firebase authentication provider."""

    def __init__(self, config: AuthConfig):
        if not config.firebase:
            raise ConfigurationException("Firebase auth configuration is missing")

        self.config = config.firebase
        # Firebase provider implementation would go here
        # This is a placeholder - full implementation would use Firebase Admin SDK

    async def get_login_url(self) -> str:
        """Return information for Firebase auth (used differently than OAuth)."""
        # Firebase auth is typically handled on the client side
        return json.dumps(
            {"apiKey": self.config.api_key, "authDomain": self.config.auth_domain, "projectId": self.config.project_id}
        )

    async def process_callback(self, code: str, state: str | None = None) -> User:
        """Process a Firebase ID token."""
        # Placeholder - would verify Firebase ID token and get user info
        return User(id="firebase_user_id", name="Firebase User", provider="firebase")

    async def validate_token(self, token: str) -> bool:
        """Validate a Firebase ID token."""
        # Placeholder - would validate token with Firebase Admin SDK
        return False


# From auth/exceptions.py
class AuthException(HTTPException):
    """Base class for authentication exceptions."""

    def __init__(self, detail: str, headers: dict | None = None):
        super().__init__(status_code=401, detail=detail, headers=headers)

# From auth/exceptions.py
class InvalidTokenException(AuthException):
    """Exception raised when token is invalid."""

    def __init__(self):
        super().__init__(detail="Invalid or expired token")

# From auth/exceptions.py
class MissingTokenException(AuthException):
    """Exception raised when token is missing."""

    def __init__(self):
        super().__init__(detail="Authentication token is missing")

# From auth/exceptions.py
class ProviderAuthException(AuthException):
    """Exception raised when authentication with provider fails."""

    def __init__(self, provider: str, detail: str):
        super().__init__(detail=f"Authentication failed with {provider}: {detail}")

# From auth/exceptions.py
class ConfigurationException(Exception):
    """Exception raised when there's an issue with auth configuration."""

    pass

# From auth/exceptions.py
class ForbiddenException(HTTPException):
    """Exception raised when user doesn't have permission."""

    def __init__(self, detail: str = "You don't have permission to access this resource"):
        super().__init__(status_code=403, detail=detail)


# From auth/models.py
class GithubAuthConfig(BaseModel):
    client_id: str
    client_secret: str
    callback_url: str
    scopes: List[str] = ["user:email"]

# From auth/models.py
class MSALAuthConfig(BaseModel):
    tenant_id: str
    client_id: str
    client_secret: str
    callback_url: str
    scopes: List[str] = ["User.Read"]

# From auth/models.py
class FirebaseAuthConfig(BaseModel):
    api_key: str
    auth_domain: str
    project_id: str

# From auth/models.py
class AuthConfig(BaseModel):
    """Authentication configuration model for the application."""

    type: Literal["none", "github", "msal", "firebase"] = "none"
    github: Optional[GithubAuthConfig] = None
    msal: Optional[MSALAuthConfig] = None
    firebase: Optional[FirebaseAuthConfig] = None
    jwt_secret: Optional[str] = None
    token_expiry_minutes: int = 60
    exclude_paths: List[str] = [
        "/",  # root for serving frontend
        "/api/health",
        "/api/version",
        "/api/auth/login-url",
        "/api/auth/callback-handler",
        "/api/auth/callback",
        "/api/auth/type",
    ]

    @field_validator("github")
    @classmethod
    def validate_github_config(cls, v, info):
        """Validate GitHub config is present when github type is selected."""
        values = info.data
        if values.get("type") == "github" and v is None:
            raise ValueError("GitHub configuration required when type is 'github'")
        return v

    @field_validator("msal")
    @classmethod
    def validate_msal_config(cls, v, info):
        """Validate MSAL config is present when msal type is selected."""
        values = info.data
        if values.get("type") == "msal" and v is None:
            raise ValueError("MSAL configuration required when type is 'msal'")
        return v

    @field_validator("firebase")
    @classmethod
    def validate_firebase_config(cls, v, info):
        """Validate Firebase config is present when firebase type is selected."""
        values = info.data
        if values.get("type") == "firebase" and v is None:
            raise ValueError("Firebase configuration required when type is 'firebase'")
        return v

    @field_validator("jwt_secret")
    @classmethod
    def validate_jwt_secret(cls, v, info):
        """Validate JWT secret is present for auth types other than 'none'."""
        values = info.data
        if values.get("type") != "none" and not v:
            raise ValueError("JWT secret is required for authentication")
        return v

# From auth/models.py
class User(BaseModel):
    """User model for authenticated users."""

    id: str
    name: str
    email: Optional[str] = None
    avatar_url: Optional[str] = None
    provider: Optional[str] = None
    roles: List[str] = ["user"]
    metadata: Optional[Dict[str, Any]] = None

# From auth/models.py
def validate_github_config(cls, v, info):
        """Validate GitHub config is present when github type is selected."""
        values = info.data
        if values.get("type") == "github" and v is None:
            raise ValueError("GitHub configuration required when type is 'github'")
        return v

# From auth/models.py
def validate_msal_config(cls, v, info):
        """Validate MSAL config is present when msal type is selected."""
        values = info.data
        if values.get("type") == "msal" and v is None:
            raise ValueError("MSAL configuration required when type is 'msal'")
        return v

# From auth/models.py
def validate_firebase_config(cls, v, info):
        """Validate Firebase config is present when firebase type is selected."""
        values = info.data
        if values.get("type") == "firebase" and v is None:
            raise ValueError("Firebase configuration required when type is 'firebase'")
        return v

# From auth/models.py
def validate_jwt_secret(cls, v, info):
        """Validate JWT secret is present for auth types other than 'none'."""
        values = info.data
        if values.get("type") != "none" and not v:
            raise ValueError("JWT secret is required for authentication")
        return v

from exceptions import ForbiddenException

# From auth/dependencies.py
def get_ws_auth_manager(websocket: WebSocket) -> AuthManager:
    """Get the auth manager from app state for WebSocket connections."""
    if hasattr(websocket.app.state, "auth_manager"):
        return websocket.app.state.auth_manager
    # Similar to above, remove the global reference
    raise HTTPException(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Authentication system not initialized"
    )

# From auth/dependencies.py
def require_authenticated(user: User = Depends(get_current_user)) -> User:
    """Require that the user is authenticated (not anonymous)."""
    if user.id == "anonymous":
        raise HTTPException(status_code=401, detail="Authentication required")
    return user

# From auth/dependencies.py
def require_roles(required_roles: List[str]):
    """
    Dependency factory to require specific roles.
    Example:
        @router.get("/admin-only")
        async def admin_endpoint(user: User = Depends(require_roles(["admin"]))):
            # Only users with admin role will get here
            return {"message": "Welcome, admin!"}
    """

    def _require_roles(user: User = Depends(require_authenticated)) -> User:
        """Require that the user has at least one of the specified roles."""
        user_roles = set(user.roles or [])
        if not any(role in user_roles for role in required_roles):
            raise ForbiddenException(f"This endpoint requires one of these roles: {', '.join(required_roles)}")
        return user

    return _require_roles

# From auth/dependencies.py
def require_admin(user: User = Depends(require_roles(["admin"]))) -> User:
    """Convenience dependency to require admin role."""
    return user


# From auth/wsauth.py
class WebSocketAuthHandler:
    """
    Helper class for authenticating WebSocket connections.
    """

    def __init__(self, auth_manager: AuthManager):
        self.auth_manager = auth_manager

    async def authenticate(self, websocket: WebSocket) -> tuple[bool, User | None]:
        """
        Authenticate a WebSocket connection.
        Returns (success, user) tuple.
        """
        if self.auth_manager.config.type == "none":
            # No authentication required
            return True, User(id="guestuser@gmail.com", name="Default User", provider="none")

        try:
            # Extract token from query params or headers query_params)
            token = None
            if "token" in websocket.query_params:
                token = websocket.query_params["token"]
            elif "authorization" in websocket.headers:
                auth_header = websocket.headers["authorization"]
                if auth_header.startswith("Bearer "):
                    token = auth_header.replace("Bearer ", "")

            if not token:
                logger.warning("No token found for WebSocket connection")
                return False, None

            # Validate token
            if not self.auth_manager.config.jwt_secret:
                # Development mode with no JWT secret
                return True, User(id="guestuser@gmail.com", name="Default User", provider="none")

            try:
                # Decode and validate JWT
                if not self.auth_manager.config.jwt_secret:
                    logger.warning("Invalid token for WebSocket connection")
                    return False, None
                payload = jwt.decode(token, self.auth_manager.config.jwt_secret, algorithms=["HS256"])

                # Create User object from token payload
                user = User(
                    id=payload.get("sub"),
                    name=payload.get("name", "Unknown User"),
                    email=payload.get("email"),
                    provider=payload.get("provider", "jwt"),
                    roles=payload.get("roles", ["user"]),
                )

                return True, user

            except jwt.ExpiredSignatureError:
                logger.warning("Expired token for WebSocket connection")
                return False, None
            except jwt.InvalidTokenError:
                logger.warning("Invalid token for WebSocket connection")
                return False, None

        except Exception as e:
            logger.error(f"WebSocket auth error: {str(e)}")
            return False, None

    async def on_connect(self, websocket: WebSocket) -> User | None:
        """
        Handle WebSocket connection with authentication.
        Returns authenticated user if successful, otherwise closes the connection.
        """
        success, user = await self.authenticate(websocket)

        if not success:
            # Authentication failed, close the connection
            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason="Authentication failed")
            raise WebSocketDisconnect(code=status.WS_1008_POLICY_VIOLATION)

        # Authentication successful, return the user
        return user

import traceback
from datetime import date
from datetime import time
from autogen_agentchat.messages import HandoffMessage
from autogen_agentchat.messages import ModelClientStreamingChunkEvent
from autogen_agentchat.messages import StopMessage
from autogen_agentchat.messages import ToolCallExecutionEvent
from autogen_agentchat.messages import ToolCallRequestEvent
from datamodel import LLMCallEventMessage
from datamodel import Message
from datamodel import MessageConfig
from datamodel import Run
from datamodel import RunStatus
from datamodel import Settings
from datamodel import SettingsConfig
from datamodel import TeamResult
from run_context import RunContext

# From managers/connection.py
class WebSocketManager:
    """Manages WebSocket connections and message streaming for team task execution"""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self._connections: Dict[int, WebSocket] = {}
        self._cancellation_tokens: Dict[int, CancellationToken] = {}
        # Track explicitly closed connections
        self._closed_connections: set[int] = set()
        self._input_responses: Dict[int, asyncio.Queue] = {}

        self._cancel_message = TeamResult(
            task_result=TaskResult(
                messages=[TextMessage(source="user", content="Run cancelled by user")], stop_reason="cancelled by user"
            ),
            usage="",
            duration=0,
        ).model_dump()

    def _get_stop_message(self, reason: str) -> dict:
        return TeamResult(
            task_result=TaskResult(messages=[TextMessage(source="user", content=reason)], stop_reason=reason),
            usage="",
            duration=0,
        ).model_dump()

    async def connect(self, websocket: WebSocket, run_id: int) -> bool:
        try:
            await websocket.accept()
            self._connections[run_id] = websocket
            self._closed_connections.discard(run_id)
            # Initialize input queue for this connection
            self._input_responses[run_id] = asyncio.Queue()

            await self._send_message(
                run_id, {"type": "system", "status": "connected", "timestamp": datetime.now(timezone.utc).isoformat()}
            )

            return True
        except Exception as e:
            logger.error(f"Connection error for run {run_id}: {e}")
            return False

    async def start_stream(
        self,
        run_id: int,
        task: str | ChatMessage | Sequence[ChatMessage] | None,
        team_config: str | Path | Dict[str, Any] | ComponentModel,
    ) -> None:
        """Start streaming task execution with proper run management"""
        if run_id not in self._connections or run_id in self._closed_connections:
            raise ValueError(f"No active connection for run {run_id}")
        with RunContext.populate_context(run_id=run_id):
            team_manager = TeamManager()
            cancellation_token = CancellationToken()
            self._cancellation_tokens[run_id] = cancellation_token
            final_result = None
            env_vars = None  # Ensure env_vars is always defined

            try:
                # Update run with task and status
                run = await self._get_run(run_id)

                if run is not None and run.user_id:
                    # get user Settings
                    user_settings = await self._get_settings(run.user_id)
                    env_vars = SettingsConfig(**user_settings.config).environment if user_settings else None  # type: ignore
                    run.task = self._convert_images_in_dict(MessageConfig(content=task, source="user").model_dump())
                    run.status = RunStatus.ACTIVE
                    self.db_manager.upsert(run)

                input_func = self.create_input_func(run_id)

                async for message in team_manager.run_stream(
                    task=task,
                    team_config=team_config,
                    input_func=input_func,
                    cancellation_token=cancellation_token,
                    env_vars=env_vars,
                ):
                    if cancellation_token.is_cancelled() or run_id in self._closed_connections:
                        logger.info(f"Stream cancelled or connection closed for run {run_id}")
                        break

                    formatted_message = self._format_message(message)
                    if formatted_message:
                        await self._send_message(run_id, formatted_message)

                        # Save messages by concrete type
                        if isinstance(
                            message,
                            (
                                TextMessage,
                                MultiModalMessage,
                                StopMessage,
                                HandoffMessage,
                                ToolCallRequestEvent,
                                ToolCallExecutionEvent,
                                LLMCallEventMessage,
                            ),
                        ):
                            await self._save_message(run_id, message)
                        # Capture final result if it's a TeamResult
                        elif isinstance(message, TeamResult):
                            final_result = message.model_dump()
                if not cancellation_token.is_cancelled() and run_id not in self._closed_connections:
                    if final_result:
                        await self._update_run(run_id, RunStatus.COMPLETE, team_result=final_result)
                    else:
                        logger.warning(f"No final result captured for completed run {run_id}")
                        await self._update_run_status(run_id, RunStatus.COMPLETE)
                else:
                    await self._send_message(
                        run_id,
                        {
                            "type": "completion",
                            "status": "cancelled",
                            "data": self._cancel_message,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        },
                    )
                    # Update run with cancellation result
                    await self._update_run(run_id, RunStatus.STOPPED, team_result=self._cancel_message)

            except Exception as e:
                logger.error(f"Stream error for run {run_id}: {e}")
                traceback.print_exc()
                await self._handle_stream_error(run_id, e)
            finally:
                self._cancellation_tokens.pop(run_id, None)

    async def _save_message(
        self, run_id: int, message: Union[BaseAgentEvent | BaseChatMessage, BaseChatMessage]
    ) -> None:
        """Save a message to the database"""

        run = await self._get_run(run_id)
        if run:
            db_message = Message(
                session_id=run.session_id,
                run_id=run_id,
                config=self._convert_images_in_dict(message.model_dump()),
                user_id=None,  # You might want to pass this from somewhere
            )
            self.db_manager.upsert(db_message)

    async def _update_run(
        self, run_id: int, status: RunStatus, team_result: Optional[dict] = None, error: Optional[str] = None
    ) -> None:
        """Update run status and result"""
        run = await self._get_run(run_id)
        if run:
            run.status = status
            if team_result:
                run.team_result = self._convert_images_in_dict(team_result)
            if error:
                run.error_message = error
            self.db_manager.upsert(run)

    def create_input_func(self, run_id: int) -> Callable:
        """Creates an input function for a specific run"""

        async def input_handler(prompt: str = "", cancellation_token: Optional[CancellationToken] = None) -> str:
            try:
                # Send input request to client
                await self._send_message(
                    run_id,
                    {
                        "type": "input_request",
                        "prompt": prompt,
                        "data": {"source": "system", "content": prompt},
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    },
                )

                # Wait for response
                if run_id in self._input_responses:
                    response = await self._input_responses[run_id].get()
                    return response
                else:
                    raise ValueError(f"No input queue for run {run_id}")

            except Exception as e:
                logger.error(f"Error handling input for run {run_id}: {e}")
                raise

        return input_handler

    async def handle_input_response(self, run_id: int, response: str) -> None:
        """Handle input response from client"""
        if run_id in self._input_responses:
            await self._input_responses[run_id].put(response)
        else:
            logger.warning(f"Received input response for inactive run {run_id}")

    async def stop_run(self, run_id: int, reason: str) -> None:
        if run_id in self._cancellation_tokens:
            logger.info(f"Stopping run {run_id}")

            stop_message = self._get_stop_message(reason)

            try:
                # Update run record first
                await self._update_run(run_id, status=RunStatus.STOPPED, team_result=stop_message)

                # Then handle websocket communication if connection is active
                if run_id in self._connections and run_id not in self._closed_connections:
                    await self._send_message(
                        run_id,
                        {
                            "type": "completion",
                            "status": "cancelled",
                            "data": stop_message,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        },
                    )

                # Finally cancel the token
                self._cancellation_tokens[run_id].cancel()

            except Exception as e:
                logger.error(f"Error stopping run {run_id}: {e}")
                # We might want to force disconnect here if db update failed
                # await self.disconnect(run_id)  # Optional

    async def disconnect(self, run_id: int) -> None:
        """Clean up connection and associated resources"""
        logger.info(f"Disconnecting run {run_id}")

        # Mark as closed before cleanup to prevent any new messages
        self._closed_connections.add(run_id)

        # Cancel any running tasks
        await self.stop_run(run_id, "Connection closed")

        # Clean up resources
        self._connections.pop(run_id, None)
        self._cancellation_tokens.pop(run_id, None)
        self._input_responses.pop(run_id, None)

    def _convert_images_in_dict(self, obj: Any) -> Any:
        """Recursively find and convert Image and datetime objects in dictionaries and lists"""
        if isinstance(obj, dict):
            return {k: self._convert_images_in_dict(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_images_in_dict(item) for item in obj]
        elif isinstance(obj, AGImage):
            return {"type": "image", "url": f"data:image/png;base64,{obj.to_base64()}", "alt": "Image"}
        elif isinstance(obj, (datetime, date, time)):
            return obj.isoformat()
        else:
            return obj

    async def _send_message(self, run_id: int, message: dict) -> None:
        """Send a message through the WebSocket with connection state checking

        Args:
            run_id: id of the run
            message: Message dictionary to send
        """
        if run_id in self._closed_connections:
            logger.warning(f"Attempted to send message to closed connection for run {run_id}")
            return

        try:
            if run_id in self._connections:
                websocket = self._connections[run_id]
                await websocket.send_json(self._convert_images_in_dict(message))
        except WebSocketDisconnect:
            logger.warning(f"WebSocket disconnected while sending message for run {run_id}")
            await self.disconnect(run_id)
        except Exception as e:
            traceback.print_exc()
            logger.error(f"Error sending message for run {run_id}: {e}, {message}")
            # Don't try to send error message here to avoid potential recursive loop
            await self._update_run_status(run_id, RunStatus.ERROR, str(e))
            await self.disconnect(run_id)

    async def _handle_stream_error(self, run_id: int, error: Exception) -> None:
        """Handle stream errors with proper run updates"""
        if run_id not in self._closed_connections:
            error_result = TeamResult(
                task_result=TaskResult(
                    messages=[TextMessage(source="system", content=str(error))],
                    stop_reason="An error occurred while processing this run",
                ),
                usage="",
                duration=0,
            ).model_dump()

            await self._send_message(
                run_id,
                {
                    "type": "completion",
                    "status": "error",
                    "data": error_result,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                },
            )

            await self._update_run(run_id, RunStatus.ERROR, team_result=error_result, error=str(error))

    def _format_message(self, message: Any) -> Optional[dict]:
        """Format message for WebSocket transmission

        Args:
            message: Message to format

        Returns:
            Optional[dict]: Formatted message or None if formatting fails
        """

        try:
            if isinstance(message, MultiModalMessage):
                message_dump = message.model_dump()

                message_content = []
                for row in message_dump["content"]:
                    if isinstance(row, dict) and "data" in row:
                        message_content.append(
                            {
                                "url": f"data:image/png;base64,{row['data']}",
                                "alt": "WebSurfer Screenshot",
                            }
                        )
                    else:
                        message_content.append(row)
                message_dump["content"] = message_content

                return {"type": "message", "data": message_dump}

            elif isinstance(message, TeamResult):
                return {
                    "type": "result",
                    "data": message.model_dump(),
                    "status": "complete",
                }
            elif isinstance(message, ModelClientStreamingChunkEvent):
                return {"type": "message_chunk", "data": message.model_dump()}

            elif isinstance(
                message,
                (
                    TextMessage,
                    StopMessage,
                    HandoffMessage,
                    ToolCallRequestEvent,
                    ToolCallExecutionEvent,
                    LLMCallEventMessage,
                ),
            ):
                return {"type": "message", "data": message.model_dump()}

            return None

        except Exception as e:
            logger.error(f"Message formatting error: {e}")
            traceback.print_exc()
            return None

    async def _get_run(self, run_id: int) -> Optional[Run]:
        """Get run from database

        Args:
            run_id: id of the run to retrieve

        Returns:
            Optional[Run]: Run object if found, None otherwise
        """
        response = self.db_manager.get(Run, filters={"id": run_id}, return_json=False)
        return response.data[0] if response.status and response.data else None

    async def _get_settings(self, user_id: str) -> Optional[Settings]:
        """Get user settings from database
        Args:
            user_id: User ID to retrieve settings for
        Returns:
            Optional[dict]: User settings if found, None otherwise
        """
        response = self.db_manager.get(filters={"user_id": user_id}, model_class=Settings, return_json=False)
        return response.data[0] if response.status and response.data else None

    async def _update_run_status(self, run_id: int, status: RunStatus, error: Optional[str] = None) -> None:
        """Update run status in database

        Args:
            run_id: id of the run to update
            status: New status to set
            error: Optional error message
        """
        run = await self._get_run(run_id)
        if run:
            run.status = status
            run.error_message = error
            self.db_manager.upsert(run)

    async def cleanup(self) -> None:
        """Clean up all active connections and resources when server is shutting down"""
        logger.info(f"Cleaning up {len(self.active_connections)} active connections")

        try:
            # First cancel all running tasks
            for run_id in self.active_runs.copy():
                if run_id in self._cancellation_tokens:
                    self._cancellation_tokens[run_id].cancel()
                run = await self._get_run(run_id)
                if run and run.status == RunStatus.ACTIVE:
                    interrupted_result = TeamResult(
                        task_result=TaskResult(
                            messages=[TextMessage(source="system", content="Run interrupted by server shutdown")],
                            stop_reason="server_shutdown",
                        ),
                        usage="",
                        duration=0,
                    ).model_dump()

                    run.status = RunStatus.STOPPED
                    run.team_result = interrupted_result
                    self.db_manager.upsert(run)

            # Then disconnect all websockets with timeout
            # 10 second timeout for entire cleanup
            async with asyncio.timeout(10):
                for run_id in self.active_connections.copy():
                    try:
                        # Give each disconnect operation 2 seconds
                        async with asyncio.timeout(2):
                            await self.disconnect(run_id)
                    except asyncio.TimeoutError:
                        logger.warning(f"Timeout disconnecting run {run_id}")
                    except Exception as e:
                        logger.error(f"Error disconnecting run {run_id}: {e}")

        except asyncio.TimeoutError:
            logger.warning("WebSocketManager cleanup timed out")
        except Exception as e:
            logger.error(f"Error during WebSocketManager cleanup: {e}")
        finally:
            # Always clear internal state, even if cleanup had errors
            self._connections.clear()
            self._cancellation_tokens.clear()
            self._closed_connections.clear()
            self._input_responses.clear()

    @property
    def active_connections(self) -> set[int]:
        """Get set of active run IDs"""
        return set(self._connections.keys()) - self._closed_connections

    @property
    def active_runs(self) -> set[int]:
        """Get set of runs with active cancellation tokens"""
        return set(self._cancellation_tokens.keys())

# From managers/connection.py
def create_input_func(self, run_id: int) -> Callable:
        """Creates an input function for a specific run"""

        async def input_handler(prompt: str = "", cancellation_token: Optional[CancellationToken] = None) -> str:
            try:
                # Send input request to client
                await self._send_message(
                    run_id,
                    {
                        "type": "input_request",
                        "prompt": prompt,
                        "data": {"source": "system", "content": prompt},
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    },
                )

                # Wait for response
                if run_id in self._input_responses:
                    response = await self._input_responses[run_id].get()
                    return response
                else:
                    raise ValueError(f"No input queue for run {run_id}")

            except Exception as e:
                logger.error(f"Error handling input for run {run_id}: {e}")
                raise

        return input_handler

# From managers/connection.py
def active_connections(self) -> set[int]:
        """Get set of active run IDs"""
        return set(self._connections.keys()) - self._closed_connections

# From managers/connection.py
def active_runs(self) -> set[int]:
        """Get set of runs with active cancellation tokens"""
        return set(self._cancellation_tokens.keys())

from contextvars import ContextVar
from typing import ClassVar
from typing import Generator

# From managers/run_context.py
class RunContext:
    RUN_CONTEXT_VAR: ClassVar[ContextVar] = ContextVar("RUN_CONTEXT_VAR")

    @classmethod
    @contextmanager
    def populate_context(cls, run_id) -> Generator[None, Any, None]:
        token = RunContext.RUN_CONTEXT_VAR.set(run_id)
        try:
            yield
        finally:
            RunContext.RUN_CONTEXT_VAR.reset(token)

    @classmethod
    def current_run_id(cls) -> str:
        try:
            return cls.RUN_CONTEXT_VAR.get()
        except LookupError as e:
            raise RuntimeError("Error getting run id") from e

# From managers/run_context.py
def populate_context(cls, run_id) -> Generator[None, Any, None]:
        token = RunContext.RUN_CONTEXT_VAR.set(run_id)
        try:
            yield
        finally:
            RunContext.RUN_CONTEXT_VAR.reset(token)

# From managers/run_context.py
def current_run_id(cls) -> str:
        try:
            return cls.RUN_CONTEXT_VAR.get()
        except LookupError as e:
            raise RuntimeError("Error getting run id") from e

from gallery.builder import create_default_gallery
from deps import get_db

from datamodel import Gallery

from datamodel import Session

# From routes/runs.py
class CreateRunRequest(BaseModel):
    session_id: int
    user_id: str


from validation.component_test_service import ComponentTestRequest
from validation.component_test_service import ComponentTestResult
from validation.component_test_service import ComponentTestService
from validation.validation_service import ValidationError
from validation.validation_service import ValidationRequest
from validation.validation_service import ValidationResponse
from validation.validation_service import ValidationService

from autogen_ext.tools.mcp._config import McpServerParams
from autogen_ext.tools.mcp._config import SseServerParams
from autogen_ext.tools.mcp._config import StdioServerParams
from autogen_ext.tools.mcp._config import StreamableHttpServerParams
from mcp import StdioServerParameters
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from mcp.client.streamable_http import streamablehttp_client
from mcp.callbacks import create_elicitation_callback
from mcp.callbacks import create_message_handler
from mcp.callbacks import create_sampling_callback
from mcp.client import MCPClient
from mcp.utils import extract_real_error
from mcp.utils import is_websocket_disconnect
from mcp.utils import serialize_for_json
from mcp.wsbridge import MCPWebSocketBridge

# From routes/mcp.py
class CreateWebSocketConnectionRequest(BaseModel):
    server_params: McpServerParams


from utils.utils import construct_task
from auth.dependencies import get_ws_auth_manager
from auth.wsauth import WebSocketAuthHandler
from deps import get_websocket_manager

from autogen_agentchat.ui import Console
from autogen_agentchat.ui import UserInputManager
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_ext.ui import RichConsole

import inspect
import typing
from functools import partial
from logging import getLogger
from typing import Annotated
from typing import Set
from typing import TypeVar
from typing import cast
from typing import get_args
from typing import get_origin
from pydantic import TypeAdapter
from pydantic import create_model
from pydantic_core import PydanticUndefined
from typing_extensions import Literal

# From autogen_core/_function_utils.py
class Parameters(BaseModel):
    """Parameters of a function as defined by the OpenAI API"""

    type: Literal["object"] = "object"
    properties: Dict[str, Dict[str, Any]]
    required: List[str]

# From autogen_core/_function_utils.py
class Function(BaseModel):
    """A function as defined by the OpenAI API"""

    description: Annotated[str, Field(description="Description of the function")]
    name: Annotated[str, Field(description="Name of the function")]
    parameters: Annotated[Parameters, Field(description="Parameters of the function")]

# From autogen_core/_function_utils.py
class ToolFunction(BaseModel):
    """A function under tool as defined by the OpenAI API."""

    type: Literal["function"] = "function"
    function: Annotated[Function, Field(description="Function under tool")]

# From autogen_core/_function_utils.py
def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:
    """Get the signature of a function with type annotations.

    Args:
        call: The function to get the signature for

    Returns:
        The signature of the function with type annotations
    """
    signature = inspect.signature(call)
    globalns = getattr(call, "__globals__", {})
    func_call = call.func if isinstance(call, partial) else call
    type_hints = typing.get_type_hints(func_call, globalns, include_extras=True)
    typed_params = [
        inspect.Parameter(
            name=param.name,
            kind=param.kind,
            default=param.default,
            annotation=type_hints[param.name],
        )
        for param in signature.parameters.values()
    ]
    return_annotation = type_hints.get("return", inspect.Signature.empty)
    typed_signature = inspect.Signature(typed_params, return_annotation=return_annotation)
    return typed_signature

# From autogen_core/_function_utils.py
def get_typed_return_annotation(call: Callable[..., Any]) -> Any:
    """Get the return annotation of a function.

    Args:
        call: The function to get the return annotation for

    Returns:
        The return annotation of the function
    """
    signature = inspect.signature(call)
    annotation = signature.return_annotation

    if annotation is inspect.Signature.empty:
        return None

    globalns = getattr(call, "__globals__", {})
    type_hints = typing.get_type_hints(call, globalns, include_extras=True)
    return type_hints.get("return", inspect.Signature.empty)

# From autogen_core/_function_utils.py
def get_param_annotations(
    typed_signature: inspect.Signature,
) -> Dict[str, Union[Annotated[Type[Any], str], Type[Any]]]:
    """Get the type annotations of the parameters of a function

    Args:
        typed_signature: The signature of the function with type annotations

    Returns:
        A dictionary of the type annotations of the parameters of the function
    """
    return {
        k: v.annotation for k, v in typed_signature.parameters.items() if v.annotation is not inspect.Signature.empty
    }

# From autogen_core/_function_utils.py
def type2description(k: str, v: Union[Annotated[Type[Any], str], Type[Any]]) -> str:
    # handles Annotated
    if hasattr(v, "__metadata__"):
        retval = v.__metadata__[0]
        if isinstance(retval, str):
            return retval
        else:
            raise ValueError(f"Invalid description {retval} for parameter {k}, should be a string.")
    else:
        return k

# From autogen_core/_function_utils.py
def get_parameter_json_schema(k: str, v: Any, default_values: Dict[str, Any]) -> Dict[str, Any]:
    """Get a JSON schema for a parameter as defined by the OpenAI API

    Args:
        k: The name of the parameter
        v: The type of the parameter
        default_values: The default values of the parameters of the function

    Returns:
        A Pydanitc model for the parameter
    """

    schema = TypeAdapter(v).json_schema()
    if k in default_values:
        dv = default_values[k]
        schema["default"] = dv

    schema["description"] = type2description(k, v)

    return schema

# From autogen_core/_function_utils.py
def get_required_params(typed_signature: inspect.Signature) -> List[str]:
    """Get the required parameters of a function

    Args:
        typed_signature: The signature of the function as returned by inspect.signature

    Returns:
        A list of the required parameters of the function
    """
    return [k for k, v in typed_signature.parameters.items() if v.default == inspect.Signature.empty]

# From autogen_core/_function_utils.py
def get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any]:
    """Get default values of parameters of a function

    Args:
        typed_signature: The signature of the function as returned by inspect.signature

    Returns:
        A dictionary of the default values of the parameters of the function
    """
    return {k: v.default for k, v in typed_signature.parameters.items() if v.default != inspect.Signature.empty}

# From autogen_core/_function_utils.py
def get_parameters(
    required: List[str],
    param_annotations: Dict[str, Union[Annotated[Type[Any], str], Type[Any]]],
    default_values: Dict[str, Any],
) -> Parameters:
    """Get the parameters of a function as defined by the OpenAI API

    Args:
        required: The required parameters of the function
        param_annotations: A dictionary of the type annotations of the parameters of the function
        default_values: The default values of the parameters of the function

    Returns:
        A Pydantic model for the parameters of the function
    """
    return Parameters(
        properties={
            k: get_parameter_json_schema(k, v, default_values)
            for k, v in param_annotations.items()
            if v is not inspect.Signature.empty
        },
        required=required,
    )

# From autogen_core/_function_utils.py
def get_missing_annotations(typed_signature: inspect.Signature, required: List[str]) -> Tuple[Set[str], Set[str]]:
    """Get the missing annotations of a function

    Ignores the parameters with default values as they are not required to be annotated, but logs a warning.
    Args:
        typed_signature: The signature of the function with type annotations
        required: The required parameters of the function

    Returns:
        A set of the missing annotations of the function
    """
    all_missing = {k for k, v in typed_signature.parameters.items() if v.annotation is inspect.Signature.empty}
    missing = all_missing.intersection(set(required))
    unannotated_with_default = all_missing.difference(missing)
    return missing, unannotated_with_default

# From autogen_core/_function_utils.py
def get_function_schema(f: Callable[..., Any], *, name: Optional[str] = None, description: str) -> Dict[str, Any]:
    """Get a JSON schema for a function as defined by the OpenAI API

    Args:
        f: The function to get the JSON schema for
        name: The name of the function
        description: The description of the function

    Returns:
        A JSON schema for the function

    Raises:
        TypeError: If the function is not annotated

    Examples:

        .. code-block:: python

            def f(
                a: Annotated[str, "Parameter a"],
                b: int = 2,
                c: Annotated[float, "Parameter c"] = 0.1,
            ) -> None:
                pass


            get_function_schema(f, description="function f")

            #   {'type': 'function',
            #    'function': {'description': 'function f',
            #        'name': 'f',
            #        'parameters': {'type': 'object',
            #           'properties': {'a': {'type': 'str', 'description': 'Parameter a'},
            #               'b': {'type': 'int', 'description': 'b'},
            #               'c': {'type': 'float', 'description': 'Parameter c'}},
            #           'required': ['a']}}}

    """
    typed_signature = get_typed_signature(f)
    required = get_required_params(typed_signature)
    default_values = get_default_values(typed_signature)
    param_annotations = get_param_annotations(typed_signature)
    return_annotation = get_typed_return_annotation(f)
    missing, unannotated_with_default = get_missing_annotations(typed_signature, required)

    if return_annotation is None:
        logger.warning(
            f"The return type of the function '{f.__name__}' is not annotated. Although annotating it is "
            + "optional, the function should return either a string, a subclass of 'pydantic.BaseModel'."
        )

    if unannotated_with_default != set():
        unannotated_with_default_s = [f"'{k}'" for k in sorted(unannotated_with_default)]
        logger.warning(
            f"The following parameters of the function '{f.__name__}' with default values are not annotated: "
            + f"{', '.join(unannotated_with_default_s)}."
        )

    if missing != set():
        missing_s = [f"'{k}'" for k in sorted(missing)]
        raise TypeError(
            f"All parameters of the function '{f.__name__}' without default values must be annotated. "
            + f"The annotations are missing for the following parameters: {', '.join(missing_s)}"
        )

    fname = name if name else f.__name__

    parameters = get_parameters(required, param_annotations, default_values=default_values)

    function = ToolFunction(
        function=Function(
            description=description,
            name=fname,
            parameters=parameters,
        )
    )

    return function.model_dump()

# From autogen_core/_function_utils.py
def normalize_annotated_type(type_hint: Type[Any]) -> Type[Any]:
    """Normalize typing.Annotated types to the inner type."""
    if get_origin(type_hint) is Annotated:
        # Extract the inner type from Annotated
        return get_args(type_hint)[0]  # type: ignore
    return type_hint

# From autogen_core/_function_utils.py
def args_base_model_from_signature(name: str, sig: inspect.Signature) -> Type[BaseModel]:
    fields: Dict[str, tuple[Type[Any], Any]] = {}
    for param_name, param in sig.parameters.items():
        # This is handled externally
        if param_name == "cancellation_token":
            continue

        if param.annotation is inspect.Parameter.empty:
            raise ValueError("No annotation")

        type = normalize_annotated_type(param.annotation)
        description = type2description(param_name, param.annotation)
        default_value = param.default if param.default is not inspect.Parameter.empty else PydanticUndefined

        fields[param_name] = (type, Field(default=default_value, description=description))

    return cast(BaseModel, create_model(name, **fields))

from dataclasses import asdict
from dataclasses import dataclass
from dataclasses import fields
from typing import runtime_checkable
from google.protobuf import any_pb2
from google.protobuf.message import Message
from _type_helpers import is_union

# From autogen_core/_serialization.py
class MessageSerializer(Protocol[T]):
    @property
    def data_content_type(self) -> str: ...

    @property
    def type_name(self) -> str: ...

    def deserialize(self, payload: bytes) -> T: ...

    def serialize(self, message: T) -> bytes: ...

# From autogen_core/_serialization.py
class IsDataclass(Protocol):
    # as already noted in comments, checking for this attribute is currently
    # the most reliable way to ascertain that something is a dataclass
    __dataclass_fields__: ClassVar[Dict[str, Any]]

# From autogen_core/_serialization.py
class DataclassJsonMessageSerializer(MessageSerializer[DataclassT]):
    def __init__(self, cls: type[DataclassT]) -> None:
        if contains_a_union(cls):
            raise ValueError("Dataclass has a union type, which is not supported. To use a union, use a Pydantic model")

        if has_nested_dataclass(cls) or has_nested_base_model(cls):
            raise ValueError(
                "Dataclass has nested dataclasses or base models, which are not supported. To use nested types, use a Pydantic model"
            )

        self.cls = cls

    @property
    def data_content_type(self) -> str:
        return JSON_DATA_CONTENT_TYPE

    @property
    def type_name(self) -> str:
        return _type_name(self.cls)

    def deserialize(self, payload: bytes) -> DataclassT:
        message_str = payload.decode("utf-8")
        return self.cls(**json.loads(message_str))

    def serialize(self, message: DataclassT) -> bytes:
        return json.dumps(asdict(message)).encode("utf-8")

# From autogen_core/_serialization.py
class PydanticJsonMessageSerializer(MessageSerializer[PydanticT]):
    def __init__(self, cls: type[PydanticT]) -> None:
        self.cls = cls

    @property
    def data_content_type(self) -> str:
        return JSON_DATA_CONTENT_TYPE

    @property
    def type_name(self) -> str:
        return _type_name(self.cls)

    def deserialize(self, payload: bytes) -> PydanticT:
        message_str = payload.decode("utf-8")
        return self.cls.model_validate_json(message_str)

    def serialize(self, message: PydanticT) -> bytes:
        return message.model_dump_json().encode("utf-8")

# From autogen_core/_serialization.py
class ProtobufMessageSerializer(MessageSerializer[ProtobufT]):
    def __init__(self, cls: type[ProtobufT]) -> None:
        self.cls = cls

    @property
    def data_content_type(self) -> str:
        return PROTOBUF_DATA_CONTENT_TYPE

    @property
    def type_name(self) -> str:
        return _type_name(self.cls)

    def deserialize(self, payload: bytes) -> ProtobufT:
        # Parse payload into a proto any
        any_proto = any_pb2.Any()
        any_proto.ParseFromString(payload)

        destination_message = self.cls()

        if not any_proto.Unpack(destination_message):  # type: ignore
            raise ValueError(f"Failed to unpack payload into {self.cls}")

        return destination_message

    def serialize(self, message: ProtobufT) -> bytes:
        any_proto = any_pb2.Any()
        any_proto.Pack(message)  # type: ignore
        return any_proto.SerializeToString()

# From autogen_core/_serialization.py
class UnknownPayload:
    type_name: str
    data_content_type: str
    payload: bytes

# From autogen_core/_serialization.py
class SerializationRegistry:
    """:meta private:"""

    def __init__(self) -> None:
        # type_name, data_content_type -> serializer
        self._serializers: dict[tuple[str, str], MessageSerializer[Any]] = {}

    def add_serializer(self, serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None:
        if isinstance(serializer, Sequence):
            for c in serializer:
                self.add_serializer(c)
            return

        self._serializers[(serializer.type_name, serializer.data_content_type)] = serializer

    def deserialize(self, payload: bytes, *, type_name: str, data_content_type: str) -> Any:
        serializer = self._serializers.get((type_name, data_content_type))
        if serializer is None:
            return UnknownPayload(type_name, data_content_type, payload)

        return serializer.deserialize(payload)

    def serialize(self, message: Any, *, type_name: str, data_content_type: str) -> bytes:
        serializer = self._serializers.get((type_name, data_content_type))
        if serializer is None:
            raise ValueError(f"Unknown type {type_name} with content type {data_content_type}")

        return serializer.serialize(message)

    def is_registered(self, type_name: str, data_content_type: str) -> bool:
        return (type_name, data_content_type) in self._serializers

    def type_name(self, message: Any) -> str:
        return _type_name(message)

# From autogen_core/_serialization.py
def is_dataclass(cls: type[Any]) -> bool:
    return hasattr(cls, "__dataclass_fields__")

# From autogen_core/_serialization.py
def has_nested_dataclass(cls: type[IsDataclass]) -> bool:
    # iterate fields and check if any of them are dataclasses
    return any(is_dataclass(f.type) for f in cls.__dataclass_fields__.values())

# From autogen_core/_serialization.py
def contains_a_union(cls: type[IsDataclass]) -> bool:
    return any(is_union(f.type) for f in cls.__dataclass_fields__.values())

# From autogen_core/_serialization.py
def has_nested_base_model(cls: type[IsDataclass]) -> bool:
    for f in fields(cls):
        field_type = f.type
        # Resolve forward references and other annotations
        origin = get_origin(field_type)
        args = get_args(field_type)

        # If the field type is directly a subclass of BaseModel
        if isinstance(field_type, type) and issubclass(field_type, BaseModel):
            return True

        # If the field type is a generic type like List[BaseModel], Tuple[BaseModel, ...], etc.
        if origin is not None and args:
            for arg in args:
                # Recursively check the argument types
                if isinstance(arg, type) and issubclass(arg, BaseModel):
                    return True
                elif get_origin(arg) is not None:
                    # Handle nested generics like List[List[BaseModel]]
                    if has_nested_base_model_in_type(arg):
                        return True
        # Handle Union types
        elif args:
            for arg in args:
                if isinstance(arg, type) and issubclass(arg, BaseModel):
                    return True
                elif get_origin(arg) is not None:
                    if has_nested_base_model_in_type(arg):
                        return True
    return False

# From autogen_core/_serialization.py
def has_nested_base_model_in_type(tp: Any) -> bool:
    """Helper function to check if a type or its arguments is a BaseModel subclass."""
    origin = get_origin(tp)
    args = get_args(tp)

    if isinstance(tp, type) and issubclass(tp, BaseModel):
        return True
    if origin is not None and args:
        for arg in args:
            if has_nested_base_model_in_type(arg):
                return True
    return False

# From autogen_core/_serialization.py
def try_get_known_serializers_for_type(cls: type[Any]) -> list[MessageSerializer[Any]]:
    """:meta private:"""

    serializers: List[MessageSerializer[Any]] = []
    if issubclass(cls, BaseModel):
        serializers.append(PydanticJsonMessageSerializer(cls))
    elif is_dataclass(cls):
        serializers.append(DataclassJsonMessageSerializer(cls))
    elif issubclass(cls, Message):
        serializers.append(ProtobufMessageSerializer(cls))

    return serializers

# From autogen_core/_serialization.py
def data_content_type(self) -> str: ...

# From autogen_core/_serialization.py
def type_name(self) -> str: ...

# From autogen_core/_serialization.py
def deserialize(self, payload: bytes) -> T: ...

# From autogen_core/_serialization.py
def serialize(self, message: T) -> bytes: ...

# From autogen_core/_serialization.py
def add_serializer(self, serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None:
        if isinstance(serializer, Sequence):
            for c in serializer:
                self.add_serializer(c)
            return

        self._serializers[(serializer.type_name, serializer.data_content_type)] = serializer

# From autogen_core/_serialization.py
def is_registered(self, type_name: str, data_content_type: str) -> bool:
        return (type_name, data_content_type) in self._serializers

from collections.abc import Sequence
from types import NoneType
from types import UnionType

# From autogen_core/_type_helpers.py
class AnyType:
    pass

# From autogen_core/_type_helpers.py
def is_union(t: object) -> bool:
    origin = get_origin(t)
    return origin is Union or origin is UnionType

# From autogen_core/_type_helpers.py
def is_optional(t: object) -> bool:
    origin = get_origin(t)
    return origin is Optional

# From autogen_core/_type_helpers.py
def get_types(t: object) -> Sequence[Type[Any]] | None:
    if is_union(t):
        return get_args(t)
    elif is_optional(t):
        return tuple(list(get_args(t)) + [NoneType])
    elif t is Any:
        return (AnyType,)
    elif isinstance(t, type):
        return (t,)
    elif isinstance(t, NoneType):
        return (NoneType,)
    else:
        return None

from typing import final
from _agent_id import AgentId
from _message_context import MessageContext

# From autogen_core/_intervention.py
class DropMessage:
    """Marker type for signalling that a message should be dropped by an intervention handler. The type itself should be returned from the handler."""

    ...

# From autogen_core/_intervention.py
class InterventionHandler(Protocol):
    """An intervention handler is a class that can be used to modify, log or drop messages that are being processed by the :class:`autogen_core.base.AgentRuntime`.

    The handler is called when the message is submitted to the runtime.

    Currently the only runtime which supports this is the :class:`autogen_core.base.SingleThreadedAgentRuntime`.

    Note: Returning None from any of the intervention handler methods will result in a warning being issued and treated as "no change". If you intend to drop a message, you should return :class:`DropMessage` explicitly.

    Example:

    .. code-block:: python

        from autogen_core import DefaultInterventionHandler, MessageContext, AgentId, SingleThreadedAgentRuntime
        from dataclasses import dataclass
        from typing import Any


        @dataclass
        class MyMessage:
            content: str


        class MyInterventionHandler(DefaultInterventionHandler):
            async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
                if isinstance(message, MyMessage):
                    message.content = message.content.upper()
                return message


        runtime = SingleThreadedAgentRuntime(intervention_handlers=[MyInterventionHandler()])

    """

    async def on_send(
        self, message: Any, *, message_context: MessageContext, recipient: AgentId
    ) -> Any | type[DropMessage]:
        """Called when a message is submitted to the AgentRuntime using :meth:`autogen_core.base.AgentRuntime.send_message`."""
        ...

    async def on_publish(self, message: Any, *, message_context: MessageContext) -> Any | type[DropMessage]:
        """Called when a message is published to the AgentRuntime using :meth:`autogen_core.base.AgentRuntime.publish_message`."""
        ...

    async def on_response(self, message: Any, *, sender: AgentId, recipient: AgentId | None) -> Any | type[DropMessage]:
        """Called when a response is received by the AgentRuntime from an Agent's message handler returning a value."""
        ...

# From autogen_core/_intervention.py
class DefaultInterventionHandler(InterventionHandler):
    """Simple class that provides a default implementation for all intervention
    handler methods, that simply returns the message unchanged. Allows for easy
    subclassing to override only the desired methods."""

    async def on_send(
        self, message: Any, *, message_context: MessageContext, recipient: AgentId
    ) -> Any | type[DropMessage]:
        return message

    async def on_publish(self, message: Any, *, message_context: MessageContext) -> Any | type[DropMessage]:
        return message

    async def on_response(self, message: Any, *, sender: AgentId, recipient: AgentId | None) -> Any | type[DropMessage]:
        return message

from __future__ import annotations
from typing import Generic
from typing import TypeGuard
from typing import overload
from typing_extensions import TypeVar

# From autogen_core/_component_config.py
class ComponentModel(BaseModel):
    """Model class for a component. Contains all information required to instantiate a component."""

    provider: str
    """Describes how the component can be instantiated."""

    component_type: ComponentType | None = None
    """Logical type of the component. If missing, the component assumes the default type of the provider."""

    version: int | None = None
    """Version of the component specification. If missing, the component assumes whatever is the current version of the library used to load it. This is obviously dangerous and should be used for user authored ephmeral config. For all other configs version should be specified."""

    component_version: int | None = None
    """Version of the component. If missing, the component assumes the default version of the provider."""

    description: str | None = None
    """Description of the component."""

    label: str | None = None
    """Human readable label for the component. If missing the component assumes the class name of the provider."""

    config: dict[str, Any]
    """The schema validated config field is passed to a given class's implmentation of :py:meth:`autogen_core.ComponentConfigImpl._from_config` to create a new instance of the component class."""

# From autogen_core/_component_config.py
class ComponentFromConfig(Generic[FromConfigT]):
    @classmethod
    def _from_config(cls, config: FromConfigT) -> Self:
        """Create a new instance of the component from a configuration object.

        Args:
            config (T): The configuration object.

        Returns:
            Self: The new instance of the component.

        :meta public:
        """
        raise NotImplementedError("This component does not support dumping to config")

    @classmethod
    def _from_config_past_version(cls, config: Dict[str, Any], version: int) -> Self:
        """Create a new instance of the component from a previous version of the configuration object.

        This is only called when the version of the configuration object is less than the current version, since in this case the schema is not known.

        Args:
            config (Dict[str, Any]): The configuration object.
            version (int): The version of the configuration object.

        Returns:
            Self: The new instance of the component.

        :meta public:
        """
        raise NotImplementedError("This component does not support loading from past versions")

# From autogen_core/_component_config.py
class ComponentToConfig(Generic[ToConfigT]):
    """The two methods a class must implement to be a component.

    Args:
        Protocol (ConfigT): Type which derives from :py:class:`pydantic.BaseModel`.
    """

    component_type: ClassVar[ComponentType]
    """The logical type of the component."""
    component_version: ClassVar[int] = 1
    """The version of the component, if schema incompatibilities are introduced this should be updated."""
    component_provider_override: ClassVar[str | None] = None
    """Override the provider string for the component. This should be used to prevent internal module names being a part of the module name."""
    component_description: ClassVar[str | None] = None
    """A description of the component. If not provided, the docstring of the class will be used."""
    component_label: ClassVar[str | None] = None
    """A human readable label for the component. If not provided, the component class name will be used."""

    def _to_config(self) -> ToConfigT:
        """Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

        Returns:
            T: The configuration of the component.

        :meta public:
        """
        raise NotImplementedError("This component does not support dumping to config")

    def dump_component(self) -> ComponentModel:
        """Dump the component to a model that can be loaded back in.

        Raises:
            TypeError: If the component is a local class.

        Returns:
            ComponentModel: The model representing the component.
        """
        if self.component_provider_override is not None:
            provider = self.component_provider_override
        else:
            provider = _type_to_provider_str(self.__class__)
            # Warn if internal module name is used,
            if "._" in provider:
                warnings.warn(
                    "Internal module name used in provider string. This is not recommended and may cause issues in the future. Silence this warning by setting component_provider_override to this value.",
                    stacklevel=2,
                )

        if "<locals>" in provider:
            raise TypeError("Cannot dump component with local class")

        if not hasattr(self, "component_type"):
            raise AttributeError("component_type not defined")

        description = self.component_description
        if description is None and self.__class__.__doc__:
            # use docstring as description
            docstring = self.__class__.__doc__.strip()
            for marker in ["\n\nArgs:", "\n\nParameters:", "\n\nAttributes:", "\n\n"]:
                docstring = docstring.split(marker)[0]
            description = docstring.strip()

        obj_config = self._to_config().model_dump(exclude_none=True)
        model = ComponentModel(
            provider=provider,
            component_type=self.component_type,
            version=self.component_version,
            component_version=self.component_version,
            description=description,
            label=self.component_label or self.__class__.__name__,
            config=obj_config,
        )
        return model

# From autogen_core/_component_config.py
class ComponentLoader:
    @overload
    @classmethod
    def load_component(cls, model: ComponentModel | Dict[str, Any], expected: None = None) -> Self: ...

    @overload
    @classmethod
    def load_component(cls, model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) -> ExpectedType: ...

    @classmethod
    def load_component(
        cls, model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType] | None = None
    ) -> Self | ExpectedType:
        """Load a component from a model. Intended to be used with the return type of :py:meth:`autogen_core.ComponentConfig.dump_component`.

        Example:

            .. code-block:: python

                from autogen_core import ComponentModel
                from autogen_core.models import ChatCompletionClient

                component: ComponentModel = ...  # type: ignore

                model_client = ChatCompletionClient.load_component(component)

        Args:
            model (ComponentModel): The model to load the component from.

        Returns:
            Self: The loaded component.

        Args:
            model (ComponentModel): _description_
            expected (Type[ExpectedType] | None, optional): Explicit type only if used directly on ComponentLoader. Defaults to None.

        Raises:
            ValueError: If the provider string is invalid.
            TypeError: Provider is not a subclass of ComponentConfigImpl, or the expected type does not match.

        Returns:
            Self | ExpectedType: The loaded component.
        """

        # Use global and add further type checks

        if isinstance(model, dict):
            loaded_model = ComponentModel(**model)
        else:
            loaded_model = model

        # First, do a look up in well known providers
        if loaded_model.provider in WELL_KNOWN_PROVIDERS:
            loaded_model.provider = WELL_KNOWN_PROVIDERS[loaded_model.provider]

        output = loaded_model.provider.rsplit(".", maxsplit=1)
        if len(output) != 2:
            raise ValueError("Invalid")

        module_path, class_name = output
        module = importlib.import_module(module_path)
        component_class = module.__getattribute__(class_name)

        if not is_component_class(component_class):
            raise TypeError("Invalid component class")

        # We need to check the schema is valid
        if not hasattr(component_class, "component_config_schema"):
            raise AttributeError("component_config_schema not defined")

        if not hasattr(component_class, "component_type"):
            raise AttributeError("component_type not defined")

        loaded_config_version = loaded_model.component_version or component_class.component_version
        if loaded_config_version < component_class.component_version:
            try:
                instance = component_class._from_config_past_version(loaded_model.config, loaded_config_version)  # type: ignore
            except NotImplementedError as e:
                raise NotImplementedError(
                    f"Tried to load component {component_class} which is on version {component_class.component_version} with a config on version {loaded_config_version} but _from_config_past_version is not implemented"
                ) from e
        else:
            schema = component_class.component_config_schema  # type: ignore
            validated_config = schema.model_validate(loaded_model.config)

            # We're allowed to use the private method here
            instance = component_class._from_config(validated_config)  # type: ignore

        if expected is None and not isinstance(instance, cls):
            raise TypeError("Expected type does not match")
        elif expected is None:
            return cast(Self, instance)
        elif not isinstance(instance, expected):
            raise TypeError("Expected type does not match")
        else:
            return cast(ExpectedType, instance)

# From autogen_core/_component_config.py
class ComponentSchemaType(Generic[ConfigT]):
    # Ideally would be ClassVar[Type[ConfigT]], but this is disallowed https://github.com/python/typing/discussions/1424 (despite being valid in this context)
    component_config_schema: Type[ConfigT]
    """The Pydantic model class which represents the configuration of the component."""

    required_class_vars = ["component_config_schema", "component_type"]

    def __init_subclass__(cls, **kwargs: Any):
        super().__init_subclass__(**kwargs)

        if cls.__name__ != "Component" and not cls.__name__ == "_ConcreteComponent":
            # TODO: validate provider is loadable
            for var in cls.required_class_vars:
                if not hasattr(cls, var):
                    warnings.warn(
                        f"Class variable '{var}' must be defined in {cls.__name__} to be a valid component",
                        stacklevel=2,
                    )

# From autogen_core/_component_config.py
class ComponentBase(ComponentToConfig[ConfigT], ComponentLoader, Generic[ConfigT]): ...

# From autogen_core/_component_config.py
class Component(
    ComponentFromConfig[ConfigT],
    ComponentSchemaType[ConfigT],
    Generic[ConfigT],
):
    """To create a component class, inherit from this class for the concrete class and ComponentBase on the interface. Then implement two class variables:

    - :py:attr:`component_config_schema` - A Pydantic model class which represents the configuration of the component. This is also the type parameter of Component.
    - :py:attr:`component_type` - What is the logical type of the component.

    Example:

    .. code-block:: python

        from __future__ import annotations

        from pydantic import BaseModel
        from autogen_core import Component


        class Config(BaseModel):
            value: str


        class MyComponent(Component[Config]):
            component_type = "custom"
            component_config_schema = Config

            def __init__(self, value: str):
                self.value = value

            def _to_config(self) -> Config:
                return Config(value=self.value)

            @classmethod
            def _from_config(cls, config: Config) -> MyComponent:
                return cls(value=config.value)
    """

    def __init_subclass__(cls, **kwargs: Any):
        super().__init_subclass__(**kwargs)

        if not is_component_class(cls):
            warnings.warn(
                f"Component class '{cls.__name__}' must subclass the following: ComponentFromConfig, ComponentToConfig, ComponentSchemaType, ComponentLoader, individually or with ComponentBase and Component. Look at the component config documentation or how OpenAIChatCompletionClient does it.",
                stacklevel=2,
            )

# From autogen_core/_component_config.py
class _ConcreteComponent(
    ComponentFromConfig[ConfigT],
    ComponentSchemaType[ConfigT],
    ComponentToConfig[ConfigT],
    ComponentLoader,
    Generic[ConfigT],
): ...

# From autogen_core/_component_config.py
def is_component_instance(cls: Any) -> TypeGuard[_ConcreteComponent[BaseModel]]:
    return (
        isinstance(cls, ComponentFromConfig)
        and isinstance(cls, ComponentToConfig)
        and isinstance(cls, ComponentSchemaType)
        and isinstance(cls, ComponentLoader)
    )

# From autogen_core/_component_config.py
def is_component_class(cls: type) -> TypeGuard[Type[_ConcreteComponent[BaseModel]]]:
    return (
        issubclass(cls, ComponentFromConfig)
        and issubclass(cls, ComponentToConfig)
        and issubclass(cls, ComponentSchemaType)
        and issubclass(cls, ComponentLoader)
    )

# From autogen_core/_component_config.py
def dump_component(self) -> ComponentModel:
        """Dump the component to a model that can be loaded back in.

        Raises:
            TypeError: If the component is a local class.

        Returns:
            ComponentModel: The model representing the component.
        """
        if self.component_provider_override is not None:
            provider = self.component_provider_override
        else:
            provider = _type_to_provider_str(self.__class__)
            # Warn if internal module name is used,
            if "._" in provider:
                warnings.warn(
                    "Internal module name used in provider string. This is not recommended and may cause issues in the future. Silence this warning by setting component_provider_override to this value.",
                    stacklevel=2,
                )

        if "<locals>" in provider:
            raise TypeError("Cannot dump component with local class")

        if not hasattr(self, "component_type"):
            raise AttributeError("component_type not defined")

        description = self.component_description
        if description is None and self.__class__.__doc__:
            # use docstring as description
            docstring = self.__class__.__doc__.strip()
            for marker in ["\n\nArgs:", "\n\nParameters:", "\n\nAttributes:", "\n\n"]:
                docstring = docstring.split(marker)[0]
            description = docstring.strip()

        obj_config = self._to_config().model_dump(exclude_none=True)
        model = ComponentModel(
            provider=provider,
            component_type=self.component_type,
            version=self.component_version,
            component_version=self.component_version,
            description=description,
            label=self.component_label or self.__class__.__name__,
            config=obj_config,
        )
        return model

# From autogen_core/_component_config.py
def load_component(cls, model: ComponentModel | Dict[str, Any], expected: None = None) -> Self: ...


# From autogen_core/_topic.py
class TopicId:
    """
    TopicId defines the scope of a broadcast message. In essence, agent runtime implements a publish-subscribe model through its broadcast API: when publishing a message, the topic must be specified.

    See here for more information: :ref:`topic_and_subscription_topic`
    """

    type: str
    """Type of the event that this topic_id contains. Adhere's to the cloud event spec.

    Must match the pattern: ^[\\w\\-\\.\\:\\=]+\\Z

    Learn more here: https://github.com/cloudevents/spec/blob/main/cloudevents/spec.md#type
    """

    source: str
    """Identifies the context in which an event happened. Adhere's to the cloud event spec.

    Learn more here: https://github.com/cloudevents/spec/blob/main/cloudevents/spec.md#source-1
    """

    def __post_init__(self) -> None:
        if is_valid_topic_type(self.type) is False:
            raise ValueError(f"Invalid topic type: {self.type}. Must match the pattern: ^[\\w\\-\\.\\:\\=]+\\Z")

    def __str__(self) -> str:
        return f"{self.type}/{self.source}"

    @classmethod
    def from_str(cls, topic_id: str) -> Self:
        """Convert a string of the format ``type/source`` into a TopicId"""
        items = topic_id.split("/", maxsplit=1)
        if len(items) != 2:
            raise ValueError(f"Invalid topic id: {topic_id}")
        type, source = items[0], items[1]
        return cls(type, source)

# From autogen_core/_topic.py
def is_valid_topic_type(value: str) -> bool:
    return bool(re.match(r"^[\w\-\.\:\=]+\Z", value))

# From autogen_core/_topic.py
def from_str(cls, topic_id: str) -> Self:
        """Convert a string of the format ``type/source`` into a TopicId"""
        items = topic_id.split("/", maxsplit=1)
        if len(items) != 2:
            raise ValueError(f"Invalid topic id: {topic_id}")
        type, source = items[0], items[1]
        return cls(type, source)

from _message_handler_context import MessageHandlerContext
from _topic import TopicId

# From autogen_core/_default_topic.py
class DefaultTopicId(TopicId):
    """DefaultTopicId provides a sensible default for the topic_id and source fields of a TopicId.

    If created in the context of a message handler, the source will be set to the agent_id of the message handler, otherwise it will be set to "default".

    Args:
        type (str, optional): Topic type to publish message to. Defaults to "default".
        source (str | None, optional): Topic source to publish message to. If None, the source will be set to the agent_id of the message handler if in the context of a message handler, otherwise it will be set to "default". Defaults to None.
    """

    def __init__(self, type: str = "default", source: str | None = None) -> None:
        if source is None:
            try:
                source = MessageHandlerContext.agent_id().key
            # If we aren't in the context of a message handler, we use the default source
            except RuntimeError:
                source = "default"

        super().__init__(type, source)


# From autogen_core/exceptions.py
class CantHandleException(Exception):
    """Raised when a handler can't handle the exception."""

# From autogen_core/exceptions.py
class UndeliverableException(Exception):
    """Raised when a message can't be delivered."""

# From autogen_core/exceptions.py
class MessageDroppedException(Exception):
    """Raised when a message is dropped."""

# From autogen_core/exceptions.py
class NotAccessibleError(Exception):
    """Tried to access a value that is not accessible. For example if it is remote cannot be accessed locally."""

from _cancellation_token import CancellationToken

# From autogen_core/_message_context.py
class MessageContext:
    sender: AgentId | None
    topic_id: TopicId | None
    is_rpc: bool
    cancellation_token: CancellationToken
    message_id: str

from collections import defaultdict
from typing import DefaultDict
from _agent import Agent
from _agent_type import AgentType
from _subscription import Subscription

# From autogen_core/_runtime_impl_helpers.py
class SubscriptionManager:
    def __init__(self) -> None:
        self._subscriptions: List[Subscription] = []
        self._seen_topics: Set[TopicId] = set()
        self._subscribed_recipients: DefaultDict[TopicId, List[AgentId]] = defaultdict(list)

    @property
    def subscriptions(self) -> Sequence[Subscription]:
        return self._subscriptions

    async def add_subscription(self, subscription: Subscription) -> None:
        # Check if the subscription already exists
        if any(sub == subscription for sub in self._subscriptions):
            raise ValueError("Subscription already exists")

        self._subscriptions.append(subscription)
        self._rebuild_subscriptions(self._seen_topics)

    async def remove_subscription(self, id: str) -> None:
        # Check if the subscription exists
        if not any(sub.id == id for sub in self._subscriptions):
            raise ValueError("Subscription does not exist")

        def is_not_sub(x: Subscription) -> bool:
            return x.id != id

        self._subscriptions = list(filter(is_not_sub, self._subscriptions))

        # Rebuild the subscriptions
        self._rebuild_subscriptions(self._seen_topics)

    async def get_subscribed_recipients(self, topic: TopicId) -> List[AgentId]:
        if topic not in self._seen_topics:
            self._build_for_new_topic(topic)
        return self._subscribed_recipients[topic]

    # TODO: optimize this...
    def _rebuild_subscriptions(self, topics: Set[TopicId]) -> None:
        self._subscribed_recipients.clear()
        for topic in topics:
            self._build_for_new_topic(topic)

    def _build_for_new_topic(self, topic: TopicId) -> None:
        self._seen_topics.add(topic)
        for subscription in self._subscriptions:
            if subscription.is_match(topic):
                self._subscribed_recipients[topic].append(subscription.map_to_agent(topic))

# From autogen_core/_runtime_impl_helpers.py
def subscriptions(self) -> Sequence[Subscription]:
        return self._subscriptions

# From autogen_core/_runtime_impl_helpers.py
def is_not_sub(x: Subscription) -> bool:
            return x.id != id


from _base_agent import BaseAgent
from _base_agent import subscription_factory
from _subscription_context import SubscriptionInstantiationContext
from _type_subscription import TypeSubscription
from exceptions import CantHandleException

# From autogen_core/_default_subscription.py
class DefaultSubscription(TypeSubscription):
    """The default subscription is designed to be a sensible default for applications that only need global scope for agents.

    This topic by default uses the "default" topic type and attempts to detect the agent type to use based on the instantiation context.

    Args:
        topic_type (str, optional): The topic type to subscribe to. Defaults to "default".
        agent_type (str, optional): The agent type to use for the subscription. Defaults to None, in which case it will attempt to detect the agent type based on the instantiation context.
    """

    def __init__(self, topic_type: str = "default", agent_type: str | AgentType | None = None):
        if agent_type is None:
            try:
                agent_type = SubscriptionInstantiationContext.agent_type().type
            except RuntimeError as e:
                raise CantHandleException(
                    "If agent_type is not specified DefaultSubscription must be created within the subscription callback in AgentRuntime.register"
                ) from e

        super().__init__(topic_type, agent_type)

# From autogen_core/_default_subscription.py
def default_subscription() -> Callable[[Type[BaseAgentType]], Type[BaseAgentType]]: ...

# From autogen_core/_default_subscription.py
def type_subscription(topic_type: str) -> Callable[[Type[BaseAgentType]], Type[BaseAgentType]]:
    return subscription_factory(lambda: [DefaultSubscription(topic_type=topic_type)])


# From autogen_core/_types.py
class FunctionCall:
    id: str
    # JSON args
    arguments: str
    # Function to call
    name: str

from asyncio import Future

# From autogen_core/_cancellation_token.py
class CancellationToken:
    """A token used to cancel pending async calls"""

    def __init__(self) -> None:
        self._cancelled: bool = False
        self._lock: threading.Lock = threading.Lock()
        self._callbacks: List[Callable[[], None]] = []

    def cancel(self) -> None:
        """Cancel pending async calls linked to this cancellation token."""
        with self._lock:
            if not self._cancelled:
                self._cancelled = True
                for callback in self._callbacks:
                    callback()

    def is_cancelled(self) -> bool:
        """Check if the CancellationToken has been used"""
        with self._lock:
            return self._cancelled

    def add_callback(self, callback: Callable[[], None]) -> None:
        """Attach a callback that will be called when cancel is invoked"""
        with self._lock:
            if self._cancelled:
                callback()
            else:
                self._callbacks.append(callback)

    def link_future(self, future: Future[Any]) -> Future[Any]:
        """Link a pending async call to a token to allow its cancellation"""
        with self._lock:
            if self._cancelled:
                future.cancel()
            else:

                def _cancel() -> None:
                    future.cancel()

                self._callbacks.append(_cancel)
        return future

# From autogen_core/_cancellation_token.py
def cancel(self) -> None:
        """Cancel pending async calls linked to this cancellation token."""
        with self._lock:
            if not self._cancelled:
                self._cancelled = True
                for callback in self._callbacks:
                    callback()

# From autogen_core/_cancellation_token.py
def is_cancelled(self) -> bool:
        """Check if the CancellationToken has been used"""
        with self._lock:
            return self._cancelled

# From autogen_core/_cancellation_token.py
def add_callback(self, callback: Callable[[], None]) -> None:
        """Attach a callback that will be called when cancel is invoked"""
        with self._lock:
            if self._cancelled:
                callback()
            else:
                self._callbacks.append(callback)

# From autogen_core/_cancellation_token.py
def link_future(self, future: Future[Any]) -> Future[Any]:
        """Link a pending async call to a token to allow its cancellation"""
        with self._lock:
            if self._cancelled:
                future.cancel()
            else:

                def _cancel() -> None:
                    future.cancel()

                self._callbacks.append(_cancel)
        return future

from io import BytesIO
from pydantic import GetCoreSchemaHandler
from pydantic import ValidationInfo
from pydantic_core import core_schema

# From autogen_core/_image.py
class Image:
    """Represents an image.


    Example:

        Loading an image from a URL:

        .. code-block:: python

            from autogen_core import Image
            from PIL import Image as PILImage
            import aiohttp
            import asyncio


            async def from_url(url: str) -> Image:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url) as response:
                        content = await response.read()
                        return Image.from_pil(PILImage.open(content))


            image = asyncio.run(from_url("https://example.com/image"))

    """

    def __init__(self, image: PILImage.Image):
        self.image: PILImage.Image = image.convert("RGB")

    @classmethod
    def from_pil(cls, pil_image: PILImage.Image) -> Image:
        return cls(pil_image)

    @classmethod
    def from_uri(cls, uri: str) -> Image:
        if not re.match(r"data:image/(?:png|jpeg);base64,", uri):
            raise ValueError("Invalid URI format. It should be a base64 encoded image URI.")

        # A URI. Remove the prefix and decode the base64 string.
        base64_data = re.sub(r"data:image/(?:png|jpeg);base64,", "", uri)
        return cls.from_base64(base64_data)

    @classmethod
    def from_base64(cls, base64_str: str) -> Image:
        return cls(PILImage.open(BytesIO(base64.b64decode(base64_str))))

    def to_base64(self) -> str:
        buffered = BytesIO()
        self.image.save(buffered, format="PNG")
        content = buffered.getvalue()
        return base64.b64encode(content).decode("utf-8")

    @classmethod
    def from_file(cls, file_path: Path) -> Image:
        return cls(PILImage.open(file_path))

    def _repr_html_(self) -> str:
        # Show the image in Jupyter notebook
        return f'<img src="{self.data_uri}"/>'

    @property
    def data_uri(self) -> str:
        return _convert_base64_to_data_uri(self.to_base64())

    # Returns openai.types.chat.ChatCompletionContentPartImageParam, which is a TypedDict
    # We don't use the explicit type annotation so that we can avoid a dependency on the OpenAI Python SDK in this package.
    def to_openai_format(self, detail: Literal["auto", "low", "high"] = "auto") -> Dict[str, Any]:
        return {"type": "image_url", "image_url": {"url": self.data_uri, "detail": detail}}

    @classmethod
    def __get_pydantic_core_schema__(cls, source_type: Any, handler: GetCoreSchemaHandler) -> core_schema.CoreSchema:
        # Custom validation
        def validate(value: Any, validation_info: ValidationInfo) -> Image:
            if isinstance(value, dict):
                base_64 = cast(str | None, value.get("data"))  # type: ignore
                if base_64 is None:
                    raise ValueError("Expected 'data' key in the dictionary")
                return cls.from_base64(base_64)
            elif isinstance(value, cls):
                return value
            else:
                raise TypeError(f"Expected dict or {cls.__name__} instance, got {type(value)}")

        # Custom serialization
        def serialize(value: Image) -> dict[str, Any]:
            return {"data": value.to_base64()}

        return core_schema.with_info_after_validator_function(
            validate,
            core_schema.any_schema(),  # Accept any type; adjust if needed
            serialization=core_schema.plain_serializer_function_ser_schema(serialize),
        )

# From autogen_core/_image.py
def from_pil(cls, pil_image: PILImage.Image) -> Image:
        return cls(pil_image)

# From autogen_core/_image.py
def from_uri(cls, uri: str) -> Image:
        if not re.match(r"data:image/(?:png|jpeg);base64,", uri):
            raise ValueError("Invalid URI format. It should be a base64 encoded image URI.")

        # A URI. Remove the prefix and decode the base64 string.
        base64_data = re.sub(r"data:image/(?:png|jpeg);base64,", "", uri)
        return cls.from_base64(base64_data)

# From autogen_core/_image.py
def from_base64(cls, base64_str: str) -> Image:
        return cls(PILImage.open(BytesIO(base64.b64decode(base64_str))))

# From autogen_core/_image.py
def to_base64(self) -> str:
        buffered = BytesIO()
        self.image.save(buffered, format="PNG")
        content = buffered.getvalue()
        return base64.b64encode(content).decode("utf-8")

# From autogen_core/_image.py
def from_file(cls, file_path: Path) -> Image:
        return cls(PILImage.open(file_path))

# From autogen_core/_image.py
def data_uri(self) -> str:
        return _convert_base64_to_data_uri(self.to_base64())

# From autogen_core/_image.py
def to_openai_format(self, detail: Literal["auto", "low", "high"] = "auto") -> Dict[str, Any]:
        return {"type": "image_url", "image_url": {"url": self.data_uri, "detail": detail}}

import collections

# From autogen_core/_queue.py
class _LoopBoundMixin:
    _loop = None

    def _get_loop(self) -> asyncio.AbstractEventLoop:
        loop = asyncio.get_running_loop()

        if self._loop is None:
            with _global_lock:
                if self._loop is None:
                    self._loop = loop
        if loop is not self._loop:
            raise RuntimeError(f"{self!r} is bound to a different event loop")
        return loop

# From autogen_core/_queue.py
class QueueShutDown(Exception):
    """Raised when putting on to or getting from a shut-down Queue."""

    pass

# From autogen_core/_queue.py
class Queue(_LoopBoundMixin, Generic[T]):
    def __init__(self, maxsize: int = 0):
        self._maxsize = maxsize
        self._getters = collections.deque[asyncio.Future[None]]()
        self._putters = collections.deque[asyncio.Future[None]]()
        self._unfinished_tasks = 0
        self._finished = asyncio.Event()
        self._finished.set()
        self._queue = collections.deque[T]()
        self._is_shutdown = False

    # These three are overridable in subclasses.

    def _get(self) -> T:
        return self._queue.popleft()

    def _put(self, item: T) -> None:
        self._queue.append(item)

    # End of the overridable methods.

    def _wakeup_next(self, waiters: collections.deque[asyncio.Future[None]]) -> None:
        # Wake up the next waiter (if any) that isn't cancelled.
        while waiters:
            waiter = waiters.popleft()
            if not waiter.done():
                waiter.set_result(None)
                break

    def __repr__(self) -> str:
        return f"<{type(self).__name__} at {id(self):#x} {self._format()}>"

    def __str__(self) -> str:
        return f"<{type(self).__name__} {self._format()}>"

    def _format(self) -> str:
        result = f"maxsize={self._maxsize!r}"
        if getattr(self, "_queue", None):
            result += f" _queue={list(self._queue)!r}"
        if self._getters:
            result += f" _getters[{len(self._getters)}]"
        if self._putters:
            result += f" _putters[{len(self._putters)}]"
        if self._unfinished_tasks:
            result += f" tasks={self._unfinished_tasks}"
        if self._is_shutdown:
            result += " shutdown"
        return result

    def qsize(self) -> int:
        """Number of items in the queue."""
        return len(self._queue)

    @property
    def maxsize(self) -> int:
        """Number of items allowed in the queue."""
        return self._maxsize

    def empty(self) -> bool:
        """Return True if the queue is empty, False otherwise."""
        return not self._queue

    def full(self) -> bool:
        """Return True if there are maxsize items in the queue.

        Note: if the Queue was initialized with maxsize=0 (the default),
        then full() is never True.
        """
        if self._maxsize <= 0:
            return False
        else:
            return self.qsize() >= self._maxsize

    async def put(self, item: T) -> None:
        """Put an item into the queue.

        Put an item into the queue. If the queue is full, wait until a free
        slot is available before adding item.

        Raises QueueShutDown if the queue has been shut down.
        """
        while self.full():
            if self._is_shutdown:
                raise QueueShutDown
            putter = self._get_loop().create_future()
            self._putters.append(putter)
            try:
                await putter
            except:
                putter.cancel()  # Just in case putter is not done yet.
                try:
                    # Clean self._putters from canceled putters.
                    self._putters.remove(putter)
                except ValueError:
                    # The putter could be removed from self._putters by a
                    # previous get_nowait call or a shutdown call.
                    pass
                if not self.full() and not putter.cancelled():
                    # We were woken up by get_nowait(), but can't take
                    # the call.  Wake up the next in line.
                    self._wakeup_next(self._putters)
                raise
        return self.put_nowait(item)

    def put_nowait(self, item: T) -> None:
        """Put an item into the queue without blocking.

        If no free slot is immediately available, raise QueueFull.

        Raises QueueShutDown if the queue has been shut down.
        """
        if self._is_shutdown:
            raise QueueShutDown
        if self.full():
            raise asyncio.QueueFull
        self._put(item)
        self._unfinished_tasks += 1
        self._finished.clear()
        self._wakeup_next(self._getters)

    async def get(self) -> T:
        """Remove and return an item from the queue.

        If queue is empty, wait until an item is available.

        Raises QueueShutDown if the queue has been shut down and is empty, or
        if the queue has been shut down immediately.
        """
        while self.empty():
            if self._is_shutdown and self.empty():
                raise QueueShutDown
            getter = self._get_loop().create_future()
            self._getters.append(getter)
            try:
                await getter
            except:
                getter.cancel()  # Just in case getter is not done yet.
                try:
                    # Clean self._getters from canceled getters.
                    self._getters.remove(getter)
                except ValueError:
                    # The getter could be removed from self._getters by a
                    # previous put_nowait call, or a shutdown call.
                    pass
                if not self.empty() and not getter.cancelled():
                    # We were woken up by put_nowait(), but can't take
                    # the call.  Wake up the next in line.
                    self._wakeup_next(self._getters)
                raise
        return self.get_nowait()

    def get_nowait(self) -> T:
        """Remove and return an item from the queue.

        Return an item if one is immediately available, else raise QueueEmpty.

        Raises QueueShutDown if the queue has been shut down and is empty, or
        if the queue has been shut down immediately.
        """
        if self.empty():
            if self._is_shutdown:
                raise QueueShutDown
            raise asyncio.QueueEmpty
        item = self._get()
        self._wakeup_next(self._putters)
        return item

    def task_done(self) -> None:
        """Indicate that a formerly enqueued task is complete.

        Used by queue consumers. For each get() used to fetch a task,
        a subsequent call to task_done() tells the queue that the processing
        on the task is complete.

        If a join() is currently blocking, it will resume when all items have
        been processed (meaning that a task_done() call was received for every
        item that had been put() into the queue).

        shutdown(immediate=True) calls task_done() for each remaining item in
        the queue.

        Raises ValueError if called more times than there were items placed in
        the queue.
        """
        if self._unfinished_tasks <= 0:
            raise ValueError("task_done() called too many times")
        self._unfinished_tasks -= 1
        if self._unfinished_tasks == 0:
            self._finished.set()

    async def join(self) -> None:
        """Block until all items in the queue have been gotten and processed.

        The count of unfinished tasks goes up whenever an item is added to the
        queue. The count goes down whenever a consumer calls task_done() to
        indicate that the item was retrieved and all work on it is complete.
        When the count of unfinished tasks drops to zero, join() unblocks.
        """
        if self._unfinished_tasks > 0:
            await self._finished.wait()

    def shutdown(self, immediate: bool = False) -> None:
        """Shut-down the queue, making queue gets and puts raise QueueShutDown.

        By default, gets will only raise once the queue is empty. Set
        'immediate' to True to make gets raise immediately instead.

        All blocked callers of put() and get() will be unblocked. If
        'immediate', a task is marked as done for each item remaining in
        the queue, which may unblock callers of join().
        """
        self._is_shutdown = True
        if immediate:
            while not self.empty():
                self._get()
                if self._unfinished_tasks > 0:
                    self._unfinished_tasks -= 1
            if self._unfinished_tasks == 0:
                self._finished.set()
        # All getters need to re-check queue-empty to raise ShutDown
        while self._getters:
            getter = self._getters.popleft()
            if not getter.done():
                getter.set_result(None)
        while self._putters:
            putter = self._putters.popleft()
            if not putter.done():
                putter.set_result(None)

# From autogen_core/_queue.py
def qsize(self) -> int:
        """Number of items in the queue."""
        return len(self._queue)

# From autogen_core/_queue.py
def maxsize(self) -> int:
        """Number of items allowed in the queue."""
        return self._maxsize

# From autogen_core/_queue.py
def empty(self) -> bool:
        """Return True if the queue is empty, False otherwise."""
        return not self._queue

# From autogen_core/_queue.py
def full(self) -> bool:
        """Return True if there are maxsize items in the queue.

        Note: if the Queue was initialized with maxsize=0 (the default),
        then full() is never True.
        """
        if self._maxsize <= 0:
            return False
        else:
            return self.qsize() >= self._maxsize

# From autogen_core/_queue.py
def put_nowait(self, item: T) -> None:
        """Put an item into the queue without blocking.

        If no free slot is immediately available, raise QueueFull.

        Raises QueueShutDown if the queue has been shut down.
        """
        if self._is_shutdown:
            raise QueueShutDown
        if self.full():
            raise asyncio.QueueFull
        self._put(item)
        self._unfinished_tasks += 1
        self._finished.clear()
        self._wakeup_next(self._getters)

# From autogen_core/_queue.py
def get_nowait(self) -> T:
        """Remove and return an item from the queue.

        Return an item if one is immediately available, else raise QueueEmpty.

        Raises QueueShutDown if the queue has been shut down and is empty, or
        if the queue has been shut down immediately.
        """
        if self.empty():
            if self._is_shutdown:
                raise QueueShutDown
            raise asyncio.QueueEmpty
        item = self._get()
        self._wakeup_next(self._putters)
        return item

# From autogen_core/_queue.py
def task_done(self) -> None:
        """Indicate that a formerly enqueued task is complete.

        Used by queue consumers. For each get() used to fetch a task,
        a subsequent call to task_done() tells the queue that the processing
        on the task is complete.

        If a join() is currently blocking, it will resume when all items have
        been processed (meaning that a task_done() call was received for every
        item that had been put() into the queue).

        shutdown(immediate=True) calls task_done() for each remaining item in
        the queue.

        Raises ValueError if called more times than there were items placed in
        the queue.
        """
        if self._unfinished_tasks <= 0:
            raise ValueError("task_done() called too many times")
        self._unfinished_tasks -= 1
        if self._unfinished_tasks == 0:
            self._finished.set()

import contextlib
from typing import Iterator
from opentelemetry.trace import NoOpTracerProvider
from opentelemetry.trace import Span
from opentelemetry.trace import SpanKind
from opentelemetry.trace import TracerProvider
from opentelemetry.trace import get_tracer_provider
from opentelemetry.util import types
from _propagation import TelemetryMetadataContainer
from _propagation import get_telemetry_links
from _tracing_config import Destination
from _tracing_config import ExtraAttributes
from _tracing_config import Operation
from _tracing_config import TracingConfig

# From _telemetry/_tracing.py
class TraceHelper(Generic[Operation, Destination, ExtraAttributes]):
    """
    TraceHelper is a utility class to assist with tracing operations using OpenTelemetry.

    This class provides a context manager `trace_block` to create and manage spans for tracing operations,
    following semantic conventions and supporting nested spans through metadata contexts.

    """

    def __init__(
        self,
        tracer_provider: TracerProvider | None,
        instrumentation_builder_config: TracingConfig[Operation, Destination, ExtraAttributes],
    ) -> None:
        self.instrumentation_builder_config = instrumentation_builder_config

        disable_runtime_tracing = os.environ.get("AUTOGEN_DISABLE_RUNTIME_TRACING") == "true"
        if disable_runtime_tracing:
            self.tracer_provider: TracerProvider = NoOpTracerProvider()
            self.tracer = self.tracer_provider.get_tracer(f"autogen {instrumentation_builder_config.name}")
            return

        # Evaluate in order: first try tracer_provider param, then get_tracer_provider(), finally fallback to NoOp
        # This allows for nested tracing with a default tracer provided by the user
        self.tracer_provider = tracer_provider or get_tracer_provider() or NoOpTracerProvider()
        self.tracer = self.tracer_provider.get_tracer(f"autogen {instrumentation_builder_config.name}")

    @contextlib.contextmanager
    def trace_block(
        self,
        operation: Operation,
        destination: Destination,
        parent: Optional[TelemetryMetadataContainer],
        *,
        extraAttributes: ExtraAttributes | None = None,
        kind: Optional[SpanKind] = None,
        attributes: Optional[types.Attributes] = None,
        start_time: Optional[int] = None,
        record_exception: bool = True,
        set_status_on_exception: bool = True,
        end_on_exit: bool = True,
    ) -> Iterator[Span]:
        """
        Thin wrapper on top of start_as_current_span.
        1. It helps us follow semantic conventions
        2. It helps us get contexts from metadata so we can get nested spans

        Args:
            operation (MessagingOperation): The messaging operation being performed.
            destination (MessagingDestination): The messaging destination being used.
            parent Optional[TelemetryMetadataContainer]: The parent telemetry metadta context
            kind (SpanKind, optional): The kind of span. If not provided, it maps to PRODUCER or CONSUMER depending on the operation.
            extraAttributes (ExtraAttributes, optional): Additional defined attributes for the span. Defaults to None.
            attributes (Optional[types.Attributes], optional): Additional non-defined attributes for the span. Defaults to None.
            start_time (Optional[int], optional): The start time of the span. Defaults to None.
            record_exception (bool, optional): Whether to record exceptions. Defaults to True.
            set_status_on_exception (bool, optional): Whether to set the status on exception. Defaults to True.
            end_on_exit (bool, optional): Whether to end the span on exit. Defaults to True.

        Yields:
            Iterator[Span]: The span object.

        """
        span_name = self.instrumentation_builder_config.get_span_name(operation, destination)
        span_kind = kind or self.instrumentation_builder_config.get_span_kind(operation)
        # context = get_telemetry_context(parent) if parent else None
        context = None  # TODO: we may need to remove other code for using custom context.
        links = get_telemetry_links(parent) if parent else None
        attributes_with_defaults: Dict[str, types.AttributeValue] = {}
        for key, value in (attributes or {}).items():
            attributes_with_defaults[key] = value
        instrumentation_attributes = self.instrumentation_builder_config.build_attributes(
            operation, destination, extraAttributes
        )
        for key, value in instrumentation_attributes.items():
            attributes_with_defaults[key] = value
        with self.tracer.start_as_current_span(
            span_name,
            context,
            span_kind,
            attributes_with_defaults,
            links,
            start_time,
            record_exception,
            set_status_on_exception,
            end_on_exit,
        ) as span:
            yield span

# From _telemetry/_tracing.py
def trace_block(
        self,
        operation: Operation,
        destination: Destination,
        parent: Optional[TelemetryMetadataContainer],
        *,
        extraAttributes: ExtraAttributes | None = None,
        kind: Optional[SpanKind] = None,
        attributes: Optional[types.Attributes] = None,
        start_time: Optional[int] = None,
        record_exception: bool = True,
        set_status_on_exception: bool = True,
        end_on_exit: bool = True,
    ) -> Iterator[Span]:
        """
        Thin wrapper on top of start_as_current_span.
        1. It helps us follow semantic conventions
        2. It helps us get contexts from metadata so we can get nested spans

        Args:
            operation (MessagingOperation): The messaging operation being performed.
            destination (MessagingDestination): The messaging destination being used.
            parent Optional[TelemetryMetadataContainer]: The parent telemetry metadta context
            kind (SpanKind, optional): The kind of span. If not provided, it maps to PRODUCER or CONSUMER depending on the operation.
            extraAttributes (ExtraAttributes, optional): Additional defined attributes for the span. Defaults to None.
            attributes (Optional[types.Attributes], optional): Additional non-defined attributes for the span. Defaults to None.
            start_time (Optional[int], optional): The start time of the span. Defaults to None.
            record_exception (bool, optional): Whether to record exceptions. Defaults to True.
            set_status_on_exception (bool, optional): Whether to set the status on exception. Defaults to True.
            end_on_exit (bool, optional): Whether to end the span on exit. Defaults to True.

        Yields:
            Iterator[Span]: The span object.

        """
        span_name = self.instrumentation_builder_config.get_span_name(operation, destination)
        span_kind = kind or self.instrumentation_builder_config.get_span_kind(operation)
        # context = get_telemetry_context(parent) if parent else None
        context = None  # TODO: we may need to remove other code for using custom context.
        links = get_telemetry_links(parent) if parent else None
        attributes_with_defaults: Dict[str, types.AttributeValue] = {}
        for key, value in (attributes or {}).items():
            attributes_with_defaults[key] = value
        instrumentation_attributes = self.instrumentation_builder_config.build_attributes(
            operation, destination, extraAttributes
        )
        for key, value in instrumentation_attributes.items():
            attributes_with_defaults[key] = value
        with self.tracer.start_as_current_span(
            span_name,
            context,
            span_kind,
            attributes_with_defaults,
            links,
            start_time,
            record_exception,
            set_status_on_exception,
            end_on_exit,
        ) as span:
            yield span


from typing_extensions import NotRequired
from _constants import NAMESPACE

# From _telemetry/_tracing_config.py
class TracingConfig(ABC, Generic[Operation, Destination, ExtraAttributes]):
    """
    A protocol that defines the configuration for instrumentation.

    This protocol specifies the required properties and methods that any
    instrumentation configuration class must implement. It includes a
    property to get the name of the module being instrumented and a method
    to build attributes for the instrumentation configuration.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """
        Returns:
            The name of the module that is being instrumented.
        """
        ...

    @abstractmethod
    def build_attributes(
        self,
        operation: Operation,
        destination: Destination,
        extraAttributes: ExtraAttributes | None,
    ) -> Dict[str, types.AttributeValue]:
        """
        Builds the attributes for the instrumentation configuration.

        Returns:
            Dict[str, str]: The attributes for the instrumentation configuration.
        """
        ...

    @abstractmethod
    def get_span_name(
        self,
        operation: Operation,
        destination: Destination,
    ) -> str:
        """
        Returns the span name based on the given operation and destination.

        Parameters:
            operation (MessagingOperation): The messaging operation.
            destination (Optional[MessagingDestination]): The messaging destination.

        Returns:
            str: The span name.
        """
        ...

    @abstractmethod
    def get_span_kind(
        self,
        operation: Operation,
    ) -> SpanKind:
        """
        Determines the span kind based on the given messaging operation.

        Parameters:
            operation (MessagingOperation): The messaging operation.

        Returns:
            SpanKind: The span kind based on the messaging operation.
        """

# From _telemetry/_tracing_config.py
class ExtraMessageRuntimeAttributes(TypedDict):
    message_size: NotRequired[int]
    message_type: NotRequired[str]

# From _telemetry/_tracing_config.py
class MessageRuntimeTracingConfig(
    TracingConfig[MessagingOperation, MessagingDestination, ExtraMessageRuntimeAttributes]
):
    """
    A class that defines the configuration for message runtime instrumentation.

    This class implements the TracingConfig protocol and provides
    the name of the module being instrumented and the attributes for the
    instrumentation configuration.
    """

    def __init__(self, runtime_name: str) -> None:
        self._runtime_name = runtime_name

    @property
    def name(self) -> str:
        return self._runtime_name

    def build_attributes(
        self,
        operation: MessagingOperation,
        destination: MessagingDestination,
        extraAttributes: ExtraMessageRuntimeAttributes | None,
    ) -> Dict[str, types.AttributeValue]:
        attrs: Dict[str, types.AttributeValue] = {
            "messaging.operation": self._get_operation_type(operation),
            "messaging.destination": self._get_destination_str(destination),
        }
        if extraAttributes:
            # TODO: Make this more pythonic?
            if "message_size" in extraAttributes:
                attrs["messaging.message.envelope.size"] = extraAttributes["message_size"]
            if "message_type" in extraAttributes:
                attrs["messaging.message.type"] = extraAttributes["message_type"]
        return attrs

    def get_span_name(
        self,
        operation: MessagingOperation,
        destination: MessagingDestination,
    ) -> str:
        """
        Returns the span name based on the given operation and destination.
        Semantic Conventions - https://opentelemetry.io/docs/specs/semconv/messaging/messaging-spans/#span-name

        Parameters:
            operation (MessagingOperation): The messaging operation.
            destination (Optional[MessagingDestination]): The messaging destination.

        Returns:
            str: The span name.
        """
        span_parts: List[str] = [operation]
        destination_str = self._get_destination_str(destination)
        if destination_str:
            span_parts.append(destination_str)
        span_name = " ".join(span_parts)
        return f"{NAMESPACE} {span_name}"

    def get_span_kind(
        self,
        operation: MessagingOperation,
    ) -> SpanKind:
        """
        Determines the span kind based on the given messaging operation.
        Semantic Conventions - https://opentelemetry.io/docs/specs/semconv/messaging/messaging-spans/#span-kind

        Parameters:
            operation (MessagingOperation): The messaging operation.

        Returns:
            SpanKind: The span kind based on the messaging operation.
        """
        if operation in ["create", "send", "publish"]:
            return SpanKind.PRODUCER
        elif operation in ["receive", "intercept", "process", "ack"]:
            return SpanKind.CONSUMER
        else:
            return SpanKind.CLIENT

    # TODO: Use stringified convention
    def _get_destination_str(self, destination: MessagingDestination) -> str:
        if isinstance(destination, AgentId):
            return f"{destination.type}.({destination.key})-A"
        elif isinstance(destination, TopicId):
            return f"{destination.type}.({destination.source})-T"
        elif isinstance(destination, str):
            return destination
        elif destination is None:
            return ""
        else:
            raise ValueError(f"Unknown destination type: {type(destination)}")

    def _get_operation_type(self, operation: MessagingOperation) -> str:
        if operation in ["send", "publish"]:
            return "publish"
        if operation in ["create"]:
            return "create"
        elif operation in ["receive", "intercept", "ack"]:
            return "receive"
        elif operation in ["process"]:
            return "process"
        else:
            return "Unknown"

# From _telemetry/_tracing_config.py
def name(self) -> str:
        """
        Returns:
            The name of the module that is being instrumented.
        """
        ...

# From _telemetry/_tracing_config.py
def build_attributes(
        self,
        operation: Operation,
        destination: Destination,
        extraAttributes: ExtraAttributes | None,
    ) -> Dict[str, types.AttributeValue]:
        """
        Builds the attributes for the instrumentation configuration.

        Returns:
            Dict[str, str]: The attributes for the instrumentation configuration.
        """
        ...

# From _telemetry/_tracing_config.py
def get_span_name(
        self,
        operation: Operation,
        destination: Destination,
    ) -> str:
        """
        Returns the span name based on the given operation and destination.

        Parameters:
            operation (MessagingOperation): The messaging operation.
            destination (Optional[MessagingDestination]): The messaging destination.

        Returns:
            str: The span name.
        """
        ...

# From _telemetry/_tracing_config.py
def get_span_kind(
        self,
        operation: Operation,
    ) -> SpanKind:
        """
        Determines the span kind based on the given messaging operation.

        Parameters:
            operation (MessagingOperation): The messaging operation.

        Returns:
            SpanKind: The span kind based on the messaging operation.
        """

from typing import Mapping
from opentelemetry.context import Context
from opentelemetry.propagate import extract
from opentelemetry.trace import Link
from opentelemetry.trace import get_current_span
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator

# From _telemetry/_propagation.py
class EnvelopeMetadata:
    """Metadata for an envelope."""

    traceparent: Optional[str] = None
    tracestate: Optional[str] = None
    links: Optional[Sequence[Link]] = None

# From _telemetry/_propagation.py
def get_telemetry_envelope_metadata() -> EnvelopeMetadata:
    """
    Retrieves the telemetry envelope metadata.

    Returns:
        EnvelopeMetadata: The envelope metadata containing the traceparent and tracestate.
    """
    carrier: Dict[str, str] = {}
    TraceContextTextMapPropagator().inject(carrier)
    return EnvelopeMetadata(
        traceparent=carrier.get("traceparent"),
        tracestate=carrier.get("tracestate"),
    )

# From _telemetry/_propagation.py
def get_telemetry_grpc_metadata(existingMetadata: Optional[Mapping[str, str]] = None) -> Dict[str, str]:
    """
    Retrieves the telemetry gRPC metadata.

    Args:
        existingMetadata (Optional[Mapping[str, str]]): The existing metadata to include in the gRPC metadata.

    Returns:
        Mapping[str, str]: The gRPC metadata containing the traceparent and tracestate.
    """
    carrier: Dict[str, str] = {}
    TraceContextTextMapPropagator().inject(carrier)
    traceparent = carrier.get("traceparent")
    tracestate = carrier.get("tracestate")
    metadata: Dict[str, str] = {}
    if existingMetadata is not None:
        for key, value in existingMetadata.items():
            metadata[key] = value
    if traceparent is not None:
        metadata["traceparent"] = traceparent
    if tracestate is not None:
        metadata["tracestate"] = tracestate
    return metadata

# From _telemetry/_propagation.py
def get_telemetry_context(metadata: TelemetryMetadataContainer) -> Context:
    """
    Retrieves the telemetry context from the given metadata.

    Args:
        metadata (Optional[EnvelopeMetadata]): The metadata containing the telemetry context.

    Returns:
        Context: The telemetry context extracted from the metadata, or an empty context if the metadata is None.
    """
    if metadata is None:
        return Context()
    elif isinstance(metadata, EnvelopeMetadata):
        return extract(_get_carrier_for_envelope_metadata(metadata))
    elif hasattr(metadata, "__getitem__"):
        return extract(_get_carrier_for_remote_call_metadata(metadata))
    else:
        raise ValueError(f"Unknown metadata type: {type(metadata)}")

# From _telemetry/_propagation.py
def get_telemetry_links(
    metadata: TelemetryMetadataContainer,
) -> Optional[Sequence[Link]]:
    """
    Retrieves the telemetry links from the given metadata.

    Args:
        metadata (Optional[EnvelopeMetadata]): The metadata containing the telemetry links.

    Returns:
        Optional[Sequence[Link]]: The telemetry links extracted from the metadata, or None if there are no links.
    """
    if metadata is None:
        return None
    elif isinstance(metadata, EnvelopeMetadata):
        context = extract(_get_carrier_for_envelope_metadata(metadata))
    elif hasattr(metadata, "__getitem__"):
        context = extract(_get_carrier_for_remote_call_metadata(metadata))
    else:
        return None
    # Retrieve the extracted SpanContext from the context.
    linked_span = get_current_span(context)
    # Use the linked span to get the SpanContext.
    span_context = linked_span.get_span_context()
    # Create a Link object using the SpanContext.
    return [Link(span_context)]

from _component_config import Component
from _types import FunctionCall
from models import AssistantMessage
from models import FunctionExecutionResultMessage
from models import LLMMessage
from models import UserMessage
from _chat_completion_context import ChatCompletionContext

# From model_context/_head_and_tail_chat_completion_context.py
class HeadAndTailChatCompletionContextConfig(BaseModel):
    head_size: int
    tail_size: int
    initial_messages: List[LLMMessage] | None = None

# From model_context/_head_and_tail_chat_completion_context.py
class HeadAndTailChatCompletionContext(ChatCompletionContext, Component[HeadAndTailChatCompletionContextConfig]):
    """A chat completion context that keeps a view of the first n and last m messages,
    where n is the head size and m is the tail size. The head and tail sizes
    are set at initialization.

    Args:
        head_size (int): The size of the head.
        tail_size (int): The size of the tail.
        initial_messages (List[LLMMessage] | None): The initial messages.
    """

    component_config_schema = HeadAndTailChatCompletionContextConfig
    component_provider_override = "autogen_core.model_context.HeadAndTailChatCompletionContext"

    def __init__(self, head_size: int, tail_size: int, initial_messages: List[LLMMessage] | None = None) -> None:
        super().__init__(initial_messages)
        if head_size <= 0:
            raise ValueError("head_size must be greater than 0.")
        if tail_size <= 0:
            raise ValueError("tail_size must be greater than 0.")
        self._head_size = head_size
        self._tail_size = tail_size

    async def get_messages(self) -> List[LLMMessage]:
        """Get at most `head_size` recent messages and `tail_size` oldest messages."""
        head_messages = self._messages[: self._head_size]
        # Handle the last message is a function call message.
        if (
            head_messages
            and isinstance(head_messages[-1], AssistantMessage)
            and isinstance(head_messages[-1].content, list)
            and all(isinstance(item, FunctionCall) for item in head_messages[-1].content)
        ):
            # Remove the last message from the head.
            head_messages = head_messages[:-1]

        tail_messages = self._messages[-self._tail_size :]
        # Handle the first message is a function call result message.
        if tail_messages and isinstance(tail_messages[0], FunctionExecutionResultMessage):
            # Remove the first message from the tail.
            tail_messages = tail_messages[1:]

        num_skipped = len(self._messages) - self._head_size - self._tail_size
        if num_skipped <= 0:
            # If there are not enough messages to fill the head and tail,
            # return all messages.
            return self._messages

        placeholder_messages = [UserMessage(content=f"Skipped {num_skipped} messages.", source="System")]
        return head_messages + placeholder_messages + tail_messages

    def _to_config(self) -> HeadAndTailChatCompletionContextConfig:
        return HeadAndTailChatCompletionContextConfig(
            head_size=self._head_size, tail_size=self._tail_size, initial_messages=self._initial_messages
        )

    @classmethod
    def _from_config(cls, config: HeadAndTailChatCompletionContextConfig) -> Self:
        return cls(head_size=config.head_size, tail_size=config.tail_size, initial_messages=config.initial_messages)

from _component_config import ComponentModel
from models import ChatCompletionClient
from tools import ToolSchema

# From model_context/_token_limited_chat_completion_context.py
class TokenLimitedChatCompletionContextConfig(BaseModel):
    model_client: ComponentModel
    token_limit: int | None = None
    tool_schema: List[ToolSchema] | None = None
    initial_messages: List[LLMMessage] | None = None

# From model_context/_token_limited_chat_completion_context.py
class TokenLimitedChatCompletionContext(ChatCompletionContext, Component[TokenLimitedChatCompletionContextConfig]):
    """(Experimental) A token based chat completion context maintains a view of the context up to a token limit.

    .. note::

        Added in v0.4.10. This is an experimental component and may change in the future.

    Args:
        model_client (ChatCompletionClient): The model client to use for token counting.
            The model client must implement the :meth:`~autogen_core.models.ChatCompletionClient.count_tokens`
            and :meth:`~autogen_core.models.ChatCompletionClient.remaining_tokens` methods.
        token_limit (int | None): The maximum number of tokens to keep in the context
            using the :meth:`~autogen_core.models.ChatCompletionClient.count_tokens` method.
            If None, the context will be limited by the model client using the
            :meth:`~autogen_core.models.ChatCompletionClient.remaining_tokens` method.
        tools (List[ToolSchema] | None): A list of tool schema to use in the context.
        initial_messages (List[LLMMessage] | None): A list of initial messages to include in the context.

    """

    component_config_schema = TokenLimitedChatCompletionContextConfig
    component_provider_override = "autogen_core.model_context.TokenLimitedChatCompletionContext"

    def __init__(
        self,
        model_client: ChatCompletionClient,
        *,
        token_limit: int | None = None,
        tool_schema: List[ToolSchema] | None = None,
        initial_messages: List[LLMMessage] | None = None,
    ) -> None:
        super().__init__(initial_messages)
        if token_limit is not None and token_limit <= 0:
            raise ValueError("token_limit must be greater than 0.")
        self._token_limit = token_limit
        self._model_client = model_client
        self._tool_schema = tool_schema or []

    async def get_messages(self) -> List[LLMMessage]:
        """Get at most `token_limit` tokens in recent messages. If the token limit is not
        provided, then return as many messages as the remaining token allowed by the model client."""
        messages = list(self._messages)
        if self._token_limit is None:
            remaining_tokens = self._model_client.remaining_tokens(messages, tools=self._tool_schema)
            while remaining_tokens < 0 and len(messages) > 0:
                middle_index = len(messages) // 2
                messages.pop(middle_index)
                remaining_tokens = self._model_client.remaining_tokens(messages, tools=self._tool_schema)
        else:
            token_count = self._model_client.count_tokens(messages, tools=self._tool_schema)
            while token_count > self._token_limit and len(messages) > 0:
                middle_index = len(messages) // 2
                messages.pop(middle_index)
                token_count = self._model_client.count_tokens(messages, tools=self._tool_schema)
        if messages and isinstance(messages[0], FunctionExecutionResultMessage):
            # Handle the first message is a function call result message.
            # Remove the first message from the list.
            messages = messages[1:]
        return messages

    def _to_config(self) -> TokenLimitedChatCompletionContextConfig:
        return TokenLimitedChatCompletionContextConfig(
            model_client=self._model_client.dump_component(),
            token_limit=self._token_limit,
            tool_schema=self._tool_schema,
            initial_messages=self._initial_messages,
        )

    @classmethod
    def _from_config(cls, config: TokenLimitedChatCompletionContextConfig) -> Self:
        return cls(
            model_client=ChatCompletionClient.load_component(config.model_client),
            token_limit=config.token_limit,
            tool_schema=config.tool_schema,
            initial_messages=config.initial_messages,
        )


# From model_context/_buffered_chat_completion_context.py
class BufferedChatCompletionContextConfig(BaseModel):
    buffer_size: int
    initial_messages: List[LLMMessage] | None = None

# From model_context/_buffered_chat_completion_context.py
class BufferedChatCompletionContext(ChatCompletionContext, Component[BufferedChatCompletionContextConfig]):
    """A buffered chat completion context that keeps a view of the last n messages,
    where n is the buffer size. The buffer size is set at initialization.

    Args:
        buffer_size (int): The size of the buffer.
        initial_messages (List[LLMMessage] | None): The initial messages.
    """

    component_config_schema = BufferedChatCompletionContextConfig
    component_provider_override = "autogen_core.model_context.BufferedChatCompletionContext"

    def __init__(self, buffer_size: int, initial_messages: List[LLMMessage] | None = None) -> None:
        super().__init__(initial_messages)
        if buffer_size <= 0:
            raise ValueError("buffer_size must be greater than 0.")
        self._buffer_size = buffer_size

    async def get_messages(self) -> List[LLMMessage]:
        """Get at most `buffer_size` recent messages."""
        messages = self._messages[-self._buffer_size :]
        # Handle the first message is a function call result message.
        if messages and isinstance(messages[0], FunctionExecutionResultMessage):
            # Remove the first message from the list.
            messages = messages[1:]
        return messages

    def _to_config(self) -> BufferedChatCompletionContextConfig:
        return BufferedChatCompletionContextConfig(
            buffer_size=self._buffer_size, initial_messages=self._initial_messages
        )

    @classmethod
    def _from_config(cls, config: BufferedChatCompletionContextConfig) -> Self:
        return cls(**config.model_dump())

from _component_config import ComponentBase

# From model_context/_chat_completion_context.py
class ChatCompletionContext(ABC, ComponentBase[BaseModel]):
    """An abstract base class for defining the interface of a chat completion context.
    A chat completion context lets agents store and retrieve LLM messages.
    It can be implemented with different recall strategies.

    Args:
        initial_messages (List[LLMMessage] | None): The initial messages.

    Example:

        To create a custom model context that filters out the thought field from AssistantMessage.
        This is useful for reasoning models like DeepSeek R1, which produces
        very long thought that is not needed for subsequent completions.

        .. code-block:: python

            from typing import List

            from autogen_core.model_context import UnboundedChatCompletionContext
            from autogen_core.models import AssistantMessage, LLMMessage


            class ReasoningModelContext(UnboundedChatCompletionContext):
                \"\"\"A model context for reasoning models.\"\"\"

                async def get_messages(self) -> List[LLMMessage]:
                    messages = await super().get_messages()
                    # Filter out thought field from AssistantMessage.
                    messages_out: List[LLMMessage] = []
                    for message in messages:
                        if isinstance(message, AssistantMessage):
                            message.thought = None
                        messages_out.append(message)
                    return messages_out

    """

    component_type = "chat_completion_context"

    def __init__(self, initial_messages: List[LLMMessage] | None = None) -> None:
        self._messages: List[LLMMessage] = []
        if initial_messages is not None:
            self._messages.extend(initial_messages)
        self._initial_messages = initial_messages

    async def add_message(self, message: LLMMessage) -> None:
        """Add a message to the context."""
        self._messages.append(message)

    @abstractmethod
    async def get_messages(self) -> List[LLMMessage]: ...

    async def clear(self) -> None:
        """Clear the context."""
        self._messages = []

    async def save_state(self) -> Mapping[str, Any]:
        return ChatCompletionContextState(messages=self._messages).model_dump()

    async def load_state(self, state: Mapping[str, Any]) -> None:
        self._messages = ChatCompletionContextState.model_validate(state).messages

# From model_context/_chat_completion_context.py
class ChatCompletionContextState(BaseModel):
    messages: List[LLMMessage] = Field(default_factory=list)


# From model_context/_unbounded_chat_completion_context.py
class UnboundedChatCompletionContextConfig(BaseModel):
    initial_messages: List[LLMMessage] | None = None

# From model_context/_unbounded_chat_completion_context.py
class UnboundedChatCompletionContext(ChatCompletionContext, Component[UnboundedChatCompletionContextConfig]):
    """An unbounded chat completion context that keeps a view of the all the messages."""

    component_config_schema = UnboundedChatCompletionContextConfig
    component_provider_override = "autogen_core.model_context.UnboundedChatCompletionContext"

    async def get_messages(self) -> List[LLMMessage]:
        """Get at most `buffer_size` recent messages."""
        return self._messages

    def _to_config(self) -> UnboundedChatCompletionContextConfig:
        return UnboundedChatCompletionContextConfig(initial_messages=self._initial_messages)

    @classmethod
    def _from_config(cls, config: UnboundedChatCompletionContextConfig) -> Self:
        return cls(initial_messages=config.initial_messages)

import builtins
from _base import BaseTool
from _base import StreamTool
from _base import ToolOverride
from _base import ToolSchema
from _workbench import StreamWorkbench
from _workbench import TextResultContent
from _workbench import ToolResult
from _workbench import Workbench

# From tools/_static_workbench.py
class StaticWorkbenchConfig(BaseModel):
    tools: List[ComponentModel] = []
    tool_overrides: Dict[str, ToolOverride] = Field(default_factory=dict)

# From tools/_static_workbench.py
class StateicWorkbenchState(BaseModel):
    type: Literal["StaticWorkbenchState"] = "StaticWorkbenchState"
    tools: Dict[str, Mapping[str, Any]] = {}

# From tools/_static_workbench.py
class StaticWorkbench(Workbench, Component[StaticWorkbenchConfig]):
    """
    A workbench that provides a static set of tools that do not change after
    each tool execution.

    Args:
        tools (List[BaseTool[Any, Any]]): A list of tools to be included in the workbench.
            The tools should be subclasses of :class:`~autogen_core.tools.BaseTool`.
        tool_overrides (Optional[Dict[str, ToolOverride]]): Optional mapping of original tool
            names to override configurations for name and/or description. This allows
            customizing how tools appear to consumers while maintaining the underlying
            tool functionality.
    """

    component_provider_override = "autogen_core.tools.StaticWorkbench"
    component_config_schema = StaticWorkbenchConfig

    def __init__(
        self, tools: List[BaseTool[Any, Any]], tool_overrides: Optional[Dict[str, ToolOverride]] = None
    ) -> None:
        self._tools = tools
        self._tool_overrides = tool_overrides or {}

        # Build reverse mapping from override names to original names for call_tool
        self._override_name_to_original: Dict[str, str] = {}
        existing_tool_names = {tool.name for tool in self._tools}

        for original_name, override in self._tool_overrides.items():
            if override.name and override.name != original_name:
                # Check for conflicts with existing tool names
                if override.name in existing_tool_names and override.name != original_name:
                    raise ValueError(
                        f"Tool override name '{override.name}' conflicts with existing tool name. "
                        f"Override names must not conflict with any tool names."
                    )
                # Check for conflicts with other override names
                if override.name in self._override_name_to_original:
                    existing_original = self._override_name_to_original[override.name]
                    raise ValueError(
                        f"Tool override name '{override.name}' is used by multiple tools: "
                        f"'{existing_original}' and '{original_name}'. Override names must be unique."
                    )
                self._override_name_to_original[override.name] = original_name

    async def list_tools(self) -> List[ToolSchema]:
        result_schemas: List[ToolSchema] = []
        for tool in self._tools:
            original_schema = tool.schema

            # Apply overrides if they exist for this tool
            if tool.name in self._tool_overrides:
                override = self._tool_overrides[tool.name]
                # Create a new ToolSchema with overrides applied
                schema: ToolSchema = {
                    "name": override.name if override.name is not None else original_schema["name"],
                    "description": override.description
                    if override.description is not None
                    else original_schema.get("description", ""),
                }
                # Copy optional fields
                if "parameters" in original_schema:
                    schema["parameters"] = original_schema["parameters"]
                if "strict" in original_schema:
                    schema["strict"] = original_schema["strict"]
            else:
                schema = original_schema

            result_schemas.append(schema)
        return result_schemas

    async def call_tool(
        self,
        name: str,
        arguments: Mapping[str, Any] | None = None,
        cancellation_token: CancellationToken | None = None,
        call_id: str | None = None,
    ) -> ToolResult:
        # Check if the name is an override name and map it back to the original
        original_name = self._override_name_to_original.get(name, name)

        tool = next((tool for tool in self._tools if tool.name == original_name), None)
        if tool is None:
            return ToolResult(
                name=name,  # Return the requested name (which might be overridden)
                result=[TextResultContent(content=f"Tool {name} not found.")],
                is_error=True,
            )
        if not cancellation_token:
            cancellation_token = CancellationToken()
        if not arguments:
            arguments = {}
        try:
            result_future = asyncio.ensure_future(tool.run_json(arguments, cancellation_token, call_id=call_id))
            cancellation_token.link_future(result_future)
            actual_tool_output = await result_future
            is_error = False
            result_str = tool.return_value_as_string(actual_tool_output)
        except Exception as e:
            result_str = self._format_errors(e)
            is_error = True
        return ToolResult(name=name, result=[TextResultContent(content=result_str)], is_error=is_error)

    async def start(self) -> None:
        return None

    async def stop(self) -> None:
        return None

    async def reset(self) -> None:
        return None

    async def save_state(self) -> Mapping[str, Any]:
        tool_states = StateicWorkbenchState()
        for tool in self._tools:
            tool_states.tools[tool.name] = await tool.save_state_json()
        return tool_states.model_dump()

    async def load_state(self, state: Mapping[str, Any]) -> None:
        parsed_state = StateicWorkbenchState.model_validate(state)
        for tool in self._tools:
            if tool.name in parsed_state.tools:
                await tool.load_state_json(parsed_state.tools[tool.name])

    def _to_config(self) -> StaticWorkbenchConfig:
        return StaticWorkbenchConfig(
            tools=[tool.dump_component() for tool in self._tools], tool_overrides=self._tool_overrides
        )

    @classmethod
    def _from_config(cls, config: StaticWorkbenchConfig) -> Self:
        return cls(tools=[BaseTool.load_component(tool) for tool in config.tools], tool_overrides=config.tool_overrides)

    def _format_errors(self, error: Exception) -> str:
        """Recursively format errors into a string."""

        error_message = ""
        if hasattr(builtins, "ExceptionGroup") and isinstance(error, builtins.ExceptionGroup):
            # ExceptionGroup is available in Python 3.11+.
            # TODO: how to make this compatible with Python 3.10?
            for sub_exception in error.exceptions:  # type: ignore
                error_message += self._format_errors(sub_exception)  # type: ignore
        else:
            error_message += f"{str(error)}\n"
        return error_message.strip()

# From tools/_static_workbench.py
class StaticStreamWorkbench(StaticWorkbench, StreamWorkbench):
    """
    A workbench that provides a static set of tools that do not change after
    each tool execution, and supports streaming results.
    """

    component_provider_override = "autogen_core.tools.StaticStreamWorkbench"

    async def call_tool_stream(
        self,
        name: str,
        arguments: Mapping[str, Any] | None = None,
        cancellation_token: CancellationToken | None = None,
        call_id: str | None = None,
    ) -> AsyncGenerator[Any | ToolResult, None]:
        tool = next((tool for tool in self._tools if tool.name == name), None)
        if tool is None:
            yield ToolResult(
                name=name,
                result=[TextResultContent(content=f"Tool {name} not found.")],
                is_error=True,
            )
            return
        if not cancellation_token:
            cancellation_token = CancellationToken()
        if not arguments:
            arguments = {}
        try:
            actual_tool_output: Any | None = None
            if isinstance(tool, StreamTool):
                previous_result: Any | None = None
                try:
                    async for result in tool.run_json_stream(arguments, cancellation_token, call_id=call_id):
                        if previous_result is not None:
                            yield previous_result
                        previous_result = result
                    actual_tool_output = previous_result
                except Exception as e:
                    # If there was a previous result before the exception, yield it first
                    if previous_result is not None:
                        yield previous_result
                    # Then yield the error result
                    result_str = self._format_errors(e)
                    yield ToolResult(name=tool.name, result=[TextResultContent(content=result_str)], is_error=True)
                    return
            else:
                # If the tool is not a stream tool, we run it normally and yield the result
                result_future = asyncio.ensure_future(tool.run_json(arguments, cancellation_token, call_id=call_id))
                cancellation_token.link_future(result_future)
                actual_tool_output = await result_future
            is_error = False
            result_str = tool.return_value_as_string(actual_tool_output)
        except Exception as e:
            result_str = self._format_errors(e)
            is_error = True
        yield ToolResult(name=tool.name, result=[TextResultContent(content=result_str)], is_error=is_error)

from types import TracebackType
from _image import Image

# From tools/_workbench.py
class TextResultContent(BaseModel):
    """
    Text result content of a tool execution.
    """

    type: Literal["TextResultContent"] = "TextResultContent"

    content: str
    """The text content of the result."""

# From tools/_workbench.py
class ImageResultContent(BaseModel):
    """
    Image result content of a tool execution.
    """

    type: Literal["ImageResultContent"] = "ImageResultContent"

    content: Image
    """The image content of the result."""

# From tools/_workbench.py
class ToolResult(BaseModel):
    """
    A result of a tool execution by a workbench.
    """

    type: Literal["ToolResult"] = "ToolResult"

    name: str
    """The name of the tool that was executed."""

    result: List[ResultContent]
    """The result of the tool execution."""

    is_error: bool = False
    """Whether the tool execution resulted in an error."""

    def to_text(self, replace_image: str | None = None) -> str:
        """
        Convert the result to a text string.

        Args:
            replace_image (str | None): The string to replace the image content with.
                If None, the image content will be included in the text as base64 string.

        Returns:
            str: The text representation of the result.
        """
        parts: List[str] = []
        for content in self.result:
            if isinstance(content, TextResultContent):
                parts.append(content.content)
            elif isinstance(content, ImageResultContent):
                if replace_image is not None:
                    parts.append(replace_image)
                else:
                    parts.append(f"[Image: {content.content.to_base64()}]")
        return "\n".join(parts)

# From tools/_workbench.py
class Workbench(ABC, ComponentBase[BaseModel]):
    """
    A workbench is a component that provides a set of tools that may share
    resources and state.

    A workbench is responsible for managing the lifecycle of the tools and
    providing a single interface to call them. The tools provided by the workbench
    may be dynamic and their availabilities may change after each tool execution.

    A workbench can be started by calling the :meth:`~autogen_core.tools.Workbench.start` method
    and stopped by calling the :meth:`~autogen_core.tools.Workbench.stop` method.
    It can also be used as an asynchronous context manager, which will automatically
    start and stop the workbench when entering and exiting the context.
    """

    component_type = "workbench"

    @abstractmethod
    async def list_tools(self) -> List[ToolSchema]:
        """
        List the currently available tools in the workbench as :class:`ToolSchema`
        objects.

        The list of tools may be dynamic, and their content may change after
        tool execution.
        """
        ...

    @abstractmethod
    async def call_tool(
        self,
        name: str,
        arguments: Mapping[str, Any] | None = None,
        cancellation_token: CancellationToken | None = None,
        call_id: str | None = None,
    ) -> ToolResult:
        """
        Call a tool in the workbench.

        Args:
            name (str): The name of the tool to call.
            arguments (Mapping[str, Any] | None): The arguments to pass to the tool.
                If None, the tool will be called with no arguments.
            cancellation_token (CancellationToken | None): An optional cancellation token
                to cancel the tool execution.
            call_id (str | None): An optional identifier for the tool call, used for tracing.
        Returns:
            ToolResult: The result of the tool execution.
        """
        ...

    @abstractmethod
    async def start(self) -> None:
        """
        Start the workbench and initialize any resources.

        This method should be called before using the workbench.
        """
        ...

    @abstractmethod
    async def stop(self) -> None:
        """
        Stop the workbench and release any resources.

        This method should be called when the workbench is no longer needed.
        """
        ...

    @abstractmethod
    async def reset(self) -> None:
        """
        Reset the workbench to its initialized, started state.
        """
        ...

    @abstractmethod
    async def save_state(self) -> Mapping[str, Any]:
        """
        Save the state of the workbench.

        This method should be called to persist the state of the workbench.
        """
        ...

    @abstractmethod
    async def load_state(self, state: Mapping[str, Any]) -> None:
        """
        Load the state of the workbench.

        Args:
            state (Mapping[str, Any]): The state to load into the workbench.
        """
        ...

    async def __aenter__(self) -> Self:
        """
        Enter the workbench context manager.

        This method is called when the workbench is used in a `with` statement.
        It calls the :meth:`~autogen_core.tools.WorkBench.start` method to start the workbench.
        """
        await self.start()
        return self

    async def __aexit__(
        self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]
    ) -> None:
        """
        Exit the workbench context manager.
        This method is called when the workbench is used in a `with` statement.
        It calls the :meth:`~autogen_core.tools.WorkBench.stop` method to stop the workbench.
        """
        await self.stop()

# From tools/_workbench.py
class StreamWorkbench(Workbench, ABC):
    """A workbench that supports streaming results from tool calls."""

    @abstractmethod
    def call_tool_stream(
        self,
        name: str,
        arguments: Mapping[str, Any] | None = None,
        cancellation_token: CancellationToken | None = None,
        call_id: str | None = None,
    ) -> AsyncGenerator[Any | ToolResult, None]:
        """
        Call a tool in the workbench and return a stream of results.

        Args:
            name (str): The name of the tool to call.
            arguments (Mapping[str, Any] | None): The arguments to pass to the tool
                If None, the tool will be called with no arguments.
            cancellation_token (CancellationToken | None): An optional cancellation token
                to cancel the tool execution.
            call_id (str | None): An optional identifier for the tool call, used for tracing.
        """
        ...

# From tools/_workbench.py
def call_tool_stream(
        self,
        name: str,
        arguments: Mapping[str, Any] | None = None,
        cancellation_token: CancellationToken | None = None,
        call_id: str | None = None,
    ) -> AsyncGenerator[Any | ToolResult, None]:
        """
        Call a tool in the workbench and return a stream of results.

        Args:
            name (str): The name of the tool to call.
            arguments (Mapping[str, Any] | None): The arguments to pass to the tool
                If None, the tool will be called with no arguments.
            cancellation_token (CancellationToken | None): An optional cancellation token
                to cancel the tool execution.
            call_id (str | None): An optional identifier for the tool call, used for tracing.
        """
        ...

import functools
from textwrap import dedent
from  import CancellationToken
from _function_utils import args_base_model_from_signature
from _function_utils import get_typed_signature
from code_executor._func_with_reqs import Import
from code_executor._func_with_reqs import import_to_str
from code_executor._func_with_reqs import to_code

# From tools/_function_tool.py
class FunctionToolConfig(BaseModel):
    """Configuration for a function tool."""

    source_code: str
    name: str
    description: str
    global_imports: Sequence[Import]
    has_cancellation_support: bool

# From tools/_function_tool.py
class FunctionTool(BaseTool[BaseModel, BaseModel], Component[FunctionToolConfig]):
    """
    Create custom tools by wrapping standard Python functions.

    `FunctionTool` offers an interface for executing Python functions either asynchronously or synchronously.
    Each function must include type annotations for all parameters and its return type. These annotations
    enable `FunctionTool` to generate a schema necessary for input validation, serialization, and for informing
    the LLM about expected parameters. When the LLM prepares a function call, it leverages this schema to
    generate arguments that align with the function's specifications.

    .. note::

        It is the user's responsibility to verify that the tool's output type matches the expected type.

    Args:
        func (Callable[..., ReturnT | Awaitable[ReturnT]]): The function to wrap and expose as a tool.
        description (str): A description to inform the model of the function's purpose, specifying what
            it does and the context in which it should be called.
        name (str, optional): An optional custom name for the tool. Defaults to
            the function's original name if not provided.
        strict (bool, optional): If set to True, the tool schema will only contain arguments that are explicitly
            defined in the function signature, and no default values will be allowed. Defaults to False.
            This is required to be set to True when used with models in structured output mode.

    Example:

        .. code-block:: python

            import random
            from autogen_core import CancellationToken
            from autogen_core.tools import FunctionTool
            from typing_extensions import Annotated
            import asyncio


            async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
                # Simulates a stock price retrieval by returning a random float within a specified range.
                return random.uniform(10, 200)


            async def example():
                # Initialize a FunctionTool instance for retrieving stock prices.
                stock_price_tool = FunctionTool(get_stock_price, description="Fetch the stock price for a given ticker.")

                # Execute the tool with cancellation support.
                cancellation_token = CancellationToken()
                result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

                # Output the result as a formatted string.
                print(stock_price_tool.return_value_as_string(result))


            asyncio.run(example())
    """

    component_provider_override = "autogen_core.tools.FunctionTool"
    component_config_schema = FunctionToolConfig

    def __init__(
        self,
        func: Callable[..., Any],
        description: str,
        name: str | None = None,
        global_imports: Sequence[Import] = [],
        strict: bool = False,
    ) -> None:
        self._func = func
        self._global_imports = global_imports
        self._signature = get_typed_signature(func)
        func_name = name or func.func.__name__ if isinstance(func, functools.partial) else name or func.__name__
        args_model = args_base_model_from_signature(func_name + "args", self._signature)
        self._has_cancellation_support = "cancellation_token" in self._signature.parameters
        return_type = self._signature.return_annotation
        super().__init__(args_model, return_type, func_name, description, strict)

    async def run(self, args: BaseModel, cancellation_token: CancellationToken) -> Any:
        kwargs = {}

        for name in self._signature.parameters.keys():
            if hasattr(args, name):
                kwargs[name] = getattr(args, name)

        if asyncio.iscoroutinefunction(self._func):
            if self._has_cancellation_support:
                result = await self._func(**kwargs, cancellation_token=cancellation_token)
            else:
                result = await self._func(**kwargs)
        else:
            if self._has_cancellation_support:
                result = await asyncio.get_event_loop().run_in_executor(
                    None,
                    functools.partial(
                        self._func,
                        **kwargs,
                        cancellation_token=cancellation_token,
                    ),
                )
            else:
                future = asyncio.get_event_loop().run_in_executor(None, functools.partial(self._func, **kwargs))
                cancellation_token.link_future(future)
                result = await future

        return result

    def _to_config(self) -> FunctionToolConfig:
        return FunctionToolConfig(
            source_code=dedent(to_code(self._func)),
            global_imports=self._global_imports,
            name=self.name,
            description=self.description,
            has_cancellation_support=self._has_cancellation_support,
        )

    @classmethod
    def _from_config(cls, config: FunctionToolConfig) -> Self:
        warnings.warn(
            "\n⚠️  SECURITY WARNING ⚠️\n"
            "Loading a FunctionTool from config will execute code to import the provided global imports and and function code.\n"
            "Only load configs from TRUSTED sources to prevent arbitrary code execution.",
            UserWarning,
            stacklevel=2,
        )

        exec_globals: dict[str, Any] = {}

        # Execute imports first
        for import_stmt in config.global_imports:
            import_code = import_to_str(import_stmt)
            try:
                exec(import_code, exec_globals)
            except ModuleNotFoundError as e:
                raise ModuleNotFoundError(
                    f"Failed to import {import_code}: Module not found. Please ensure the module is installed."
                ) from e
            except ImportError as e:
                raise ImportError(f"Failed to import {import_code}: {str(e)}") from e
            except Exception as e:
                raise RuntimeError(f"Unexpected error while importing {import_code}: {str(e)}") from e

        # Execute function code
        try:
            exec(config.source_code, exec_globals)
            func_name = config.source_code.split("def ")[1].split("(")[0]
        except Exception as e:
            raise ValueError(f"Could not compile and load function: {e}") from e

        # Get function and verify it's callable
        func: Callable[..., Any] = exec_globals[func_name]
        if not callable(func):
            raise TypeError(f"Expected function but got {type(func)}")

        return cls(func, name=config.name, description=config.description, global_imports=config.global_imports)

from dataclasses import field
from importlib.abc import SourceLoader
from importlib.util import module_from_spec
from importlib.util import spec_from_loader
from textwrap import indent
from typing_extensions import ParamSpec

# From code_executor/_func_with_reqs.py
class Alias:
    name: str
    alias: str

# From code_executor/_func_with_reqs.py
class ImportFromModule:
    module: str
    imports: Tuple[Union[str, Alias], ...]

    # backward compatibility
    def __init__(
        self,
        module: str,
        imports: Union[Tuple[Union[str, Alias], ...], List[Union[str, Alias]]],
    ):
        object.__setattr__(self, "module", module)
        if isinstance(imports, list):
            object.__setattr__(self, "imports", tuple(imports))
        else:
            object.__setattr__(self, "imports", imports)

# From code_executor/_func_with_reqs.py
class _StringLoader(SourceLoader):
    def __init__(self, data: str):
        self.data = data

    def get_source(self, fullname: str) -> str:
        return self.data

    def get_data(self, path: str) -> bytes:
        return self.data.encode("utf-8")

    def get_filename(self, fullname: str) -> str:
        return "<not a real path>/" + fullname + ".py"

# From code_executor/_func_with_reqs.py
class FunctionWithRequirementsStr:
    func: str
    compiled_func: Callable[..., Any]
    _func_name: str
    python_packages: Sequence[str] = field(default_factory=list)
    global_imports: Sequence[Import] = field(default_factory=list)

    def __init__(self, func: str, python_packages: Sequence[str] = [], global_imports: Sequence[Import] = []):
        self.func = func
        self.python_packages = python_packages
        self.global_imports = global_imports

        module_name = "func_module"
        loader = _StringLoader(func)
        spec = spec_from_loader(module_name, loader)
        if spec is None:
            raise ValueError("Could not create spec")
        module = module_from_spec(spec)
        if spec.loader is None:
            raise ValueError("Could not create loader")

        try:
            spec.loader.exec_module(module)
        except Exception as e:
            raise ValueError(f"Could not compile function: {e}") from e

        functions = inspect.getmembers(module, inspect.isfunction)
        if len(functions) != 1:
            raise ValueError("The string must contain exactly one function")

        self._func_name, self.compiled_func = functions[0]

    def __call__(self, *args: Any, **kwargs: Any) -> None:
        raise NotImplementedError("String based function with requirement objects are not directly callable")

# From code_executor/_func_with_reqs.py
class FunctionWithRequirements(Generic[T, P]):
    func: Callable[P, T]
    python_packages: Sequence[str] = field(default_factory=list)
    global_imports: Sequence[Import] = field(default_factory=list)

    @classmethod
    def from_callable(
        cls, func: Callable[P, T], python_packages: Sequence[str] = [], global_imports: Sequence[Import] = []
    ) -> FunctionWithRequirements[T, P]:
        return cls(python_packages=python_packages, global_imports=global_imports, func=func)

    @staticmethod
    def from_str(
        func: str, python_packages: Sequence[str] = [], global_imports: Sequence[Import] = []
    ) -> FunctionWithRequirementsStr:
        return FunctionWithRequirementsStr(func=func, python_packages=python_packages, global_imports=global_imports)

    # Type this based on F
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> T:
        return self.func(*args, **kwargs)

# From code_executor/_func_with_reqs.py
def with_requirements(
    python_packages: Sequence[str] = [], global_imports: Sequence[Import] = []
) -> Callable[[Callable[P, T]], FunctionWithRequirements[T, P]]:
    """
    Decorate a function with package and import requirements for code execution environments.

    This decorator makes a function available for reference in dynamically executed code blocks
    by wrapping it in a `FunctionWithRequirements` object that tracks its dependencies. When the
    decorated function is passed to a code executor, it can be imported by name in the executed
    code, with all dependencies automatically handled.

    Args:
        python_packages (Sequence[str], optional): Python packages required by the function.
            Can include version specifications (e.g., ["pandas>=1.0.0"]). Defaults to [].
        global_imports (Sequence[Import], optional): Import statements required by the function.
            Can be strings ("numpy"), ImportFromModule objects, or Alias objects. Defaults to [].

    Returns:
        Callable[[Callable[P, T]], FunctionWithRequirements[T, P]]: A decorator that wraps
            the target function, preserving its functionality while registering its dependencies.

    Example:

        .. code-block:: python

            import tempfile
            import asyncio
            from autogen_core import CancellationToken
            from autogen_core.code_executor import with_requirements, CodeBlock
            from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
            import pandas

            @with_requirements(python_packages=["pandas"], global_imports=["pandas"])
            def load_data() -> pandas.DataFrame:
                \"\"\"Load some sample data.

                Returns:
                    pandas.DataFrame: A DataFrame with sample data
                \"\"\"
                data = {
                    "name": ["John", "Anna", "Peter", "Linda"],
                    "location": ["New York", "Paris", "Berlin", "London"],
                    "age": [24, 13, 53, 33],
                }
                return pandas.DataFrame(data)

            async def run_example():
                # The decorated function can be used in executed code
                with tempfile.TemporaryDirectory() as temp_dir:
                    executor = LocalCommandLineCodeExecutor(work_dir=temp_dir, functions=[load_data])
                    code = f\"\"\"from {executor.functions_module} import load_data

                    # Use the imported function
                    data = load_data()
                    print(data['name'][0])\"\"\"

                    result = await executor.execute_code_blocks(
                        code_blocks=[CodeBlock(language="python", code=code)],
                        cancellation_token=CancellationToken(),
                    )
                    print(result.output)  # Output: John

            # Run the async example
            asyncio.run(run_example())
    """

    def wrapper(func: Callable[P, T]) -> FunctionWithRequirements[T, P]:
        func_with_reqs = FunctionWithRequirements(
            python_packages=python_packages, global_imports=global_imports, func=func
        )

        functools.update_wrapper(func_with_reqs, func)
        return func_with_reqs

    return wrapper

# From code_executor/_func_with_reqs.py
def build_python_functions_file(
    funcs: Sequence[Union[FunctionWithRequirements[Any, P], Callable[..., Any], FunctionWithRequirementsStr]],
) -> str:
    """:meta private:"""
    # First collect all global imports
    global_imports: Set[Import] = set()
    for func in funcs:
        if isinstance(func, (FunctionWithRequirements, FunctionWithRequirementsStr)):
            global_imports.update(func.global_imports)

    content = "\n".join(map(_import_to_str, global_imports)) + "\n\n"

    for func in funcs:
        content += _to_code(func) + "\n\n"

    return content

# From code_executor/_func_with_reqs.py
def to_stub(func: Union[Callable[..., Any], FunctionWithRequirementsStr]) -> str:
    """Generate a stub for a function as a string

    Args:
        func (Callable[..., Any]): The function to generate a stub for

    Returns:
        str: The stub for the function
    """
    if isinstance(func, FunctionWithRequirementsStr):
        return to_stub(func.compiled_func)

    content = f"def {func.__name__}{inspect.signature(func)}:\n"
    docstring = func.__doc__

    if docstring:
        docstring = dedent(docstring)
        docstring = '"""' + docstring + '"""'
        docstring = indent(docstring, "    ")
        content += docstring + "\n"

    content += "    ..."
    return content

# From code_executor/_func_with_reqs.py
def to_code(func: Union[FunctionWithRequirements[T, P], Callable[P, T], FunctionWithRequirementsStr]) -> str:
    return _to_code(func)

# From code_executor/_func_with_reqs.py
def import_to_str(im: Import) -> str:
    return _import_to_str(im)

# From code_executor/_func_with_reqs.py
def get_source(self, fullname: str) -> str:
        return self.data

# From code_executor/_func_with_reqs.py
def get_data(self, path: str) -> bytes:
        return self.data.encode("utf-8")

# From code_executor/_func_with_reqs.py
def get_filename(self, fullname: str) -> str:
        return "<not a real path>/" + fullname + ".py"

# From code_executor/_func_with_reqs.py
def from_callable(
        cls, func: Callable[P, T], python_packages: Sequence[str] = [], global_imports: Sequence[Import] = []
    ) -> FunctionWithRequirements[T, P]:
        return cls(python_packages=python_packages, global_imports=global_imports, func=func)

# From code_executor/_func_with_reqs.py
def wrapper(func: Callable[P, T]) -> FunctionWithRequirements[T, P]:
        func_with_reqs = FunctionWithRequirements(
            python_packages=python_packages, global_imports=global_imports, func=func
        )

        functools.update_wrapper(func_with_reqs, func)
        return func_with_reqs

# From code_executor/_func_with_reqs.py
def to_str(i: Union[str, Alias]) -> str:
            if isinstance(i, str):
                return i
            else:
                return f"{i.name} as {i.alias}"


# From code_executor/_base.py
class CodeBlock:
    """A code block extracted fromm an agent message."""

    code: str
    language: str

# From code_executor/_base.py
class CodeResult:
    """Result of a code execution."""

    exit_code: int
    output: str

# From code_executor/_base.py
class CodeExecutor(ABC, ComponentBase[BaseModel]):
    """Executes code blocks and returns the result.

    This is an abstract base class for code executors. It defines the interface
    for executing code blocks and returning the result. A concrete implementation
    of this class should be provided to execute code blocks in a specific
    environment. For example, :class:`~autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor` executes
    code blocks in a command line environment in a Docker container.

    It is recommended for subclass to be used as a context manager to ensure
    that resources are cleaned up properly. To do this, implement the
    :meth:`~autogen_core.code_executor.CodeExecutor.start` and
    :meth:`~autogen_core.code_executor.CodeExecutor.stop` methods
    that will be called when entering and exiting the context manager.

    """

    component_type = "code_executor"

    @abstractmethod
    async def execute_code_blocks(
        self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken
    ) -> CodeResult:
        """Execute code blocks and return the result.

        This method should be implemented by the code executor.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            CodeResult: The result of the code execution.

        Raises:
            ValueError: Errors in user inputs
            asyncio.TimeoutError: Code execution timeouts
            asyncio.CancelledError: CancellationToken evoked during execution
        """
        ...

    @abstractmethod
    async def start(self) -> None:
        """Start the code executor."""
        ...

    @abstractmethod
    async def stop(self) -> None:
        """Stop the code executor and release any resources."""
        ...

    @abstractmethod
    async def restart(self) -> None:
        """Restart the code executor.

        This method should be implemented by the code executor.

        This method is called when the agent is reset.
        """
        ...

    async def __aenter__(self) -> Self:
        await self.start()
        return self

    async def __aexit__(
        self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]
    ) -> Optional[bool]:
        await self.stop()
        return None

import datetime
from ipaddress import IPv4Address
from ipaddress import IPv6Address
from typing import ForwardRef
from pydantic import UUID1
from pydantic import UUID3
from pydantic import UUID4
from pydantic import UUID5
from pydantic import AnyUrl
from pydantic import EmailStr
from pydantic import Json
from pydantic import conbytes
from pydantic import confloat
from pydantic import conint
from pydantic import conlist
from pydantic import constr
from pydantic.fields import FieldInfo

# From utils/_json_to_pydantic.py
class SchemaConversionError(Exception):
    """Base class for schema conversion exceptions."""

    pass

# From utils/_json_to_pydantic.py
class ReferenceNotFoundError(SchemaConversionError):
    """Raised when a $ref cannot be resolved."""

    pass

# From utils/_json_to_pydantic.py
class FormatNotSupportedError(SchemaConversionError):
    """Raised when a format is not supported."""

    pass

# From utils/_json_to_pydantic.py
class UnsupportedKeywordError(SchemaConversionError):
    """Raised when an unsupported JSON Schema keyword is encountered."""

    pass

# From utils/_json_to_pydantic.py
class _JSONSchemaToPydantic:
    def __init__(self) -> None:
        self._model_cache: Dict[str, Optional[Union[Type[BaseModel], ForwardRef]]] = {}

    def _resolve_ref(self, ref: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        ref_key = ref.split("/")[-1]
        definitions = cast(dict[str, dict[str, Any]], schema.get("$defs", {}))

        if ref_key not in definitions:
            raise ReferenceNotFoundError(
                f"Reference `{ref}` not found in `$defs`. Available keys: {list(definitions.keys())}"
            )

        return definitions[ref_key]

    def get_ref(self, ref_name: str) -> Any:
        if ref_name not in self._model_cache:
            raise ReferenceNotFoundError(
                f"Reference `{ref_name}` not found in cache. Available: {list(self._model_cache.keys())}"
            )

        if self._model_cache[ref_name] is None:
            return ForwardRef(ref_name)

        return self._model_cache[ref_name]

    def _process_definitions(self, root_schema: Dict[str, Any]) -> None:
        if "$defs" in root_schema:
            for model_name in root_schema["$defs"]:
                if model_name not in self._model_cache:
                    self._model_cache[model_name] = None

            for model_name, model_schema in root_schema["$defs"].items():
                if self._model_cache[model_name] is None:
                    self._model_cache[model_name] = self.json_schema_to_pydantic(model_schema, model_name, root_schema)

    def json_schema_to_pydantic(
        self, schema: Dict[str, Any], model_name: str = "GeneratedModel", root_schema: Optional[Dict[str, Any]] = None
    ) -> Type[BaseModel]:
        if root_schema is None:
            root_schema = schema
            self._process_definitions(root_schema)

        if "$ref" in schema:
            resolved = self._resolve_ref(schema["$ref"], root_schema)
            schema = {**resolved, **{k: v for k, v in schema.items() if k != "$ref"}}

        if "allOf" in schema:
            merged: Dict[str, Any] = {"type": "object", "properties": {}, "required": []}
            for s in schema["allOf"]:
                part = self._resolve_ref(s["$ref"], root_schema) if "$ref" in s else s
                merged["properties"].update(part.get("properties", {}))
                merged["required"].extend(part.get("required", []))
            for k, v in schema.items():
                if k not in {"allOf", "properties", "required"}:
                    merged[k] = v
            merged["required"] = list(set(merged["required"]))
            schema = merged

        return self._json_schema_to_model(schema, model_name, root_schema)

    def _resolve_union_types(self, schemas: List[Dict[str, Any]]) -> List[Any]:
        types: List[Any] = []
        for s in schemas:
            if "$ref" in s:
                types.append(self.get_ref(s["$ref"].split("/")[-1]))
            elif "enum" in s:
                types.append(Literal[tuple(s["enum"])] if len(s["enum"]) > 0 else Any)
            else:
                json_type = s.get("type")
                if json_type not in TYPE_MAPPING:
                    raise UnsupportedKeywordError(f"Unsupported or missing type `{json_type}` in union")

                # Handle array types with items specification
                if json_type == "array" and "items" in s:
                    item_schema = s["items"]
                    if "$ref" in item_schema:
                        item_type = self.get_ref(item_schema["$ref"].split("/")[-1])
                    else:
                        item_type_name = item_schema.get("type")
                        if item_type_name is None:
                            item_type = str
                        elif item_type_name not in TYPE_MAPPING:
                            raise UnsupportedKeywordError(f"Unsupported item type `{item_type_name}` in union array")
                        else:
                            item_type = TYPE_MAPPING[item_type_name]

                    constraints = {}
                    if "minItems" in s:
                        constraints["min_length"] = s["minItems"]
                    if "maxItems" in s:
                        constraints["max_length"] = s["maxItems"]

                    array_type = conlist(item_type, **constraints) if constraints else List[item_type]  # type: ignore[valid-type]
                    types.append(array_type)
                else:
                    types.append(TYPE_MAPPING[json_type])
        return types

    def _extract_field_type(self, key: str, value: Dict[str, Any], model_name: str, root_schema: Dict[str, Any]) -> Any:
        json_type = value.get("type")
        if json_type not in TYPE_MAPPING:
            raise UnsupportedKeywordError(
                f"Unsupported or missing type `{json_type}` for field `{key}` in `{model_name}`"
            )

        base_type = TYPE_MAPPING[json_type]
        constraints: Dict[str, Any] = {}

        if json_type == "string":
            if "minLength" in value:
                constraints["min_length"] = value["minLength"]
            if "maxLength" in value:
                constraints["max_length"] = value["maxLength"]
            if "pattern" in value:
                constraints["pattern"] = value["pattern"]
            if constraints:
                base_type = constr(**constraints)

        elif json_type == "integer":
            if "minimum" in value:
                constraints["ge"] = value["minimum"]
            if "maximum" in value:
                constraints["le"] = value["maximum"]
            if "exclusiveMinimum" in value:
                constraints["gt"] = value["exclusiveMinimum"]
            if "exclusiveMaximum" in value:
                constraints["lt"] = value["exclusiveMaximum"]
            if constraints:
                base_type = conint(**constraints)

        elif json_type == "number":
            if "minimum" in value:
                constraints["ge"] = value["minimum"]
            if "maximum" in value:
                constraints["le"] = value["maximum"]
            if "exclusiveMinimum" in value:
                constraints["gt"] = value["exclusiveMinimum"]
            if "exclusiveMaximum" in value:
                constraints["lt"] = value["exclusiveMaximum"]
            if constraints:
                base_type = confloat(**constraints)

        elif json_type == "array":
            if "minItems" in value:
                constraints["min_length"] = value["minItems"]
            if "maxItems" in value:
                constraints["max_length"] = value["maxItems"]
            item_schema = value.get("items", {"type": "string"})
            if "$ref" in item_schema:
                item_type = self.get_ref(item_schema["$ref"].split("/")[-1])
            else:
                item_type_name = item_schema.get("type")
                if item_type_name is None:
                    item_type = str
                elif item_type_name not in TYPE_MAPPING:
                    raise UnsupportedKeywordError(
                        f"Unsupported or missing item type `{item_type_name}` for array field `{key}` in `{model_name}`"
                    )
                else:
                    item_type = TYPE_MAPPING[item_type_name]

            base_type = conlist(item_type, **constraints) if constraints else List[item_type]  # type: ignore[valid-type]

        if "format" in value:
            format_type = FORMAT_MAPPING.get(value["format"])
            if format_type is None:
                raise FormatNotSupportedError(f"Unknown format `{value['format']}` for `{key}` in `{model_name}`")
            if not isinstance(format_type, type):
                return format_type
            if not issubclass(format_type, str):
                return format_type
            return format_type

        return base_type

    def _json_schema_to_model(
        self, schema: Dict[str, Any], model_name: str, root_schema: Dict[str, Any]
    ) -> Type[BaseModel]:
        if "allOf" in schema:
            merged: Dict[str, Any] = {"type": "object", "properties": {}, "required": []}
            for s in schema["allOf"]:
                part = self._resolve_ref(s["$ref"], root_schema) if "$ref" in s else s
                merged["properties"].update(part.get("properties", {}))
                merged["required"].extend(part.get("required", []))
            for k, v in schema.items():
                if k not in {"allOf", "properties", "required"}:
                    merged[k] = v
            merged["required"] = list(set(merged["required"]))
            schema = merged

        fields: Dict[str, tuple[Any, FieldInfo]] = {}
        required_fields = set(schema.get("required", []))

        for key, value in schema.get("properties", {}).items():
            if "$ref" in value:
                ref_name = value["$ref"].split("/")[-1]
                field_type = self.get_ref(ref_name)
            elif "anyOf" in value:
                sub_models = self._resolve_union_types(value["anyOf"])
                field_type = Union[tuple(sub_models)]
            elif "oneOf" in value:
                sub_models = self._resolve_union_types(value["oneOf"])
                field_type = Union[tuple(sub_models)]
                if "discriminator" in value:
                    discriminator = value["discriminator"]["propertyName"]
                    field_type = Annotated[field_type, Field(discriminator=discriminator)]
            elif "enum" in value:
                field_type = Literal[tuple(value["enum"])]
            elif "allOf" in value:
                merged = {"type": "object", "properties": {}, "required": []}
                for s in value["allOf"]:
                    part = self._resolve_ref(s["$ref"], root_schema) if "$ref" in s else s
                    merged["properties"].update(part.get("properties", {}))
                    merged["required"].extend(part.get("required", []))
                for k, v in value.items():
                    if k not in {"allOf", "properties", "required"}:
                        merged[k] = v
                merged["required"] = list(set(merged["required"]))
                field_type = self._json_schema_to_model(merged, f"{model_name}_{key}", root_schema)
            elif value.get("type") == "object" and "properties" in value:
                field_type = self._json_schema_to_model(value, f"{model_name}_{key}", root_schema)
            else:
                field_type = self._extract_field_type(key, value, model_name, root_schema)

            if field_type is None:
                raise UnsupportedKeywordError(f"Unsupported or missing type for field `{key}` in `{model_name}`")

            default_value = value.get("default")
            is_required = key in required_fields

            if not is_required and default_value is None:
                field_type = Optional[field_type]

            field_args = {
                "default": default_value if not is_required else ...,
            }
            if "title" in value:
                field_args["title"] = value["title"]
            if "description" in value:
                field_args["description"] = value["description"]

            fields[key] = (
                field_type,
                _make_field(
                    default_value if not is_required else ...,
                    title=value.get("title"),
                    description=value.get("description"),
                ),
            )

        model: Type[BaseModel] = create_model(model_name, **cast(dict[str, Any], fields))
        model.model_rebuild()
        return model

# From utils/_json_to_pydantic.py
def schema_to_pydantic_model(schema: Dict[str, Any], model_name: str = "GeneratedModel") -> Type[BaseModel]:
    """
    Convert a JSON Schema dictionary to a fully-typed Pydantic model.

    This function handles schema translation and validation logic to produce
    a Pydantic model.

    **Supported JSON Schema Features**

    - **Primitive types**: `string`, `integer`, `number`, `boolean`, `object`, `array`, `null`
    - **String formats**:
        - `email`, `uri`, `uuid`, `uuid1`, `uuid3`, `uuid4`, `uuid5`
        - `hostname`, `ipv4`, `ipv6`, `ipv4-network`, `ipv6-network`
        - `date`, `time`, `date-time`, `duration`
        - `byte`, `binary`, `password`, `path`
    - **String constraints**:
        - `minLength`, `maxLength`, `pattern`
    - **Numeric constraints**:
        - `minimum`, `maximum`, `exclusiveMinimum`, `exclusiveMaximum`
    - **Array constraints**:
        - `minItems`, `maxItems`, `items`
    - **Object schema support**:
        - `properties`, `required`, `title`, `description`, `default`
    - **Enums**:
        - Converted to Python `Literal` type
    - **Union types**:
        - `anyOf`, `oneOf` supported with optional `discriminator`
    - **Inheritance and composition**:
        - `allOf` merges multiple schemas into one model
    - **$ref and $defs resolution**:
        - Supports references to sibling definitions and self-referencing schemas

    .. code-block:: python

        from autogen_core.utils import schema_to_pydantic_model

        # Example 1: Simple user model
        schema = {
            "title": "User",
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "email": {"type": "string", "format": "email"},
                "age": {"type": "integer", "minimum": 0},
            },
            "required": ["name", "email"],
        }

        UserModel = schema_to_pydantic_model(schema)
        user = UserModel(name="Alice", email="alice@example.com", age=30)

    .. code-block:: python

        from autogen_core.utils import schema_to_pydantic_model

        # Example 2: Nested model
        schema = {
            "title": "BlogPost",
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "tags": {"type": "array", "items": {"type": "string"}},
                "author": {
                    "type": "object",
                    "properties": {"name": {"type": "string"}, "email": {"type": "string", "format": "email"}},
                    "required": ["name"],
                },
            },
            "required": ["title", "author"],
        }

        BlogPost = schema_to_pydantic_model(schema)


    .. code-block:: python

        from autogen_core.utils import schema_to_pydantic_model

        # Example 3: allOf merging with $refs
        schema = {
            "title": "EmployeeWithDepartment",
            "allOf": [{"$ref": "#/$defs/Employee"}, {"$ref": "#/$defs/Department"}],
            "$defs": {
                "Employee": {
                    "type": "object",
                    "properties": {"id": {"type": "string"}, "name": {"type": "string"}},
                    "required": ["id", "name"],
                },
                "Department": {
                    "type": "object",
                    "properties": {"department": {"type": "string"}},
                    "required": ["department"],
                },
            },
        }

        Model = schema_to_pydantic_model(schema)

    .. code-block:: python

        from autogen_core.utils import schema_to_pydantic_model

        # Example 4: Self-referencing (recursive) model
        schema = {
            "title": "Category",
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "subcategories": {"type": "array", "items": {"$ref": "#/$defs/Category"}},
            },
            "required": ["name"],
            "$defs": {
                "Category": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "subcategories": {"type": "array", "items": {"$ref": "#/$defs/Category"}},
                    },
                    "required": ["name"],
                }
            },
        }

        Category = schema_to_pydantic_model(schema)

    .. code-block:: python

        # Example 5: Serializing and deserializing with Pydantic

        from uuid import uuid4
        from pydantic import BaseModel, EmailStr, Field
        from typing import Optional, List, Dict, Any
        from autogen_core.utils import schema_to_pydantic_model


        class Address(BaseModel):
            street: str
            city: str
            zipcode: str


        class User(BaseModel):
            id: str
            name: str
            email: EmailStr
            age: int = Field(..., ge=18)
            address: Address


        class Employee(BaseModel):
            id: str
            name: str
            manager: Optional["Employee"] = None


        class Department(BaseModel):
            name: str
            employees: List[Employee]


        class ComplexModel(BaseModel):
            user: User
            extra_info: Optional[Dict[str, Any]] = None
            sub_items: List[Employee]


        # Convert ComplexModel to JSON schema
        complex_schema = ComplexModel.model_json_schema()

        # Rebuild a new Pydantic model from JSON schema
        ReconstructedModel = schema_to_pydantic_model(complex_schema, "ComplexModel")

        # Instantiate reconstructed model
        reconstructed = ReconstructedModel(
            user={
                "id": str(uuid4()),
                "name": "Alice",
                "email": "alice@example.com",
                "age": 30,
                "address": {"street": "123 Main St", "city": "Wonderland", "zipcode": "12345"},
            },
            sub_items=[{"id": str(uuid4()), "name": "Bob", "manager": {"id": str(uuid4()), "name": "Eve"}}],
        )

        print(reconstructed.model_dump())


    Args:
        schema (Dict[str, Any]): A valid JSON Schema dictionary.
        model_name (str, optional): The name of the root model. Defaults to "GeneratedModel".

    Returns:
        Type[BaseModel]: A dynamically generated Pydantic model class.

    Raises:
        ReferenceNotFoundError: If a `$ref` key references a missing entry.
        FormatNotSupportedError: If a `format` keyword is unknown or unsupported.
        UnsupportedKeywordError: If the schema contains an unsupported `type`.

    See Also:
        - :class:`pydantic.BaseModel`
        - :func:`pydantic.create_model`
        - https://json-schema.org/
    """
    ...

    return _JSONSchemaToPydantic().json_schema_to_pydantic(schema, model_name)

# From utils/_json_to_pydantic.py
def get_ref(self, ref_name: str) -> Any:
        if ref_name not in self._model_cache:
            raise ReferenceNotFoundError(
                f"Reference `{ref_name}` not found in cache. Available: {list(self._model_cache.keys())}"
            )

        if self._model_cache[ref_name] is None:
            return ForwardRef(ref_name)

        return self._model_cache[ref_name]

# From utils/_json_to_pydantic.py
def json_schema_to_pydantic(
        self, schema: Dict[str, Any], model_name: str = "GeneratedModel", root_schema: Optional[Dict[str, Any]] = None
    ) -> Type[BaseModel]:
        if root_schema is None:
            root_schema = schema
            self._process_definitions(root_schema)

        if "$ref" in schema:
            resolved = self._resolve_ref(schema["$ref"], root_schema)
            schema = {**resolved, **{k: v for k, v in schema.items() if k != "$ref"}}

        if "allOf" in schema:
            merged: Dict[str, Any] = {"type": "object", "properties": {}, "required": []}
            for s in schema["allOf"]:
                part = self._resolve_ref(s["$ref"], root_schema) if "$ref" in s else s
                merged["properties"].update(part.get("properties", {}))
                merged["required"].extend(part.get("required", []))
            for k, v in schema.items():
                if k not in {"allOf", "properties", "required"}:
                    merged[k] = v
            merged["required"] = list(set(merged["required"]))
            schema = merged

        return self._json_schema_to_model(schema, model_name, root_schema)


# From utils/_load_json.py
def extract_json_from_str(content: str) -> List[Dict[str, Any]]:
    """Extract JSON objects from a string. Supports backtick enclosed JSON objects"""
    pattern = re.compile(r"```(?:\s*([\w\+\-]+))?\n([\s\S]*?)```")
    matches = pattern.findall(content)
    ret: List[Dict[str, Any]] = []
    # If no matches found, assume the entire content is a JSON object
    if not matches:
        ret.append(json.loads(content))
    for match in matches:
        language = match[0].strip() if match[0] else None
        if language and language.lower() != "json":
            raise ValueError(f"Expected JSON object, but found language: {language}")
        content = match[1]
        ret.append(json.loads(content))
    return ret

from  import AgentId
from  import AgentRuntime
from  import BaseAgent
from  import FunctionCall
from models import FunctionExecutionResult
from tools import Tool
from _tool_agent import ToolException

from typing import TypeAlias
from typing_extensions import Any
from typing_extensions import AsyncGenerator
from typing_extensions import Required
from typing_extensions import TypedDict
from typing_extensions import Union
from typing_extensions import deprecated
from _types import CreateResult
from _types import LLMMessage
from _types import RequestUsage

# From models/_model_client.py
class ModelFamily:
    """A model family is a group of models that share similar characteristics from a capabilities perspective. This is different to discrete supported features such as vision, function calling, and JSON output.

    This namespace class holds constants for the model families that AutoGen understands. Other families definitely exist and can be represented by a string, however, AutoGen will treat them as unknown."""

    GPT_41 = "gpt-41"
    GPT_45 = "gpt-45"
    GPT_4O = "gpt-4o"
    O1 = "o1"
    O3 = "o3"
    O4 = "o4"
    GPT_4 = "gpt-4"
    GPT_35 = "gpt-35"
    R1 = "r1"
    GEMINI_1_5_FLASH = "gemini-1.5-flash"
    GEMINI_1_5_PRO = "gemini-1.5-pro"
    GEMINI_2_0_FLASH = "gemini-2.0-flash"
    GEMINI_2_5_PRO = "gemini-2.5-pro"
    GEMINI_2_5_FLASH = "gemini-2.5-flash"
    CLAUDE_3_HAIKU = "claude-3-haiku"
    CLAUDE_3_SONNET = "claude-3-sonnet"
    CLAUDE_3_OPUS = "claude-3-opus"
    CLAUDE_3_5_HAIKU = "claude-3-5-haiku"
    CLAUDE_3_5_SONNET = "claude-3-5-sonnet"
    CLAUDE_3_7_SONNET = "claude-3-7-sonnet"
    CLAUDE_4_OPUS = "claude-4-opus"
    CLAUDE_4_SONNET = "claude-4-sonnet"
    LLAMA_3_3_8B = "llama-3.3-8b"
    LLAMA_3_3_70B = "llama-3.3-70b"
    LLAMA_4_SCOUT = "llama-4-scout"
    LLAMA_4_MAVERICK = "llama-4-maverick"
    CODESRAL = "codestral"
    OPEN_CODESRAL_MAMBA = "open-codestral-mamba"
    MISTRAL = "mistral"
    MINISTRAL = "ministral"
    PIXTRAL = "pixtral"
    UNKNOWN = "unknown"

    ANY: TypeAlias = Literal[
        # openai_models
        "gpt-41",
        "gpt-45",
        "gpt-4o",
        "o1",
        "o3",
        "o4",
        "gpt-4",
        "gpt-35",
        "r1",
        # google_models
        "gemini-1.5-flash",
        "gemini-1.5-pro",
        "gemini-2.0-flash",
        "gemini-2.5-pro",
        "gemini-2.5-flash",
        # anthropic_models
        "claude-3-haiku",
        "claude-3-sonnet",
        "claude-3-opus",
        "claude-3-5-haiku",
        "claude-3-5-sonnet",
        "claude-3-7-sonnet",
        "claude-4-opus",
        "claude-4-sonnet",
        # llama_models
        "llama-3.3-8b",
        "llama-3.3-70b",
        "llama-4-scout",
        "llama-4-maverick",
        # mistral_models
        "codestral",
        "open-codestral-mamba",
        "mistral",
        "ministral",
        "pixtral",
        # unknown
        "unknown",
    ]

    def __new__(cls, *args: Any, **kwargs: Any) -> ModelFamily:
        raise TypeError(f"{cls.__name__} is a namespace class and cannot be instantiated.")

    @staticmethod
    def is_claude(family: str) -> bool:
        return family in (
            ModelFamily.CLAUDE_3_HAIKU,
            ModelFamily.CLAUDE_3_SONNET,
            ModelFamily.CLAUDE_3_OPUS,
            ModelFamily.CLAUDE_3_5_HAIKU,
            ModelFamily.CLAUDE_3_5_SONNET,
            ModelFamily.CLAUDE_3_7_SONNET,
            ModelFamily.CLAUDE_4_OPUS,
            ModelFamily.CLAUDE_4_SONNET,
        )

    @staticmethod
    def is_gemini(family: str) -> bool:
        return family in (
            ModelFamily.GEMINI_1_5_FLASH,
            ModelFamily.GEMINI_1_5_PRO,
            ModelFamily.GEMINI_2_0_FLASH,
            ModelFamily.GEMINI_2_5_PRO,
            ModelFamily.GEMINI_2_5_FLASH,
        )

    @staticmethod
    def is_openai(family: str) -> bool:
        return family in (
            ModelFamily.GPT_45,
            ModelFamily.GPT_41,
            ModelFamily.GPT_4O,
            ModelFamily.O1,
            ModelFamily.O3,
            ModelFamily.O4,
            ModelFamily.GPT_4,
            ModelFamily.GPT_35,
        )

    @staticmethod
    def is_llama(family: str) -> bool:
        return family in (
            ModelFamily.LLAMA_3_3_8B,
            ModelFamily.LLAMA_3_3_70B,
            ModelFamily.LLAMA_4_SCOUT,
            ModelFamily.LLAMA_4_MAVERICK,
        )

    @staticmethod
    def is_mistral(family: str) -> bool:
        return family in (
            ModelFamily.CODESRAL,
            ModelFamily.OPEN_CODESRAL_MAMBA,
            ModelFamily.MISTRAL,
            ModelFamily.MINISTRAL,
            ModelFamily.PIXTRAL,
        )

# From models/_model_client.py
class ModelCapabilities(TypedDict, total=False):
    vision: Required[bool]
    function_calling: Required[bool]
    json_output: Required[bool]

# From models/_model_client.py
class ModelInfo(TypedDict, total=False):
    """ModelInfo is a dictionary that contains information about a model's properties.
    It is expected to be used in the model_info property of a model client.

    We are expecting this to grow over time as we add more features.
    """

    vision: Required[bool]
    """True if the model supports vision, aka image input, otherwise False."""
    function_calling: Required[bool]
    """True if the model supports function calling, otherwise False."""
    json_output: Required[bool]
    """True if the model supports json output, otherwise False. Note: this is different to structured json."""
    family: Required[ModelFamily.ANY | str]
    """Model family should be one of the constants from :py:class:`ModelFamily` or a string representing an unknown model family."""
    structured_output: Required[bool]
    """True if the model supports structured output, otherwise False. This is different to json_output."""
    multiple_system_messages: Optional[bool]
    """True if the model supports multiple, non-consecutive system messages, otherwise False."""

# From models/_model_client.py
class ChatCompletionClient(ComponentBase[BaseModel], ABC):
    # Caching has to be handled internally as they can depend on the create args that were stored in the constructor
    @abstractmethod
    async def create(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> CreateResult:
        """Creates a single response from the model.

        Args:
            messages (Sequence[LLMMessage]): The messages to send to the model.
            tools (Sequence[Tool | ToolSchema], optional): The tools to use with the model. Defaults to [].
            tool_choice (Tool | Literal["auto", "required", "none"], optional): A single Tool object to force the model to use, "auto" to let the model choose any available tool, "required" to force tool usage, or "none" to disable tool usage. Defaults to "auto".
            json_output (Optional[bool | type[BaseModel]], optional): Whether to use JSON mode, structured output, or neither.
                Defaults to None. If set to a `Pydantic BaseModel <https://docs.pydantic.dev/latest/usage/models/#model>`_ type,
                it will be used as the output type for structured output.
                If set to a boolean, it will be used to determine whether to use JSON mode or not.
                If set to `True`, make sure to instruct the model to produce JSON output in the instruction or prompt.
            extra_create_args (Mapping[str, Any], optional): Extra arguments to pass to the underlying client. Defaults to {}.
            cancellation_token (Optional[CancellationToken], optional): A token for cancellation. Defaults to None.

        Returns:
            CreateResult: The result of the model call.
        """
        ...

    @abstractmethod
    def create_stream(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> AsyncGenerator[Union[str, CreateResult], None]:
        """Creates a stream of string chunks from the model ending with a CreateResult.

        Args:
            messages (Sequence[LLMMessage]): The messages to send to the model.
            tools (Sequence[Tool | ToolSchema], optional): The tools to use with the model. Defaults to [].
            tool_choice (Tool | Literal["auto", "required", "none"], optional): A single Tool object to force the model to use, "auto" to let the model choose any available tool, "required" to force tool usage, or "none" to disable tool usage. Defaults to "auto".
            json_output (Optional[bool | type[BaseModel]], optional): Whether to use JSON mode, structured output, or neither.
                Defaults to None. If set to a `Pydantic BaseModel <https://docs.pydantic.dev/latest/usage/models/#model>`_ type,
                it will be used as the output type for structured output.
                If set to a boolean, it will be used to determine whether to use JSON mode or not.
                If set to `True`, make sure to instruct the model to produce JSON output in the instruction or prompt.
            extra_create_args (Mapping[str, Any], optional): Extra arguments to pass to the underlying client. Defaults to {}.
            cancellation_token (Optional[CancellationToken], optional): A token for cancellation. Defaults to None.

        Returns:
            AsyncGenerator[Union[str, CreateResult], None]: A generator that yields string chunks and ends with a :py:class:`CreateResult`.
        """
        ...

    @abstractmethod
    async def close(self) -> None: ...

    @abstractmethod
    def actual_usage(self) -> RequestUsage: ...

    @abstractmethod
    def total_usage(self) -> RequestUsage: ...

    @abstractmethod
    def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int: ...

    @abstractmethod
    def remaining_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int: ...

    # Deprecated
    @property
    @abstractmethod
    def capabilities(self) -> ModelCapabilities: ...  # type: ignore

    @property
    @abstractmethod
    def model_info(self) -> ModelInfo:
        warnings.warn(
            "Model client in use does not implement model_info property. Falling back to capabilities property. The capabilities property is deprecated and will be removed soon, please implement model_info instead in the model client class.",
            stacklevel=2,
        )
        base_info: ModelInfo = self.capabilities  # type: ignore
        base_info["family"] = ModelFamily.UNKNOWN
        return base_info

# From models/_model_client.py
def validate_model_info(model_info: ModelInfo) -> None:
    """Validates the model info dictionary.

    Raises:
        ValueError: If the model info dictionary is missing required fields.
    """
    required_fields = ["vision", "function_calling", "json_output", "family"]
    for field in required_fields:
        if field not in model_info:
            raise ValueError(
                f"Missing required field '{field}' in ModelInfo. "
                "Starting in v0.4.7, the required fields are enforced."
            )
    new_required_fields = ["structured_output"]
    for field in new_required_fields:
        if field not in model_info:
            warnings.warn(
                f"Missing required field '{field}' in ModelInfo. "
                "This field will be required in a future version of AutoGen.",
                UserWarning,
                stacklevel=2,
            )

# From models/_model_client.py
def is_claude(family: str) -> bool:
        return family in (
            ModelFamily.CLAUDE_3_HAIKU,
            ModelFamily.CLAUDE_3_SONNET,
            ModelFamily.CLAUDE_3_OPUS,
            ModelFamily.CLAUDE_3_5_HAIKU,
            ModelFamily.CLAUDE_3_5_SONNET,
            ModelFamily.CLAUDE_3_7_SONNET,
            ModelFamily.CLAUDE_4_OPUS,
            ModelFamily.CLAUDE_4_SONNET,
        )

# From models/_model_client.py
def is_gemini(family: str) -> bool:
        return family in (
            ModelFamily.GEMINI_1_5_FLASH,
            ModelFamily.GEMINI_1_5_PRO,
            ModelFamily.GEMINI_2_0_FLASH,
            ModelFamily.GEMINI_2_5_PRO,
            ModelFamily.GEMINI_2_5_FLASH,
        )

# From models/_model_client.py
def is_openai(family: str) -> bool:
        return family in (
            ModelFamily.GPT_45,
            ModelFamily.GPT_41,
            ModelFamily.GPT_4O,
            ModelFamily.O1,
            ModelFamily.O3,
            ModelFamily.O4,
            ModelFamily.GPT_4,
            ModelFamily.GPT_35,
        )

# From models/_model_client.py
def is_llama(family: str) -> bool:
        return family in (
            ModelFamily.LLAMA_3_3_8B,
            ModelFamily.LLAMA_3_3_70B,
            ModelFamily.LLAMA_4_SCOUT,
            ModelFamily.LLAMA_4_MAVERICK,
        )

# From models/_model_client.py
def is_mistral(family: str) -> bool:
        return family in (
            ModelFamily.CODESRAL,
            ModelFamily.OPEN_CODESRAL_MAMBA,
            ModelFamily.MISTRAL,
            ModelFamily.MINISTRAL,
            ModelFamily.PIXTRAL,
        )

# From models/_model_client.py
def create_stream(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> AsyncGenerator[Union[str, CreateResult], None]:
        """Creates a stream of string chunks from the model ending with a CreateResult.

        Args:
            messages (Sequence[LLMMessage]): The messages to send to the model.
            tools (Sequence[Tool | ToolSchema], optional): The tools to use with the model. Defaults to [].
            tool_choice (Tool | Literal["auto", "required", "none"], optional): A single Tool object to force the model to use, "auto" to let the model choose any available tool, "required" to force tool usage, or "none" to disable tool usage. Defaults to "auto".
            json_output (Optional[bool | type[BaseModel]], optional): Whether to use JSON mode, structured output, or neither.
                Defaults to None. If set to a `Pydantic BaseModel <https://docs.pydantic.dev/latest/usage/models/#model>`_ type,
                it will be used as the output type for structured output.
                If set to a boolean, it will be used to determine whether to use JSON mode or not.
                If set to `True`, make sure to instruct the model to produce JSON output in the instruction or prompt.
            extra_create_args (Mapping[str, Any], optional): Extra arguments to pass to the underlying client. Defaults to {}.
            cancellation_token (Optional[CancellationToken], optional): A token for cancellation. Defaults to None.

        Returns:
            AsyncGenerator[Union[str, CreateResult], None]: A generator that yields string chunks and ends with a :py:class:`CreateResult`.
        """
        ...

# From models/_model_client.py
def actual_usage(self) -> RequestUsage: ...

# From models/_model_client.py
def total_usage(self) -> RequestUsage: ...

# From models/_model_client.py
def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int: ...

# From models/_model_client.py
def remaining_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int: ...

# From models/_model_client.py
def model_info(self) -> ModelInfo:
        warnings.warn(
            "Model client in use does not implement model_info property. Falling back to capabilities property. The capabilities property is deprecated and will be removed soon, please implement model_info instead in the model client class.",
            stacklevel=2,
        )
        base_info: ModelInfo = self.capabilities  # type: ignore
        base_info["family"] = ModelFamily.UNKNOWN
        return base_info

import grpc
from grpc._utilities import first_version_is_lower

from google.protobuf import descriptor
from google.protobuf import descriptor_pool
from google.protobuf import runtime_version
from google.protobuf import symbol_database
from google.protobuf.internal import builder

from autogen_agentchat.base._task import TaskResult
from autogen_agentchat.messages import BaseTextChatMessage

# From tests/utils.py
class FileLogHandler(logging.Handler):
    def __init__(self, filename: str) -> None:
        super().__init__()
        self.filename = filename
        self.file_handler = logging.FileHandler(filename)

    def emit(self, record: logging.LogRecord) -> None:
        ts = datetime.fromtimestamp(record.created).isoformat()
        if isinstance(record.msg, BaseModel):
            record.msg = json.dumps(
                {
                    "timestamp": ts,
                    "message": record.msg.model_dump_json(indent=2),
                    "type": record.msg.__class__.__name__,
                },
            )
        self.file_handler.emit(record)

# From tests/utils.py
class ConsoleLogHandler(logging.Handler):
    def emit(self, record: logging.LogRecord) -> None:
        ts = datetime.fromtimestamp(record.created).isoformat()
        if isinstance(record.msg, BaseModel):
            record.msg = json.dumps(
                {
                    "timestamp": ts,
                    "message": record.msg.model_dump_json(indent=2),
                    "type": record.msg.__class__.__name__,
                },
            )
        sys.stdout.write(f"{record.msg}\n")

# From tests/utils.py
def compare_messages(
    msg1: BaseAgentEvent | BaseChatMessage | BaseTextChatMessage,
    msg2: BaseAgentEvent | BaseChatMessage | BaseTextChatMessage,
) -> bool:
    if isinstance(msg1, BaseTextChatMessage) and isinstance(msg2, BaseTextChatMessage):
        if msg1.content != msg2.content:
            return False
    return (
        (msg1.source == msg2.source) and (msg1.models_usage == msg2.models_usage) and (msg1.metadata == msg2.metadata)
    )

# From tests/utils.py
def compare_message_lists(
    msgs1: Sequence[BaseAgentEvent | BaseChatMessage],
    msgs2: Sequence[BaseAgentEvent | BaseChatMessage],
) -> bool:
    if len(msgs1) != len(msgs2):
        return False
    for i in range(len(msgs1)):
        if not compare_messages(msgs1[i], msgs2[i]):
            return False
    return True

# From tests/utils.py
def compare_task_results(
    res1: TaskResult,
    res2: TaskResult,
) -> bool:
    if res1.stop_reason != res2.stop_reason:
        return False
    return compare_message_lists(res1.messages, res2.messages)

from base import TerminatedException
from base import TerminationCondition
from messages import BaseAgentEvent
from messages import BaseChatMessage
from messages import HandoffMessage
from messages import StopMessage
from messages import TextMessage
from messages import ToolCallExecutionEvent

# From conditions/_terminations.py
class StopMessageTerminationConfig(BaseModel):
    pass

# From conditions/_terminations.py
class StopMessageTermination(TerminationCondition, Component[StopMessageTerminationConfig]):
    """Terminate the conversation if a StopMessage is received."""

    component_config_schema = StopMessageTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.StopMessageTermination"

    def __init__(self) -> None:
        self._terminated = False

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        for message in messages:
            if isinstance(message, StopMessage):
                self._terminated = True
                return StopMessage(content="Stop message received", source="StopMessageTermination")
        return None

    async def reset(self) -> None:
        self._terminated = False

    def _to_config(self) -> StopMessageTerminationConfig:
        return StopMessageTerminationConfig()

    @classmethod
    def _from_config(cls, config: StopMessageTerminationConfig) -> Self:
        return cls()

# From conditions/_terminations.py
class MaxMessageTerminationConfig(BaseModel):
    max_messages: int
    include_agent_event: bool = False

# From conditions/_terminations.py
class MaxMessageTermination(TerminationCondition, Component[MaxMessageTerminationConfig]):
    """Terminate the conversation after a maximum number of messages have been exchanged.

    Args:
        max_messages: The maximum number of messages allowed in the conversation.
        include_agent_event: If True, include :class:`~autogen_agentchat.messages.BaseAgentEvent` in the message count.
            Otherwise, only include :class:`~autogen_agentchat.messages.BaseChatMessage`. Defaults to False.
    """

    component_config_schema = MaxMessageTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.MaxMessageTermination"

    def __init__(self, max_messages: int, include_agent_event: bool = False) -> None:
        self._max_messages = max_messages
        self._message_count = 0
        self._include_agent_event = include_agent_event

    @property
    def terminated(self) -> bool:
        return self._message_count >= self._max_messages

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self.terminated:
            raise TerminatedException("Termination condition has already been reached")
        self._message_count += len([m for m in messages if self._include_agent_event or isinstance(m, BaseChatMessage)])
        if self._message_count >= self._max_messages:
            return StopMessage(
                content=f"Maximum number of messages {self._max_messages} reached, current message count: {self._message_count}",
                source="MaxMessageTermination",
            )
        return None

    async def reset(self) -> None:
        self._message_count = 0

    def _to_config(self) -> MaxMessageTerminationConfig:
        return MaxMessageTerminationConfig(
            max_messages=self._max_messages, include_agent_event=self._include_agent_event
        )

    @classmethod
    def _from_config(cls, config: MaxMessageTerminationConfig) -> Self:
        return cls(max_messages=config.max_messages, include_agent_event=config.include_agent_event)

# From conditions/_terminations.py
class TextMentionTerminationConfig(BaseModel):
    text: str

# From conditions/_terminations.py
class TextMentionTermination(TerminationCondition, Component[TextMentionTerminationConfig]):
    """Terminate the conversation if a specific text is mentioned.


    Args:
        text: The text to look for in the messages.
        sources: Check only messages of the specified agents for the text to look for.
    """

    component_config_schema = TextMentionTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.TextMentionTermination"

    def __init__(self, text: str, sources: Sequence[str] | None = None) -> None:
        self._termination_text = text
        self._terminated = False
        self._sources = sources

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        for message in messages:
            if self._sources is not None and message.source not in self._sources:
                continue

            content = message.to_text()
            if self._termination_text in content:
                self._terminated = True
                return StopMessage(
                    content=f"Text '{self._termination_text}' mentioned", source="TextMentionTermination"
                )
        return None

    async def reset(self) -> None:
        self._terminated = False

    def _to_config(self) -> TextMentionTerminationConfig:
        return TextMentionTerminationConfig(text=self._termination_text)

    @classmethod
    def _from_config(cls, config: TextMentionTerminationConfig) -> Self:
        return cls(text=config.text)

# From conditions/_terminations.py
class FunctionalTermination(TerminationCondition):
    """Terminate the conversation if an functional expression is met.

    Args:
        func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]]): A function that takes a sequence of messages
            and returns True if the termination condition is met, False otherwise.
            The function can be a callable or an async callable.

    Example:

        .. code-block:: python

            import asyncio
            from typing import Sequence

            from autogen_agentchat.conditions import FunctionalTermination
            from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


            def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
                # Check if the last message is a stop message
                return isinstance(messages[-1], StopMessage)


            termination = FunctionalTermination(expression)


            async def run() -> None:
                messages = [
                    StopMessage(source="agent1", content="Stop"),
                ]
                result = await termination(messages)
                print(result)


            asyncio.run(run())

        .. code-block:: text

            StopMessage(source="FunctionalTermination", content="Functional termination condition met")

    """

    def __init__(
        self,
        func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool]
        | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]],
    ) -> None:
        self._func = func
        self._terminated = False

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        if asyncio.iscoroutinefunction(self._func):
            result = await self._func(messages)
        else:
            result = self._func(messages)
        if result is True:
            self._terminated = True
            return StopMessage(content="Functional termination condition met", source="FunctionalTermination")
        return None

    async def reset(self) -> None:
        self._terminated = False

# From conditions/_terminations.py
class TokenUsageTerminationConfig(BaseModel):
    max_total_token: int | None
    max_prompt_token: int | None
    max_completion_token: int | None

# From conditions/_terminations.py
class TokenUsageTermination(TerminationCondition, Component[TokenUsageTerminationConfig]):
    """Terminate the conversation if a token usage limit is reached.

    Args:
        max_total_token: The maximum total number of tokens allowed in the conversation.
        max_prompt_token: The maximum number of prompt tokens allowed in the conversation.
        max_completion_token: The maximum number of completion tokens allowed in the conversation.

    Raises:
        ValueError: If none of max_total_token, max_prompt_token, or max_completion_token is provided.
    """

    component_config_schema = TokenUsageTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.TokenUsageTermination"

    def __init__(
        self,
        max_total_token: int | None = None,
        max_prompt_token: int | None = None,
        max_completion_token: int | None = None,
    ) -> None:
        if max_total_token is None and max_prompt_token is None and max_completion_token is None:
            raise ValueError(
                "At least one of max_total_token, max_prompt_token, or max_completion_token must be provided"
            )
        self._max_total_token = max_total_token
        self._max_prompt_token = max_prompt_token
        self._max_completion_token = max_completion_token
        self._total_token_count = 0
        self._prompt_token_count = 0
        self._completion_token_count = 0

    @property
    def terminated(self) -> bool:
        return (
            (self._max_total_token is not None and self._total_token_count >= self._max_total_token)
            or (self._max_prompt_token is not None and self._prompt_token_count >= self._max_prompt_token)
            or (self._max_completion_token is not None and self._completion_token_count >= self._max_completion_token)
        )

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self.terminated:
            raise TerminatedException("Termination condition has already been reached")
        for message in messages:
            if message.models_usage is not None:
                self._prompt_token_count += message.models_usage.prompt_tokens
                self._completion_token_count += message.models_usage.completion_tokens
                self._total_token_count += message.models_usage.prompt_tokens + message.models_usage.completion_tokens
        if self.terminated:
            content = f"Token usage limit reached, total token count: {self._total_token_count}, prompt token count: {self._prompt_token_count}, completion token count: {self._completion_token_count}."
            return StopMessage(content=content, source="TokenUsageTermination")
        return None

    async def reset(self) -> None:
        self._total_token_count = 0
        self._prompt_token_count = 0
        self._completion_token_count = 0

    def _to_config(self) -> TokenUsageTerminationConfig:
        return TokenUsageTerminationConfig(
            max_total_token=self._max_total_token,
            max_prompt_token=self._max_prompt_token,
            max_completion_token=self._max_completion_token,
        )

    @classmethod
    def _from_config(cls, config: TokenUsageTerminationConfig) -> Self:
        return cls(
            max_total_token=config.max_total_token,
            max_prompt_token=config.max_prompt_token,
            max_completion_token=config.max_completion_token,
        )

# From conditions/_terminations.py
class HandoffTerminationConfig(BaseModel):
    target: str

# From conditions/_terminations.py
class HandoffTermination(TerminationCondition, Component[HandoffTerminationConfig]):
    """Terminate the conversation if a :class:`~autogen_agentchat.messages.HandoffMessage`
    with the given target is received.

    Args:
        target (str): The target of the handoff message.
    """

    component_config_schema = HandoffTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.HandoffTermination"

    def __init__(self, target: str) -> None:
        self._terminated = False
        self._target = target

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        for message in messages:
            if isinstance(message, HandoffMessage) and message.target == self._target:
                self._terminated = True
                return StopMessage(
                    content=f"Handoff to {self._target} from {message.source} detected.", source="HandoffTermination"
                )
        return None

    async def reset(self) -> None:
        self._terminated = False

    def _to_config(self) -> HandoffTerminationConfig:
        return HandoffTerminationConfig(target=self._target)

    @classmethod
    def _from_config(cls, config: HandoffTerminationConfig) -> Self:
        return cls(target=config.target)

# From conditions/_terminations.py
class TimeoutTerminationConfig(BaseModel):
    timeout_seconds: float

# From conditions/_terminations.py
class TimeoutTermination(TerminationCondition, Component[TimeoutTerminationConfig]):
    """Terminate the conversation after a specified duration has passed.

    Args:
        timeout_seconds: The maximum duration in seconds before terminating the conversation.
    """

    component_config_schema = TimeoutTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.TimeoutTermination"

    def __init__(self, timeout_seconds: float) -> None:
        self._timeout_seconds = timeout_seconds
        self._start_time = time.monotonic()
        self._terminated = False

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")

        if (time.monotonic() - self._start_time) >= self._timeout_seconds:
            self._terminated = True
            return StopMessage(
                content=f"Timeout of {self._timeout_seconds} seconds reached", source="TimeoutTermination"
            )
        return None

    async def reset(self) -> None:
        self._start_time = time.monotonic()
        self._terminated = False

    def _to_config(self) -> TimeoutTerminationConfig:
        return TimeoutTerminationConfig(timeout_seconds=self._timeout_seconds)

    @classmethod
    def _from_config(cls, config: TimeoutTerminationConfig) -> Self:
        return cls(timeout_seconds=config.timeout_seconds)

# From conditions/_terminations.py
class ExternalTerminationConfig(BaseModel):
    pass

# From conditions/_terminations.py
class ExternalTermination(TerminationCondition, Component[ExternalTerminationConfig]):
    """A termination condition that is externally controlled
    by calling the :meth:`set` method.

    Example:

    .. code-block:: python

        from autogen_agentchat.conditions import ExternalTermination

        termination = ExternalTermination()

        # Run the team in an asyncio task.
        ...

        # Set the termination condition externally
        termination.set()

    """

    component_config_schema = ExternalTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.ExternalTermination"

    def __init__(self) -> None:
        self._terminated = False
        self._setted = False

    @property
    def terminated(self) -> bool:
        return self._terminated

    def set(self) -> None:
        """Set the termination condition to terminated."""
        self._setted = True

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        if self._setted:
            self._terminated = True
            return StopMessage(content="External termination requested", source="ExternalTermination")
        return None

    async def reset(self) -> None:
        self._terminated = False
        self._setted = False

    def _to_config(self) -> ExternalTerminationConfig:
        return ExternalTerminationConfig()

    @classmethod
    def _from_config(cls, config: ExternalTerminationConfig) -> Self:
        return cls()

# From conditions/_terminations.py
class SourceMatchTerminationConfig(BaseModel):
    sources: List[str]

# From conditions/_terminations.py
class SourceMatchTermination(TerminationCondition, Component[SourceMatchTerminationConfig]):
    """Terminate the conversation after a specific source responds.

    Args:
        sources (List[str]): List of source names to terminate the conversation.

    Raises:
        TerminatedException: If the termination condition has already been reached.
    """

    component_config_schema = SourceMatchTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.SourceMatchTermination"

    def __init__(self, sources: List[str]) -> None:
        self._sources = sources
        self._terminated = False

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        if not messages:
            return None
        for message in messages:
            if message.source in self._sources:
                self._terminated = True
                return StopMessage(content=f"'{message.source}' answered", source="SourceMatchTermination")
        return None

    async def reset(self) -> None:
        self._terminated = False

    def _to_config(self) -> SourceMatchTerminationConfig:
        return SourceMatchTerminationConfig(sources=self._sources)

    @classmethod
    def _from_config(cls, config: SourceMatchTerminationConfig) -> Self:
        return cls(sources=config.sources)

# From conditions/_terminations.py
class TextMessageTerminationConfig(BaseModel):
    """Configuration for the TextMessageTermination termination condition."""

    source: str | None = None
    """The source of the text message to terminate the conversation."""

# From conditions/_terminations.py
class TextMessageTermination(TerminationCondition, Component[TextMessageTerminationConfig]):
    """Terminate the conversation if a :class:`~autogen_agentchat.messages.TextMessage` is received.

    This termination condition checks for TextMessage instances in the message sequence. When a TextMessage is found,
    it terminates the conversation if either:
    - No source was specified (terminates on any TextMessage)
    - The message source matches the specified source

    Args:
        source (str | None, optional): The source name to match against incoming messages. If None, matches any source.
            Defaults to None.
    """

    component_config_schema = TextMessageTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.TextMessageTermination"

    def __init__(self, source: str | None = None) -> None:
        self._terminated = False
        self._source = source

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        for message in messages:
            if isinstance(message, TextMessage) and (self._source is None or message.source == self._source):
                self._terminated = True
                return StopMessage(
                    content=f"Text message received from '{message.source}'", source="TextMessageTermination"
                )
        return None

    async def reset(self) -> None:
        self._terminated = False

    def _to_config(self) -> TextMessageTerminationConfig:
        return TextMessageTerminationConfig(source=self._source)

    @classmethod
    def _from_config(cls, config: TextMessageTerminationConfig) -> Self:
        return cls(source=config.source)

# From conditions/_terminations.py
class FunctionCallTerminationConfig(BaseModel):
    """Configuration for the :class:`FunctionCallTermination` termination condition."""

    function_name: str

# From conditions/_terminations.py
class FunctionCallTermination(TerminationCondition, Component[FunctionCallTerminationConfig]):
    """Terminate the conversation if a :class:`~autogen_core.models.FunctionExecutionResult`
    with a specific name was received.

    Args:
        function_name (str): The name of the function to look for in the messages.

    Raises:
        TerminatedException: If the termination condition has already been reached.
    """

    component_config_schema = FunctionCallTerminationConfig
    component_provider_override = "autogen_agentchat.conditions.FunctionCallTermination"
    """The schema for the component configuration."""

    def __init__(self, function_name: str) -> None:
        self._terminated = False
        self._function_name = function_name

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")
        for message in messages:
            if isinstance(message, ToolCallExecutionEvent):
                for execution in message.content:
                    if execution.name == self._function_name:
                        self._terminated = True
                        return StopMessage(
                            content=f"Function '{self._function_name}' was executed.",
                            source="FunctionCallTermination",
                        )
        return None

    async def reset(self) -> None:
        self._terminated = False

    def _to_config(self) -> FunctionCallTerminationConfig:
        return FunctionCallTerminationConfig(
            function_name=self._function_name,
        )

    @classmethod
    def _from_config(cls, config: FunctionCallTerminationConfig) -> Self:
        return cls(
            function_name=config.function_name,
        )

# From conditions/_terminations.py
def terminated(self) -> bool:
        return self._terminated

# From conditions/_terminations.py
def set(self) -> None:
        """Set the termination condition to terminated."""
        self._setted = True

from _task_runner_tool import TaskRunnerTool

# From tools/_team.py
class TeamToolConfig(BaseModel):
    """Configuration for the TeamTool."""

    name: str
    """The name of the tool."""
    description: str
    """The name and description of the tool."""
    team: ComponentModel
    """The team to be used for running the task."""
    return_value_as_last_message: bool = False
    """Whether to return the value as the last message of the task result."""

# From tools/_team.py
class TeamTool(TaskRunnerTool, Component[TeamToolConfig]):
    """Tool that can be used to run a task.

    The tool returns the result of the task execution as a :class:`~autogen_agentchat.base.TaskResult` object.

    .. important::
        When using TeamTool, you **must** disable parallel tool calls in the model client configuration
        to avoid concurrency issues. Teams cannot run concurrently as they maintain internal state
        that would conflict with parallel execution. For example, set ``parallel_tool_calls=False``
        for :class:`~autogen_ext.models.openai.OpenAIChatCompletionClient` and
        :class:`~autogen_ext.models.openai.AzureOpenAIChatCompletionClient`.

    Args:
        team (BaseGroupChat): The team to be used for running the task.
        name (str): The name of the tool.
        description (str): The description of the tool.
        return_value_as_last_message (bool): Whether to use the last message content of the task result
            as the return value of the tool in :meth:`~autogen_agentchat.tools.TaskRunnerTool.return_value_as_string`.
            If set to True, the last message content will be returned as a string.
            If set to False, the tool will return all messages in the task result as a string concatenated together,
            with each message prefixed by its source (e.g., "writer: ...", "assistant: ...").

    Example:

        .. code-block:: python

            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import SourceMatchTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_agentchat.tools import TeamTool
            from autogen_agentchat.ui import Console
            from autogen_ext.models.openai import OpenAIChatCompletionClient


            async def main() -> None:
                # Disable parallel tool calls when using TeamTool
                model_client = OpenAIChatCompletionClient(model="gpt-4.1")

                writer = AssistantAgent(name="writer", model_client=model_client, system_message="You are a helpful assistant.")
                reviewer = AssistantAgent(
                    name="reviewer", model_client=model_client, system_message="You are a critical reviewer."
                )
                summarizer = AssistantAgent(
                    name="summarizer",
                    model_client=model_client,
                    system_message="You combine the review and produce a revised response.",
                )
                team = RoundRobinGroupChat(
                    [writer, reviewer, summarizer], termination_condition=SourceMatchTermination(sources=["summarizer"])
                )

                # Create a TeamTool that uses the team to run tasks, returning the last message as the result.
                tool = TeamTool(
                    team=team,
                    name="writing_team",
                    description="A tool for writing tasks.",
                    return_value_as_last_message=True,
                )

                # Create model client with parallel tool calls disabled for the main agent
                main_model_client = OpenAIChatCompletionClient(model="gpt-4.1", parallel_tool_calls=False)
                main_agent = AssistantAgent(
                    name="main_agent",
                    model_client=main_model_client,
                    system_message="You are a helpful assistant that can use the writing tool.",
                    tools=[tool],
                )
                # For handling each events manually.
                # async for message in main_agent.run_stream(
                #     task="Write a short story about a robot learning to love.",
                # ):
                #     print(message)
                # Use Console to display the messages in a more readable format.
                await Console(
                    main_agent.run_stream(
                        task="Write a short story about a robot learning to love.",
                    )
                )


            if __name__ == "__main__":
                import asyncio

                asyncio.run(main())
    """

    component_config_schema = TeamToolConfig
    component_provider_override = "autogen_agentchat.tools.TeamTool"

    def __init__(
        self, team: BaseGroupChat, name: str, description: str, return_value_as_last_message: bool = False
    ) -> None:
        self._team = team
        super().__init__(team, name, description, return_value_as_last_message=return_value_as_last_message)

    def _to_config(self) -> TeamToolConfig:
        return TeamToolConfig(
            name=self._name,
            description=self._description,
            team=self._team.dump_component(),
            return_value_as_last_message=self._return_value_as_last_message,
        )

    @classmethod
    def _from_config(cls, config: TeamToolConfig) -> Self:
        return cls(
            BaseGroupChat.load_component(config.team),
            config.name,
            config.description,
            config.return_value_as_last_message,
        )

from autogen_core.tools import BaseStreamTool
from agents import BaseChatAgent
from base import TaskResult
from teams import BaseGroupChat

# From tools/_task_runner_tool.py
class TaskRunnerToolArgs(BaseModel):
    """Input for the TaskRunnerTool."""

    task: Annotated[str, "The task to be executed."]

# From tools/_task_runner_tool.py
class TaskRunnerTool(BaseStreamTool[TaskRunnerToolArgs, BaseAgentEvent | BaseChatMessage, TaskResult], ABC):
    """An base class for tool that can be used to run a task using a team or an agent."""

    component_type = "tool"

    def __init__(
        self,
        task_runner: BaseGroupChat | BaseChatAgent,
        name: str,
        description: str,
        return_value_as_last_message: bool,
    ) -> None:
        self._task_runner = task_runner
        self._return_value_as_last_message = return_value_as_last_message
        super().__init__(
            args_type=TaskRunnerToolArgs,
            return_type=TaskResult,
            name=name,
            description=description,
        )

    async def run(self, args: TaskRunnerToolArgs, cancellation_token: CancellationToken) -> TaskResult:
        """Run the task and return the result."""
        return await self._task_runner.run(task=args.task, cancellation_token=cancellation_token)

    async def run_stream(
        self, args: TaskRunnerToolArgs, cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None]:
        """Run the task and yield events or messages as they are produced, the final :class:`TaskResult`
        will be yielded at the end."""
        async for event in self._task_runner.run_stream(task=args.task, cancellation_token=cancellation_token):
            yield event

    def return_value_as_string(self, value: TaskResult) -> str:
        """Convert the task result to a string."""
        if self._return_value_as_last_message:
            if value.messages and isinstance(value.messages[-1], BaseChatMessage):
                return value.messages[-1].to_model_text()
            raise ValueError("The last message is not a BaseChatMessage.")
        parts: List[str] = []
        for message in value.messages:
            if isinstance(message, BaseChatMessage):
                if message.source == "user":
                    continue
                parts.append(f"{message.source}: {message.to_model_text()}")
        return "\n\n".join(parts)

    async def save_state_json(self) -> Mapping[str, Any]:
        return await self._task_runner.save_state()

    async def load_state_json(self, state: Mapping[str, Any]) -> None:
        await self._task_runner.load_state(state)

# From tools/_task_runner_tool.py
def return_value_as_string(self, value: TaskResult) -> str:
        """Convert the task result to a string."""
        if self._return_value_as_last_message:
            if value.messages and isinstance(value.messages[-1], BaseChatMessage):
                return value.messages[-1].to_model_text()
            raise ValueError("The last message is not a BaseChatMessage.")
        parts: List[str] = []
        for message in value.messages:
            if isinstance(message, BaseChatMessage):
                if message.source == "user":
                    continue
                parts.append(f"{message.source}: {message.to_model_text()}")
        return "\n\n".join(parts)

from inspect import iscoroutinefunction
from autogen_core.models import RequestUsage
from autogen_agentchat.base import Response
from autogen_agentchat.messages import UserInputRequestedEvent

# From ui/_console.py
class UserInputManager:
    def __init__(self, callback: InputFuncType):
        self.input_events: Dict[str, asyncio.Event] = {}
        self.callback = callback

    def get_wrapped_callback(self) -> AsyncInputFunc:
        async def user_input_func_wrapper(prompt: str, cancellation_token: Optional[CancellationToken]) -> str:
            # Lookup the event for the prompt, if it exists wait for it.
            # If it doesn't exist, create it and store it.
            # Get request ID:
            request_id = UserProxyAgent.InputRequestContext.request_id()
            if request_id in self.input_events:
                event = self.input_events[request_id]
            else:
                event = asyncio.Event()
                self.input_events[request_id] = event

            await event.wait()

            del self.input_events[request_id]

            if iscoroutinefunction(self.callback):
                # Cast to AsyncInputFunc for proper typing
                async_func = cast(AsyncInputFunc, self.callback)
                return await async_func(prompt, cancellation_token)
            else:
                # Cast to SyncInputFunc for proper typing
                sync_func = cast(SyncInputFunc, self.callback)
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, sync_func, prompt)

        return user_input_func_wrapper

    def notify_event_received(self, request_id: str) -> None:
        if request_id in self.input_events:
            self.input_events[request_id].set()
        else:
            event = asyncio.Event()
            self.input_events[request_id] = event

# From ui/_console.py
def aprint(output: str, end: str = "\n", flush: bool = False) -> Awaitable[None]:
    return asyncio.to_thread(print, output, end=end, flush=flush)

# From ui/_console.py
def get_wrapped_callback(self) -> AsyncInputFunc:
        async def user_input_func_wrapper(prompt: str, cancellation_token: Optional[CancellationToken]) -> str:
            # Lookup the event for the prompt, if it exists wait for it.
            # If it doesn't exist, create it and store it.
            # Get request ID:
            request_id = UserProxyAgent.InputRequestContext.request_id()
            if request_id in self.input_events:
                event = self.input_events[request_id]
            else:
                event = asyncio.Event()
                self.input_events[request_id] = event

            await event.wait()

            del self.input_events[request_id]

            if iscoroutinefunction(self.callback):
                # Cast to AsyncInputFunc for proper typing
                async_func = cast(AsyncInputFunc, self.callback)
                return await async_func(prompt, cancellation_token)
            else:
                # Cast to SyncInputFunc for proper typing
                sync_func = cast(SyncInputFunc, self.callback)
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, sync_func, prompt)

        return user_input_func_wrapper

# From ui/_console.py
def notify_event_received(self, request_id: str) -> None:
        if request_id in self.input_events:
            self.input_events[request_id].set()
        else:
            event = asyncio.Event()
            self.input_events[request_id] = event

from autogen_core import FunctionCall
from autogen_core.models import FunctionExecutionResult
from autogen_core.models import LLMMessage

# From utils/_utils.py
def content_to_str(
    content: _UserContent | _AssistantContent | _FunctionExecutionContent | _SystemContent | _StructuredContent,
) -> str:
    """Convert the content of an LLMMessage to a string."""
    if isinstance(content, str):
        return content
    elif isinstance(content, BaseModel):
        return content.model_dump_json()
    else:
        result: List[str] = []
        for c in content:
            if isinstance(c, str):
                result.append(c)
            elif isinstance(c, Image):
                result.append("<image>")
            else:
                result.append(str(c))

    return "\n".join(result)

# From utils/_utils.py
def remove_images(messages: List[LLMMessage]) -> List[LLMMessage]:
    """Remove images from a list of LLMMessages"""
    str_messages: List[LLMMessage] = []
    for message in messages:
        if isinstance(message, UserMessage) and isinstance(message.content, list):
            str_messages.append(UserMessage(content=content_to_str(message.content), source=message.source))
        else:
            str_messages.append(message)
    return str_messages

from _task import TaskRunner

# From base/_team.py
def description(self) -> str:
        """A description of the team. This is used to provide context about the
        team and its purpose to its parent orchestrator."""
        ...

from pydantic import SerializeAsAny

# From base/_task.py
class TaskResult(BaseModel):
    """Result of running a task."""

    messages: Sequence[SerializeAsAny[BaseAgentEvent | BaseChatMessage]]
    """Messages produced by the task."""

    stop_reason: str | None = None
    """The reason the task stopped."""

# From base/_task.py
class TaskRunner(Protocol):
    """A task runner."""

    async def run(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> TaskResult:
        """Run the task and return the result.

        The task can be a string, a single message, or a sequence of messages.

        The runner is stateful and a subsequent call to this method will continue
        from where the previous call left off. If the task is not specified,
        the runner will continue with the current task.

        Args:
            task: The task to run. Can be a string, a single message, or a sequence of messages.
            cancellation_token: The cancellation token to kill the task immediately.
            output_task_messages: Whether to include task messages in :attr:`TaskResult.messages`. Defaults to True for backward compatibility.
        """
        ...

    def run_stream(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None]:
        """Run the task and produces a stream of messages and the final result
        :class:`TaskResult` as the last item in the stream.

        The task can be a string, a single message, or a sequence of messages.

        The runner is stateful and a subsequent call to this method will continue
        from where the previous call left off. If the task is not specified,
        the runner will continue with the current task.

        Args:
            task: The task to run. Can be a string, a single message, or a sequence of messages.
            cancellation_token: The cancellation token to kill the task immediately.
            output_task_messages: Whether to include task messages in the output stream. Defaults to True for backward compatibility.
        """
        ...

# From base/_task.py
def run_stream(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None]:
        """Run the task and produces a stream of messages and the final result
        :class:`TaskResult` as the last item in the stream.

        The task can be a string, a single message, or a sequence of messages.

        The runner is stateful and a subsequent call to this method will continue
        from where the previous call left off. If the task is not specified,
        the runner will continue with the current task.

        Args:
            task: The task to run. Can be a string, a single message, or a sequence of messages.
            cancellation_token: The cancellation token to kill the task immediately.
            output_task_messages: Whether to include task messages in the output stream. Defaults to True for backward compatibility.
        """
        ...


# From base/_termination.py
class TerminatedException(BaseException): ...

# From base/_termination.py
class TerminationCondition(ABC, ComponentBase[BaseModel]):
    """A stateful condition that determines when a conversation should be terminated.

    A termination condition is a callable that takes a sequence of BaseChatMessage objects
    since the last time the condition was called, and returns a StopMessage if the
    conversation should be terminated, or None otherwise.
    Once a termination condition has been reached, it must be reset before it can be used again.

    Termination conditions can be combined using the AND and OR operators.

    Example:

        .. code-block:: python

            import asyncio
            from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


            async def main() -> None:
                # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
                cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

                # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
                cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

                # ...

                # Reset the termination condition.
                await cond1.reset()
                await cond2.reset()


            asyncio.run(main())
    """

    component_type = "termination"

    @property
    @abstractmethod
    def terminated(self) -> bool:
        """Check if the termination condition has been reached"""
        ...

    @abstractmethod
    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        """Check if the conversation should be terminated based on the messages received
        since the last time the condition was called.
        Return a StopMessage if the conversation should be terminated, or None otherwise.

        Args:
            messages: The messages received since the last time the condition was called.

        Returns:
            StopMessage | None: A StopMessage if the conversation should be terminated, or None otherwise.

        Raises:
            TerminatedException: If the termination condition has already been reached."""
        ...

    @abstractmethod
    async def reset(self) -> None:
        """Reset the termination condition."""
        ...

    def __and__(self, other: "TerminationCondition") -> "TerminationCondition":
        """Combine two termination conditions with an AND operation."""
        return AndTerminationCondition(self, other)

    def __or__(self, other: "TerminationCondition") -> "TerminationCondition":
        """Combine two termination conditions with an OR operation."""
        return OrTerminationCondition(self, other)

# From base/_termination.py
class AndTerminationConditionConfig(BaseModel):
    conditions: List[ComponentModel]

# From base/_termination.py
class AndTerminationCondition(TerminationCondition, Component[AndTerminationConditionConfig]):
    component_config_schema = AndTerminationConditionConfig
    component_type = "termination"
    component_provider_override = "autogen_agentchat.base.AndTerminationCondition"

    def __init__(self, *conditions: TerminationCondition) -> None:
        self._conditions = conditions
        self._stop_messages: List[StopMessage] = []

    @property
    def terminated(self) -> bool:
        return all(condition.terminated for condition in self._conditions)

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self.terminated:
            raise TerminatedException("Termination condition has already been reached.")
        # Check all remaining conditions.
        stop_messages = await asyncio.gather(
            *[condition(messages) for condition in self._conditions if not condition.terminated]
        )
        # Collect stop messages.
        for stop_message in stop_messages:
            if stop_message is not None:
                self._stop_messages.append(stop_message)
        if any(stop_message is None for stop_message in stop_messages):
            # If any remaining condition has not reached termination, it is not terminated.
            return None
        content = ", ".join(stop_message.content for stop_message in self._stop_messages)
        source = ", ".join(stop_message.source for stop_message in self._stop_messages)
        return StopMessage(content=content, source=source)

    async def reset(self) -> None:
        for condition in self._conditions:
            await condition.reset()
        self._stop_messages.clear()

    def _to_config(self) -> AndTerminationConditionConfig:
        """Convert the AND termination condition to a config."""
        return AndTerminationConditionConfig(conditions=[condition.dump_component() for condition in self._conditions])

    @classmethod
    def _from_config(cls, config: AndTerminationConditionConfig) -> Self:
        """Create an AND termination condition from a config."""
        conditions = [TerminationCondition.load_component(condition_model) for condition_model in config.conditions]
        return cls(*conditions)

# From base/_termination.py
class OrTerminationConditionConfig(BaseModel):
    conditions: List[ComponentModel]
    """List of termination conditions where any one being satisfied is sufficient."""

# From base/_termination.py
class OrTerminationCondition(TerminationCondition, Component[OrTerminationConditionConfig]):
    component_config_schema = OrTerminationConditionConfig
    component_type = "termination"
    component_provider_override = "autogen_agentchat.base.OrTerminationCondition"

    def __init__(self, *conditions: TerminationCondition) -> None:
        self._conditions = conditions

    @property
    def terminated(self) -> bool:
        return any(condition.terminated for condition in self._conditions)

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self.terminated:
            raise RuntimeError("Termination condition has already been reached")
        stop_messages = await asyncio.gather(*[condition(messages) for condition in self._conditions])
        stop_messages_filter = [stop_message for stop_message in stop_messages if stop_message is not None]
        if len(stop_messages_filter) > 0:
            content = ", ".join(stop_message.content for stop_message in stop_messages_filter)
            source = ", ".join(stop_message.source for stop_message in stop_messages_filter)
            return StopMessage(content=content, source=source)
        return None

    async def reset(self) -> None:
        for condition in self._conditions:
            await condition.reset()

    def _to_config(self) -> OrTerminationConditionConfig:
        """Convert the OR termination condition to a config."""
        return OrTerminationConditionConfig(conditions=[condition.dump_component() for condition in self._conditions])

    @classmethod
    def _from_config(cls, config: OrTerminationConditionConfig) -> Self:
        """Create an OR termination condition from a config."""
        conditions = [TerminationCondition.load_component(condition_model) for condition_model in config.conditions]
        return cls(*conditions)

from autogen_core.tools import BaseTool
from pydantic import model_validator
from  import EVENT_LOGGER_NAME

# From base/_handoff.py
class Handoff(BaseModel):
    """Handoff configuration."""

    target: str
    """The name of the target agent to handoff to."""

    description: str = Field(default="")
    """The description of the handoff such as the condition under which it should happen and the target agent's ability.
    If not provided, it is generated from the target agent's name."""

    name: str = Field(default="")
    """The name of this handoff configuration. If not provided, it is generated from the target agent's name."""

    message: str = Field(default="")
    """The message to the target agent.
    By default, it will be the result for the handoff tool.
    If not provided, it is generated from the target agent's name."""

    @model_validator(mode="before")
    @classmethod
    def set_defaults(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        if not values.get("description"):
            values["description"] = f"Handoff to {values['target']}."
        if not values.get("name"):
            values["name"] = f"transfer_to_{values['target']}".lower()
        else:
            name = values["name"]
            if not isinstance(name, str):
                raise ValueError(f"Handoff name must be a string: {values['name']}")
            # Check if name is a valid identifier.
            if not name.isidentifier():
                raise ValueError(f"Handoff name must be a valid identifier: {values['name']}")
        if not values.get("message"):
            values["message"] = (
                f"Transferred to {values['target']}, adopting the role of {values['target']} immediately."
            )
        return values

    @property
    def handoff_tool(self) -> BaseTool[BaseModel, BaseModel]:
        """Create a handoff tool from this handoff configuration."""

        def _handoff_tool() -> str:
            return self.message

        return FunctionTool(_handoff_tool, name=self.name, description=self.description, strict=True)

    """
    The tool that can be used to handoff to the target agent.
    Typically, the results of the tool's execution are provided to the target agent.
    """

# From base/_handoff.py
def set_defaults(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        if not values.get("description"):
            values["description"] = f"Handoff to {values['target']}."
        if not values.get("name"):
            values["name"] = f"transfer_to_{values['target']}".lower()
        else:
            name = values["name"]
            if not isinstance(name, str):
                raise ValueError(f"Handoff name must be a string: {values['name']}")
            # Check if name is a valid identifier.
            if not name.isidentifier():
                raise ValueError(f"Handoff name must be a valid identifier: {values['name']}")
        if not values.get("message"):
            values["message"] = (
                f"Transferred to {values['target']}, adopting the role of {values['target']} immediately."
            )
        return values

# From base/_handoff.py
def handoff_tool(self) -> BaseTool[BaseModel, BaseModel]:
        """Create a handoff tool from this handoff configuration."""

        def _handoff_tool() -> str:
            return self.message

        return FunctionTool(_handoff_tool, name=self.name, description=self.description, strict=True)

from autogen_core import event
from autogen_core import rpc
from messages import MessageFactory
from messages import SelectSpeakerEvent
from _events import GroupChatAgentResponse
from _events import GroupChatError
from _events import GroupChatMessage
from _events import GroupChatPause
from _events import GroupChatRequestPublish
from _events import GroupChatReset
from _events import GroupChatResume
from _events import GroupChatStart
from _events import GroupChatTeamResponse
from _events import GroupChatTermination
from _events import SerializableException
from _sequential_routed_agent import SequentialRoutedAgent

# From _group_chat/_base_group_chat_manager.py
class BaseGroupChatManager(SequentialRoutedAgent, ABC):
    """Base class for a group chat manager that manages a group chat with multiple participants.

    It is the responsibility of the caller to ensure:
    - All participants must subscribe to the group chat topic and each of their own topics.
    - The group chat manager must subscribe to the group chat topic.
    - The agent types of the participants must be unique.
    - For each participant, the agent type must be the same as the topic type.

    Without the above conditions, the group chat will not function correctly.
    """

    def __init__(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        participant_topic_types: List[str],
        participant_names: List[str],
        participant_descriptions: List[str],
        output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination],
        termination_condition: TerminationCondition | None,
        max_turns: int | None,
        message_factory: MessageFactory,
        emit_team_events: bool = False,
    ):
        super().__init__(
            description="Group chat manager",
            sequential_message_types=[
                GroupChatStart,
                GroupChatAgentResponse,
                GroupChatTeamResponse,
                GroupChatMessage,
                GroupChatReset,
            ],
        )
        if max_turns is not None and max_turns <= 0:
            raise ValueError("The maximum number of turns must be greater than 0.")
        if len(participant_topic_types) != len(participant_descriptions):
            raise ValueError("The number of participant topic types, agent types, and descriptions must be the same.")
        if len(set(participant_topic_types)) != len(participant_topic_types):
            raise ValueError("The participant topic types must be unique.")
        if group_topic_type in participant_topic_types:
            raise ValueError("The group topic type must not be in the participant topic types.")
        self._name = name
        self._group_topic_type = group_topic_type
        self._output_topic_type = output_topic_type
        self._participant_names = participant_names
        self._participant_name_to_topic_type = {
            name: topic_type for name, topic_type in zip(participant_names, participant_topic_types, strict=True)
        }
        self._participant_descriptions = participant_descriptions
        self._message_thread: List[BaseAgentEvent | BaseChatMessage] = []
        self._output_message_queue = output_message_queue
        self._termination_condition = termination_condition
        self._max_turns = max_turns
        self._current_turn = 0
        self._message_factory = message_factory
        self._emit_team_events = emit_team_events
        self._active_speakers: List[str] = []

    @rpc
    async def handle_start(self, message: GroupChatStart, ctx: MessageContext) -> None:
        """Handle the start of a group chat by selecting a speaker to start the conversation."""

        # Check if the conversation has already terminated.
        if self._termination_condition is not None and self._termination_condition.terminated:
            early_stop_message = StopMessage(
                content="The group chat has already terminated.",
                source=self._name,
            )
            # Signal termination to the caller of the team.
            await self._signal_termination(early_stop_message)
            # Stop the group chat.
            return

        # Validate the group state given the start messages
        await self.validate_group_state(message.messages)

        if message.messages is not None:
            # Log all messages at once
            await self.publish_message(
                GroupChatStart(messages=message.messages),
                topic_id=DefaultTopicId(type=self._output_topic_type),
            )

            # Only put messages in output queue if output_task_messages is True
            if message.output_task_messages:
                for msg in message.messages:
                    await self._output_message_queue.put(msg)

            # Relay all messages at once to participants
            await self.publish_message(
                GroupChatStart(messages=message.messages),
                topic_id=DefaultTopicId(type=self._group_topic_type),
                cancellation_token=ctx.cancellation_token,
            )

            # Append all messages to thread
            await self.update_message_thread(message.messages)

            # Check termination condition after processing all messages
            if await self._apply_termination_condition(message.messages):
                # Stop the group chat.
                return

        # Select speakers to start/continue the conversation
        await self._transition_to_next_speakers(ctx.cancellation_token)

    @event
    async def handle_agent_response(
        self, message: GroupChatAgentResponse | GroupChatTeamResponse, ctx: MessageContext
    ) -> None:
        try:
            # Construct the detla from the agent response.
            delta: List[BaseAgentEvent | BaseChatMessage] = []
            if isinstance(message, GroupChatAgentResponse):
                if message.response.inner_messages is not None:
                    for inner_message in message.response.inner_messages:
                        delta.append(inner_message)
                delta.append(message.response.chat_message)
            else:
                delta.extend(message.result.messages)

            # Append the messages to the message thread.
            await self.update_message_thread(delta)

            # Remove the agent from the active speakers list.
            self._active_speakers.remove(message.name)
            if len(self._active_speakers) > 0:
                # If there are still active speakers, return without doing anything.
                return

            # Check if the conversation should be terminated.
            if await self._apply_termination_condition(delta, increment_turn_count=True):
                # Stop the group chat.
                return

            # Select speakers to continue the conversation.
            await self._transition_to_next_speakers(ctx.cancellation_token)
        except Exception as e:
            # Handle the exception and signal termination with an error.
            error = SerializableException.from_exception(e)
            await self._signal_termination_with_error(error)
            # Raise the exception to the runtime.
            raise

    async def _transition_to_next_speakers(self, cancellation_token: CancellationToken) -> None:
        speaker_names_future = asyncio.ensure_future(self.select_speaker(self._message_thread))
        # Link the select speaker future to the cancellation token.
        cancellation_token.link_future(speaker_names_future)
        speaker_names = await speaker_names_future
        if isinstance(speaker_names, str):
            # If only one speaker is selected, convert it to a list.
            speaker_names = [speaker_names]
        for speaker_name in speaker_names:
            if speaker_name not in self._participant_name_to_topic_type:
                raise RuntimeError(f"Speaker {speaker_name} not found in participant names.")
        await self._log_speaker_selection(speaker_names)

        # Send request to publish message to the next speakers
        for speaker_name in speaker_names:
            speaker_topic_type = self._participant_name_to_topic_type[speaker_name]
            await self.publish_message(
                GroupChatRequestPublish(),
                topic_id=DefaultTopicId(type=speaker_topic_type),
                cancellation_token=cancellation_token,
            )
            self._active_speakers.append(speaker_name)

    async def _apply_termination_condition(
        self, delta: Sequence[BaseAgentEvent | BaseChatMessage], increment_turn_count: bool = False
    ) -> bool:
        """Apply the termination condition to the delta and return True if the conversation should be terminated.
        It also resets the termination condition and turn count, and signals termination to the caller of the team."""
        if self._termination_condition is not None:
            stop_message = await self._termination_condition(delta)
            if stop_message is not None:
                # Reset the termination conditions and turn count.
                await self._termination_condition.reset()
                self._current_turn = 0
                # Signal termination to the caller of the team.
                await self._signal_termination(stop_message)
                # Stop the group chat.
                return True
        if increment_turn_count:
            # Increment the turn count.
            self._current_turn += 1
        # Check if the maximum number of turns has been reached.
        if self._max_turns is not None:
            if self._current_turn >= self._max_turns:
                stop_message = StopMessage(
                    content=f"Maximum number of turns {self._max_turns} reached.",
                    source=self._name,
                )
                # Reset the termination conditions and turn count.
                if self._termination_condition is not None:
                    await self._termination_condition.reset()
                self._current_turn = 0
                # Signal termination to the caller of the team.
                await self._signal_termination(stop_message)
                # Stop the group chat.
                return True
        return False

    async def _log_speaker_selection(self, speaker_names: List[str]) -> None:
        """Log the selected speaker to the output message queue."""
        select_msg = SelectSpeakerEvent(content=speaker_names, source=self._name)
        if self._emit_team_events:
            await self.publish_message(
                GroupChatMessage(message=select_msg),
                topic_id=DefaultTopicId(type=self._output_topic_type),
            )
            await self._output_message_queue.put(select_msg)

    async def _signal_termination(self, message: StopMessage) -> None:
        termination_event = GroupChatTermination(message=message)
        # Log the early stop message.
        await self.publish_message(
            termination_event,
            topic_id=DefaultTopicId(type=self._output_topic_type),
        )
        # Put the termination event in the output message queue.
        await self._output_message_queue.put(termination_event)

    async def _signal_termination_with_error(self, error: SerializableException) -> None:
        termination_event = GroupChatTermination(
            message=StopMessage(content="An error occurred in the group chat.", source=self._name), error=error
        )
        # Log the termination event.
        await self.publish_message(
            termination_event,
            topic_id=DefaultTopicId(type=self._output_topic_type),
        )
        # Put the termination event in the output message queue.
        await self._output_message_queue.put(termination_event)

    @event
    async def handle_group_chat_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:
        """Handle a group chat message by appending the content to its output message queue."""
        await self._output_message_queue.put(message.message)

    @event
    async def handle_group_chat_error(self, message: GroupChatError, ctx: MessageContext) -> None:
        """Handle a group chat error by logging the error and signaling termination."""
        await self._signal_termination_with_error(message.error)

    @rpc
    async def handle_reset(self, message: GroupChatReset, ctx: MessageContext) -> None:
        """Reset the group chat manager. Calling :meth:`reset` to reset the group chat manager
        and clear the message thread."""
        await self.reset()

    @rpc
    async def handle_pause(self, message: GroupChatPause, ctx: MessageContext) -> None:
        """Pause the group chat manager. This is a no-op in the base class."""
        pass

    @rpc
    async def handle_resume(self, message: GroupChatResume, ctx: MessageContext) -> None:
        """Resume the group chat manager. This is a no-op in the base class."""
        pass

    @abstractmethod
    async def validate_group_state(self, messages: List[BaseChatMessage] | None) -> None:
        """Validate the state of the group chat given the start messages.
        This is executed when the group chat manager receives a GroupChatStart event.

        Args:
            messages: A list of chat messages to validate, or None if no messages are provided.
        """
        ...

    async def update_message_thread(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> None:
        """Update the message thread with the new messages.
        This is called when the group chat receives a GroupChatStart or GroupChatAgentResponse event,
        before calling the select_speakers method.
        """
        self._message_thread.extend(messages)

    @abstractmethod
    async def select_speaker(self, thread: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str] | str:
        """Select speakers from the participants and return the topic types of the selected speaker.
        This is called when the group chat manager have received all responses from the participants
        for a turn and is ready to select the next speakers for the next turn.

        Args:
            thread: The message thread of the group chat.

        Returns:
            A list of topic types of the selected speakers.
            If only one speaker is selected, a single string is returned instead of a list.
        """
        ...

    @abstractmethod
    async def reset(self) -> None:
        """Reset the group chat manager."""
        ...

    async def on_unhandled_message(self, message: Any, ctx: MessageContext) -> None:
        raise ValueError(f"Unhandled message in group chat manager: {type(message)}")

from autogen_core import AgentId
from autogen_core import AgentRuntime
from autogen_core import AgentType
from autogen_core import SingleThreadedAgentRuntime
from autogen_core import TypeSubscription
from pydantic import ValidationError
from base import ChatAgent
from base import Team
from messages import ModelClientStreamingChunkEvent
from messages import StructuredMessage
from state import TeamState
from _chat_agent_container import ChatAgentContainer

# From _group_chat/_base_group_chat.py
class BaseGroupChat(Team, ABC, ComponentBase[BaseModel]):
    """The base class for group chat teams.

    In a group chat team, participants share context by publishing their messages
    to all other participants.

    If an :class:`~autogen_agentchat.base.ChatAgent` is a participant,
    the :class:`~autogen_agentchat.messages.BaseChatMessage` from the agent response's
    :attr:`~autogen_agentchat.base.Response.chat_message` will be published
    to other participants in the group chat.

    If a :class:`~autogen_agentchat.base.Team` is a participant,
    the :class:`~autogen_agentchat.messages.BaseChatMessage`
    from the team result' :attr:`~autogen_agentchat.base.TaskResult.messages` will be published
    to other participants in the group chat.

    To implement a group chat team, first create a subclass of :class:`BaseGroupChatManager` and then
    create a subclass of :class:`BaseGroupChat` that uses the group chat manager.

    This base class provides the mapping between the agents of the AgentChat API
    and the agent runtime of the Core API, and handles high-level features like
    running, pausing, resuming, and resetting the team.
    """

    component_type = "team"

    def __init__(
        self,
        name: str,
        description: str,
        participants: List[ChatAgent | Team],
        group_chat_manager_name: str,
        group_chat_manager_class: type[SequentialRoutedAgent],
        termination_condition: TerminationCondition | None = None,
        max_turns: int | None = None,
        runtime: AgentRuntime | None = None,
        custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None,
        emit_team_events: bool = False,
    ):
        self._name = name
        self._description = description
        if len(participants) == 0:
            raise ValueError("At least one participant is required.")
        if len(participants) != len(set(participant.name for participant in participants)):
            raise ValueError("The participant names must be unique.")
        self._participants = participants
        self._base_group_chat_manager_class = group_chat_manager_class
        self._termination_condition = termination_condition
        self._max_turns = max_turns
        self._message_factory = MessageFactory()
        if custom_message_types is not None:
            for message_type in custom_message_types:
                self._message_factory.register(message_type)

        for agent in participants:
            if isinstance(agent, ChatAgent):
                for message_type in agent.produced_message_types:
                    try:
                        is_registered = self._message_factory.is_registered(message_type)  # type: ignore[reportUnknownArgumentType]
                        if issubclass(message_type, StructuredMessage) and not is_registered:
                            self._message_factory.register(message_type)  # type: ignore[reportUnknownArgumentType]
                    except TypeError:
                        # Not a class or not a valid subclassable type (skip)
                        pass

        # The team ID is a UUID that is used to identify the team and its participants
        # in the agent runtime. It is used to create unique topic types for each participant.
        # Currently, team ID is binded to an object instance of the group chat class.
        # So if you create two instances of group chat, there will be two teams with different IDs.
        self._team_id = str(uuid.uuid4())

        # Constants for the group chat team.
        # The names are used to identify the agents within the team.
        # The names may not be unique across different teams.
        self._group_chat_manager_name = group_chat_manager_name
        self._participant_names: List[str] = [participant.name for participant in participants]
        self._participant_descriptions: List[str] = [participant.description for participant in participants]
        # The group chat topic type is used for broadcast communication among all participants and the group chat manager.
        self._group_topic_type = f"group_topic_{self._team_id}"
        # The group chat manager topic type is used for direct communication with the group chat manager.
        self._group_chat_manager_topic_type = f"{self._group_chat_manager_name}_{self._team_id}"
        # The participant topic types are used for direct communication with each participant.
        self._participant_topic_types: List[str] = [
            f"{participant.name}_{self._team_id}" for participant in participants
        ]
        # The output topic type is used for emitting streaming messages from the group chat.
        # The group chat manager will relay the messages to the output message queue.
        self._output_topic_type = f"output_topic_{self._team_id}"

        # The queue for collecting the output messages.
        self._output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination] = (
            asyncio.Queue()
        )

        # Create a runtime for the team.
        if runtime is not None:
            self._runtime = runtime
            self._embedded_runtime = False
        else:
            # Use a embedded single-threaded runtime for the group chat.
            # Background exceptions must not be ignored as it results in non-surfaced exceptions and early team termination.
            self._runtime = SingleThreadedAgentRuntime(ignore_unhandled_exceptions=False)
            self._embedded_runtime = True

        # Flag to track if the group chat has been initialized.
        self._initialized = False

        # Flag to track if the group chat is running.
        self._is_running = False

        # Flag to track if the team events should be emitted.
        self._emit_team_events = emit_team_events

    @property
    def name(self) -> str:
        """The name of the group chat team."""
        return self._name

    @property
    def description(self) -> str:
        """A description of the group chat team."""
        return self._description

    @abstractmethod
    def _create_group_chat_manager_factory(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        participant_topic_types: List[str],
        participant_names: List[str],
        participant_descriptions: List[str],
        output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination],
        termination_condition: TerminationCondition | None,
        max_turns: int | None,
        message_factory: MessageFactory,
    ) -> Callable[[], SequentialRoutedAgent]: ...

    def _create_participant_factory(
        self,
        parent_topic_type: str,
        output_topic_type: str,
        agent: ChatAgent | Team,
        message_factory: MessageFactory,
    ) -> Callable[[], ChatAgentContainer]:
        def _factory() -> ChatAgentContainer:
            container = ChatAgentContainer(parent_topic_type, output_topic_type, agent, message_factory)
            return container

        return _factory

    async def _init(self, runtime: AgentRuntime) -> None:
        # Constants for the group chat manager.
        group_chat_manager_agent_type = AgentType(self._group_chat_manager_topic_type)

        # Register participants.
        # Use the participant topic type as the agent type.
        for participant, agent_type in zip(self._participants, self._participant_topic_types, strict=True):
            # Register the participant factory.
            await ChatAgentContainer.register(
                runtime,
                type=agent_type,
                factory=self._create_participant_factory(
                    self._group_topic_type, self._output_topic_type, participant, self._message_factory
                ),
            )
            # Add subscriptions for the participant.
            # The participant should be able to receive messages from its own topic.
            await runtime.add_subscription(TypeSubscription(topic_type=agent_type, agent_type=agent_type))
            # The participant should be able to receive messages from the group topic.
            await runtime.add_subscription(TypeSubscription(topic_type=self._group_topic_type, agent_type=agent_type))

        # Register the group chat manager.
        await self._base_group_chat_manager_class.register(
            runtime,
            type=group_chat_manager_agent_type.type,
            factory=self._create_group_chat_manager_factory(
                name=self._group_chat_manager_name,
                group_topic_type=self._group_topic_type,
                output_topic_type=self._output_topic_type,
                participant_names=self._participant_names,
                participant_topic_types=self._participant_topic_types,
                participant_descriptions=self._participant_descriptions,
                output_message_queue=self._output_message_queue,
                termination_condition=self._termination_condition,
                max_turns=self._max_turns,
                message_factory=self._message_factory,
            ),
        )
        # Add subscriptions for the group chat manager.
        # The group chat manager should be able to receive messages from the its own topic.
        await runtime.add_subscription(
            TypeSubscription(
                topic_type=self._group_chat_manager_topic_type, agent_type=group_chat_manager_agent_type.type
            )
        )
        # The group chat manager should be able to receive messages from the group topic.
        await runtime.add_subscription(
            TypeSubscription(topic_type=self._group_topic_type, agent_type=group_chat_manager_agent_type.type)
        )
        # The group chat manager will relay the messages from output topic to the output message queue.
        await runtime.add_subscription(
            TypeSubscription(topic_type=self._output_topic_type, agent_type=group_chat_manager_agent_type.type)
        )

        self._initialized = True

    async def run(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> TaskResult:
        """Run the team and return the result. The base implementation uses
        :meth:`run_stream` to run the team and then returns the final result.
        Once the team is stopped, the termination condition is reset.

        Args:
            task (str | BaseChatMessage | Sequence[BaseChatMessage] | None): The task to run the team with. Can be a string, a single :class:`BaseChatMessage` , or a list of :class:`BaseChatMessage`.
            cancellation_token (CancellationToken | None): The cancellation token to kill the task immediately.
                Setting the cancellation token potentially put the team in an inconsistent state,
                and it may not reset the termination condition.
                To gracefully stop the team, use :class:`~autogen_agentchat.conditions.ExternalTermination` instead.

        Returns:
            result: The result of the task as :class:`~autogen_agentchat.base.TaskResult`. The result contains the messages produced by the team and the stop reason.

        Example using the :class:`~autogen_agentchat.teams.RoundRobinGroupChat` team:


        .. code-block:: python

            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import MaxMessageTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_ext.models.openai import OpenAIChatCompletionClient


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent1 = AssistantAgent("Assistant1", model_client=model_client)
                agent2 = AssistantAgent("Assistant2", model_client=model_client)
                termination = MaxMessageTermination(3)
                team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

                result = await team.run(task="Count from 1 to 10, respond one at a time.")
                print(result)

                # Run the team again without a task to continue the previous task.
                result = await team.run()
                print(result)


            asyncio.run(main())


        Example using the :class:`~autogen_core.CancellationToken` to cancel the task:

        .. code-block:: python

            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import MaxMessageTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_core import CancellationToken
            from autogen_ext.models.openai import OpenAIChatCompletionClient


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent1 = AssistantAgent("Assistant1", model_client=model_client)
                agent2 = AssistantAgent("Assistant2", model_client=model_client)
                termination = MaxMessageTermination(3)
                team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

                cancellation_token = CancellationToken()

                # Create a task to run the team in the background.
                run_task = asyncio.create_task(
                    team.run(
                        task="Count from 1 to 10, respond one at a time.",
                        cancellation_token=cancellation_token,
                    )
                )

                # Wait for 1 second and then cancel the task.
                await asyncio.sleep(1)
                cancellation_token.cancel()

                # This will raise a cancellation error.
                await run_task


            asyncio.run(main())
        """
        result: TaskResult | None = None
        async for message in self.run_stream(
            task=task,
            cancellation_token=cancellation_token,
            output_task_messages=output_task_messages,
        ):
            if isinstance(message, TaskResult):
                result = message
        if result is not None:
            return result
        raise AssertionError("The stream should have returned the final result.")

    async def run_stream(
        self,
        *,
        task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None,
        cancellation_token: CancellationToken | None = None,
        output_task_messages: bool = True,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None]:
        """Run the team and produces a stream of messages and the final result
        of the type :class:`~autogen_agentchat.base.TaskResult` as the last item in the stream. Once the
        team is stopped, the termination condition is reset.

        .. note::

            If an agent produces :class:`~autogen_agentchat.messages.ModelClientStreamingChunkEvent`,
            the message will be yielded in the stream but it will not be included in the
            :attr:`~autogen_agentchat.base.TaskResult.messages`.

        Args:
            task (str | BaseChatMessage | Sequence[BaseChatMessage] | None): The task to run the team with. Can be a string, a single :class:`BaseChatMessage` , or a list of :class:`BaseChatMessage`.
            cancellation_token (CancellationToken | None): The cancellation token to kill the task immediately.
                Setting the cancellation token potentially put the team in an inconsistent state,
                and it may not reset the termination condition.
                To gracefully stop the team, use :class:`~autogen_agentchat.conditions.ExternalTermination` instead.
            output_task_messages (bool): Whether to include task messages in the output stream. Defaults to True for backward compatibility.

        Returns:
            stream: an :class:`~collections.abc.AsyncGenerator` that yields :class:`~autogen_agentchat.messages.BaseAgentEvent`, :class:`~autogen_agentchat.messages.BaseChatMessage`, and the final result :class:`~autogen_agentchat.base.TaskResult` as the last item in the stream.

        Example using the :class:`~autogen_agentchat.teams.RoundRobinGroupChat` team:

        .. code-block:: python

            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import MaxMessageTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_ext.models.openai import OpenAIChatCompletionClient


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent1 = AssistantAgent("Assistant1", model_client=model_client)
                agent2 = AssistantAgent("Assistant2", model_client=model_client)
                termination = MaxMessageTermination(3)
                team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

                stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
                async for message in stream:
                    print(message)

                # Run the team again without a task to continue the previous task.
                stream = team.run_stream()
                async for message in stream:
                    print(message)


            asyncio.run(main())


        Example using the :class:`~autogen_core.CancellationToken` to cancel the task:

        .. code-block:: python

            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import MaxMessageTermination
            from autogen_agentchat.ui import Console
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_core import CancellationToken
            from autogen_ext.models.openai import OpenAIChatCompletionClient


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent1 = AssistantAgent("Assistant1", model_client=model_client)
                agent2 = AssistantAgent("Assistant2", model_client=model_client)
                termination = MaxMessageTermination(3)
                team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

                cancellation_token = CancellationToken()

                # Create a task to run the team in the background.
                run_task = asyncio.create_task(
                    Console(
                        team.run_stream(
                            task="Count from 1 to 10, respond one at a time.",
                            cancellation_token=cancellation_token,
                        )
                    )
                )

                # Wait for 1 second and then cancel the task.
                await asyncio.sleep(1)
                cancellation_token.cancel()

                # This will raise a cancellation error.
                await run_task


            asyncio.run(main())

        """
        # Create the messages list if the task is a string or a chat message.
        messages: List[BaseChatMessage] | None = None
        if task is None:
            pass
        elif isinstance(task, str):
            messages = [TextMessage(content=task, source="user")]
        elif isinstance(task, BaseChatMessage):
            messages = [task]
        elif isinstance(task, list):
            if not task:
                raise ValueError("Task list cannot be empty.")
            messages = []
            for msg in task:
                if not isinstance(msg, BaseChatMessage):
                    raise ValueError("All messages in task list must be valid BaseChatMessage types")
                messages.append(msg)
        else:
            raise ValueError("Task must be a string, a BaseChatMessage, or a list of BaseChatMessage.")
        # Check if the messages types are registered with the message factory.
        if messages is not None:
            for msg in messages:
                if not self._message_factory.is_registered(msg.__class__):
                    raise ValueError(
                        f"Message type {msg.__class__} is not registered with the message factory. "
                        "Please register it with the message factory by adding it to the "
                        "custom_message_types list when creating the team."
                    )

        if self._is_running:
            raise ValueError("The team is already running, it cannot run again until it is stopped.")
        self._is_running = True

        if self._embedded_runtime:
            # Start the embedded runtime.
            assert isinstance(self._runtime, SingleThreadedAgentRuntime)
            self._runtime.start()

        if not self._initialized:
            await self._init(self._runtime)

        shutdown_task: asyncio.Task[None] | None = None
        if self._embedded_runtime:

            async def stop_runtime() -> None:
                assert isinstance(self._runtime, SingleThreadedAgentRuntime)
                try:
                    # This will propagate any exceptions raised.
                    await self._runtime.stop_when_idle()
                    # Put a termination message in the queue to indicate that the group chat is stopped for whatever reason
                    # but not due to an exception.
                    await self._output_message_queue.put(
                        GroupChatTermination(
                            message=StopMessage(
                                content="The group chat is stopped.", source=self._group_chat_manager_name
                            )
                        )
                    )
                except Exception as e:
                    # Stop the consumption of messages and end the stream.
                    # NOTE: we also need to put a GroupChatTermination event here because when the runtime
                    # has an exception, the group chat manager may not be able to put a GroupChatTermination event in the queue.
                    # This may not be necessary if the group chat manager is able to handle the exception and put the event in the queue.
                    await self._output_message_queue.put(
                        GroupChatTermination(
                            message=StopMessage(
                                content="An exception occurred in the runtime.", source=self._group_chat_manager_name
                            ),
                            error=SerializableException.from_exception(e),
                        )
                    )

            # Create a background task to stop the runtime when the group chat
            # is stopped or has an exception.
            shutdown_task = asyncio.create_task(stop_runtime())

        try:
            # Run the team by sending the start message to the group chat manager.
            # The group chat manager will start the group chat by relaying the message to the participants
            # and the group chat manager.
            await self._runtime.send_message(
                GroupChatStart(messages=messages, output_task_messages=output_task_messages),
                recipient=AgentId(type=self._group_chat_manager_topic_type, key=self._team_id),
                cancellation_token=cancellation_token,
            )
            # Collect the output messages in order.
            output_messages: List[BaseAgentEvent | BaseChatMessage] = []
            stop_reason: str | None = None

            # Yield the messages until the queue is empty.
            while True:
                message_future = asyncio.ensure_future(self._output_message_queue.get())
                if cancellation_token is not None:
                    cancellation_token.link_future(message_future)
                # Wait for the next message, this will raise an exception if the task is cancelled.
                message = await message_future
                if isinstance(message, GroupChatTermination):
                    # If the message contains an error, we need to raise it here.
                    # This will stop the team and propagate the error.
                    if message.error is not None:
                        raise RuntimeError(str(message.error))
                    stop_reason = message.message.content
                    break
                yield message
                if isinstance(message, ModelClientStreamingChunkEvent):
                    # Skip the model client streaming chunk events.
                    continue
                output_messages.append(message)

            # Yield the final result.
            yield TaskResult(messages=output_messages, stop_reason=stop_reason)

        finally:
            try:
                if shutdown_task is not None:
                    # Wait for the shutdown task to finish.
                    # This will propagate any exceptions raised.
                    await shutdown_task
            finally:
                # Clear the output message queue.
                while not self._output_message_queue.empty():
                    self._output_message_queue.get_nowait()

                # Indicate that the team is no longer running.
                self._is_running = False

    async def reset(self) -> None:
        """Reset the team and its participants to their initial state.

        The team must be stopped before it can be reset.

        Raises:
            RuntimeError: If the team has not been initialized or is currently running.

        Example using the :class:`~autogen_agentchat.teams.RoundRobinGroupChat` team:

        .. code-block:: python

            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import MaxMessageTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_ext.models.openai import OpenAIChatCompletionClient


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent1 = AssistantAgent("Assistant1", model_client=model_client)
                agent2 = AssistantAgent("Assistant2", model_client=model_client)
                termination = MaxMessageTermination(3)
                team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
                stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
                async for message in stream:
                    print(message)

                # Reset the team.
                await team.reset()
                stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
                async for message in stream:
                    print(message)


            asyncio.run(main())
        """

        if not self._initialized:
            await self._init(self._runtime)

        if self._is_running:
            raise RuntimeError("The group chat is currently running. It must be stopped before it can be reset.")
        self._is_running = True

        if self._embedded_runtime:
            # Start the runtime.
            assert isinstance(self._runtime, SingleThreadedAgentRuntime)
            self._runtime.start()

        try:
            # Send a reset messages to all participants.
            for participant_topic_type in self._participant_topic_types:
                await self._runtime.send_message(
                    GroupChatReset(),
                    recipient=AgentId(type=participant_topic_type, key=self._team_id),
                )
            # Send a reset message to the group chat manager.
            await self._runtime.send_message(
                GroupChatReset(),
                recipient=AgentId(type=self._group_chat_manager_topic_type, key=self._team_id),
            )
        finally:
            if self._embedded_runtime:
                # Stop the runtime.
                assert isinstance(self._runtime, SingleThreadedAgentRuntime)
                await self._runtime.stop_when_idle()

            # Reset the output message queue.
            while not self._output_message_queue.empty():
                self._output_message_queue.get_nowait()

            # Indicate that the team is no longer running.
            self._is_running = False

    async def pause(self) -> None:
        """Pause its participants when the team is running by calling their
        :meth:`~autogen_agentchat.base.ChatAgent.on_pause` method via direct RPC calls.

        .. attention::

            This is an experimental feature introduced in v0.4.9 and may subject
            to change or removal in the future.

        The team must be initialized before it can be paused.

        Different from termination, pausing the team does not cause the
        :meth:`run` or :meth:`run_stream` method to return. It calls the
        :meth:`~autogen_agentchat.base.ChatAgent.on_pause` method on each
        participant, and if the participant does not implement the method, it
        will be a no-op.

        .. note::

            It is the responsibility of the agent class to handle the pause
            and ensure that the agent can be resumed later.
            Make sure to implement the :meth:`~autogen_agentchat.agents.BaseChatAgent.on_pause`
            method in your agent class for custom pause behavior.
            By default, the agent will not do anything when called.

        Raises:
            RuntimeError: If the team has not been initialized. Exceptions from
                the participants when calling their implementations of
                :class:`~autogen_agentchat.base.ChatAgent.on_pause` are
                propagated to this method and raised.
        """
        if not self._initialized:
            raise RuntimeError("The group chat has not been initialized. It must be run before it can be paused.")

        # Send a pause message to all participants.
        for participant_topic_type in self._participant_topic_types:
            await self._runtime.send_message(
                GroupChatPause(),
                recipient=AgentId(type=participant_topic_type, key=self._team_id),
            )
        # Send a pause message to the group chat manager.
        await self._runtime.send_message(
            GroupChatPause(),
            recipient=AgentId(type=self._group_chat_manager_topic_type, key=self._team_id),
        )

    async def resume(self) -> None:
        """Resume its participants when the team is running and paused by calling their
        :meth:`~autogen_agentchat.base.ChatAgent.on_resume` method via direct RPC calls.

        .. attention::

            This is an experimental feature introduced in v0.4.9 and may subject
            to change or removal in the future.

        The team must be initialized before it can be resumed.

        Different from termination and restart with a new task, resuming the team
        does not cause the :meth:`run` or :meth:`run_stream` method to return.
        It calls the :meth:`~autogen_agentchat.base.ChatAgent.on_resume` method on each
        participant, and if the participant does not implement the method, it
        will be a no-op.

        .. note::

            It is the responsibility of the agent class to handle the resume
            and ensure that the agent continues from where it was paused.
            Make sure to implement the :meth:`~autogen_agentchat.agents.BaseChatAgent.on_resume`
            method in your agent class for custom resume behavior.

        Raises:
            RuntimeError: If the team has not been initialized. Exceptions from
                the participants when calling their implementations of :class:`~autogen_agentchat.base.ChatAgent.on_resume`
                method are propagated to this method and raised.

        """
        if not self._initialized:
            raise RuntimeError("The group chat has not been initialized. It must be run before it can be resumed.")

        # Send a resume message to all participants.
        for participant_topic_type in self._participant_topic_types:
            await self._runtime.send_message(
                GroupChatResume(),
                recipient=AgentId(type=participant_topic_type, key=self._team_id),
            )
        # Send a resume message to the group chat manager.
        await self._runtime.send_message(
            GroupChatResume(),
            recipient=AgentId(type=self._group_chat_manager_topic_type, key=self._team_id),
        )

    async def save_state(self) -> Mapping[str, Any]:
        """Save the state of the group chat team.

        The state is saved by calling the :meth:`~autogen_core.AgentRuntime.agent_save_state` method
        on each participant and the group chat manager with their internal agent ID.
        The state is returned as a nested dictionary: a dictionary with key `agent_states`,
        which is a dictionary the agent names as keys and the state as values.

        .. code-block:: text

            {
                "agent_states": {
                    "agent1": ...,
                    "agent2": ...,
                    "RoundRobinGroupChatManager": ...
                }
            }

        .. note::

            Starting v0.4.9, the state is using the agent name as the key instead of the agent ID,
            and the `team_id` field is removed from the state. This is to allow the state to be
            portable across different teams and runtimes. States saved with the old format
            may not be compatible with the new format in the future.

        .. caution::

            When calling :func:`~autogen_agentchat.teams.BaseGroupChat.save_state` on a team
            while it is running, the state may not be consistent and may result in an unexpected state.
            It is recommended to call this method when the team is not running or after it is stopped.

        """
        if not self._initialized:
            await self._init(self._runtime)

        # Store state of each agent by their name.
        # NOTE: we don't use the agent ID as the key here because we need to be able to decouple
        # the state of the agents from their identities in the agent runtime.
        agent_states: Dict[str, Mapping[str, Any]] = {}
        # Save the state of all participants.
        for name, agent_type in zip(self._participant_names, self._participant_topic_types, strict=True):
            agent_id = AgentId(type=agent_type, key=self._team_id)
            # NOTE: We are using the runtime's save state method rather than the agent instance's
            # save_state method because we want to support saving state of remote agents.
            agent_states[name] = await self._runtime.agent_save_state(agent_id)
        # Save the state of the group chat manager.
        agent_id = AgentId(type=self._group_chat_manager_topic_type, key=self._team_id)
        agent_states[self._group_chat_manager_name] = await self._runtime.agent_save_state(agent_id)
        return TeamState(agent_states=agent_states).model_dump()

    async def load_state(self, state: Mapping[str, Any]) -> None:
        """Load an external state and overwrite the current state of the group chat team.

        The state is loaded by calling the :meth:`~autogen_core.AgentRuntime.agent_load_state` method
        on each participant and the group chat manager with their internal agent ID.
        See :meth:`~autogen_agentchat.teams.BaseGroupChat.save_state` for the expected format of the state.
        """
        if not self._initialized:
            await self._init(self._runtime)

        if self._is_running:
            raise RuntimeError("The team cannot be loaded while it is running.")
        self._is_running = True

        try:
            team_state = TeamState.model_validate(state)
            # Load the state of all participants.
            for name, agent_type in zip(self._participant_names, self._participant_topic_types, strict=True):
                agent_id = AgentId(type=agent_type, key=self._team_id)
                if name not in team_state.agent_states:
                    raise ValueError(f"Agent state for {name} not found in the saved state.")
                await self._runtime.agent_load_state(agent_id, team_state.agent_states[name])
            # Load the state of the group chat manager.
            agent_id = AgentId(type=self._group_chat_manager_topic_type, key=self._team_id)
            if self._group_chat_manager_name not in team_state.agent_states:
                raise ValueError(f"Agent state for {self._group_chat_manager_name} not found in the saved state.")
            await self._runtime.agent_load_state(agent_id, team_state.agent_states[self._group_chat_manager_name])

        except ValidationError as e:
            raise ValueError(
                "Invalid state format. The expected state format has changed since v0.4.9. "
                "Please read the release note on GitHub."
            ) from e

        finally:
            # Indicate that the team is no longer running.
            self._is_running = False

from state import SwarmManagerState
from _base_group_chat import BaseGroupChat
from _base_group_chat_manager import BaseGroupChatManager

# From _group_chat/_swarm_group_chat.py
class SwarmGroupChatManager(BaseGroupChatManager):
    """A group chat manager that selects the next speaker based on handoff message only."""

    def __init__(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        participant_topic_types: List[str],
        participant_names: List[str],
        participant_descriptions: List[str],
        output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination],
        termination_condition: TerminationCondition | None,
        max_turns: int | None,
        message_factory: MessageFactory,
        emit_team_events: bool,
    ) -> None:
        super().__init__(
            name,
            group_topic_type,
            output_topic_type,
            participant_topic_types,
            participant_names,
            participant_descriptions,
            output_message_queue,
            termination_condition,
            max_turns,
            message_factory,
            emit_team_events,
        )
        self._current_speaker = self._participant_names[0]

    async def validate_group_state(self, messages: List[BaseChatMessage] | None) -> None:
        """Validate the start messages for the group chat."""
        # Check if any of the start messages is a handoff message.
        if messages:
            for message in messages:
                if isinstance(message, HandoffMessage):
                    if message.target not in self._participant_names:
                        raise ValueError(
                            f"The target {message.target} is not one of the participants {self._participant_names}. "
                            "If you are resuming Swarm with a new HandoffMessage make sure to set the target to a valid participant as the target."
                        )
                    return

        # Check if there is a handoff message in the thread that is not targeting a valid participant.
        for existing_message in reversed(self._message_thread):
            if isinstance(existing_message, HandoffMessage):
                if existing_message.target not in self._participant_names:
                    raise ValueError(
                        f"The existing handoff target {existing_message.target} is not one of the participants {self._participant_names}. "
                        "If you are resuming Swarm with a new task make sure to include in your task "
                        "a HandoffMessage with a valid participant as the target. For example, if you are "
                        "resuming from a HandoffTermination, make sure the new task is a HandoffMessage "
                        "with a valid participant as the target."
                    )
                # The latest handoff message should always target a valid participant.
                # Do not look past the latest handoff message.
                return

    async def reset(self) -> None:
        self._current_turn = 0
        self._message_thread.clear()
        if self._termination_condition is not None:
            await self._termination_condition.reset()
        self._current_speaker = self._participant_names[0]

    async def select_speaker(self, thread: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str] | str:
        """Select a speaker from the participants based on handoff message.
        Looks for the last handoff message in the thread to determine the next speaker.

        .. note::

            This method always returns a single speaker.
        """
        if len(thread) == 0:
            return [self._current_speaker]
        for message in reversed(thread):
            if isinstance(message, HandoffMessage):
                self._current_speaker = message.target
                # The latest handoff message should always target a valid participant.
                assert self._current_speaker in self._participant_names
                return [self._current_speaker]
        return self._current_speaker

    async def save_state(self) -> Mapping[str, Any]:
        state = SwarmManagerState(
            message_thread=[msg.dump() for msg in self._message_thread],
            current_turn=self._current_turn,
            current_speaker=self._current_speaker,
        )
        return state.model_dump()

    async def load_state(self, state: Mapping[str, Any]) -> None:
        swarm_state = SwarmManagerState.model_validate(state)
        self._message_thread = [self._message_factory.create(message) for message in swarm_state.message_thread]
        self._current_turn = swarm_state.current_turn
        self._current_speaker = swarm_state.current_speaker

# From _group_chat/_swarm_group_chat.py
class SwarmConfig(BaseModel):
    """The declarative configuration for Swarm."""

    name: str | None = None
    description: str | None = None
    participants: List[ComponentModel]
    termination_condition: ComponentModel | None = None
    max_turns: int | None = None
    emit_team_events: bool = False

# From _group_chat/_swarm_group_chat.py
class Swarm(BaseGroupChat, Component[SwarmConfig]):
    """A group chat team that selects the next speaker based on handoff message only.

    The first participant in the list of participants is the initial speaker.
    The next speaker is selected based on the :class:`~autogen_agentchat.messages.HandoffMessage` message
    sent by the current speaker. If no handoff message is sent, the current speaker
    continues to be the speaker.

    .. note::

        Unlike :class:`~autogen_agentchat.teams.RoundRobinGroupChat` and
        :class:`~autogen_agentchat.teams.SelectorGroupChat`, this group chat
        team does not support inner teams as participants.

    Args:
        participants (List[ChatAgent]): The agents participating in the group chat. The first agent in the list is the initial speaker.
        name (str | None, optional): The name of the group chat, using :attr:`~autogen_agentchat.teams.Swarm.DEFAULT_NAME` if not provided.
            The name is used by a parent team to identify this group chat so it must be unique within the parent team.
        description (str | None, optional): The description of the group chat, using :attr:`~autogen_agentchat.teams.Swarm.DEFAULT_DESCRIPTION` if not provided.
        termination_condition (TerminationCondition, optional): The termination condition for the group chat. Defaults to None.
            Without a termination condition, the group chat will run indefinitely.
        max_turns (int, optional): The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
        custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional): A list of custom message types that will be used in the group chat.
            If you are using custom message types or your agents produces custom message types, you need to specify them here.
            Make sure your custom message types are subclasses of :class:`~autogen_agentchat.messages.BaseAgentEvent` or :class:`~autogen_agentchat.messages.BaseChatMessage`.
        emit_team_events (bool, optional): Whether to emit team events through :meth:`BaseGroupChat.run_stream`. Defaults to False.

    Basic example:

        .. code-block:: python

            import asyncio
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.teams import Swarm
            from autogen_agentchat.conditions import MaxMessageTermination


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent1 = AssistantAgent(
                    "Alice",
                    model_client=model_client,
                    handoffs=["Bob"],
                    system_message="You are Alice and you only answer questions about yourself.",
                )
                agent2 = AssistantAgent(
                    "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January."
                )

                termination = MaxMessageTermination(3)
                team = Swarm([agent1, agent2], termination_condition=termination)

                stream = team.run_stream(task="What is bob's birthday?")
                async for message in stream:
                    print(message)


            asyncio.run(main())


    Using the :class:`~autogen_agentchat.conditions.HandoffTermination` for human-in-the-loop handoff:

        .. code-block:: python

            import asyncio
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.teams import Swarm
            from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination
            from autogen_agentchat.ui import Console
            from autogen_agentchat.messages import HandoffMessage


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                agent = AssistantAgent(
                    "Alice",
                    model_client=model_client,
                    handoffs=["user"],
                    system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.",
                )
                termination = HandoffTermination(target="user") | MaxMessageTermination(3)
                team = Swarm([agent], termination_condition=termination)

                # Start the conversation.
                await Console(team.run_stream(task="What is bob's birthday?"))

                # Resume with user feedback.
                await Console(
                    team.run_stream(
                        task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.")
                    )
                )


            asyncio.run(main())
    """

    component_config_schema = SwarmConfig
    component_provider_override = "autogen_agentchat.teams.Swarm"

    DEFAULT_NAME = "Swarm"
    DEFAULT_DESCRIPTION = "A team of agents."

    def __init__(
        self,
        participants: List[ChatAgent],
        *,
        name: str | None = None,
        description: str | None = None,
        termination_condition: TerminationCondition | None = None,
        max_turns: int | None = None,
        runtime: AgentRuntime | None = None,
        custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None,
        emit_team_events: bool = False,
    ) -> None:
        for participant in participants:
            if not isinstance(participant, ChatAgent):
                raise TypeError(f"Participant {participant} must be a ChatAgent.")
        super().__init__(
            name=name or self.DEFAULT_NAME,
            description=description or self.DEFAULT_DESCRIPTION,
            participants=[participant for participant in participants],
            group_chat_manager_name="SwarmGroupChatManager",
            group_chat_manager_class=SwarmGroupChatManager,
            termination_condition=termination_condition,
            max_turns=max_turns,
            runtime=runtime,
            custom_message_types=custom_message_types,
            emit_team_events=emit_team_events,
        )
        # The first participant must be able to produce handoff messages.
        first_participant = self._participants[0]
        assert isinstance(first_participant, ChatAgent)
        if HandoffMessage not in first_participant.produced_message_types:
            raise ValueError("The first participant must be able to produce a handoff messages.")

    def _create_group_chat_manager_factory(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        participant_topic_types: List[str],
        participant_names: List[str],
        participant_descriptions: List[str],
        output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination],
        termination_condition: TerminationCondition | None,
        max_turns: int | None,
        message_factory: MessageFactory,
    ) -> Callable[[], SwarmGroupChatManager]:
        def _factory() -> SwarmGroupChatManager:
            return SwarmGroupChatManager(
                name,
                group_topic_type,
                output_topic_type,
                participant_topic_types,
                participant_names,
                participant_descriptions,
                output_message_queue,
                termination_condition,
                max_turns,
                message_factory,
                self._emit_team_events,
            )

        return _factory

    def _to_config(self) -> SwarmConfig:
        participants = [participant.dump_component() for participant in self._participants]
        termination_condition = self._termination_condition.dump_component() if self._termination_condition else None
        return SwarmConfig(
            name=self._name,
            description=self._description,
            participants=participants,
            termination_condition=termination_condition,
            max_turns=self._max_turns,
            emit_team_events=self._emit_team_events,
        )

    @classmethod
    def _from_config(cls, config: SwarmConfig) -> "Swarm":
        participants = [ChatAgent.load_component(participant) for participant in config.participants]
        termination_condition = (
            TerminationCondition.load_component(config.termination_condition) if config.termination_condition else None
        )
        return cls(
            participants,
            name=config.name,
            description=config.description,
            termination_condition=termination_condition,
            max_turns=config.max_turns,
            emit_team_events=config.emit_team_events,
        )

from state import RoundRobinManagerState

# From _group_chat/_round_robin_group_chat.py
class RoundRobinGroupChatManager(BaseGroupChatManager):
    """A group chat manager that selects the next speaker in a round-robin fashion."""

    def __init__(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        participant_topic_types: List[str],
        participant_names: List[str],
        participant_descriptions: List[str],
        output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination],
        termination_condition: TerminationCondition | None,
        max_turns: int | None,
        message_factory: MessageFactory,
        emit_team_events: bool,
    ) -> None:
        super().__init__(
            name,
            group_topic_type,
            output_topic_type,
            participant_topic_types,
            participant_names,
            participant_descriptions,
            output_message_queue,
            termination_condition,
            max_turns,
            message_factory,
            emit_team_events,
        )
        self._next_speaker_index = 0

    async def validate_group_state(self, messages: List[BaseChatMessage] | None) -> None:
        pass

    async def reset(self) -> None:
        self._current_turn = 0
        self._message_thread.clear()
        if self._termination_condition is not None:
            await self._termination_condition.reset()
        self._next_speaker_index = 0

    async def save_state(self) -> Mapping[str, Any]:
        state = RoundRobinManagerState(
            message_thread=[message.dump() for message in self._message_thread],
            current_turn=self._current_turn,
            next_speaker_index=self._next_speaker_index,
        )
        return state.model_dump()

    async def load_state(self, state: Mapping[str, Any]) -> None:
        round_robin_state = RoundRobinManagerState.model_validate(state)
        self._message_thread = [self._message_factory.create(message) for message in round_robin_state.message_thread]
        self._current_turn = round_robin_state.current_turn
        self._next_speaker_index = round_robin_state.next_speaker_index

    async def select_speaker(self, thread: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str] | str:
        """Select a speaker from the participants in a round-robin fashion.

        .. note::

            This method always returns a single speaker.
        """
        current_speaker_index = self._next_speaker_index
        self._next_speaker_index = (current_speaker_index + 1) % len(self._participant_names)
        current_speaker = self._participant_names[current_speaker_index]
        return current_speaker

# From _group_chat/_round_robin_group_chat.py
class RoundRobinGroupChatConfig(BaseModel):
    """The declarative configuration RoundRobinGroupChat."""

    name: str | None = None
    description: str | None = None
    participants: List[ComponentModel]
    termination_condition: ComponentModel | None = None
    max_turns: int | None = None
    emit_team_events: bool = False

# From _group_chat/_round_robin_group_chat.py
class RoundRobinGroupChat(BaseGroupChat, Component[RoundRobinGroupChatConfig]):
    """A team that runs a group chat with participants taking turns in a round-robin fashion
    to publish a message to all.

    If an :class:`~autogen_agentchat.base.ChatAgent` is a participant,
    the :class:`~autogen_agentchat.messages.BaseChatMessage` from the agent response's
    :attr:`~autogen_agentchat.base.Response.chat_message` will be published
    to other participants in the group chat.

    If a :class:`~autogen_agentchat.base.Team` is a participant,
    the :class:`~autogen_agentchat.messages.BaseChatMessage`
    from the team result' :attr:`~autogen_agentchat.base.TaskResult.messages` will be published
    to other participants in the group chat.

    If a single participant is in the team, the participant will be the only speaker.

    Args:
        participants (List[ChatAgent | Team]): The participants in the group chat.
        name (str | None, optional): The name of the group chat, using :attr:`~autogen_agentchat.teams.RoundRobinGroupChat.DEFAULT_NAME` if not provided.
            The name is used by a parent team to identify this group chat so it must be unique within the parent team.
        description (str | None, optional): The description of the group chat, using :attr:`~autogen_agentchat.teams.RoundRobinGroupChat.DEFAULT_DESCRIPTION` if not provided.
        termination_condition (TerminationCondition, optional): The termination condition for the group chat. Defaults to None.
            Without a termination condition, the group chat will run indefinitely.
        max_turns (int, optional): The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
        custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional): A list of custom message types that will be used in the group chat.
            If you are using custom message types or your agents produces custom message types, you need to specify them here.
            Make sure your custom message types are subclasses of :class:`~autogen_agentchat.messages.BaseAgentEvent` or :class:`~autogen_agentchat.messages.BaseChatMessage`.
        emit_team_events (bool, optional): Whether to emit team events through :meth:`BaseGroupChat.run_stream`. Defaults to False.

    Raises:
        ValueError: If no participants are provided or if participant names are not unique.

    Examples:

        A team with one participant with tools:

            .. code-block:: python

                import asyncio
                from autogen_ext.models.openai import OpenAIChatCompletionClient
                from autogen_agentchat.agents import AssistantAgent
                from autogen_agentchat.teams import RoundRobinGroupChat
                from autogen_agentchat.conditions import TextMentionTermination
                from autogen_agentchat.ui import Console


                async def main() -> None:
                    model_client = OpenAIChatCompletionClient(model="gpt-4o")

                    async def get_weather(location: str) -> str:
                        return f"The weather in {location} is sunny."

                    assistant = AssistantAgent(
                        "Assistant",
                        model_client=model_client,
                        tools=[get_weather],
                    )
                    termination = TextMentionTermination("TERMINATE")
                    team = RoundRobinGroupChat([assistant], termination_condition=termination)
                    await Console(team.run_stream(task="What's the weather in New York?"))


                asyncio.run(main())

        A team with multiple participants:

            .. code-block:: python

                import asyncio
                from autogen_ext.models.openai import OpenAIChatCompletionClient
                from autogen_agentchat.agents import AssistantAgent
                from autogen_agentchat.teams import RoundRobinGroupChat
                from autogen_agentchat.conditions import TextMentionTermination
                from autogen_agentchat.ui import Console


                async def main() -> None:
                    model_client = OpenAIChatCompletionClient(model="gpt-4o")

                    agent1 = AssistantAgent("Assistant1", model_client=model_client)
                    agent2 = AssistantAgent("Assistant2", model_client=model_client)
                    termination = TextMentionTermination("TERMINATE")
                    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
                    await Console(team.run_stream(task="Tell me some jokes."))


                asyncio.run(main())

        A team of user proxy and a nested team of writer and reviewer agents:

            .. code-block:: python

                import asyncio

                from autogen_agentchat.agents import UserProxyAgent, AssistantAgent
                from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination
                from autogen_agentchat.teams import RoundRobinGroupChat
                from autogen_agentchat.ui import Console
                from autogen_ext.models.openai import OpenAIChatCompletionClient


                async def main() -> None:
                    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")

                    writer = AssistantAgent(
                        "writer", model_client=model_client, system_message="You are a writer.", model_client_stream=True
                    )

                    reviewer = AssistantAgent(
                        "reviewer",
                        model_client=model_client,
                        system_message="Provide feedback to the input and suggest improvements.",
                        model_client_stream=True,
                    )

                    # NOTE: you can skip input by pressing Enter.
                    user_proxy = UserProxyAgent("user_proxy")

                    # Maximum 1 round of review and revision.
                    inner_termination = MaxMessageTermination(max_messages=4)

                    # The outter-loop termination condition that will terminate the team when the user types "exit".
                    outter_termination = TextMentionTermination("exit", sources=["user_proxy"])

                    team = RoundRobinGroupChat(
                        [
                            # For each turn, the writer writes a summary and the reviewer reviews it.
                            RoundRobinGroupChat([writer, reviewer], termination_condition=inner_termination),
                            # The user proxy gets user input once the writer and reviewer have finished their actions.
                            user_proxy,
                        ],
                        termination_condition=outter_termination,
                    )
                    # Start the team and wait for it to terminate.
                    await Console(team.run_stream(task="Write a short essay about the impact of AI on society."))


                asyncio.run(main())
    """

    component_config_schema = RoundRobinGroupChatConfig
    component_provider_override = "autogen_agentchat.teams.RoundRobinGroupChat"

    DEFAULT_NAME = "RoundRobinGroupChat"
    DEFAULT_DESCRIPTION = "A team of agents."

    def __init__(
        self,
        participants: List[ChatAgent | Team],
        *,
        name: str | None = None,
        description: str | None = None,
        termination_condition: TerminationCondition | None = None,
        max_turns: int | None = None,
        runtime: AgentRuntime | None = None,
        custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None,
        emit_team_events: bool = False,
    ) -> None:
        super().__init__(
            name=name or self.DEFAULT_NAME,
            description=description or self.DEFAULT_DESCRIPTION,
            participants=participants,
            group_chat_manager_name="RoundRobinGroupChatManager",
            group_chat_manager_class=RoundRobinGroupChatManager,
            termination_condition=termination_condition,
            max_turns=max_turns,
            runtime=runtime,
            custom_message_types=custom_message_types,
            emit_team_events=emit_team_events,
        )

    def _create_group_chat_manager_factory(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        participant_topic_types: List[str],
        participant_names: List[str],
        participant_descriptions: List[str],
        output_message_queue: asyncio.Queue[BaseAgentEvent | BaseChatMessage | GroupChatTermination],
        termination_condition: TerminationCondition | None,
        max_turns: int | None,
        message_factory: MessageFactory,
    ) -> Callable[[], RoundRobinGroupChatManager]:
        def _factory() -> RoundRobinGroupChatManager:
            return RoundRobinGroupChatManager(
                name,
                group_topic_type,
                output_topic_type,
                participant_topic_types,
                participant_names,
                participant_descriptions,
                output_message_queue,
                termination_condition,
                max_turns,
                message_factory,
                self._emit_team_events,
            )

        return _factory

    def _to_config(self) -> RoundRobinGroupChatConfig:
        participants = [participant.dump_component() for participant in self._participants]
        termination_condition = self._termination_condition.dump_component() if self._termination_condition else None
        return RoundRobinGroupChatConfig(
            name=self._name,
            description=self._description,
            participants=participants,
            termination_condition=termination_condition,
            max_turns=self._max_turns,
            emit_team_events=self._emit_team_events,
        )

    @classmethod
    def _from_config(cls, config: RoundRobinGroupChatConfig) -> Self:
        participants: List[ChatAgent | Team] = []
        for participant in config.participants:
            if participant.component_type == Team.component_type:
                participants.append(Team.load_component(participant))
            else:
                participants.append(ChatAgent.load_component(participant))

        termination_condition = (
            TerminationCondition.load_component(config.termination_condition) if config.termination_condition else None
        )
        return cls(
            participants,
            name=config.name,
            description=config.description,
            termination_condition=termination_condition,
            max_turns=config.max_turns,
            emit_team_events=config.emit_team_events,
        )

from autogen_agentchat.base import ChatAgent
from _digraph_group_chat import DiGraph
from _digraph_group_chat import DiGraphEdge
from _digraph_group_chat import DiGraphNode

# From _graph/_graph_builder.py
class DiGraphBuilder:
    """
    A fluent builder for constructing :class:`DiGraph` execution graphs used in :class:`GraphFlow`.

    .. warning::

        This is an experimental feature, and the API will change in the future releases.

    This utility provides a convenient way to programmatically build a graph of agent interactions,
    including complex execution flows such as:

    - Sequential chains
    - Parallel fan-outs
    - Conditional branching
    - Cyclic loops with safe exits

    Each node in the graph represents an agent. Edges define execution paths between agents,
    and can optionally be conditioned on message content using callable functions.

    The builder is compatible with the `Graph` runner and supports both standard and filtered agents.

    Methods:
        - add_node(agent, activation): Add an agent node to the graph.
        - add_edge(source, target, condition): Connect two nodes optionally with a condition.
        - add_conditional_edges(source, condition_to_target): Add multiple conditional edges from a source.
        - set_entry_point(agent): Define the default start node (optional).
        - build(): Generate a validated `DiGraph`.
        - get_participants(): Return the list of added agents.

    Example — Sequential Flow A → B → C:
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
        >>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
        >>> team = Graph(
        ...     participants=builder.get_participants(),
        ...     graph=builder.build(),
        ...     termination_condition=MaxMessageTermination(5),
        ... )

    Example — Parallel Fan-out A → (B, C):
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
        >>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)

    Example — Conditional Branching A → B or A → C:
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
        >>> # Add conditional edges using keyword check
        >>> builder.add_edge(agent_a, agent_b, condition="keyword1")
        >>> builder.add_edge(agent_a, agent_c, condition="keyword2")


    Example — Using Custom String Conditions:
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
        >>> # Add condition strings to check in messages
        >>> builder.add_edge(agent_a, agent_b, condition="big")
        >>> builder.add_edge(agent_a, agent_c, condition="small")

    Example — Loop: A → B → A or B → C:
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
        >>> builder.add_edge(agent_a, agent_b)
        >> # Add a loop back to agent A
        >>> builder.add_edge(agent_b, agent_a, condition=lambda msg: "loop" in msg.to_model_text())
        >>> # Add exit condition to break the loop
        >>> builder.add_edge(agent_b, agent_c, condition=lambda msg: "loop" not in msg.to_model_text())

    Example — Loop with multiple paths to the same node: A → B → C → B:
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
        >>> builder.add_edge(agent_a, agent_b)
        >>> builder.add_edge(agent_b, agent_c)
        >>> builder.add_edge(agent_c, agent_b, activation_group="loop_back")

    Example — Loop with multiple paths to the same node with any activation condition: A → B → (C1, C2) → B → E(exit):
        >>> builder = GraphBuilder()
        >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c1).add_node(agent_c2).add_node(agent_e)
        >>> builder.add_edge(agent_a, agent_b)
        >>> builder.add_edge(agent_b, agent_c1)
        >>> builder.add_edge(agent_b, agent_c2)
        >>> builder.add_edge(agent_b, agent_e, condition="exit")
        >>> builder.add_edge(agent_c1, agent_b, activation_group="loop_back_group", activation_condition="any")
        >>> builder.add_edge(agent_c2, agent_b, activation_group="loop_back_group", activation_condition="any")
    """

    def __init__(self) -> None:
        self.nodes: Dict[str, DiGraphNode] = {}
        self.agents: Dict[str, ChatAgent] = {}
        self._default_start_node: Optional[str] = None

    def _get_name(self, obj: Union[str, ChatAgent]) -> str:
        return obj if isinstance(obj, str) else obj.name

    def add_node(self, agent: ChatAgent, activation: Literal["all", "any"] = "all") -> "DiGraphBuilder":
        """Add a node to the graph and register its agent."""
        name = agent.name
        if name not in self.nodes:
            self.nodes[name] = DiGraphNode(name=name, edges=[], activation=activation)
            self.agents[name] = agent
        return self

    def add_edge(
        self,
        source: Union[str, ChatAgent],
        target: Union[str, ChatAgent],
        condition: Optional[Union[str, Callable[[BaseChatMessage], bool]]] = None,
        activation_group: Optional[str] = None,
        activation_condition: Optional[Literal["all", "any"]] = None,
    ) -> "DiGraphBuilder":
        """Add a directed edge from source to target, optionally with a condition.

        Args:
            source: Source node (agent name or agent object)
            target: Target node (agent name or agent object)
            condition: Optional condition for edge activation.
                If string, activates when substring is found in message.
                If callable, activates when function returns True for the message.

        Returns:
            Self for method chaining

        Raises:
            ValueError: If source or target node doesn't exist in the builder
        """
        source_name = self._get_name(source)
        target_name = self._get_name(target)

        if source_name not in self.nodes:
            raise ValueError(f"Source node '{source_name}' must be added before adding an edge.")
        if target_name not in self.nodes:
            raise ValueError(f"Target node '{target_name}' must be added before adding an edge.")
        if activation_group is None:
            activation_group = target_name
        if activation_condition is None:
            activation_condition = "all"
        self.nodes[source_name].edges.append(
            DiGraphEdge(
                target=target_name,
                condition=condition,
                activation_group=activation_group,
                activation_condition=activation_condition,
            )
        )
        return self

    def add_conditional_edges(
        self, source: Union[str, ChatAgent], condition_to_target: Dict[str, Union[str, ChatAgent]]
    ) -> "DiGraphBuilder":
        """Add multiple conditional edges from a source node based on keyword checks.

        .. warning::

            This method interface will be changed in the future to support callable conditions.
            Please use `add_edge` if you need to specify custom conditions.

        Args:
            source: Source node (agent name or agent object)
            condition_to_target: Mapping from condition strings to target nodes
                Each key is a keyword that will be checked in the message content
                Each value is the target node to activate when condition is met

                For each key (keyword), a lambda will be created that checks
                if the keyword is in the message text.

        Returns:
            Self for method chaining
        """

        warnings.warn(
            "add_conditional_edges will be changed in the future to support callable conditions. "
            "For now, please use add_edge if you need to specify custom conditions.",
            DeprecationWarning,
            stacklevel=2,
        )

        for condition_keyword, target in condition_to_target.items():
            self.add_edge(source, target, condition=condition_keyword)
        return self

    def set_entry_point(self, name: Union[str, ChatAgent]) -> "DiGraphBuilder":
        """Set the default start node of the graph."""
        node_name = self._get_name(name)
        if node_name not in self.nodes:
            raise ValueError(f"Start node '{node_name}' must be added before setting as entry point.")
        self._default_start_node = node_name
        return self

    def build(self) -> DiGraph:
        """Build and validate the DiGraph."""
        graph = DiGraph(
            nodes=self.nodes,
            default_start_node=self._default_start_node,
        )
        graph.graph_validate()
        return graph

    def get_participants(self) -> list[ChatAgent]:
        """Return the list of agents in the builder, in insertion order."""
        return list(self.agents.values())

# From _graph/_graph_builder.py
def add_node(self, agent: ChatAgent, activation: Literal["all", "any"] = "all") -> "DiGraphBuilder":
        """Add a node to the graph and register its agent."""
        name = agent.name
        if name not in self.nodes:
            self.nodes[name] = DiGraphNode(name=name, edges=[], activation=activation)
            self.agents[name] = agent
        return self

# From _graph/_graph_builder.py
def add_edge(
        self,
        source: Union[str, ChatAgent],
        target: Union[str, ChatAgent],
        condition: Optional[Union[str, Callable[[BaseChatMessage], bool]]] = None,
        activation_group: Optional[str] = None,
        activation_condition: Optional[Literal["all", "any"]] = None,
    ) -> "DiGraphBuilder":
        """Add a directed edge from source to target, optionally with a condition.

        Args:
            source: Source node (agent name or agent object)
            target: Target node (agent name or agent object)
            condition: Optional condition for edge activation.
                If string, activates when substring is found in message.
                If callable, activates when function returns True for the message.

        Returns:
            Self for method chaining

        Raises:
            ValueError: If source or target node doesn't exist in the builder
        """
        source_name = self._get_name(source)
        target_name = self._get_name(target)

        if source_name not in self.nodes:
            raise ValueError(f"Source node '{source_name}' must be added before adding an edge.")
        if target_name not in self.nodes:
            raise ValueError(f"Target node '{target_name}' must be added before adding an edge.")
        if activation_group is None:
            activation_group = target_name
        if activation_condition is None:
            activation_condition = "all"
        self.nodes[source_name].edges.append(
            DiGraphEdge(
                target=target_name,
                condition=condition,
                activation_group=activation_group,
                activation_condition=activation_condition,
            )
        )
        return self

# From _graph/_graph_builder.py
def add_conditional_edges(
        self, source: Union[str, ChatAgent], condition_to_target: Dict[str, Union[str, ChatAgent]]
    ) -> "DiGraphBuilder":
        """Add multiple conditional edges from a source node based on keyword checks.

        .. warning::

            This method interface will be changed in the future to support callable conditions.
            Please use `add_edge` if you need to specify custom conditions.

        Args:
            source: Source node (agent name or agent object)
            condition_to_target: Mapping from condition strings to target nodes
                Each key is a keyword that will be checked in the message content
                Each value is the target node to activate when condition is met

                For each key (keyword), a lambda will be created that checks
                if the keyword is in the message text.

        Returns:
            Self for method chaining
        """

        warnings.warn(
            "add_conditional_edges will be changed in the future to support callable conditions. "
            "For now, please use add_edge if you need to specify custom conditions.",
            DeprecationWarning,
            stacklevel=2,
        )

        for condition_keyword, target in condition_to_target.items():
            self.add_edge(source, target, condition=condition_keyword)
        return self

# From _graph/_graph_builder.py
def set_entry_point(self, name: Union[str, ChatAgent]) -> "DiGraphBuilder":
        """Set the default start node of the graph."""
        node_name = self._get_name(name)
        if node_name not in self.nodes:
            raise ValueError(f"Start node '{node_name}' must be added before setting as entry point.")
        self._default_start_node = node_name
        return self

# From _graph/_graph_builder.py
def build(self) -> DiGraph:
        """Build and validate the DiGraph."""
        graph = DiGraph(
            nodes=self.nodes,
            default_start_node=self._default_start_node,
        )
        graph.graph_validate()
        return graph

# From _graph/_graph_builder.py
def get_participants(self) -> list[ChatAgent]:
        """Return the list of agents in the builder, in insertion order."""
        return list(self.agents.values())


# From _magentic_one/_prompts.py
class LedgerEntryBooleanAnswer(BaseModel):
    reason: str
    answer: bool

# From _magentic_one/_prompts.py
class LedgerEntryStringAnswer(BaseModel):
    reason: str
    answer: str

# From _magentic_one/_prompts.py
class LedgerEntry(BaseModel):
    is_request_satisfied: LedgerEntryBooleanAnswer
    is_in_loop: LedgerEntryBooleanAnswer
    is_progress_being_made: LedgerEntryBooleanAnswer
    next_speaker: LedgerEntryStringAnswer
    instruction_or_question: LedgerEntryStringAnswer

import pandas
import tabulate
from load_module import load_module

# From agbench/tabulate_cmd.py
def find_tabulate_module(search_dir: str, stop_dir: Optional[str] = None) -> Optional[str]:
    """Hunt for the tabulate script."""

    search_dir = os.path.abspath(search_dir)
    if not os.path.isdir(search_dir):
        raise ValueError(f"'{search_dir}' is not a directory.")

    stop_dir = None if stop_dir is None else os.path.abspath(stop_dir)

    while True:
        path = os.path.join(search_dir, TABULATE_FILE)
        if os.path.isfile(path):
            return path

        path = os.path.join(search_dir, "Scripts", TABULATE_FILE)
        if os.path.isfile(path):
            return path

        path = os.path.join(search_dir, "scripts", TABULATE_FILE)
        if os.path.isfile(path):
            return path

        # Stop if we hit the stop_dir
        if search_dir == stop_dir:
            break

        # Stop if we hit the root
        parent_dir = os.path.abspath(os.path.join(search_dir, os.pardir))
        if parent_dir == search_dir:
            break

        search_dir = parent_dir

    return None

# From agbench/tabulate_cmd.py
def default_scorer(instance_dir: str, success_strings: List[str] = SUCCESS_STRINGS) -> Optional[bool]:
    console_log = os.path.join(instance_dir, "console_log.txt")
    if os.path.isfile(console_log):
        with open(console_log, "rt") as fh:
            content = fh.read()

            # It succeeded
            for s in success_strings:
                if s in content:
                    return True

            # It completed without succeeding
            for s in COMPLETED_STRINGS:
                if s in content:
                    return False

            # Has not, or did not, complete
            return None
    else:
        return None

# From agbench/tabulate_cmd.py
def default_timer(instance_dir: str, timer_regex: str = TIMER_REGEX) -> Optional[float]:
    console_log = os.path.join(instance_dir, "console_log.txt")
    if os.path.isfile(console_log):
        with open(console_log, "rt") as fh:
            content = fh.read()

            # It succeeded
            m = re.search(timer_regex, content)
            if m:
                return float(m.group(1))
            else:
                return None
    else:
        return None

# From agbench/tabulate_cmd.py
def default_tabulate(
    args: List[str],
    scorer: ScorerFunc = default_scorer,
    timer: TimerFunc = default_timer,
    exclude_dir_names: List[str] = EXCLUDE_DIR_NAMES,
) -> None:
    invocation_cmd = args[0]
    args = args[1:]

    warning = f"CAUTION: '{invocation_cmd}' is in early preview and is not thoroughly tested.\nPlease do not cite values from these calculations in academic work without first inspecting and verifying the results in the run logs yourself."

    # Prepare the argument parser
    parser = argparse.ArgumentParser(
        prog=invocation_cmd,
        description=f"{invocation_cmd} will tabulate the results of a previous run.",
    )

    parser.add_argument(
        "runlogs",
        help="The path where the run's logs are stored.",
    )
    parser.add_argument(
        "-c",
        "--csv",
        action="store_true",
        help="Output the results in CSV format.",
    )

    parser.add_argument(
        "-e", "--excel", help="Output the results in Excel format. Please specify a path for the Excel file.", type=str
    )

    parsed_args = parser.parse_args(args)
    runlogs: str = parsed_args.runlogs

    all_results: List[Dict[str, Any]] = list()
    max_instances = 0

    for task_id in sorted(
        os.listdir(runlogs),
        key=lambda s: os.path.getmtime(os.path.join(runlogs, s)),
    ):
        if task_id in exclude_dir_names:
            continue

        task_path = os.path.join(runlogs, task_id)

        if not os.path.isdir(task_path):
            continue

        # Collect the results vector
        results: Dict[str, Any] = {"Task Id": task_id}

        # Collect the results for each instance.
        instance_dirs = sorted(
            os.listdir(task_path),
            key=lambda s: os.path.getmtime(os.path.join(task_path, s)),
        )
        instances = [int(d) for d in instance_dirs if d.isdigit()]

        for instance in instances:
            instance_dir = os.path.join(task_path, str(instance))
            results[f"Trial {instance} Success"] = scorer(instance_dir)
            results[f"Trial {instance} Time"] = timer(instance_dir)

        max_instances = max(instances)

        # Buffer the results
        all_results.append(results)

    num_instances = max_instances + 1

    # Pad the results to max_instances
    for result in all_results:
        for i in range(num_instances):
            if f"Trial {i} Success" not in result:
                result[f"Trial {i} Success"] = None
            if f"Trial {i} Time" not in result:
                result[f"Trial {i} Time"] = None

    # Create dataframe from results.
    df = pd.DataFrame(all_results)

    if parsed_args.csv:
        # Print out the dataframe in CSV format
        print(df.to_csv(index=False))
        # Print out alpha-version warning
        sys.stderr.write("\n" + warning + "\n\n")
    else:
        # Tabulate the results.
        print(tb.tabulate(df, headers="keys", tablefmt="simple"))  # type: ignore

        def _check_true(x: Any) -> Any:
            if isinstance(x, pd.Series):
                return x.apply(lambda y: y is True)  # type: ignore
            else:
                return x is True

        def _check_false(x: Any) -> Any:
            if isinstance(x, pd.Series):
                return x.apply(lambda y: y is False)  # type: ignore
            else:
                return x is False

        # Aggregate statistics for all tasks for each trials.
        print("\nSummary Statistics\n")
        score_columns = ["Trial " + str(i) + " Success" for i in range(num_instances)]
        # Count the number of successes when the value is True.
        successes = df[score_columns].apply(_check_true).sum(axis=0)  # type: ignore
        # Count the number of failures when the value is False.
        failures: pd.Series = df[score_columns].apply(_check_false).sum(axis=0)  # type: ignore
        # Count the number of missing
        missings = df[score_columns].isna().sum(axis=0)  # type: ignore
        # Count the total number of instances
        totals = successes + failures + missings  # type: ignore
        # Calculate the average success rates
        avg_success_rates = successes / (successes + failures)  # type: ignore
        time_columns = ["Trial " + str(i) + " Time" for i in range(num_instances)]  # type: ignore
        # Count the total time of non-null values
        total_times = df[time_columns].sum(axis=0, skipna=True)  # type: ignore
        # Calculate the average time of non-null values
        avg_times = df[time_columns].mean(axis=0, skipna=True)  # type: ignore

        def _list(series: Any) -> List[Any]:
            # If iteraable, convert to list
            if hasattr(series, "__iter__") and not isinstance(series, str):
                return list(series)
            else:
                # If not iterable, return the series
                return [series]

        # Create a per-trial summary dataframe
        trial_df = pd.DataFrame(
            {
                "Successes": _list(successes),  # type: ignore
                "Failures": _list(failures),  # type: ignore
                "Missing": _list(missings),  # type: ignore
                "Total": _list(totals),  # type: ignore
                "Average Success Rate": _list(avg_success_rates),  # type: ignore
                "Average Time": _list(avg_times),  # type: ignore
                "Total Time": _list(total_times),  # type: ignore
            },
            index=[f"Trial {i}" for i in range(num_instances)],
        )
        # Print out the per-trial summary dataframe.
        print(tb.tabulate(trial_df, headers="keys", tablefmt="simple"))  # type: ignore

        # Aggregate statistics across tasks for all trials.
        # At least one success for each trial, averaged across tasks.
        average_at_least_one_success = df[score_columns].any(axis=1).mean(skipna=True)  # type: ignore
        # All successes for each trial
        average_all_successes = df[score_columns].all(axis=1).mean(skipna=True)  # type: ignore

        # Create a dataframe
        trial_aggregated_df = pd.DataFrame(
            {
                "At Least One Success": [average_at_least_one_success],  # type: ignore
                "All Successes": [average_all_successes],  # type: ignore
            },
            index=["Trial Aggregated"],
        )
        # Print out the trial-aggregated dataframe.
        print(tb.tabulate(trial_aggregated_df, headers="keys", tablefmt="simple"))  # type: ignore

        # Print out alpha-version warning
        sys.stderr.write("\n" + warning + "\n\n")

# From agbench/tabulate_cmd.py
def tabulate_cli(args: Sequence[str]) -> None:
    invocation_cmd = args[0]
    args = args[1:]

    # We won't assume much about the arguments, letting the dynamically-loaded
    # tabulate modules parse the arguments however they want. But, we will use
    # bare arguments (not starting a "-"), to help us find what module to load.
    module_path = find_tabulate_module(os.getcwd(), stop_dir=os.getcwd())
    for arg in reversed(args):
        if module_path is not None:
            break
        if arg.startswith("-"):
            continue
        module_path = find_tabulate_module(arg)

    # Load the module and hand over control
    if module_path is None:
        sys.stderr.write("Using default tabulation method.\n\n")
        default_tabulate([invocation_cmd] + list(args))
    else:
        sys.stderr.write(f"Using tabulation method defined in '{module_path}'\n\n")
        load_module(module_path).main([invocation_cmd] + list(args))

import importlib.util
from types import ModuleType

# From agbench/load_module.py
def load_module(module_path: str) -> ModuleType:
    module_name = os.path.basename(module_path).replace(".py", "")
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    if spec is None:
        raise ValueError(f"Could not load module from path: {module_path}")
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    assert spec.loader is not None
    spec.loader.exec_module(module)
    return module

import errno
import pathlib
import random
import stat
from multiprocessing import Pool
import docker
from azure.core.exceptions import ClientAuthenticationError
from azure.identity import DefaultAzureCredential
from azure.identity import get_bearer_token_provider
from docker.errors import APIError
from docker.errors import DockerException
from docker.errors import ImageNotFound
from version import __version__

# From agbench/run_cmd.py
class ScenarioInstance(TypedDict):
    id: str
    template: Union[str, List[Union[str, List[str]]]]
    substitutions: Dict[str, Dict[str, str]]
    values: Dict[str, Dict[str, str]]

# From agbench/run_cmd.py
def run_scenarios(
    scenario: str,
    n_repeats: int,
    is_native: bool,
    config_file: Union[None, str],
    token_provider: Optional[Callable[[], str]],
    docker_image: Optional[str] = None,
    results_dir: str = "Results",
    subsample: Union[None, int, float] = None,
    env_file: Union[None, str] = None,
) -> None:
    """
    Run a set agbench scenarios a given number of times.

    Args:
        scenario (path):    The file or folder containing the scenario JSONL instances. If given a folder, then
                            all JSONL files in the folder will be loaded and run.
        n_repeats (int):    The number of times each scenario instance will be repeated
        is_native (bool):   True if the scenario should be run locally rather than in Docker (proceed with caution!)
        results_dir (path): The folder were results will be saved.
    """

    files: List[str] = []

    # Figure out which files or folders we are working with
    if scenario == "-" or os.path.isfile(scenario):
        files.append(scenario)
    elif os.path.isdir(scenario):
        for f in os.listdir(scenario):
            scenario_file = os.path.join(scenario, f)

            if not os.path.isfile(scenario_file):
                continue

            if not scenario_file.lower().endswith(".jsonl"):
                continue

            files.append(scenario_file)
    else:
        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), scenario)

    # Run all the scenario files
    for scenario_file in files:
        scenario_name: Optional[str] = None
        scenario_dir: Optional[str] = None
        file_handle = None

        # stdin
        if scenario_file == "-":
            scenario_name = "stdin"
            scenario_dir = "."
            file_handle = sys.stdin
        else:
            scenario_name_parts = os.path.basename(scenario_file).split(".")
            scenario_name_parts.pop()
            scenario_name = ".".join(scenario_name_parts)
            scenario_dir = os.path.dirname(os.path.realpath(scenario_file))
            file_handle = open(scenario_file, "rt")

        # Read all the lines, then subsample if needed
        lines = [line for line in file_handle]
        if subsample is not None:
            # How many lines are we sampling
            n = 0
            # It's a proportion
            if 0 <= subsample < 1:
                n = int(len(lines) * subsample + 0.5)
            # It's a raw count
            else:
                n = int(subsample)
            n = max(0, min(n, len(lines)))
            lines = subsample_rng.sample(lines, n)

        for line in lines:
            instance = json.loads(line)

            # Create a folder to store the results
            # Results base
            if not os.path.isdir(results_dir):
                os.mkdir(results_dir)

            # Results for the scenario
            results_scenario = os.path.join(results_dir, scenario_name)
            if not os.path.isdir(results_scenario):
                os.mkdir(results_scenario)

            # Results for the instance
            results_instance = os.path.join(results_scenario, instance["id"])
            if not os.path.isdir(results_instance):
                os.mkdir(results_instance)

            # Results for the repeats
            for i in range(0, n_repeats):
                results_repetition = os.path.join(results_instance, str(i))

                # Skip it if it already exists
                if os.path.isdir(results_repetition):
                    print(f"Found folder {results_repetition} ... Skipping.")
                    continue
                print(f"Running scenario {results_repetition}")

                # Expand the scenario
                expand_scenario(scenario_dir, instance, results_repetition, config_file)

                # Prepare the environment (keys/values that need to be added)
                env = get_scenario_env(token_provider=token_provider, env_file=env_file)

                # Run the scenario
                if is_native:
                    run_scenario_natively(results_repetition, env)
                else:
                    run_scenario_in_docker(
                        results_repetition,
                        env,
                        docker_image=docker_image,
                    )

        # Close regular files
        if scenario_file != "-":
            file_handle.close()

# From agbench/run_cmd.py
def expand_scenario(
    scenario_dir: str, scenario: ScenarioInstance, output_dir: str, config_file: Union[str, None]
) -> None:
    """
    Expand a scenario into a folder.
    Despite some awkwardness created by backwards compatibility and notational conveniences, expansion is conceptually simple.
    It is a series of copy commands (similar to `cp -R`), followed by a series of in-place fine and replace operations.
    """

    template = scenario["template"]

    # Either key works for finding the substiturions list. "values" may be deprecated in the future
    substitutions = scenario["substitutions"] if "substitutions" in scenario else scenario["values"]

    # Older versions are only one-level deep. Convert them,
    if len(substitutions) > 0 and isinstance(substitutions[next(iter(substitutions))], str):
        substitutions = {"scenario.py": cast(Dict[str, str], substitutions)}

    copy_operations: List[Tuple[str, str]] = []

    # Handle file (str), folder (str), or mapping (List) templates
    if isinstance(template, str):
        template_path = os.path.join(scenario_dir, template)
        if os.path.isdir(template_path):
            copy_operations.append((template, ""))
        else:
            copy_operations.append((template, "scenario.py"))
    elif isinstance(template, list):
        for elm in template:
            if isinstance(elm, list):
                copy_operations.append((elm[0], elm[1]))
            else:
                copy_operations.append((elm, ""))
    else:
        raise ValueError("expand_scenario expects an str or list for 'template'")

    # The global includes folder is always copied
    shutil.copytree(
        BASE_TEMPLATE_PATH,
        output_dir,
        ignore=shutil.ignore_patterns("*.example"),
        dirs_exist_ok=False,
    )

    # Expand other folders
    for items in copy_operations:
        src_path = pathlib.Path(os.path.join(scenario_dir, items[0])).absolute()
        dest_path = pathlib.Path(os.path.join(output_dir, items[1])).absolute()

        if os.path.isdir(src_path):
            shutil.copytree(src_path, dest_path, dirs_exist_ok=True)
        else:
            if os.path.isdir(dest_path):
                # If the destination is a directory, use the same filename
                shutil.copyfile(src_path, os.path.join(dest_path, os.path.basename(src_path)))
            else:
                # Otherwuse use the filename provided
                shutil.copyfile(src_path, dest_path)

    # Expand templated files
    for templated_file in substitutions.keys():  # Keys are relative file paths
        # Read the templated file into memory
        template_contents: List[str] = list()
        with open(os.path.join(output_dir, templated_file), "rt") as fh:
            for line in fh:
                template_contents.append(line)

        # Rewrite the templated file with substitutions
        values = substitutions[templated_file]
        with open(os.path.join(output_dir, templated_file), "wt") as fh:
            for line in template_contents:
                for k, v in values.items():
                    line = line.replace(k, v)
                fh.write(line)

    # Copy the config
    if config_file is None:
        if os.path.isfile(DEFAULT_CONFIG_YAML):
            config_file = DEFAULT_CONFIG_YAML

    if config_file is not None:
        src_path = pathlib.Path(config_file).absolute()
        dest_path = pathlib.Path(os.path.join(output_dir, "config.yaml")).absolute()
        shutil.copyfile(src_path, dest_path)
    else:
        logging.warning(f"No {DEFAULT_CONFIG_YAML} file found.")

# From agbench/run_cmd.py
def get_scenario_env(token_provider: Optional[Callable[[], str]] = None, env_file: str | None = None) -> Dict[str, str]:
    """
    Return a dictionary of environment variables needed to run a scenario.

    Args:
        config_list (list): An AutoGen OAI_CONFIG_LIST to be used when running scenarios.
        env_file (str): The path to the env_file to read. (if None, default to DEFAULT_ENV_FILE)

    Returns: A dictionary of keys and values that need to be added to the system environment.
    """
    env: Dict[str, str] = dict()

    # Populate with commonly needed keys
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    if openai_api_key is not None and len(openai_api_key.strip()) > 0:
        env["OPENAI_API_KEY"] = openai_api_key

    ## Support Azure auth tokens
    azure_openai_ad_token = os.environ.get("AZURE_OPENAI_AD_TOKEN")
    if azure_openai_ad_token is None and token_provider is not None:
        azure_openai_ad_token = token_provider()
    if azure_openai_ad_token is not None and len(azure_openai_ad_token.strip()) > 0:
        env["AZURE_OPENAI_AD_TOKEN"] = azure_openai_ad_token

    # Update with any values from the ENV.json file
    env_file_contents: Dict[str, Any] = {}
    if env_file is None:
        # Env file was not specified, so read the default, or warn if the default file is missing.
        if os.path.isfile(DEFAULT_ENV_FILE_YAML):
            with open(DEFAULT_ENV_FILE_YAML, "r") as fh:
                env_file_contents = yaml.safe_load(fh)
        elif os.path.isfile(DEFAULT_ENV_FILE_JSON):
            with open(DEFAULT_ENV_FILE_JSON, "rt") as fh:
                env_file_contents = json.loads(fh.read())
            logging.warning(f"JSON environment files are deprecated. Migrate to '{DEFAULT_ENV_FILE_YAML}'")
        else:
            logging.warning(
                f"The environment file '{DEFAULT_ENV_FILE_YAML}' was not found. A default environment will be provided, containing the keys: {env.keys()}"
            )
    else:
        # Env file was specified. Throw an error if the file can't be read.
        with open(env_file, "rt") as fh:
            if env_file.endswith(".json"):
                logging.warning("JSON environment files are deprecated. Migrate to YAML")
                env_file_contents = json.loads(fh.read())
            else:
                env_file_contents = yaml.safe_load(fh)

    # Apply substitutions in-place
    substitute_env_variables(env_file_contents)

    # Flatten any structures
    for key, value in env_file_contents.items():
        if isinstance(value, dict) or isinstance(value, list):
            env_file_contents[key] = json.dumps(value)

    # Warn about carrying env variables
    if "OPENAI_API_KEY" in env and "OPENAI_API_KEY" not in env_file_contents:
        logging.warning(
            f"Implicit inclusion of OPENAI_API_KEY in the task environment is deprecated. Add it to {DEFAULT_ENV_FILE_YAML} instead. E.g.,\n"
            + """

OPENAI_API_KEY: ${OPENAI_API_KEY}

"""
        )

    # Apply the loaded variables
    env.update(cast(Dict[str, str], env_file_contents))

    return env

# From agbench/run_cmd.py
def substitute_env_variables(json_data: Any) -> None:
    """
    Recursively replaces any instance of "${ENV_VARIABLE}" with os.environ("ENV_VARIABLE") in a structure returned from json.loads()
    """

    def replace_env_var(match: Any) -> str:
        var_name = match.group(1)
        return os.environ.get(var_name, "")

    pattern = re.compile(r"\$\{(\w+)\}")

    def replace_in_dict(d: Dict[str, Any]) -> None:
        for key, value in d.items():
            if isinstance(value, str):
                d[key] = pattern.sub(replace_env_var, value)
            elif isinstance(value, dict):
                replace_in_dict(cast(Dict[str, Any], value))
            elif isinstance(value, list):
                # Note: with the task mypy complains of a redundant cast
                # without the cast, pyright complains the type is unknown
                replace_in_list(cast(List[Any], value))  # type: ignore

    def replace_in_list(lst: List[Any]) -> None:
        for i, item in enumerate(lst):
            if isinstance(item, str):
                lst[i] = pattern.sub(replace_env_var, item)
            elif isinstance(item, dict):
                replace_in_dict(cast(Dict[str, Any], item))
            elif isinstance(item, list):
                replace_in_list(cast(List[Any], item))  # type: ignore

    if isinstance(json_data, dict):
        replace_in_dict(cast(Dict[str, Any], json_data))
    elif isinstance(json_data, list):
        replace_in_list(cast(List[Any], json_data))

# From agbench/run_cmd.py
def run_scenario_natively(work_dir: str, env: Dict[str, str], timeout: int = TASK_TIMEOUT) -> None:
    """
    Run a scenario in the native environment.

    Args:
        work_dir (path): the path to the working directory previously created to house this sceario instance
    """

    # Get the current working directory
    cwd = os.getcwd()

    # Prepare the environment variables
    full_env = os.environ.copy()
    full_env.update(env)

    # Navigate to the scenario
    os.chdir(work_dir)
    print("\n\n" + os.getcwd() + "\n===================================================================")

    # Prepare the run script
    with open(os.path.join("run.sh"), "wt") as f:
        f.write(
            f"""#
echo RUN.SH STARTING !#!#
export AUTOGEN_TESTBED_SETTING="Native"
echo "agbench version: {__version__}" > timestamp.txt

# Create and activate the virtual environment
# This is called in a subprocess, and will not impact the parent
{sys.executable} -m venv .agbench_venv
. .agbench_venv/bin/activate

# Run the global init script if it exists
if [ -f global_init.sh ] ; then
    . ./global_init.sh
fi

# Run the scenario init script if it exists
if [ -f scenario_init.sh ] ; then
    . ./scenario_init.sh
fi

# Run the scenario
pip install -r requirements.txt
echo SCENARIO.PY STARTING !#!#
start_time=$(date +%s)
timeout --preserve-status --kill-after {timeout  + 30}s {timeout}s python scenario.py
end_time=$(date +%s)
EXIT_CODE=$?
if [ $EXIT_CODE -ne 0 ]; then
    echo SCENARIO.PY EXITED WITH CODE: $EXIT_CODE !#!#
else
    echo SCENARIO.PY COMPLETE !#!#
fi
elapsed_time=$((end_time - start_time))
echo "SCENARIO.PY RUNTIME: $elapsed_time !#!#"

# Clean up
if [ -d .cache ] ; then
    rm -Rf .cache
fi

if [ -d __pycache__ ] ; then
    rm -Rf __pycache__
fi

# Run the scenario finalize script if it exists
if [ -f scenario_finalize.sh ] ; then
    . ./scenario_finalize.sh
fi

# Run the global finalize script if it exists
if [ -f global_finalize.sh ] ; then
    . ./global_finalize.sh
fi

# We don't need to deactivate the venv because it's
# contained in the subprocess; but we should clean it up
if [ -d .agbench_venv ] ; then
    rm -Rf .agbench_venv
fi

echo RUN.SH COMPLETE !#!#
"""
        )

    # Run the script and log the output
    with open("console_log.txt", "wb") as f:
        process = subprocess.Popen(
            ["sh", "run.sh"],
            env=full_env,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        for c in iter(lambda: process.stdout.read(1), b""):  # type: ignore
            f.write(c)
            os.write(sys.stdout.fileno(), c)  # Write binary to stdout

    # Return where we started
    os.chdir(cwd)
    return

# From agbench/run_cmd.py
def run_scenario_in_docker(
    work_dir: str, env: Dict[str, str], timeout: int = TASK_TIMEOUT, docker_image: Optional[str] = None
) -> None:
    """
    Run a scenario in a Docker environment.

    Args:
        work_dir (path): the path to the working directory previously created to house this sceario instance
        timeout (Optional, int): the number of seconds to allow a Docker container to run before timing out
    """

    client = docker.from_env()
    image = None

    # If the docker_image is None, then we will fetch DEFAULT_DOCKER_IMAGE_TAG, if present,
    # or build it if missing.
    if docker_image is None:
        # Pull a suitable image
        try:
            image = client.images.get(DEFAULT_DOCKER_IMAGE_TAG)
        except ImageNotFound:
            print(f"Building default Docker image '{DEFAULT_DOCKER_IMAGE_TAG}'. This may take a few minutes...")
            try:
                build_default_docker_image(client, DEFAULT_DOCKER_IMAGE_TAG)
                image = client.images.get(DEFAULT_DOCKER_IMAGE_TAG)
            except DockerException:
                print(f"Failed to build image '{DEFAULT_DOCKER_IMAGE_TAG}'")

    # Otherwise get the requested image
    else:
        try:
            image = client.images.get(docker_image)
        except ImageNotFound:
            # pull the image
            print(f"Pulling image '{docker_image}'")
            try:
                image = client.images.pull(docker_image)
            except DockerException:
                print(f"Failed to pull image '{docker_image}'")

    # Prepare the run script
    with open(os.path.join(work_dir, "run.sh"), "wt", newline="\n") as f:
        f.write(
            f"""#
echo RUN.SH STARTING !#!#
export AUTOGEN_TESTBED_SETTING="Docker"

umask 000
echo "agbench version: {__version__}" > timestamp.txt

# Run the global init script if it exists
if [ -f global_init.sh ] ; then
    . ./global_init.sh
fi

# Run the scenario init script if it exists
if [ -f scenario_init.sh ] ; then
    . ./scenario_init.sh
fi

# Run the scenario
pip install -r requirements.txt
echo SCENARIO.PY STARTING !#!#
start_time=$(date +%s)
timeout --preserve-status --kill-after {timeout  + 30}s {timeout}s python scenario.py
end_time=$(date +%s)
EXIT_CODE=$?
if [ $EXIT_CODE -ne 0 ]; then
    echo SCENARIO.PY EXITED WITH CODE: $EXIT_CODE !#!#
else
    echo SCENARIO.PY COMPLETE !#!#
fi
elapsed_time=$((end_time - start_time))
echo "SCENARIO.PY RUNTIME: $elapsed_time !#!#"

# Clean up
if [ -d .cache ] ; then
    rm -Rf .cache
fi

if [ -d __pycache__ ] ; then
    rm -Rf __pycache__
fi

# Run the scenario finalize script if it exists
if [ -f scenario_finalize.sh ] ; then
    . ./scenario_finalize.sh
fi

# Run the global finalize script if it exists
if [ -f global_finalize.sh ] ; then
    . ./global_finalize.sh
fi

echo RUN.SH COMPLETE !#!#
"""
        )

    # Figure out what folders to mount
    volumes = {str(pathlib.Path(work_dir).absolute()): {"bind": "/workspace", "mode": "rw"}}

    # Add the autogen repo if we can find it
    autogen_repo_base = os.environ.get("AUTOGEN_REPO_BASE")
    if autogen_repo_base is None:
        autogen_repo_base = find_autogen_repo(os.getcwd())
    elif not os.path.isdir(autogen_repo_base):
        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), autogen_repo_base)

    if autogen_repo_base is None:
        raise ValueError(
            "Could not find AutoGen repo base. Please set the environment variable AUTOGEN_REPO_BASE to the correct value."
        )

    autogen_repo_base = os.path.join(autogen_repo_base, "python")
    volumes[str(pathlib.Path(autogen_repo_base).absolute())] = {"bind": "/autogen_python", "mode": "rw"}

    # Add the Docker socket if we are running on Linux
    # This allows docker-out-of-docker to work, but provides access to the Docker daemon on the host.
    # This maintains good isolation for experiment purposes (e.g., ensuring consistent initial conditions),
    # but deminishes the security benefits of using Docker (e.g., when facing a deliberately malicious agent).
    # since it would allow clients to mount privalaged images, volumes, etc.
    docker_host = os.environ.get("DOCKER_HOST", "unix:///var/run/docker.sock")
    if docker_host.startswith("unix://"):
        docker_socket = os.path.abspath(docker_host[7:])
        if os.path.exists(docker_socket):
            st_mode = os.stat(docker_socket).st_mode
            if stat.S_ISSOCK(st_mode):
                volumes[docker_socket] = {"bind": "/var/run/docker.sock", "mode": "rw"}

                # Update the environment variables so that the inner docker client can
                # mount the workspace
                env = {k: v for k, v in env.items()}
                env["HOST_WORKSPACE"] = str(pathlib.Path(work_dir).absolute())

    print("Mounting:")
    for k in volumes.keys():
        bind = volumes[k]["bind"]
        mode = volumes[k]["mode"].upper()
        if bind == "/workspace":
            k = os.path.relpath(k)
        print(f"[{mode}]\t'{k}' => '{bind}'")
    print("===================================================================")

    assert image is not None
    # Create and run the container
    container = client.containers.run(
        image,
        command=["sh", "run.sh"],
        working_dir="/workspace",
        environment=env,
        detach=True,
        remove=True,
        auto_remove=True,
        # Type hint of docker is wrong here
        volumes=volumes,  # type: ignore
        network="host",  # Use the host network to avoid issues with localhost.
    )

    # Read the logs in a streaming fashion. Keep an eye on the time to make sure we don't need to stop.
    docker_timeout: float = timeout + 60  # One full minute after the bash timeout command should have already triggered
    start_time = time.time()
    logs = container.logs(stream=True)
    log_file = open(os.path.join(work_dir, "console_log.txt"), "wt", encoding="utf-8")
    stopping = False
    exiting = False

    while True:
        try:
            chunk = next(logs)  # Manually step the iterator so it is captures with the try-catch

            # Stream the data to the log file and the console
            chunk_str = chunk.decode("utf-8")
            log_file.write(chunk_str)
            log_file.flush()
            sys.stdout.reconfigure(encoding="utf-8")  # type: ignore
            sys.stdout.write(chunk_str)
            sys.stdout.flush()

            # Check if we need to terminate
            if not stopping and time.time() - start_time >= docker_timeout:
                container.stop()

                # Don't exit the loop right away, as there are things we may still want to read from the logs
                # but remember how we got here.
                stopping = True
        except KeyboardInterrupt:
            log_file.write("\nKeyboard interrupt (Ctrl-C). Attempting to exit gracefully.\n")
            log_file.flush()
            sys.stdout.write("\nKeyboard interrupt (Ctrl-C). Attempting to exit gracefully.\n")
            sys.stdout.flush()

            # Start the exit process, and give it a minute, but keep iterating
            container.stop()
            exiting = True
            docker_timeout = time.time() - start_time + 60
        except StopIteration:
            break

    # Clean up the container
    try:
        container.remove()
    except APIError:
        pass

    if stopping:  # By this line we've exited the loop, and the container has actually stopped.
        log_file.write("\nDocker timed out.\n")
        log_file.flush()
        sys.stdout.write("\nDocker timed out.\n")
        sys.stdout.flush()

    if exiting:  # User hit ctrl-C
        sys.exit(1)

# From agbench/run_cmd.py
def build_default_docker_image(docker_client: docker.DockerClient, image_tag: str) -> None:
    for segment in docker_client.api.build(
        path=RESOURCES_PATH,
        dockerfile="Dockerfile",
        rm=True,
        tag=image_tag,
        decode=True,
    ):
        if "stream" in segment:
            sys.stdout.write(segment["stream"])

# From agbench/run_cmd.py
def find_autogen_repo(path: str) -> Optional[str]:
    """
    Utility for identifying if the path is a subdirectory of the autogen_core repo.

    Returns: the path to the root of the autogen_core repo if one is found, otherwise None
    """

    # Normalize the path (we expect a directory)
    path = os.path.abspath(path)
    if os.path.isfile(path):
        path = os.path.dirname(path)

    while True:
        test_path = os.path.join(path, "python", "packages", "autogen-core")  # We found autogen_core
        if os.path.isdir(test_path):
            return path

        # Stop if we hit the root
        parent_dir = os.path.abspath(os.path.join(path, os.pardir))
        if parent_dir == path:
            break

        # Keep searching
        path = parent_dir

    return None

# From agbench/run_cmd.py
def split_jsonl(file_path: str, num_parts: int) -> List[List[Dict[str, Any]]]:
    """
    Split a JSONL file into num_parts approximately equal parts.
    """
    with open(file_path, "r") as f:
        data = [json.loads(line) for line in f]

    random.shuffle(data)  # Shuffle the data for better distribution
    chunk_size = len(data) // num_parts
    return [data[i : i + chunk_size] for i in range(0, len(data), chunk_size)]

# From agbench/run_cmd.py
def mkdir_p(path: str) -> None:
    """
    Create a directory if it doesn't exist, handling race conditions.
    """
    try:
        os.makedirs(path, exist_ok=True)
    except OSError as exc:
        if exc.errno != errno.EEXIST:
            raise

# From agbench/run_cmd.py
def run_scenarios_subset(
    scenario_name: str,
    scenarios: List[Dict[str, Any]],
    n_repeats: int,
    is_native: bool,
    config_file: Union[None, str],
    docker_image: Optional[str] = None,
    results_dir: str = "Results",
    subsample: Union[None, int, float] = None,
    env_file: Union[None, str] = None,
) -> None:
    """
    Run a subset of agbench scenarios a given number of times.
    """
    for instance in scenarios:
        # Create a folder to store the results
        # Results base

        mkdir_p(results_dir)

        # Results for the scenario

        results_scenario = os.path.join(results_dir, scenario_name)
        mkdir_p(results_scenario)

        # Results for the instance
        results_instance = os.path.join(results_scenario, instance["id"])
        mkdir_p(results_instance)

        # Results for the repeats
        for i in range(0, n_repeats):
            results_repetition = os.path.join(results_instance, str(i))

            # Skip it if it already exists
            if os.path.isdir(results_repetition):
                print(f"Found folder {results_repetition} ... Skipping.")
                continue
            print(f"Running scenario {results_repetition}")

            # Expand the scenario
            expand_scenario(".", instance, results_repetition, config_file)  # type: ignore

            # Prepare the environment (keys/values that need to be added)
            env = get_scenario_env(env_file=env_file)

            # Run the scenario
            if is_native:
                run_scenario_natively(results_repetition, env)
            else:
                run_scenario_in_docker(
                    results_repetition,
                    env,
                    docker_image=docker_image,
                )

# From agbench/run_cmd.py
def run_parallel(args: argparse.Namespace) -> None:
    """
    Run scenarios in parallel.
    """
    # Read and split the JSONL file
    scenarios = split_jsonl(args.scenario, args.parallel)
    scenario_name_parts = os.path.basename(args.scenario).split(".")
    scenario_name_parts.pop()
    scenario_name = ".".join(scenario_name_parts)

    # Create a pool of worker processes
    with Pool(processes=args.parallel) as pool:
        # Prepare arguments for each worker
        worker_args = [
            (
                scenario_name,
                scenario_subset,
                args.repeat,
                args.native,
                args.config,
                args.docker_image,
                "Results",
                args.subsample,
                args.env,
            )
            for scenario_subset in scenarios
        ]

        # Run scenarios in parallel
        pool.starmap(run_scenarios_subset, worker_args)

# From agbench/run_cmd.py
def get_azure_token_provider() -> Optional[Callable[[], str]]:
    """
    Get the Azure bearer token generator if a token wasn't provided and there's any evidence of using Azure.
    """
    if not os.environ.get("AZURE_OPENAI_AD_TOKEN") and os.path.isdir(pathlib.Path("~/.azure").expanduser()):
        logging.disable(logging.CRITICAL)
        try:
            azure_token_provider = get_bearer_token_provider(
                DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
            )
            azure_token_provider()  # Call it once to warm it up, and make sure it doesn't throw an error
            print("Found Azure token provider.")
            return azure_token_provider
        except ClientAuthenticationError:
            error_message = traceback.format_exc()
            print(
                f"Azure token provider failed loading. Try using 'az login --use-device-code'\n\nError details:\n{error_message}\n\nContinuing without Azure token provider..."
            )
        logging.disable(logging.NOTSET)
    return None

# From agbench/run_cmd.py
def run_cli(args: Sequence[str]) -> None:
    invocation_cmd = args[0]
    args = args[1:]

    # Prepare the argument parser
    parser = argparse.ArgumentParser(
        prog=invocation_cmd,
        description=f"{invocation_cmd} will run the specified AutoGen scenarios for a given number of repetitions and record all logs and trace information. When running in a Docker environment (default), each run will begin from a common, tightly controlled, environment. The resultant logs can then be further processed by other scripts to produce metrics.".strip(),
    )

    parser.add_argument(
        "scenario",
        help="The JSONL scenario file to run. If a directory is specified, then all JSONL scenarios in the directory are run. If set to '-', then read from stdin.",
    )
    parser.add_argument(
        "-r",
        "--repeat",
        type=int,
        help="The number of repetitions to run for each scenario (default: 1).",
        default=1,
    )
    parser.add_argument(
        "-s",
        "--subsample",
        type=str,
        help='Run on a subsample of the tasks in the JSONL file(s). If a decimal value is specified, then run on the given proportion of tasks in each file. For example "0.7" would run on 70%% of tasks, and "1.0" would run on 100%% of tasks. If an integer value is specified, then randomly select *that* number of tasks from each specified JSONL file. For example "7" would run tasks, while "1" would run only 1 task from each specified JSONL file. (default: 1.0; which is 100%%)',
        default=None,
    )
    parser.add_argument(
        "-p",
        "--parallel",
        type=int,
        help="The number of parallel processes to run (default: 1).",
        default=1,
    )
    parser.add_argument(
        "-a",
        "--azure",
        action="store_true",
        help="Use Azure identity to pass an AZURE_OPENAI_AD_TOKEN to the task environment. This is necessary when using Azure-hosted OpenAI models rather than those hosted by OpenAI.",
    )
    parser.add_argument(
        "-e",
        "--env",
        type=str,
        help="The environment file to load into Docker, or into the native task context (default: '"
        + DEFAULT_ENV_FILE_YAML
        + "').",
        default=None,
    )
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        help="The config file to copy into the Task (default: '" + DEFAULT_CONFIG_YAML + "').",
        default=None,
    )
    parser.add_argument(
        "-d",
        "--docker-image",
        type=str,
        help="The Docker image to use when running scenarios. Can not be used together with --native. (default: '"
        + DEFAULT_DOCKER_IMAGE_TAG
        + "', which will be created if not present)",
        default=None,
    )
    parser.add_argument(
        "--native",
        action="store_true",
        help="Run the scenarios natively rather than in docker. NOTE: This is not advisable, and should be done with great caution.",
    )

    parsed_args = parser.parse_args(args)

    if parsed_args.config is not None:
        # Make sure the config file is readable, so that we fail early
        with open(parsed_args.config, "r"):
            pass

    # don't support parallel and subsample together
    if parsed_args.parallel > 1 and parsed_args.subsample is not None:
        sys.exit("The options --parallel and --subsample can not be used together currently. Exiting.")

    # Don't allow both --docker-image and --native on the same command
    if parsed_args.docker_image is not None and parsed_args.native:
        sys.exit("The options --native and --docker-image can not be used together. Exiting.")

    # Warn if running natively
    if parsed_args.native:
        if IS_WIN32:
            sys.exit("Running scenarios with --native is not supported in Windows. Exiting.")

        sys.stderr.write(
            "WARNING: Running natively, without Docker, not only poses the usual risks of executing arbitrary AI generated code on your machine, it also makes it impossible to ensure that each test starts from a known and consistent set of initial conditions. For example, if the agents spend time debugging and installing Python libraries to solve the task, then those libraries will be available to all other runs. In other words, earlier runs can influence later runs, leading to many confounds in testing.\n\n"
        )

        # Does an environment variable override the prompt?
        allow_native = os.environ.get("AGBENCH_ALLOW_NATIVE")
        if allow_native is None or allow_native == "":
            choice = input(
                'Are you absolutely sure you want to continue with native execution? Type "Yes" exactly, and in full, to proceed: '
            )
            if choice.strip().lower() != "yes":
                sys.exit("Received '" + choice + "'. Exiting.")
        elif allow_native.strip().lower() != "yes":
            sys.exit(f"Exiting because AGBENCH_ALLOW_NATIVE is '{allow_native}'\n")
        else:
            sys.stderr.write(f"Continuing because AGBENCH_ALLOW_NATIVE is '{allow_native}'\n")
            time.sleep(0.75)  # Pause very briefly so the message isn't lost in the noise

    # Parse the subsample
    subsample = None
    if parsed_args.subsample is not None:
        subsample = float(parsed_args.subsample)
        if "." in parsed_args.subsample:  # Intention is to run on a proportion
            if subsample == 1.0:  # Intention is to run 100%, which is the default
                subsample = None  # None means 100% ... which use None to differentiate from the integer 1
            elif subsample < 0 or subsample > 1.0:
                raise (
                    ValueError(
                        "Subsample must either be an integer (specified without a decimal), or a Real number between 0.0 and 1.0"
                    )
                )

    # Get the Azure bearer token generator if a token wasn't provided and there's any evidence of using Azure
    azure_token_provider = None
    if parsed_args.azure:
        azure_token_provider = get_azure_token_provider()

    # Run the scenario
    if parsed_args.parallel > 1:
        run_parallel(parsed_args)
    else:
        run_scenarios(
            scenario=parsed_args.scenario,
            n_repeats=parsed_args.repeat,
            is_native=True if parsed_args.native else False,
            config_file=parsed_args.config,
            token_provider=azure_token_provider,
            docker_image=parsed_args.docker_image,
            subsample=subsample,
            env_file=parsed_args.env,
        )

# From agbench/run_cmd.py
def replace_env_var(match: Any) -> str:
        var_name = match.group(1)
        return os.environ.get(var_name, "")

# From agbench/run_cmd.py
def replace_in_dict(d: Dict[str, Any]) -> None:
        for key, value in d.items():
            if isinstance(value, str):
                d[key] = pattern.sub(replace_env_var, value)
            elif isinstance(value, dict):
                replace_in_dict(cast(Dict[str, Any], value))
            elif isinstance(value, list):
                # Note: with the task mypy complains of a redundant cast
                # without the cast, pyright complains the type is unknown
                replace_in_list(cast(List[Any], value))

# From agbench/run_cmd.py
def replace_in_list(lst: List[Any]) -> None:
        for i, item in enumerate(lst):
            if isinstance(item, str):
                lst[i] = pattern.sub(replace_env_var, item)
            elif isinstance(item, dict):
                replace_in_dict(cast(Dict[str, Any], item))
            elif isinstance(item, list):
                replace_in_list(cast(List[Any], item))

from linter.cli import lint_cli
from remove_missing_cmd import remove_missing_cli
from run_cmd import run_cli
from tabulate_cmd import tabulate_cli

# From agbench/cli.py
class CommandSpec(TypedDict):
    command: str
    description: str
    function: Optional[Callable[[Sequence[str]], None]]


# From agbench/remove_missing_cmd.py
def delete_folders_with_missing_results(runlogs_path: str, noconfirm: bool = False) -> None:
    deleted_folders = 0

    for task_id in os.listdir(runlogs_path):
        task_path = os.path.join(runlogs_path, task_id)

        if not os.path.isdir(task_path):
            continue

        instance = 0
        has_missing_results = False

        while True:
            instance_dir = os.path.join(task_path, str(instance))
            if not os.path.isdir(instance_dir):
                if instance == 0:
                    print(f"Empty folder: {task_path}")
                    has_missing_results = True
                break
            if not default_scorer(instance_dir):
                has_missing_results = True
                break

            instance += 1
        if has_missing_results:
            if not noconfirm:
                print(f"Missing Results in : {task_path}")
                user_confirmation = input("Press 1 to delete, anything else to skip...")
                if user_confirmation == "1":
                    shutil.rmtree(task_path)
                    print(f"Deleted folder: {task_path}")
                    deleted_folders += 1
                else:
                    print(f"Skipping folder: {task_path}")
            else:
                shutil.rmtree(task_path)
                print(f"Deleted folder: {task_path}")
                deleted_folders += 1

    print(f"Total folders deleted: {deleted_folders}")

# From agbench/remove_missing_cmd.py
def remove_missing_cli(args: Sequence[str]) -> None:
    invocation_cmd = args[0]
    args = args[1:]
    runlogs_path = args[0]

    parser = argparse.ArgumentParser(
        prog=invocation_cmd,
        description=f"{invocation_cmd} will remove folders with missing results.",
    )

    parser.add_argument(
        "runlogs",
        help="The path where the run's logs are stored.",
    )
    parser.add_argument(
        "-c",
        "--noconfirm",
        action="store_true",
        help="Disable confirmation prompt before deleting folders.",
    )

    parsed_args = parser.parse_args(args)
    print(parsed_args)
    if not os.path.isdir(parsed_args.runlogs):
        print(f"Error: '{runlogs_path}' is not a valid directory.")
        print("Usage: agbench remove_missing <path_to_runlogs>")

        sys.exit(1)
    if not parsed_args.noconfirm:
        input(
            "Did you modify the default_scorer function to match the expected ending pattern? Press Enter to continue..."
        )

    delete_folders_with_missing_results(parsed_args.runlogs, parsed_args.noconfirm)


from _base import CodedDocument
from _base import Document
from coders.oai_coder import OAIQualitativeCoder

# From linter/cli.py
def prepend_line_numbers(lines: List[str]) -> List[str]:
    """
    Returns a list of strings with each line prefixed by its right-justified
      line number.
    """
    width = len(str(len(lines)))
    new_lines = [f"{i+1:>{width}}: {line}" for i, line in enumerate(lines)]
    return new_lines

# From linter/cli.py
def load_log_file(path: str, prepend_numbers: bool = False) -> Document:
    with open(path, "r") as f:
        lines = f.readlines()
    if prepend_numbers:
        lines = prepend_line_numbers(lines)

    text = "".join(lines)
    return Document(text=text, name=os.path.abspath(path))

# From linter/cli.py
def code_log(path: str) -> Optional[CodedDocument]:
    coder = OAIQualitativeCoder()

    if os.path.isfile(path):
        doc = load_log_file(path, prepend_numbers=True)
        coded_doc = coder.code_document(doc)
        return coded_doc
    else:
        raise FileNotFoundError(f"File {path} does not exist.")

# From linter/cli.py
def print_coded_results(input_path: str, coded_doc: CodedDocument) -> None:
    num_errors: int = 0
    # define map from severity to ANSI color
    severity_color_map = {2: "\033[31m", 1: "\033[33m", 0: "\033[32m"}

    # sort the codes by severity with the most severe first
    sorted_codes = sorted(coded_doc.codes, key=lambda x: x.severity, reverse=True)

    for code in sorted_codes:
        # select color based on severity, default to white if missing
        color = severity_color_map.get(code.severity, "\033[37m")
        print(f"{color}[{code.severity}]: {code.name}\033[0m: {code.definition}")
        for example in code.examples:
            print(f"\033[1m{input_path}\033[0m:{example.line}" f":{example.line_end}\t{example.reason}")
            num_errors += 1
    print("\n")
    print(f"Found {num_errors} errors in {input_path}.")
    print("\n")

# From linter/cli.py
def get_log_summary(input_path: str) -> str:
    """
    Generate a single sentence of summary for the given log file.
    """
    client = OpenAI()

    text = load_log_file(input_path, prepend_numbers=False).text

    response = client.responses.create(
        model="gpt-4o",
        input=f"Summarize the following log file in one sentence.\n{text}",
    )
    return response.output_text

# From linter/cli.py
def code_command(input_path: str) -> None:
    """
    Process the given input path by coding log files.
    """
    if os.path.isfile(input_path):
        print(f"Processing file: {input_path}")
        print(get_log_summary(input_path))
        coded_doc = code_log(input_path)
        if coded_doc is None:
            raise ValueError("Failed to code the document.")
        print_coded_results(input_path, coded_doc)
    else:
        print("Invalid input path.")

# From linter/cli.py
def lint_cli(args: Sequence[str]) -> None:
    invocation_cmd = args[0]

    args = args[1:]

    parser = argparse.ArgumentParser(
        prog=invocation_cmd,
        description=f"{invocation_cmd} will analyze a console log."
        " And detect errors/inefficiencies in the log files.",
    )

    parser.add_argument("logfile", type=str, help="Path to a log file.")

    parsed_args = parser.parse_args(args)

    code_command(parsed_args.logfile)

import hashlib

# From linter/_base.py
class Document(BaseModel):
    text: str = Field(..., description="Text content of the document.")
    name: Optional[str] = Field(None, description="Optional name of the document.")

    def __hash__(self) -> int:
        return int(hashlib.md5(self.text.encode("utf-8")).hexdigest(), 16)

# From linter/_base.py
class CodeExample(BaseModel):
    """
    Represents an example associated with a code.
    """

    reason: str = Field(
        ..., description="A two sentence, human-readable explanation why this example and lines relate to the code."
    )
    line_content: str = Field(
        ..., description="The exact content of the line where the error is found. This should be a single line."
    )
    line: int = Field(..., description="The most important line number where a human would say the error is.")
    line_end: int = Field(..., description="Line number where the issue ends.")

# From linter/_base.py
class Code(BaseModel):
    name: str = Field(..., description="Normalized unique name for the code (lowercase, hyphen separated).")
    definition: str = Field(..., description="Definition of the code.")
    examples: List[CodeExample] = Field(
        ..., description="List of code examples associated with the code. Cannot be empty."
    )
    severity: int = Field(
        ..., description="Severity rating of the error identified using the code. Valid values: 0, 1, 2."
    )
    id: Optional[int] = Field(None, description="Identifier computed using MD5 of name and definition.")
    merged_from: Optional[List[int]] = Field(None, description="List of code ids from which this code is merged.")

    def __init__(
        self,
        name: str,
        definition: str,
        examples: List[CodeExample],
        severity: int,
        id: Optional[int] = None,
        merged_from: Optional[List[int]] = None,
    ):
        super().__init__(name=name, definition=definition, examples=examples, severity=severity)
        self.name = re.sub(r"[^a-z-]", "", self.name.lower().replace(" ", "-"))
        self.id = int(hashlib.md5((self.name + self.definition).encode("utf-8")).hexdigest(), 16)
        self.merged_from = None

    def __hash__(self) -> int:
        if self.id is None:
            raise ValueError("Code ID is not set.")
        return self.id

    def add_merged_from(self, code_id: int) -> None:
        if self.merged_from is None:
            self.merged_from = []
        if code_id not in self.merged_from:
            self.merged_from.append(code_id)

# From linter/_base.py
class CodedDocument(BaseModel):
    doc: Document
    codes: Set[Code]

    @classmethod
    def from_json(cls, json_str: str) -> "CodedDocument":
        data = json.loads(json_str)
        doc = Document(**data["doc"])
        codes = {Code(**code) for code in data["codes"]}
        return cls(doc=doc, codes=codes)

# From linter/_base.py
class BaseQualitativeCoder(Protocol):
    def code_document(self, doc: Document, code_set: Optional[Set[Code]]) -> Optional[CodedDocument]: ...

# From linter/_base.py
def add_merged_from(self, code_id: int) -> None:
        if self.merged_from is None:
            self.merged_from = []
        if code_id not in self.merged_from:
            self.merged_from.append(code_id)

# From linter/_base.py
def from_json(cls, json_str: str) -> "CodedDocument":
        data = json.loads(json_str)
        doc = Document(**data["doc"])
        codes = {Code(**code) for code in data["codes"]}
        return cls(doc=doc, codes=codes)

# From linter/_base.py
def code_document(self, doc: Document, code_set: Optional[Set[Code]]) -> Optional[CodedDocument]: ...

from _base import BaseQualitativeCoder
from _base import Code

# From coders/oai_coder.py
class CodeList(BaseModel):
    code_list: List[Code]

# From coders/oai_coder.py
class OAIQualitativeCoder(BaseQualitativeCoder):
    DEFAULT_MODEL = "gpt-4o"

    def __init__(self, cache_dir: str = ".cache", model: str = DEFAULT_MODEL, cache_enabled: bool = False) -> None:
        self.client = OpenAI()
        self.cache_dir = cache_dir
        self.model = model
        self.cache_enabled = cache_enabled

    def code_document(
        self,
        doc: Document,
        code_set: Optional[Set[Code]] = None,
    ) -> Optional[CodedDocument]:
        # get hash of the document
        doc_hash = hash(doc)
        cache_file = os.path.join(self.cache_dir, f"{doc_hash}.json") if self.cache_enabled else None

        if self.cache_enabled:
            if not os.path.exists(self.cache_dir):
                os.makedirs(self.cache_dir)
            if cache_file and os.path.exists(cache_file):
                with open(cache_file, "r") as f:
                    cached_coded_doc_json = f.read()
                    return CodedDocument.from_json(cached_coded_doc_json)

        # sanitize the doc before passing it to openai
        doc.text = remove_control_characters(doc.text)

        coded_document: Optional[CodedDocument] = None

        if code_set is None:
            completion = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """You are an expert qualitative researcher.

Given a list of dcocuments containing errors below, generate a list of (error) codes.
Each code should contains:
- at least 3 words, max 4 word, hyphenated.

For example, the name could be of the format "lack-of-word2",
"failed-to-bar", "excessive-use-of-magenta". Name should adhere to
Joseph M. Williams' writing principles of clarity, conciseness, and coherence.

Ensure each code name is lower-case, hyphenated, and directly reflects the
concept it represents. Avoid ambiguous or overly complex terms, and prioritize
simplicity, precision, and readability in the naming.

The code names should pass the 'clarity and grace' test by being easy to
understand, descriptive, and reflective of the content they categorize.
- suggest codes that are similar to good code names. avoid code names that are
similar to bad code names.
- The definition should be simple worded and practical. At least 2 sentences,
 max 3. It should be written in past tense.

It should convey how a labeller could apply this code to future logs, without
mentioning the word "labeller". The definition should be specific enough to be
useful in debugging. It should be very concrete. And should be well thought and
make sense. Bull shitting will not earn you any points.

- The examples should be a list. Each example should be descriptive between
2-3 sentences. Examples should be concrete, informative and not vague. Provide
at max 20 salient examples. Examples should contain a lot of detail about what
happened and should refer to incidents in the log.

- The list of codes must mutually exclusive.

# GOOD EXAMPLES OF FINAL CODE NAMES/CLUSTERS
* looped-without-progress
* repeated-unsuccessful-actions
* repeated-syntax-errors
* exceeded-context-window-limits
* encountered-security-risks
* failure-to-switch-strategy
* exceeded-resource-limits
* attempted-to-handle-excessive-data
* no-errors-detected
These names are high-level but also concrete. They exactly mention the type of
error, issue, gap that has been identified.

## BAD EXAMPLES OF FINAL CODE NAMES/CLUSTERS
* mismanaged-data-utilization -- too high level
* incomplete-or-misguided-execution -- too high level
* misaligned-agent-interactions -- too high level
* mismanaged-task-strategies -- too high level
* resource-inefficiencies -- vague
* communication-issues -- vague
* coordination-issues -- too high level and vague
* operational-failures
* execution-errors -- too high level
* navigation-issues -- too concise
* adaptive-failures -- too concise
* successful-processes -- I dont like the word processes
* system-constraints
* configuration-issues
* information-inaccuracies -- too high level
* process-improvements -- vague, not an error
* inadequate-error-response -- too high-level, unclear what kind of errors
* specific-access-issues -- makes no sense
* strategy-inefficiency -- strategy is too high level
* error-management-gaps -- unclear what error management means
* error-handling-deficiency -- unclear what kind of errors
* coordination-breakdown -- unclear what coordination means
* muddled-task-execution -- unclear what kind of tasks were muddled
* task-completion-gaps -- too high level
The above names are too high level and unclear. Please DO NOT use such names.
    """,
                    },
                    {
                        "role": "user",
                        "content": doc.text,
                    },
                ],
                response_format=CodeList,
            )

            message = completion.choices[0].message
            if message.parsed and len(message.parsed.code_list) > 0:
                coded_document = CodedDocument(doc=doc, codes=set(message.parsed.code_list))
            else:
                print(message.refusal)
                raise ValueError("Error in coding document with OpenAI")
        else:
            code_to_str = "\n".join(
                [
                    (
                        f"\n---\nCode Name: {code.name}\n"
                        f"Definition: {code.definition}\n"
                        f"Examples: {code.examples}\n---\n"
                    )
                    for code in code_set
                ]
            )

            completion = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """You are an expert qualitative researcher.
                        You can answer any questions about coding logs.""",
                    },
                    {
                        "role": "user",
                        "content": f"""
## Context
The text below shows a log containing errors. Your task is to code the log with
the following codes. Generate a list of codes for the log below.

Only use the codes from the list below. Do not create new codes.
Modify the examples of the codes to fit the context of the log.

Your example should be informative to narrow down the details of the error in
the context of the example.

## Codes

{code_to_str}

## Log

{doc.text}
""",
                    },
                ],
                response_format=CodeList,
            )

            message = completion.choices[0].message
            if message.parsed and len(message.parsed.code_list) > 0:
                code_list = message.parsed.code_list
                # filter out codes whose names are not in the code_set
                code_set_names = {code.name for code in code_set}
                code_list = [code for code in code_list if code.name in code_set_names]

                coded_document = CodedDocument(doc=doc, codes=set(code_list))

        if coded_document is None:
            raise ValueError("Error in coding document with OpenAI")

        if self.cache_enabled and cache_file:
            with open(cache_file, "w") as f:
                f.write(coded_document.model_dump_json(indent=4))
        return coded_document

# From coders/oai_coder.py
def remove_control_characters(text: str) -> str:
    """
    Remove control characters from the text.
    """
    return re.sub(r"[\x00-\x1F\x7F]", "", text)

from agbench.tabulate_cmd import default_tabulate
import sqlite3
import string
import numpy

# From Scripts/custom_tabulate.py
def in_house_normalize_answer(a):
    # Lower case
    # Trim (left and right)
    # standardize comma separated values
    # Replace multiple spaces with one space
    # Remove trailing punctuation
    norm_answer = ", ".join(a.strip().lower().split(","))
    norm_answer = re.sub(r"[\.\!\?]+$", "", re.sub(r"\s+", " ", norm_answer))
    return norm_answer

# From Scripts/custom_tabulate.py
def in_house_question_scorer(
    model_answer: str,
    ground_truth: str,
) -> bool:
     n_ma = in_house_normalize_answer(model_answer)
     n_gt = in_house_normalize_answer(ground_truth)
     return (n_gt != "" and n_gt == n_ma)

# From Scripts/custom_tabulate.py
def gaia_question_scorer(
    model_answer: str,
    ground_truth: str,
) -> bool:
    #FROM: https://huggingface.co/spaces/gaia-benchmark/leaderboard/blob/main/scorer.py

    def normalize_number_str(number_str: str) -> float:
        # we replace these common units and commas to allow
        # conversion to float
        for char in ["$", "%", ","]:
            number_str = number_str.replace(char, "")
        try:
            return float(number_str)
        except ValueError:
            print(f"String {number_str} cannot be normalized to number str.")
            return float("inf")

    def split_string(s: str, char_list: list[str] = [",", ";"],) -> list[str]:
        pattern = f"[{''.join(char_list)}]"
        return re.split(pattern, s)

    def normalize_str(input_str, remove_punct=True) -> str:
        """
        Normalize a string by:
        - Removing all white spaces
        - Optionally removing punctuation (if remove_punct is True)
        - Converting to lowercase
        Parameters:
        - input_str: str, the string to normalize
        - remove_punct: bool, whether to remove punctuation (default: True)
        Returns:
        - str, the normalized string
        """
        # Remove all white spaces. Required e.g for seagull vs. sea gull
        no_spaces = re.sub(r"\s", "", input_str)

        # Remove punctuation, if specified.
        if remove_punct:
            translator = str.maketrans("", "", string.punctuation)
            return no_spaces.lower().translate(translator)
        else:
            return no_spaces.lower()


    def is_float(element: any) -> bool:
        try:
            float(element)
            return True
        except ValueError:
            return False

    # if gt is a number
    if is_float(ground_truth):
        normalized_answer = normalize_number_str(model_answer)
        return normalized_answer == float(ground_truth)

    # if gt is a list
    elif any(char in ground_truth for char in [",", ";"]):
        # question with the fish: normalization removes punct

        gt_elems = split_string(ground_truth)
        ma_elems = split_string(model_answer)

        # check length is the same
        if len(gt_elems) != len(ma_elems):
            #warnings.warn(
            #    "Answer lists have different lengths, returning False.", UserWarning
            #)
            return False

        # compare each element as float or str
        comparisons = []
        for ma_elem, gt_elem in zip(ma_elems, gt_elems):
            if is_float(gt_elem):
                normalized_ma_elem = normalize_number_str(ma_elem)
                comparisons.append(normalized_ma_elem == float(gt_elem))
            else:
                # we do not remove punct since comparisons can include punct
                comparisons.append(
                    normalize_str(ma_elem, remove_punct=False)
                    == normalize_str(gt_elem, remove_punct=False)
                )
        return all(comparisons)

    # if gt is a str
    else:
        return normalize_str(model_answer) == normalize_str(ground_truth)

# From Scripts/custom_tabulate.py
def scorer(instance_dir):
    # Read the expected answer
    expected_answer_file = os.path.join(instance_dir, "expected_answer.txt")
    if not os.path.isfile(expected_answer_file):
        return None

    expected_answer = None
    with open(expected_answer_file, "rt") as fh:
        expected_answer = fh.read().strip()

    # Read the console
    console_log_file = os.path.join(instance_dir, "console_log.txt")
    if not os.path.isfile(console_log_file):
        return None

    console_log = ""
    with open(console_log_file, "rt") as fh:
        console_log = fh.read()

        final_answer = None 
        m = re.search(r"FINAL ANSWER:(.*?)\n", console_log, re.DOTALL)
        if m:
            final_answer = m.group(1).strip()

        # Missing the final answer line
        if final_answer is None:
            return None

        # Return true if they are equal after normalization
        # return in_house_question_scorer(final_answer, expected_answer)
        return gaia_question_scorer(final_answer, expected_answer)

# From Scripts/custom_tabulate.py
def normalize_number_str(number_str: str) -> float:
        # we replace these common units and commas to allow
        # conversion to float
        for char in ["$", "%", ","]:
            number_str = number_str.replace(char, "")
        try:
            return float(number_str)
        except ValueError:
            print(f"String {number_str} cannot be normalized to number str.")
            return float("inf")

# From Scripts/custom_tabulate.py
def split_string(s: str, char_list: list[str] = [",", ";"],) -> list[str]:
        pattern = f"[{''.join(char_list)}]"
        return re.split(pattern, s)

# From Scripts/custom_tabulate.py
def normalize_str(input_str, remove_punct=True) -> str:
        """
        Normalize a string by:
        - Removing all white spaces
        - Optionally removing punctuation (if remove_punct is True)
        - Converting to lowercase
        Parameters:
        - input_str: str, the string to normalize
        - remove_punct: bool, whether to remove punctuation (default: True)
        Returns:
        - str, the normalized string
        """
        # Remove all white spaces. Required e.g for seagull vs. sea gull
        no_spaces = re.sub(r"\s", "", input_str)

        # Remove punctuation, if specified.
        if remove_punct:
            translator = str.maketrans("", "", string.punctuation)
            return no_spaces.lower().translate(translator)
        else:
            return no_spaces.lower()

# From Scripts/custom_tabulate.py
def is_float(element: any) -> bool:
        try:
            float(element)
            return True
        except ValueError:
            return False

from huggingface_hub import snapshot_download

# From Scripts/init_tasks.py
def download_gaia():
    """Download the GAIA benchmark from Hugging Face."""

    if not os.path.isdir(DOWNLOADS_DIR):
        os.mkdir(DOWNLOADS_DIR)

    """Download the GAIA dataset from Hugging Face Hub"""
    snapshot_download(
        repo_id="gaia-benchmark/GAIA",
        repo_type="dataset",
        local_dir=REPO_DIR,
        local_dir_use_symlinks=True,
    )

# From Scripts/init_tasks.py
def create_jsonl(name, tasks, files_dir, template):
    """Creates a JSONL scenario file with a given name, and template path."""

    if not os.path.isdir(TASKS_DIR):
        os.mkdir(TASKS_DIR)

    with open(os.path.join(TASKS_DIR, name + ".jsonl"), "wt") as fh:
        for task in tasks:
            print(f"Converting: [{name}] {task['task_id']}")

            # Figure out what files we need to copy
            template_cp_list = [template]
            if len(task["file_name"].strip()) > 0:
                template_cp_list.append(
                    [
                        os.path.join(files_dir, task["file_name"].strip()),
                        task["file_name"].strip(),
                        #os.path.join("coding", task["file_name"].strip()),
                    ]
                )

            record = {
                "id": task["task_id"],
                "template": template_cp_list,
                "substitutions": {
                    "scenario.py": {
                        "__FILE_NAME__": task["file_name"],
                    },
                    "expected_answer.txt": {"__EXPECTED_ANSWER__": task["Final answer"]},
                    "prompt.txt": {"__PROMPT__": task["Question"]},
                },
            }

            fh.write(json.dumps(record).strip() + "\n")

from autogen_ext.agents.magentic_one import MagenticOneCoderAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.utils import content_to_str
from autogen_core.models import ModelFamily
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.base import TerminationCondition
from autogen_agentchat.base import TerminatedException
from autogen_ext.agents.web_surfer import MultimodalWebSurfer
from autogen_ext.agents.file_surfer import FileSurfer
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_core.models import AssistantMessage

# From SelectorGroupChat/scenario.py
class LLMTermination(TerminationCondition):
    """Terminate the conversation if an LLM determines the task is complete.

    Args:
        prompt: The prompt to evaluate in the llm
        model_client: The LLM model_client to use
        termination_phrase: The phrase to look for in the LLM output to trigger termination
    """

    def __init__(self, prompt: str, model_client: ChatCompletionClient, termination_phrase: str = "TERMINATE") -> None:
        self._prompt = prompt
        self._model_client = model_client
        self._termination_phrase = termination_phrase
        self._terminated = False
        self._context: Sequence[LLMMessage] = []

    @property
    def terminated(self) -> bool:
        return self._terminated

    async def __call__(self, messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> StopMessage | None:
        if self._terminated:
            raise TerminatedException("Termination condition has already been reached")

        # Build the context
        for message in messages:
            if isinstance(message, TextMessage):
                self._context.append(UserMessage(content=message.content, source=message.source))
            elif isinstance(message, MultiModalMessage):
                if self._model_client.model_info["vision"]:
                    self._context.append(UserMessage(content=message.content, source=message.source))
                else:
                    self._context.append(UserMessage(content=content_to_str(message.content), source=message.source))

        if len(self._context) == 0:
            return None

        # Call the model to evaluate
        response = await self._model_client.create(self._context + [UserMessage(content=self._prompt, source="user")]) 

        # Check for termination
        if isinstance(message.content, str) and self._termination_phrase in response.content:
            self._terminated = True
            return StopMessage(content=message.content, source="LLMTermination")
        return None

    async def reset(self) -> None:
        self._terminated = False
        self._context = []

from autogen_agentchat.teams import MagenticOneGroupChat


import gzip
import requests

# From Scripts/init_tasks.py
def download_human_eval():
    """Download the HumanEval dataset, un-gzips it, and returns a list of its parsed JSON objects."""

    # Send a HTTP request to the URL of the file
    response = requests.get(URL)

    # Ensure we raise an error if the download failed
    response.raise_for_status()

    # Create a BytesIO object from the response content
    buffer = io.BytesIO(response.content)

    # Read the file, line by line, populating a list of parsed JSON objects
    results = []
    with gzip.GzipFile(fileobj=buffer) as f_in:
        for line in f_in:
            # Parse each line as JSON
            results.append(json.loads(line))

    return results

from autogen_core.model_context import UnboundedChatCompletionContext

# From AgentChat/reasoning_model_context.py
class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out

from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core.model_context import ChatCompletionContext
from custom_code_executor import CustomCodeExecutorAgent
from reasoning_model_context import ReasoningModelContext

from mcp import PromptsCapability
from mcp import ResourcesCapability
from mcp import ServerCapabilities
from mcp import ToolsCapability
from mcp.server import Server
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import GetPromptResult
from mcp.types import Prompt
from mcp.types import PromptArgument
from mcp.types import PromptMessage
from mcp.types import Resource
from mcp.types import Tool

# From tests/mcp_server_comprehensive.py
class SimpleMcpServer:
    """A simple MCP server demonstrating basic functionality."""

    def __init__(self) -> None:
        self.server: Server[object] = Server("simple-mcp-server")
        self.register_handlers()  # type: ignore[no-untyped-call]

    def register_handlers(self) -> None:
        """Register all MCP handlers."""

        # Prompts
        @self.server.list_prompts()  # type: ignore[no-untyped-call,misc]
        async def list_prompts() -> list[Prompt]:  # pyright: ignore[reportUnusedFunction]
            """List available prompts."""
            return [
                Prompt(
                    name="code_review",
                    description="Generate a comprehensive code review for a given piece of code",
                    arguments=[
                        PromptArgument(
                            name="code",
                            description="The code to review",
                            required=True,
                        ),
                        PromptArgument(
                            name="language",
                            description="Programming language of the code",
                            required=True,
                        ),
                    ],
                ),
                Prompt(
                    name="documentation",
                    description="Generate documentation for code or APIs",
                    arguments=[
                        PromptArgument(
                            name="content",
                            description="The content to document",
                            required=True,
                        ),
                    ],
                ),
            ]

        @self.server.get_prompt()  # type: ignore[no-untyped-call,misc]
        async def get_prompt(name: str, arguments: Optional[Dict[str, str]] = None) -> GetPromptResult:  # pyright: ignore[reportUnusedFunction]
            """Get a specific prompt with arguments."""
            if not arguments:
                arguments = {}

            if name == "code_review":
                code = arguments.get("code", "// No code provided")
                language = arguments.get("language", "unknown")

                return GetPromptResult(
                    description=f"Code review for {language} code",
                    messages=[
                        PromptMessage(
                            role="user",
                            content=TextContent(
                                type="text",
                                text=f"Please review this {language} code:\n\n```{language}\n{code}\n```",
                            ),
                        ),
                    ],
                )

            elif name == "documentation":
                content = arguments.get("content", "No content provided")

                return GetPromptResult(
                    description="Documentation generation",
                    messages=[
                        PromptMessage(
                            role="user",
                            content=TextContent(
                                type="text",
                                text=f"Please generate documentation for:\n\n{content}",
                            ),
                        ),
                    ],
                )

            else:
                raise ValueError(f"Unknown prompt: {name}")

        # Resources
        @self.server.list_resources()  # type: ignore[no-untyped-call,misc]
        async def list_resources() -> list[Resource]:  # pyright: ignore[reportUnusedFunction]
            """List available resources."""
            return [
                Resource(
                    uri=AnyUrl("file:///company/users.json"),
                    name="Company Users",
                    description="List of all company users",
                    mimeType="application/json",
                ),
                Resource(
                    uri=AnyUrl("file:///company/projects.json"),
                    name="Active Projects",
                    description="Current projects",
                    mimeType="application/json",
                ),
            ]

        @self.server.read_resource()  # type: ignore[no-untyped-call,misc]
        async def read_resource(uri: AnyUrl) -> str:  # pyright: ignore[reportUnusedFunction]
            """Read a specific resource."""
            uri_str = str(uri)

            if uri_str == "file:///company/users.json":
                return json.dumps(SAMPLE_DATA["users"], indent=2)

            elif uri_str == "file:///company/projects.json":
                return json.dumps(SAMPLE_DATA["projects"], indent=2)

            else:
                raise ValueError(f"Unknown resource: {uri_str}")

        # Tools
        @self.server.list_tools()  # type: ignore[no-untyped-call,misc]
        async def list_tools() -> list[Tool]:  # pyright: ignore[reportUnusedFunction]
            """List available tools."""
            return [
                Tool(
                    name="echo",
                    description="Echo back the input text",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "Text to echo back",
                            }
                        },
                        "required": ["text"],
                    },
                ),
                Tool(
                    name="get_time",
                    description="Get the current time",
                    inputSchema={
                        "type": "object",
                        "properties": {},
                    },
                ),
            ]

        @self.server.call_tool()  # type: ignore[no-untyped-call,misc]
        async def call_tool(name: str, arguments: Optional[Dict[str, Any]] = None) -> list[TextContent]:  # pyright: ignore[reportUnusedFunction]
            """Call a specific tool."""
            if not arguments:
                arguments = {}

            if name == "echo":
                text = arguments.get("text", "")
                return [
                    TextContent(
                        type="text",
                        text=f"Echo: {text}",
                    )
                ]

            elif name == "get_time":
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                return [
                    TextContent(
                        type="text",
                        text=f"Current time: {current_time}",
                    )
                ]

            else:
                raise ValueError(f"Unknown tool: {name}")

    async def run(self) -> None:
        """Run the MCP server."""
        # Server capabilities
        init_options = InitializationOptions(
            server_name="simple-mcp-server",
            server_version="1.0.0",
            capabilities=ServerCapabilities(
                prompts=PromptsCapability(listChanged=True),
                resources=ResourcesCapability(
                    subscribe=True,
                    listChanged=True,
                ),
                tools=ToolsCapability(listChanged=True),
            ),
        )

        # Run the server
        async with stdio_server() as (read_stream, write_stream):
            await self.server.run(
                read_stream,
                write_stream,
                init_options,
            )

# From tests/mcp_server_comprehensive.py
def register_handlers(self) -> None:
        """Register all MCP handlers."""

        # Prompts
        @self.server.list_prompts()  # type: ignore[no-untyped-call,misc]
        async def list_prompts() -> list[Prompt]:  # pyright: ignore[reportUnusedFunction]
            """List available prompts."""
            return [
                Prompt(
                    name="code_review",
                    description="Generate a comprehensive code review for a given piece of code",
                    arguments=[
                        PromptArgument(
                            name="code",
                            description="The code to review",
                            required=True,
                        ),
                        PromptArgument(
                            name="language",
                            description="Programming language of the code",
                            required=True,
                        ),
                    ],
                ),
                Prompt(
                    name="documentation",
                    description="Generate documentation for code or APIs",
                    arguments=[
                        PromptArgument(
                            name="content",
                            description="The content to document",
                            required=True,
                        ),
                    ],
                ),
            ]

        @self.server.get_prompt()  # type: ignore[no-untyped-call,misc]
        async def get_prompt(name: str, arguments: Optional[Dict[str, str]] = None) -> GetPromptResult:  # pyright: ignore[reportUnusedFunction]
            """Get a specific prompt with arguments."""
            if not arguments:
                arguments = {}

            if name == "code_review":
                code = arguments.get("code", "// No code provided")
                language = arguments.get("language", "unknown")

                return GetPromptResult(
                    description=f"Code review for {language} code",
                    messages=[
                        PromptMessage(
                            role="user",
                            content=TextContent(
                                type="text",
                                text=f"Please review this {language} code:\n\n```{language}\n{code}\n```",
                            ),
                        ),
                    ],
                )

            elif name == "documentation":
                content = arguments.get("content", "No content provided")

                return GetPromptResult(
                    description="Documentation generation",
                    messages=[
                        PromptMessage(
                            role="user",
                            content=TextContent(
                                type="text",
                                text=f"Please generate documentation for:\n\n{content}",
                            ),
                        ),
                    ],
                )

            else:
                raise ValueError(f"Unknown prompt: {name}")

        # Resources
        @self.server.list_resources()  # type: ignore[no-untyped-call,misc]
        async def list_resources() -> list[Resource]:  # pyright: ignore[reportUnusedFunction]
            """List available resources."""
            return [
                Resource(
                    uri=AnyUrl("file:///company/users.json"),
                    name="Company Users",
                    description="List of all company users",
                    mimeType="application/json",
                ),
                Resource(
                    uri=AnyUrl("file:///company/projects.json"),
                    name="Active Projects",
                    description="Current projects",
                    mimeType="application/json",
                ),
            ]

        @self.server.read_resource()  # type: ignore[no-untyped-call,misc]
        async def read_resource(uri: AnyUrl) -> str:  # pyright: ignore[reportUnusedFunction]
            """Read a specific resource."""
            uri_str = str(uri)

            if uri_str == "file:///company/users.json":
                return json.dumps(SAMPLE_DATA["users"], indent=2)

            elif uri_str == "file:///company/projects.json":
                return json.dumps(SAMPLE_DATA["projects"], indent=2)

            else:
                raise ValueError(f"Unknown resource: {uri_str}")

        # Tools
        @self.server.list_tools()  # type: ignore[no-untyped-call,misc]
        async def list_tools() -> list[Tool]:  # pyright: ignore[reportUnusedFunction]
            """List available tools."""
            return [
                Tool(
                    name="echo",
                    description="Echo back the input text",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "Text to echo back",
                            }
                        },
                        "required": ["text"],
                    },
                ),
                Tool(
                    name="get_time",
                    description="Get the current time",
                    inputSchema={
                        "type": "object",
                        "properties": {},
                    },
                ),
            ]

        @self.server.call_tool()  # type: ignore[no-untyped-call,misc]
        async def call_tool(name: str, arguments: Optional[Dict[str, Any]] = None) -> list[TextContent]:  # pyright: ignore[reportUnusedFunction]
            """Call a specific tool."""
            if not arguments:
                arguments = {}

            if name == "echo":
                text = arguments.get("text", "")
                return [
                    TextContent(
                        type="text",
                        text=f"Echo: {text}",
                    )
                ]

            elif name == "get_time":
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                return [
                    TextContent(
                        type="text",
                        text=f"Current time: {current_time}",
                    )
                ]

            else:
                raise ValueError(f"Unknown tool: {name}")


# From tests/conftest.py
def pytest_configure(config: pytest.Config) -> None:
    config.addinivalue_line("markers", "windows: mark test as requiring Windows")

import redis
from autogen_core import CacheStore

# From cache_store/redis.py
class RedisStoreConfig(BaseModel):
    """Configuration for RedisStore"""

    host: str = "localhost"
    port: int = 6379
    db: int = 0
    # Add other relevant redis connection parameters
    username: Optional[str] = None
    password: Optional[str] = None
    ssl: bool = False
    socket_timeout: Optional[float] = None

# From cache_store/redis.py
class RedisStore(CacheStore[T], Component[RedisStoreConfig]):
    """
    A typed CacheStore implementation that uses redis as the underlying storage.
    See :class:`~autogen_ext.models.cache.ChatCompletionCache` for an example of usage.

    Args:
        cache_instance: An instance of `redis.Redis`.
                        The user is responsible for managing the Redis instance's lifetime.
    """

    component_config_schema = RedisStoreConfig
    component_provider_override = "autogen_ext.cache_store.redis.RedisStore"

    def __init__(self, redis_instance: redis.Redis):
        self.cache = redis_instance

    def get(self, key: str, default: Optional[T] = None) -> Optional[T]:
        value = cast(Optional[T], self.cache.get(key))
        if value is None:
            return default
        return value

    def set(self, key: str, value: T) -> None:
        self.cache.set(key, cast(Any, value))

    def _to_config(self) -> RedisStoreConfig:
        # Extract connection info from redis instance
        connection_pool = self.cache.connection_pool
        connection_kwargs: Dict[str, Any] = connection_pool.connection_kwargs  # type: ignore[reportUnknownMemberType]

        username = connection_kwargs.get("username")
        password = connection_kwargs.get("password")
        socket_timeout = connection_kwargs.get("socket_timeout")

        return RedisStoreConfig(
            host=str(connection_kwargs.get("host", "localhost")),
            port=int(connection_kwargs.get("port", 6379)),
            db=int(connection_kwargs.get("db", 0)),
            username=str(username) if username is not None else None,
            password=str(password) if password is not None else None,
            ssl=bool(connection_kwargs.get("ssl", False)),
            socket_timeout=float(socket_timeout) if socket_timeout is not None else None,
        )

    @classmethod
    def _from_config(cls, config: RedisStoreConfig) -> Self:
        # Create new redis instance from config
        redis_instance = redis.Redis(
            host=config.host,
            port=config.port,
            db=config.db,
            username=config.username,
            password=config.password,
            ssl=config.ssl,
            socket_timeout=config.socket_timeout,
        )
        return cls(redis_instance=redis_instance)

import diskcache

# From cache_store/diskcache.py
class DiskCacheStoreConfig(BaseModel):
    """Configuration for DiskCacheStore"""

    directory: str

# From cache_store/diskcache.py
class DiskCacheStore(CacheStore[T], Component[DiskCacheStoreConfig]):
    """
    A typed CacheStore implementation that uses diskcache as the underlying storage.
    See :class:`~autogen_ext.models.cache.ChatCompletionCache` for an example of usage.

    Args:
        cache_instance: An instance of diskcache.Cache.
                        The user is responsible for managing the DiskCache instance's lifetime.
    """

    component_config_schema = DiskCacheStoreConfig
    component_provider_override = "autogen_ext.cache_store.diskcache.DiskCacheStore"

    def __init__(self, cache_instance: diskcache.Cache):  # type: ignore[no-any-unimported]
        self.cache = cache_instance

    def get(self, key: str, default: Optional[T] = None) -> Optional[T]:
        return cast(Optional[T], self.cache.get(key, default))  # type: ignore[reportUnknownMemberType]

    def set(self, key: str, value: T) -> None:
        self.cache.set(key, cast(Any, value))  # type: ignore[reportUnknownMemberType]

    def _to_config(self) -> DiskCacheStoreConfig:
        # Get directory from cache instance
        return DiskCacheStoreConfig(directory=self.cache.directory)

    @classmethod
    def _from_config(cls, config: DiskCacheStoreConfig) -> Self:
        return cls(cache_instance=diskcache.Cache(config.directory))

from autogen_core.code_executor import Alias
from autogen_core.code_executor import CodeResult
from autogen_core.code_executor import FunctionWithRequirements
from autogen_core.code_executor import FunctionWithRequirementsStr
from autogen_core.code_executor import Import

# From code_executors/_common.py
class CommandLineCodeResult(CodeResult):
    """A code result class for command line code executor."""

    code_file: Optional[str]

# From code_executors/_common.py
def get_file_name_from_content(code: str, workspace_path: Path) -> Optional[str]:
    first_line = code.split("\n")[0]
    # TODO - support other languages
    if first_line.startswith("# filename:"):
        filename = first_line.split(":")[1].strip()

        # Handle relative paths in the filename
        path = Path(filename)
        if not path.is_absolute():
            path = workspace_path / path
        path = path.resolve()
        # Throws an error if the file is not in the workspace
        relative = path.relative_to(workspace_path.resolve())
        return str(relative)

    return None

# From code_executors/_common.py
def silence_pip(code: str, lang: str) -> str:
    """Apply -qqq flag to pip install commands."""
    if lang == "python":
        regex = r"^! ?pip install"
    elif lang in ["bash", "shell", "sh", "pwsh", "powershell", "ps1"]:
        regex = r"^pip install"
    else:
        return code

    # Find lines that start with pip install and make sure "-qqq" flag is added.
    lines = code.split("\n")
    for i, line in enumerate(lines):
        # use regex to find lines that start with pip install.
        match = re.search(regex, line)
        if match is not None:
            if "-qqq" not in line:
                lines[i] = line.replace(match.group(0), match.group(0) + " -qqq")
    return "\n".join(lines)

# From code_executors/_common.py
def get_required_packages(code: str, lang: str) -> set[str]:
    ret: set[str] = set()
    if lang == "python":
        regex = r"^! ?pip install(.*)$"
    else:
        return ret

    # Find lines that start with pip install and make sure "-qqq" flag is added.
    lines = code.split("\n")
    for _, line in enumerate(lines):
        # use regex to find lines that start with pip install.
        match = re.search(regex, line)
        if match is not None:
            reqs = match.group(1).split(",")
            ret = {req.strip(" ") for req in reqs}
    return ret

# From code_executors/_common.py
def lang_to_cmd(lang: str) -> str:
    if lang in PYTHON_VARIANTS:
        return "python"
    if lang.startswith("python") or lang in ["bash", "sh"]:
        return lang
    if lang in ["shell"]:
        return "sh"
    if lang in ["pwsh", "powershell", "ps1"]:
        # Check if pwsh is available, otherwise fall back to powershell
        if shutil.which("pwsh") is not None:
            return "pwsh"
        elif shutil.which("powershell") is not None:
            return "powershell"
        else:
            raise ValueError("Powershell or pwsh is not installed. Please install one of them.")
    else:
        raise ValueError(f"Unsupported language: {lang}")

# From code_executors/_common.py
def infer_lang(code: str) -> str:
    """infer the language for the code.
    TODO: make it robust.
    """
    if code.startswith("python ") or code.startswith("pip") or code.startswith("python3 "):
        return "sh"

    # check if code is a valid python code
    try:
        compile(code, "test", "exec")
        return "python"
    except SyntaxError:
        # not a valid python code
        return "unknown"

from autogen_agentchat.ui._console import UserInputManager
from rich.align import AlignMethod
from rich.console import Console
from rich.panel import Panel


# From canvas/_canvas.py
class BaseCanvas(ABC):
    """
    An abstract protocol for "canvas" objects that maintain
    revision history for file-like data. Concrete subclasses
    can handle text, images, structured data, etc.

    .. warning::

        This is an experimental API and may change in the future.

    """

    @abstractmethod
    def list_files(self) -> Dict[str, int]:
        """
        Returns a dict of filename -> latest revision number.
        """
        raise NotImplementedError

    @abstractmethod
    def get_latest_content(self, filename: str) -> Union[str, bytes, Any]:
        """
        Returns the latest version of a file's content.
        """
        raise NotImplementedError

    @abstractmethod
    def add_or_update_file(self, filename: str, new_content: Union[str, bytes, Any]) -> None:
        """
        Creates or updates the file content with a new revision.
        """
        raise NotImplementedError

    @abstractmethod
    def get_diff(self, filename: str, from_revision: int, to_revision: int) -> str:
        """
        Returns a diff (in some format) between two revisions.
        """
        raise NotImplementedError

    @abstractmethod
    def apply_patch(self, filename: str, patch_data: Union[str, bytes, Any]) -> None:
        """
        Applies a patch/diff to the latest revision and increments the revision.
        """
        raise NotImplementedError

# From canvas/_canvas.py
def list_files(self) -> Dict[str, int]:
        """
        Returns a dict of filename -> latest revision number.
        """
        raise NotImplementedError

# From canvas/_canvas.py
def get_latest_content(self, filename: str) -> Union[str, bytes, Any]:
        """
        Returns the latest version of a file's content.
        """
        raise NotImplementedError

# From canvas/_canvas.py
def add_or_update_file(self, filename: str, new_content: Union[str, bytes, Any]) -> None:
        """
        Creates or updates the file content with a new revision.
        """
        raise NotImplementedError

# From canvas/_canvas.py
def get_diff(self, filename: str, from_revision: int, to_revision: int) -> str:
        """
        Returns a diff (in some format) between two revisions.
        """
        raise NotImplementedError

# From canvas/_canvas.py
def apply_patch(self, filename: str, patch_data: Union[str, bytes, Any]) -> None:
        """
        Applies a patch/diff to the latest revision and increments the revision.
        """
        raise NotImplementedError

import difflib
from _canvas import BaseCanvas
from unidiff import PatchSet

# From canvas/_text_canvas.py
class FileRevision:
    """Tracks the history of one file's content."""

    __slots__ = ("content", "revision")

    def __init__(self, content: str, revision: int) -> None:
        self.content: str = content
        self.revision: int = revision

# From canvas/_text_canvas.py
class TextCanvas(BaseCanvas):
    """An in‑memory canvas that stores *text* files with full revision history.

    .. warning::

        This is an experimental API and may change in the future.

    Besides the original CRUD‑like operations, this enhanced implementation adds:

    * **apply_patch** – applies patches using the ``unidiff`` library for accurate
      hunk application and context line validation.
    * **get_revision_content** – random access to any historical revision.
    * **get_revision_diffs** – obtain the list of diffs applied between every
      consecutive pair of revisions so that a caller can replay or audit the
      full change history.
    """

    # ----------------------------------------------------------------------------------
    # Construction helpers
    # ----------------------------------------------------------------------------------

    def __init__(self) -> None:
        # For each file we keep an *ordered* list of FileRevision where the last
        # element is the most recent.  Using a list keeps the memory footprint
        # small and preserves order without any extra bookkeeping.
        self._files: Dict[str, List[FileRevision]] = {}

    # ----------------------------------------------------------------------------------
    # Internal utilities
    # ----------------------------------------------------------------------------------

    def _latest_idx(self, filename: str) -> int:
        """Return the index (not revision number) of the newest revision."""
        return len(self._files.get(filename, [])) - 1

    def _ensure_file(self, filename: str) -> None:
        if filename not in self._files:
            raise ValueError(f"File '{filename}' does not exist on the canvas; create it first.")

    # ----------------------------------------------------------------------------------
    # Revision inspection helpers
    # ----------------------------------------------------------------------------------

    def get_revision_content(self, filename: str, revision: int) -> str:  # NEW 🚀
        """Return the exact content stored in *revision*.

        If the revision does not exist an empty string is returned so that
        downstream code can handle the "not found" case without exceptions.
        """
        for rev in self._files.get(filename, []):
            if rev.revision == revision:
                return rev.content
        return ""

    def get_revision_diffs(self, filename: str) -> List[str]:  # NEW 🚀
        """Return a *chronological* list of unified‑diffs for *filename*.

        Each element in the returned list represents the diff that transformed
        revision *n* into revision *n+1* (starting at revision 1 → 2).
        """
        revisions = self._files.get(filename, [])
        diffs: List[str] = []
        for i in range(1, len(revisions)):
            older, newer = revisions[i - 1], revisions[i]
            diff = difflib.unified_diff(
                older.content.splitlines(keepends=True),
                newer.content.splitlines(keepends=True),
                fromfile=f"{filename}@r{older.revision}",
                tofile=f"{filename}@r{newer.revision}",
            )
            diffs.append("".join(diff))
        return diffs

    # ----------------------------------------------------------------------------------
    # BaseCanvas interface implementation
    # ----------------------------------------------------------------------------------

    def list_files(self) -> Dict[str, int]:
        """Return a mapping of *filename → latest revision number*."""
        return {fname: revs[-1].revision for fname, revs in self._files.items() if revs}

    def get_latest_content(self, filename: str) -> str:  # noqa: D401 – keep API identical
        """Return the most recent content or an empty string if the file is new."""
        revs = self._files.get(filename, [])
        return revs[-1].content if revs else ""

    def add_or_update_file(self, filename: str, new_content: Union[str, bytes, Any]) -> None:
        """Create *filename* or append a new revision containing *new_content*."""
        if isinstance(new_content, bytes):
            new_content = new_content.decode("utf-8")
        if not isinstance(new_content, str):
            raise ValueError(f"Expected str or bytes, got {type(new_content)}")
        if filename not in self._files:
            self._files[filename] = [FileRevision(new_content, 1)]
        else:
            last_rev_num = self._files[filename][-1].revision
            self._files[filename].append(FileRevision(new_content, last_rev_num + 1))

    def get_diff(self, filename: str, from_revision: int, to_revision: int) -> str:
        """Return a unified diff between *from_revision* and *to_revision*."""
        revisions = self._files.get(filename, [])
        if not revisions:
            return ""
        # Fetch the contents for the requested revisions.
        from_content = self.get_revision_content(filename, from_revision)
        to_content = self.get_revision_content(filename, to_revision)
        if from_content == "" and to_content == "":  # one (or both) revision ids not found
            return ""
        diff = difflib.unified_diff(
            from_content.splitlines(keepends=True),
            to_content.splitlines(keepends=True),
            fromfile=f"{filename}@r{from_revision}",
            tofile=f"{filename}@r{to_revision}",
        )
        return "".join(diff)

    def apply_patch(self, filename: str, patch_data: Union[str, bytes, Any]) -> None:
        """Apply *patch_text* (unified diff) to the latest revision and save a new revision.

        Uses the *unidiff* library to accurately apply hunks and validate context lines.
        """
        if isinstance(patch_data, bytes):
            patch_data = patch_data.decode("utf-8")
        if not isinstance(patch_data, str):
            raise ValueError(f"Expected str or bytes, got {type(patch_data)}")
        self._ensure_file(filename)
        original_content = self.get_latest_content(filename)

        if PatchSet is None:
            raise ImportError(
                "The 'unidiff' package is required for patch application. Install with 'pip install unidiff'."
            )

        patch = PatchSet(patch_data)
        # Our canvas stores exactly one file per patch operation so we
        # use the first (and only) patched_file object.
        if not patch:
            raise ValueError("Empty patch text provided.")
        patched_file = patch[0]
        working_lines = original_content.splitlines(keepends=True)
        line_offset = 0
        for hunk in patched_file:
            # Calculate the slice boundaries in the *current* working copy.
            start = hunk.source_start - 1 + line_offset
            end = start + hunk.source_length
            # Build the replacement block for this hunk.
            replacement: List[str] = []
            for line in hunk:
                if line.is_added or line.is_context:
                    replacement.append(line.value)
                # removed lines (line.is_removed) are *not* added.
            # Replace the slice with the hunk‑result.
            working_lines[start:end] = replacement
            line_offset += len(replacement) - (end - start)
        new_content = "".join(working_lines)

        # Finally commit the new revision.
        self.add_or_update_file(filename, new_content)

    # ----------------------------------------------------------------------------------
    # Convenience helpers
    # ----------------------------------------------------------------------------------

    def get_all_contents_for_context(self) -> str:  # noqa: D401 – keep public API stable
        """Return a summarised view of every file and its *latest* revision."""
        out: List[str] = ["=== CANVAS FILES ==="]
        for fname, revs in self._files.items():
            latest = revs[-1]
            out.append(f"File: {fname} (rev {latest.revision}):\n{latest.content}\n")
        out.append("=== END OF CANVAS ===")
        return "\n".join(out)

# From canvas/_text_canvas.py
def get_revision_content(self, filename: str, revision: int) -> str:  # NEW 🚀
        """Return the exact content stored in *revision*.

        If the revision does not exist an empty string is returned so that
        downstream code can handle the "not found" case without exceptions.
        """
        for rev in self._files.get(filename, []):
            if rev.revision == revision:
                return rev.content
        return ""

# From canvas/_text_canvas.py
def get_revision_diffs(self, filename: str) -> List[str]:  # NEW 🚀
        """Return a *chronological* list of unified‑diffs for *filename*.

        Each element in the returned list represents the diff that transformed
        revision *n* into revision *n+1* (starting at revision 1 → 2).
        """
        revisions = self._files.get(filename, [])
        diffs: List[str] = []
        for i in range(1, len(revisions)):
            older, newer = revisions[i - 1], revisions[i]
            diff = difflib.unified_diff(
                older.content.splitlines(keepends=True),
                newer.content.splitlines(keepends=True),
                fromfile=f"{filename}@r{older.revision}",
                tofile=f"{filename}@r{newer.revision}",
            )
            diffs.append("".join(diff))
        return diffs

# From canvas/_text_canvas.py
def get_all_contents_for_context(self) -> str:  # noqa: D401 – keep public API stable
        """Return a summarised view of every file and its *latest* revision."""
        out: List[str] = ["=== CANVAS FILES ==="]
        for fname, revs in self._files.items():
            latest = revs[-1]
            out.append(f"File: {fname} (rev {latest.revision}):\n{latest.content}\n")
        out.append("=== END OF CANVAS ===")
        return "\n".join(out)

from _text_canvas import TextCanvas

# From canvas/_canvas_writer.py
class UpdateFileArgs(BaseModel):
    filename: str
    new_content: str

# From canvas/_canvas_writer.py
class UpdateFileResult(BaseModel):
    status: str

# From canvas/_canvas_writer.py
class UpdateFileTool(BaseTool[UpdateFileArgs, UpdateFileResult]):
    """
    Overwrites or creates a file in the canvas.
    """

    def __init__(self, canvas: TextCanvas):
        super().__init__(
            args_type=UpdateFileArgs,
            return_type=UpdateFileResult,
            name="update_file",
            description="Create/update a file on the canvas with the provided content.",
        )
        self._canvas = canvas

    async def run(self, args: UpdateFileArgs, cancellation_token: CancellationToken) -> UpdateFileResult:
        self._canvas.add_or_update_file(args.filename, args.new_content)
        return UpdateFileResult(status="OK")

# From canvas/_canvas_writer.py
class ApplyPatchArgs(BaseModel):
    filename: str
    patch_text: str

# From canvas/_canvas_writer.py
class ApplyPatchResult(BaseModel):
    status: str

# From canvas/_canvas_writer.py
class ApplyPatchTool(BaseTool[ApplyPatchArgs, ApplyPatchResult]):
    """
    Applies a unified diff patch to the given file on the canvas.
    """

    def __init__(self, canvas: TextCanvas):
        super().__init__(
            args_type=ApplyPatchArgs,
            return_type=ApplyPatchResult,
            name="apply_patch",
            description=(
                "Apply a unified diff patch to an existing file on the canvas. "
                "The patch must be in diff/patch format. The file must exist or be created first."
            ),
        )
        self._canvas = canvas

    async def run(self, args: ApplyPatchArgs, cancellation_token: CancellationToken) -> ApplyPatchResult:
        self._canvas.apply_patch(args.filename, args.patch_text)
        return ApplyPatchResult(status="PATCH APPLIED")

from mcp import Tool
from _base import McpToolAdapter
from _config import SseServerParams

# From mcp/_sse.py
class SseMcpToolAdapterConfig(BaseModel):
    """Configuration for the MCP tool adapter."""

    server_params: SseServerParams
    tool: Tool

# From mcp/_sse.py
class SseMcpToolAdapter(
    McpToolAdapter[SseServerParams],
    Component[SseMcpToolAdapterConfig],
):
    """
    Allows you to wrap an MCP tool running over Server-Sent Events (SSE) and make it available to AutoGen.

    This adapter enables using MCP-compatible tools that communicate over HTTP with SSE
    with AutoGen agents. Common use cases include integrating with remote MCP services,
    cloud-based tools, and web APIs that implement the Model Context Protocol (MCP).

    .. note::

        To use this class, you need to install `mcp` extra for the `autogen-ext` package.

        .. code-block:: bash

            pip install -U "autogen-ext[mcp]"

    Args:
        server_params (SseServerParameters): Parameters for the MCP server connection,
            including URL, headers, and timeouts.
        tool (Tool): The MCP tool to wrap.
        session (ClientSession, optional): The MCP client session to use. If not provided,
            it will create a new session. This is useful for testing or when you want to
            manage the session lifecycle yourself.

    Examples:
        Use a remote translation service that implements MCP over SSE to create tools
        that allow AutoGen agents to perform translations:

        .. code-block:: python

            import asyncio
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.ui import Console
            from autogen_core import CancellationToken


            async def main() -> None:
                # Create server params for the remote MCP service
                server_params = SseServerParams(
                    url="https://api.example.com/mcp",
                    headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
                    timeout=30,  # Connection timeout in seconds
                )

                # Get the translation tool from the server
                adapter = await SseMcpToolAdapter.from_server_params(server_params, "translate")

                # Create an agent that can use the translation tool
                model_client = OpenAIChatCompletionClient(model="gpt-4")
                agent = AssistantAgent(
                    name="translator",
                    model_client=model_client,
                    tools=[adapter],
                    system_message="You are a helpful translation assistant.",
                )

                # Let the agent translate some text
                await Console(
                    agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
                )


            if __name__ == "__main__":
                asyncio.run(main())

    """

    component_config_schema = SseMcpToolAdapterConfig
    component_provider_override = "autogen_ext.tools.mcp.SseMcpToolAdapter"

    def __init__(self, server_params: SseServerParams, tool: Tool, session: ClientSession | None = None) -> None:
        super().__init__(server_params=server_params, tool=tool, session=session)

    def _to_config(self) -> SseMcpToolAdapterConfig:
        """
        Convert the adapter to its configuration representation.

        Returns:
            SseMcpToolAdapterConfig: The configuration of the adapter.
        """
        return SseMcpToolAdapterConfig(server_params=self._server_params, tool=self._tool)

    @classmethod
    def _from_config(cls, config: SseMcpToolAdapterConfig) -> Self:
        """
        Create an instance of SseMcpToolAdapter from its configuration.

        Args:
            config (SseMcpToolAdapterConfig): The configuration of the adapter.

        Returns:
            SseMcpToolAdapter: An instance of SseMcpToolAdapter.
        """
        return cls(server_params=config.server_params, tool=config.tool)

from autogen_core import trace_tool_span
from autogen_core.tools import ImageResultContent
from autogen_core.tools import ParametersSchema
from autogen_core.tools import TextResultContent
from autogen_core.tools import ToolOverride
from autogen_core.tools import ToolResult
from autogen_core.tools import ToolSchema
from autogen_core.tools import Workbench
from mcp.types import CallToolResult
from mcp.types import EmbeddedResource
from mcp.types import ImageContent
from mcp.types import ListPromptsResult
from mcp.types import ListResourcesResult
from mcp.types import ListResourceTemplatesResult
from mcp.types import ListToolsResult
from mcp.types import ReadResourceResult
from _actor import McpSessionActor
from _config import McpServerParams
from _config import StdioServerParams
from _config import StreamableHttpServerParams

# From mcp/_workbench.py
class McpWorkbenchConfig(BaseModel):
    server_params: McpServerParams
    tool_overrides: Dict[str, ToolOverride] = Field(default_factory=dict)
    model_client: ComponentModel | Dict[str, Any] | None = None

# From mcp/_workbench.py
class McpWorkbenchState(BaseModel):
    type: Literal["McpWorkBenchState"] = "McpWorkBenchState"

# From mcp/_workbench.py
class McpWorkbench(Workbench, Component[McpWorkbenchConfig]):
    """A workbench that wraps an MCP server and provides an interface
    to list and call tools provided by the server.

    .. warning::

        Only connect to trusted MCP servers, especially when using
        `StdioServerParams` as it executes commands in the local environment.

    This workbench should be used as a context manager to ensure proper
    initialization and cleanup of the underlying MCP session.

    .. list-table:: MCP Support
       :header-rows: 1
       :widths: 30 70

       * - MCP Capability
         - Supported Features
       * - Tools
         - list_tools, call_tool
       * - Resources
         - list_resources, read_resource
       * - ResourceTemplates
         - list_resource_templates, read_resource_template
       * - Prompts
         - list_prompts, get_prompt
       * - Sampling
         - Optional support via model_client
       * - Roots
         - not supported
       * - Ellicitation
         - not supported

    Args:
        server_params (McpServerParams): The parameters to connect to the MCP server.
            This can be either a :class:`StdioServerParams` or :class:`SseServerParams`.
        tool_overrides (Optional[Dict[str, ToolOverride]]): Optional mapping of original tool
            names to override configurations for name and/or description. This allows
            customizing how server tools appear to consumers while maintaining the underlying
            tool functionality.
        model_client: Optional chat completion client to handle sampling requests
            from MCP servers that support the sampling capability. This allows MCP
            servers to request text generation from a language model during tool
            execution. If not provided, sampling requests will return an error.

    Raises:
        ValueError: If there are conflicts in tool override names.

    Examples:

        Here is a simple example of how to use the workbench with a `mcp-server-fetch` server:

        .. code-block:: python

            import asyncio

            from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


            async def main() -> None:
                params = StdioServerParams(
                    command="uvx",
                    args=["mcp-server-fetch"],
                    read_timeout_seconds=60,
                )

                # You can also use `start()` and `stop()` to manage the session.
                async with McpWorkbench(server_params=params) as workbench:
                    tools = await workbench.list_tools()
                    print(tools)
                    result = await workbench.call_tool(tools[0]["name"], {"url": "https://github.com/"})
                    print(result)


            asyncio.run(main())

        Example of using tool overrides:

        .. code-block:: python

            import asyncio
            from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams
            from autogen_core.tools import ToolOverride


            async def main() -> None:
                params = StdioServerParams(
                    command="uvx",
                    args=["mcp-server-fetch"],
                    read_timeout_seconds=60,
                )

                # Override the fetch tool's name and description
                overrides = {
                    "fetch": ToolOverride(name="web_fetch", description="Enhanced web fetching tool with better error handling")
                }

                async with McpWorkbench(server_params=params, tool_overrides=overrides) as workbench:
                    tools = await workbench.list_tools()
                    # The tool will now appear as "web_fetch" with the new description
                    print(tools)
                    # Call the overridden tool
                    result = await workbench.call_tool("web_fetch", {"url": "https://github.com/"})
                    print(result)


            asyncio.run(main())

        Example of using the workbench with the `GitHub MCP Server <https://github.com/github/github-mcp-server>`_:

        .. code-block:: python

            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.ui import Console
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
                server_params = StdioServerParams(
                    command="docker",
                    args=[
                        "run",
                        "-i",
                        "--rm",
                        "-e",
                        "GITHUB_PERSONAL_ACCESS_TOKEN",
                        "ghcr.io/github/github-mcp-server",
                    ],
                    env={
                        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
                    },
                )
                async with McpWorkbench(server_params) as mcp:
                    agent = AssistantAgent(
                        "github_assistant",
                        model_client=model_client,
                        workbench=mcp,
                        reflect_on_tool_use=True,
                        model_client_stream=True,
                    )
                    await Console(agent.run_stream(task="Is there a repository named Autogen"))


            asyncio.run(main())

        Example of using the workbench with the `Playwright MCP Server <https://github.com/microsoft/playwright-mcp>`_:

        .. code-block:: python

            # First run `npm install -g @playwright/mcp@latest` to install the MCP server.
            import asyncio
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_agentchat.conditions import TextMessageTermination
            from autogen_agentchat.ui import Console
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


            async def main() -> None:
                model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
                server_params = StdioServerParams(
                    command="npx",
                    args=[
                        "@playwright/mcp@latest",
                        "--headless",
                    ],
                )
                async with McpWorkbench(server_params) as mcp:
                    agent = AssistantAgent(
                        "web_browsing_assistant",
                        model_client=model_client,
                        workbench=mcp,
                        model_client_stream=True,
                    )
                    team = RoundRobinGroupChat(
                        [agent],
                        termination_condition=TextMessageTermination(source="web_browsing_assistant"),
                    )
                    await Console(team.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


            asyncio.run(main())

    """

    component_provider_override = "autogen_ext.tools.mcp.McpWorkbench"
    component_config_schema = McpWorkbenchConfig

    def __init__(
        self,
        server_params: McpServerParams,
        tool_overrides: Optional[Dict[str, ToolOverride]] = None,
        model_client: ChatCompletionClient | None = None,
    ) -> None:
        self._server_params = server_params
        self._tool_overrides = tool_overrides or {}
        self._model_client = model_client

        # Build reverse mapping from override names to original names for call_tool
        self._override_name_to_original: Dict[str, str] = {}
        for original_name, override in self._tool_overrides.items():
            override_name = override.name
            if override_name and override_name != original_name:
                # Check for conflicts with other override names
                if override_name in self._override_name_to_original:
                    existing_original = self._override_name_to_original[override_name]
                    raise ValueError(
                        f"Tool override name '{override_name}' is used by multiple tools: "
                        f"'{existing_original}' and '{original_name}'. Override names must be unique."
                    )
                self._override_name_to_original[override_name] = original_name

        # self._session: ClientSession | None = None
        self._actor: McpSessionActor | None = None
        self._actor_loop: asyncio.AbstractEventLoop | None = None
        self._read = None
        self._write = None

    @property
    def server_params(self) -> McpServerParams:
        return self._server_params

    async def list_tools(self) -> List[ToolSchema]:
        if not self._actor:
            await self.start()  # fallback to start the actor if not initialized instead of raising an error
            # Why? Because when deserializing the workbench, the actor might not be initialized yet.
            # raise RuntimeError("Actor is not initialized. Call start() first.")
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")
        result_future = await self._actor.call("list_tools", None)
        list_tool_result = await result_future
        assert isinstance(
            list_tool_result, ListToolsResult
        ), f"list_tools must return a CallToolResult, instead of : {str(type(list_tool_result))}"
        schema: List[ToolSchema] = []
        for tool in list_tool_result.tools:
            original_name = tool.name
            name = original_name
            description = tool.description or ""

            # Apply overrides if they exist for this tool
            if original_name in self._tool_overrides:
                override = self._tool_overrides[original_name]
                if override.name is not None:
                    name = override.name
                if override.description is not None:
                    description = override.description

            parameters = ParametersSchema(
                type="object",
                properties=tool.inputSchema.get("properties", {}),
                required=tool.inputSchema.get("required", []),
                additionalProperties=tool.inputSchema.get("additionalProperties", False),
            )
            tool_schema = ToolSchema(
                name=name,
                description=description,
                parameters=parameters,
            )
            schema.append(tool_schema)
        return schema

    async def call_tool(
        self,
        name: str,
        arguments: Mapping[str, Any] | None = None,
        cancellation_token: CancellationToken | None = None,
        call_id: str | None = None,
    ) -> ToolResult:
        if not self._actor:
            await self.start()  # fallback to start the actor if not initialized instead of raising an error
            # Why? Because when deserializing the workbench, the actor might not be initialized yet.
            # raise RuntimeError("Actor is not initialized. Call start() first.")
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")
        if not cancellation_token:
            cancellation_token = CancellationToken()
        if not arguments:
            arguments = {}

        # Check if the name is an override name and map it back to the original
        original_name = self._override_name_to_original.get(name, name)

        with trace_tool_span(
            tool_name=name,  # Use the requested name for tracing
            tool_call_id=call_id,
        ):
            try:
                result_future = await self._actor.call("call_tool", {"name": original_name, "kargs": arguments})
                cancellation_token.link_future(result_future)
                result = await result_future
                assert isinstance(
                    result, CallToolResult
                ), f"call_tool must return a CallToolResult, instead of : {str(type(result))}"
                result_parts: List[TextResultContent | ImageResultContent] = []
                is_error = result.isError
                for content in result.content:
                    if isinstance(content, TextContent):
                        result_parts.append(TextResultContent(content=content.text))
                    elif isinstance(content, ImageContent):
                        result_parts.append(ImageResultContent(content=Image.from_base64(content.data)))
                    elif isinstance(content, EmbeddedResource):
                        # TODO: how to handle embedded resources?
                        # For now we just use text representation.
                        result_parts.append(TextResultContent(content=content.model_dump_json()))
                    else:
                        raise ValueError(f"Unknown content type from server: {type(content)}")
            except Exception as e:
                error_message = self._format_errors(e)
                is_error = True
                result_parts = [TextResultContent(content=error_message)]
        return ToolResult(name=name, result=result_parts, is_error=is_error)  # Return the requested name

    @property
    def initialize_result(self) -> Any:
        if self._actor:
            return self._actor.initialize_result

        return None

    async def list_prompts(self) -> ListPromptsResult:
        """List available prompts from the MCP server."""
        if not self._actor:
            await self.start()
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")

        result_future = await self._actor.call("list_prompts", None)
        list_prompts_result = await result_future
        assert isinstance(
            list_prompts_result, ListPromptsResult
        ), f"list_prompts must return a ListPromptsResult, instead of: {str(type(list_prompts_result))}"

        return list_prompts_result

    async def list_resources(self) -> ListResourcesResult:
        """List available resources from the MCP server."""
        if not self._actor:
            await self.start()
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")

        result_future = await self._actor.call("list_resources", None)
        list_resources_result = await result_future
        assert isinstance(
            list_resources_result, ListResourcesResult
        ), f"list_resources must return a ListResourcesResult, instead of: {str(type(list_resources_result))}"

        return list_resources_result

    async def list_resource_templates(self) -> ListResourceTemplatesResult:
        """List available resource templates from the MCP server."""
        if not self._actor:
            await self.start()
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")

        result_future = await self._actor.call("list_resource_templates", None)
        list_templates_result = await result_future
        assert isinstance(
            list_templates_result, ListResourceTemplatesResult
        ), f"list_resource_templates must return a ListResourceTemplatesResult, instead of: {str(type(list_templates_result))}"

        return list_templates_result

    async def read_resource(self, uri: str) -> ReadResourceResult:
        """Read a resource from the MCP server."""
        if not self._actor:
            await self.start()
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")

        result_future = await self._actor.call("read_resource", {"name": None, "kargs": {"uri": uri}})
        read_resource_result = await result_future
        assert isinstance(
            read_resource_result, ReadResourceResult
        ), f"read_resource must return a ReadResourceResult, instead of: {str(type(read_resource_result))}"

        return read_resource_result

    async def get_prompt(self, name: str, arguments: Optional[Dict[str, str]] = None) -> GetPromptResult:
        """Get a prompt from the MCP server."""
        if not self._actor:
            await self.start()
        if self._actor is None:
            raise RuntimeError("Actor is not initialized. Please check the server connection.")

        result_future = await self._actor.call("get_prompt", {"name": name, "kargs": {"arguments": arguments}})
        get_prompt_result = await result_future
        assert isinstance(
            get_prompt_result, GetPromptResult
        ), f"get_prompt must return a GetPromptResult, instead of: {str(type(get_prompt_result))}"

        return get_prompt_result

    def _format_errors(self, error: Exception) -> str:
        """Recursively format errors into a string."""

        error_message = ""
        if hasattr(builtins, "ExceptionGroup") and isinstance(error, builtins.ExceptionGroup):
            # ExceptionGroup is available in Python 3.11+.
            # TODO: how to make this compatible with Python 3.10?
            for sub_exception in error.exceptions:  # type: ignore
                error_message += self._format_errors(sub_exception)  # type: ignore
        else:
            error_message += f"{str(error)}\n"
        return error_message

    async def start(self) -> None:
        if self._actor:
            warnings.warn(
                "McpWorkbench is already started. No need to start again.",
                UserWarning,
                stacklevel=2,
            )
            return  # Already initialized, no need to start again

        if isinstance(self._server_params, (StdioServerParams, SseServerParams, StreamableHttpServerParams)):
            self._actor = McpSessionActor(self._server_params, model_client=self._model_client)
            await self._actor.initialize()
            self._actor_loop = asyncio.get_event_loop()
        else:
            raise ValueError(f"Unsupported server params type: {type(self._server_params)}")

    async def stop(self) -> None:
        if self._actor:
            # Close the actor
            await self._actor.close()
            self._actor = None
        else:
            raise RuntimeError("McpWorkbench is not started. Call start() first.")

    async def reset(self) -> None:
        pass

    async def save_state(self) -> Mapping[str, Any]:
        return McpWorkbenchState().model_dump()

    async def load_state(self, state: Mapping[str, Any]) -> None:
        pass

    def _to_config(self) -> McpWorkbenchConfig:
        model_client_config = None
        if self._model_client is not None:
            model_client_config = self._model_client.dump_component()
        return McpWorkbenchConfig(
            server_params=self._server_params, tool_overrides=self._tool_overrides, model_client=model_client_config
        )

    @classmethod
    def _from_config(cls, config: McpWorkbenchConfig) -> Self:
        model_client = None
        if config.model_client is not None:
            model_client = ChatCompletionClient.load_component(config.model_client)
        return cls(server_params=config.server_params, tool_overrides=config.tool_overrides, model_client=model_client)

    def __del__(self) -> None:
        # Ensure the actor is stopped when the workbench is deleted
        # Use getattr to safely handle cases where attributes may not be set (e.g., if __init__ failed)
        actor = getattr(self, "_actor", None)
        actor_loop = getattr(self, "_actor_loop", None)

        if actor and actor_loop:
            if actor_loop.is_running() and not actor_loop.is_closed():
                actor_loop.call_soon_threadsafe(lambda: asyncio.create_task(self.stop()))
            else:
                msg = "Cannot safely stop actor at [McpWorkbench.__del__]: loop is closed or not running"
                warnings.warn(msg, RuntimeWarning, stacklevel=2)

# From mcp/_workbench.py
def server_params(self) -> McpServerParams:
        return self._server_params

# From mcp/_workbench.py
def initialize_result(self) -> Any:
        if self._actor:
            return self._actor.initialize_result

        return None


# From mcp/_streamable_http.py
class StreamableHttpMcpToolAdapterConfig(BaseModel):
    """Configuration for the MCP tool adapter."""

    server_params: StreamableHttpServerParams
    tool: Tool

# From mcp/_streamable_http.py
class StreamableHttpMcpToolAdapter(
    McpToolAdapter[StreamableHttpServerParams],
    Component[StreamableHttpMcpToolAdapterConfig],
):
    """
    Allows you to wrap an MCP tool running over Streamable HTTP and make it available to AutoGen.

    This adapter enables using MCP-compatible tools that communicate over Streamable HTTP
    with AutoGen agents. Common use cases include integrating with remote MCP services,
    cloud-based tools, and web APIs that implement the Model Context Protocol (MCP).

    .. note::

        To use this class, you need to install `mcp` extra for the `autogen-ext` package.

        .. code-block:: bash

            pip install -U "autogen-ext[mcp]"


    Args:
        server_params (StreamableHttpServerParams): Parameters for the MCP server connection,
            including URL, headers, and timeouts.
        tool (Tool): The MCP tool to wrap.
        session (ClientSession, optional): The MCP client session to use. If not provided,
            it will create a new session. This is useful for testing or when you want to
            manage the session lifecycle yourself.

    Examples:
        Use a remote translation service that implements MCP over Streamable HTTP to
        create tools that allow AutoGen agents to perform translations:

        .. code-block:: python

            import asyncio
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.tools.mcp import StreamableHttpMcpToolAdapter, StreamableHttpServerParams
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.ui import Console
            from autogen_core import CancellationToken


            async def main() -> None:
                # Create server params for the remote MCP service
                server_params = StreamableHttpServerParams(
                    url="https://api.example.com/mcp",
                    headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
                    timeout=30.0,  # HTTP timeout in seconds
                    sse_read_timeout=300.0,  # SSE read timeout in seconds (5 minutes)
                    terminate_on_close=True,
                )

                # Get the translation tool from the server
                adapter = await StreamableHttpMcpToolAdapter.from_server_params(server_params, "translate")

                # Create an agent that can use the translation tool
                model_client = OpenAIChatCompletionClient(model="gpt-4")
                agent = AssistantAgent(
                    name="translator",
                    model_client=model_client,
                    tools=[adapter],
                    system_message="You are a helpful translation assistant.",
                )

                # Let the agent translate some text
                await Console(
                    agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
                )


            if __name__ == "__main__":
                asyncio.run(main())

    """

    component_config_schema = StreamableHttpMcpToolAdapterConfig
    component_provider_override = "autogen_ext.tools.mcp.StreamableHttpMcpToolAdapter"

    def __init__(
        self, server_params: StreamableHttpServerParams, tool: Tool, session: ClientSession | None = None
    ) -> None:
        super().__init__(server_params=server_params, tool=tool, session=session)

    def _to_config(self) -> StreamableHttpMcpToolAdapterConfig:
        """
        Convert the adapter to its configuration representation.

        Returns:
            StreamableHttpMcpToolAdapterConfig: The configuration of the adapter.
        """
        return StreamableHttpMcpToolAdapterConfig(server_params=self._server_params, tool=self._tool)

    @classmethod
    def _from_config(cls, config: StreamableHttpMcpToolAdapterConfig) -> Self:
        """
        Create an instance of StreamableHttpMcpToolAdapter from its configuration.

        Args:
            config (StreamableHttpMcpToolAdapterConfig): The configuration of the adapter.

        Returns:
            StreamableHttpMcpToolAdapter: An instance of StreamableHttpMcpToolAdapter.
        """
        return cls(server_params=config.server_params, tool=config.tool)

from autogen_core.utils import schema_to_pydantic_model
from mcp.types import AudioContent
from mcp.types import ContentBlock
from mcp.types import ResourceLink
from _session import create_mcp_server_session

# From mcp/_base.py
class McpToolAdapter(BaseTool[BaseModel, Any], ABC, Generic[TServerParams]):
    """
    Base adapter class for MCP tools to make them compatible with AutoGen.

    Args:
        server_params (TServerParams): Parameters for the MCP server connection.
        tool (Tool): The MCP tool to wrap.
    """

    component_type = "tool"

    def __init__(self, server_params: TServerParams, tool: Tool, session: ClientSession | None = None) -> None:
        self._tool = tool
        self._server_params = server_params
        self._session = session

        # Extract name and description
        name = tool.name
        description = tool.description or ""

        # Create the input model from the tool's schema
        input_model = schema_to_pydantic_model(tool.inputSchema)

        # Use Any as return type since MCP tool returns can vary
        return_type: Type[Any] = object

        super().__init__(input_model, return_type, name, description)

    async def run(self, args: BaseModel, cancellation_token: CancellationToken) -> Any:
        """
        Run the MCP tool with the provided arguments.

        Args:
            args (BaseModel): The arguments to pass to the tool.
            cancellation_token (CancellationToken): Token to signal cancellation.

        Returns:
            Any: The result of the tool execution.

        Raises:
            Exception: If the operation is cancelled or the tool execution fails.
        """
        # Convert the input model to a dictionary
        # Exclude unset values to avoid sending them to the MCP servers which may cause errors
        # for many servers.
        kwargs = args.model_dump(exclude_unset=True)

        if self._session is not None:
            # If a session is provided, use it directly.
            session = self._session
            return await self._run(args=kwargs, cancellation_token=cancellation_token, session=session)

        async with create_mcp_server_session(self._server_params) as session:
            await session.initialize()
            return await self._run(args=kwargs, cancellation_token=cancellation_token, session=session)

    def _normalize_payload_to_content_list(self, payload: Sequence[ContentBlock]) -> list[ContentBlock]:
        """
        Normalizes a raw tool output payload into a list of content items.
        - If payload is already a sequence of ContentBlock items, it's converted to a list and returned.
        - If payload is a single ContentBlock item, it's wrapped in a list.
        - If payload is a string, it's wrapped in [TextContent(text=payload)].
        - Otherwise, the payload is stringified and wrapped in [TextContent(text=str(payload))].
        """
        if isinstance(payload, Sequence) and all(
            isinstance(item, (TextContent, ImageContent, EmbeddedResource, AudioContent, ResourceLink))
            for item in payload
        ):
            return list(payload)
        elif isinstance(payload, (TextContent, ImageContent, EmbeddedResource, AudioContent, ResourceLink)):
            return [payload]
        elif isinstance(payload, str):
            return [TextContent(text=payload, type="text")]
        else:
            return [TextContent(text=str(payload), type="text")]

    async def _run(self, args: Dict[str, Any], cancellation_token: CancellationToken, session: ClientSession) -> Any:
        exceptions_to_catch: tuple[Type[BaseException], ...]
        if hasattr(builtins, "ExceptionGroup"):
            exceptions_to_catch = (asyncio.CancelledError, builtins.ExceptionGroup)
        else:
            exceptions_to_catch = (asyncio.CancelledError,)

        try:
            if cancellation_token.is_cancelled():
                raise asyncio.CancelledError("Operation cancelled")

            result_future = asyncio.ensure_future(session.call_tool(name=self._tool.name, arguments=args))
            cancellation_token.link_future(result_future)
            result = await result_future

            normalized_content_list = self._normalize_payload_to_content_list(result.content)

            if result.isError:
                serialized_error_message = self.return_value_as_string(normalized_content_list)
                raise Exception(serialized_error_message)
            return normalized_content_list

        except exceptions_to_catch:
            # Re-raise these specific exception types directly.
            raise

    @classmethod
    async def from_server_params(cls, server_params: TServerParams, tool_name: str) -> "McpToolAdapter[TServerParams]":
        """
        Create an instance of McpToolAdapter from server parameters and tool name.

        Args:
            server_params (TServerParams): Parameters for the MCP server connection.
            tool_name (str): The name of the tool to wrap.

        Returns:
            McpToolAdapter[TServerParams]: An instance of McpToolAdapter.

        Raises:
            ValueError: If the tool with the specified name is not found.
        """
        async with create_mcp_server_session(server_params) as session:
            await session.initialize()

            tools_response = await session.list_tools()
            matching_tool = next((t for t in tools_response.tools if t.name == tool_name), None)

            if matching_tool is None:
                raise ValueError(
                    f"Tool '{tool_name}' not found, available tools: {', '.join([t.name for t in tools_response.tools])}"
                )

        return cls(server_params=server_params, tool=matching_tool)

    def return_value_as_string(self, value: list[Any]) -> str:
        """Return a string representation of the result."""

        def serialize_item(item: Any) -> dict[str, Any]:
            if isinstance(item, (TextContent, ImageContent, AudioContent)):
                dumped = item.model_dump()
                # Remove the 'meta' field if it exists and is None (for backward compatibility)
                if dumped.get("meta") is None:
                    dumped.pop("meta", None)
                return dumped
            elif isinstance(item, EmbeddedResource):
                type = item.type
                resource = {}
                for key, val in item.resource.model_dump().items():
                    # Skip 'meta' field if it's None (for backward compatibility)
                    if key == "meta" and val is None:
                        continue
                    if isinstance(val, AnyUrl):
                        resource[key] = str(val)
                    else:
                        resource[key] = val
                dumped_annotations = item.annotations.model_dump() if item.annotations else None
                # Remove 'meta' from annotations if it exists and is None
                if dumped_annotations and dumped_annotations.get("meta") is None:
                    dumped_annotations.pop("meta", None)
                return {"type": type, "resource": resource, "annotations": dumped_annotations}
            elif isinstance(item, ResourceLink):
                dumped = item.model_dump()
                # Remove the 'meta' field if it exists and is None (for backward compatibility)
                if dumped.get("meta") is None:
                    dumped.pop("meta", None)
                # Convert AnyUrl to string for JSON serialization
                if "uri" in dumped and isinstance(dumped["uri"], AnyUrl):
                    dumped["uri"] = str(dumped["uri"])
                return dumped
            else:
                return {}

        return json.dumps([serialize_item(item) for item in value])

# From mcp/_base.py
def serialize_item(item: Any) -> dict[str, Any]:
            if isinstance(item, (TextContent, ImageContent, AudioContent)):
                dumped = item.model_dump()
                # Remove the 'meta' field if it exists and is None (for backward compatibility)
                if dumped.get("meta") is None:
                    dumped.pop("meta", None)
                return dumped
            elif isinstance(item, EmbeddedResource):
                type = item.type
                resource = {}
                for key, val in item.resource.model_dump().items():
                    # Skip 'meta' field if it's None (for backward compatibility)
                    if key == "meta" and val is None:
                        continue
                    if isinstance(val, AnyUrl):
                        resource[key] = str(val)
                    else:
                        resource[key] = val
                dumped_annotations = item.annotations.model_dump() if item.annotations else None
                # Remove 'meta' from annotations if it exists and is None
                if dumped_annotations and dumped_annotations.get("meta") is None:
                    dumped_annotations.pop("meta", None)
                return {"type": type, "resource": resource, "annotations": dumped_annotations}
            elif isinstance(item, ResourceLink):
                dumped = item.model_dump()
                # Remove the 'meta' field if it exists and is None (for backward compatibility)
                if dumped.get("meta") is None:
                    dumped.pop("meta", None)
                # Convert AnyUrl to string for JSON serialization
                if "uri" in dumped and isinstance(dumped["uri"], AnyUrl):
                    dumped["uri"] = str(dumped["uri"])
                return dumped
            else:
                return {}

from _sse import SseMcpToolAdapter
from _stdio import StdioMcpToolAdapter
from _streamable_http import StreamableHttpMcpToolAdapter


# From mcp/_config.py
class StdioServerParams(StdioServerParameters):
    """Parameters for connecting to an MCP server over STDIO."""

    type: Literal["StdioServerParams"] = "StdioServerParams"

    read_timeout_seconds: float = 5

# From mcp/_config.py
class SseServerParams(BaseModel):
    """Parameters for connecting to an MCP server over SSE."""

    type: Literal["SseServerParams"] = "SseServerParams"

    url: str  # The SSE endpoint URL.
    headers: dict[str, Any] | None = None  # Optional headers to include in requests.
    timeout: float = 5  # HTTP timeout for regular operations.
    sse_read_timeout: float = 60 * 5

# From mcp/_config.py
class StreamableHttpServerParams(BaseModel):
    """Parameters for connecting to an MCP server over Streamable HTTP."""

    type: Literal["StreamableHttpServerParams"] = "StreamableHttpServerParams"

    url: str  # The endpoint URL.
    headers: dict[str, Any] | None = None  # Optional headers to include in requests.
    timeout: float = 30.0  # HTTP timeout for regular operations in seconds.
    sse_read_timeout: float = 300.0  # Timeout for SSE read operations in seconds.
    terminate_on_close: bool = True

import atexit
from typing import Coroutine
from autogen_core.models import ModelInfo
from autogen_core.models import SystemMessage
from mcp import types
from mcp.client.session import ClientSession

# From mcp/_actor.py
class McpActorArgs(TypedDict):
    name: str | None
    kargs: Mapping[str, Any]

# From mcp/_actor.py
class McpSessionActorConfig(BaseModel):
    server_params: McpServerParams
    model_client: ComponentModel | Dict[str, Any] | None = None

# From mcp/_actor.py
class McpSessionActor(ComponentBase[BaseModel], Component[McpSessionActorConfig]):
    component_type = "mcp_session_actor"
    component_config_schema = McpSessionActorConfig
    component_provider_override = "autogen_ext.tools.mcp.McpSessionActor"

    server_params: McpServerParams

    # model_config = ConfigDict(arbitrary_types_allowed=True)

    def __init__(self, server_params: McpServerParams, model_client: ChatCompletionClient | None = None) -> None:
        self.server_params: McpServerParams = server_params
        self._model_client = model_client
        self.name = "mcp_session_actor"
        self.description = "MCP session actor"
        self._command_queue: asyncio.Queue[Dict[str, Any]] = asyncio.Queue()
        self._actor_task: asyncio.Task[Any] | None = None
        self._shutdown_future: asyncio.Future[Any] | None = None
        self._active = False
        self._initialize_result: mcp_types.InitializeResult | None = None
        atexit.register(self._sync_shutdown)

    @property
    def initialize_result(self) -> mcp_types.InitializeResult | None:
        return self._initialize_result

    async def initialize(self) -> None:
        if not self._active:
            self._active = True
            self._actor_task = asyncio.create_task(self._run_actor())

    async def call(self, type: str, args: McpActorArgs | None = None) -> McpFuture:
        if not self._active:
            raise RuntimeError("MCP Actor not running, call initialize() first")
        if self._actor_task and self._actor_task.done():
            raise RuntimeError("MCP actor task crashed", self._actor_task.exception())
        fut: asyncio.Future[McpFuture] = asyncio.Future()
        if type in {"list_tools", "list_prompts", "list_resources", "list_resource_templates", "shutdown"}:
            await self._command_queue.put({"type": type, "future": fut})
            res = await fut
        elif type in {"call_tool", "read_resource", "get_prompt"}:
            if args is None:
                raise ValueError(f"args is required for {type}")
            name = args.get("name", None)
            kwargs = args.get("kargs", {})
            if type == "call_tool" and name is None:
                raise ValueError("name is required for call_tool")
            elif type == "read_resource":
                uri = kwargs.get("uri", None)
                if uri is None:
                    raise ValueError("uri is required for read_resource")
                await self._command_queue.put({"type": type, "uri": uri, "future": fut})
            elif type == "get_prompt":
                if name is None:
                    raise ValueError("name is required for get_prompt")
                prompt_args = kwargs.get("arguments", None)
                await self._command_queue.put({"type": type, "name": name, "args": prompt_args, "future": fut})
            else:  # call_tool
                await self._command_queue.put({"type": type, "name": name, "args": kwargs, "future": fut})
            res = await fut
        else:
            raise ValueError(f"Unknown command type: {type}")
        return res

    async def close(self) -> None:
        if not self._active or self._actor_task is None:
            return
        self._shutdown_future = asyncio.Future()
        await self._command_queue.put({"type": "shutdown", "future": self._shutdown_future})
        await self._shutdown_future
        await self._actor_task
        self._active = False

    async def _sampling_callback(
        self,
        context: RequestContext[ClientSession, Any],
        params: mcp_types.CreateMessageRequestParams,
    ) -> mcp_types.CreateMessageResult | mcp_types.ErrorData:
        """Handle sampling requests using the provided model client."""
        if self._model_client is None:
            # Return an error when no model client is available
            return mcp_types.ErrorData(
                code=mcp_types.INVALID_REQUEST,
                message="No model client available for sampling.",
                data=None,
            )

        llm_messages: list[LLMMessage] = []

        try:
            if params.systemPrompt:
                llm_messages.append(SystemMessage(content=params.systemPrompt))

            for mcp_message in params.messages:
                llm_messages.append(_parse_sampling_message(mcp_message, model_info=self._model_client.model_info))

        except Exception as e:
            return mcp_types.ErrorData(
                code=mcp_types.INVALID_PARAMS,
                message="Error processing sampling messages.",
                data=f"{type(e).__name__}: {e}",
            )

        try:
            result = await self._model_client.create(messages=llm_messages)

            content = result.content
            if not isinstance(content, str):
                content = str(content)

            return mcp_types.CreateMessageResult(
                role="assistant",
                content=mcp_types.TextContent(type="text", text=content),
                model=self._model_client.model_info["family"],
                stopReason=result.finish_reason,
            )
        except Exception as e:
            return mcp_types.ErrorData(
                code=mcp_types.INTERNAL_ERROR,
                message="Error sampling from model client.",
                data=f"{type(e).__name__}: {e}",
            )

    async def _run_actor(self) -> None:
        result: McpResult
        try:
            async with create_mcp_server_session(
                self.server_params, sampling_callback=self._sampling_callback
            ) as session:
                # Save the initialize result
                self._initialize_result = await session.initialize()
                while True:
                    cmd = await self._command_queue.get()
                    if cmd["type"] == "shutdown":
                        cmd["future"].set_result("ok")
                        break
                    elif cmd["type"] == "call_tool":
                        try:
                            result = session.call_tool(name=cmd["name"], arguments=cmd["args"])
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
                    elif cmd["type"] == "read_resource":
                        try:
                            result = session.read_resource(uri=cmd["uri"])
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
                    elif cmd["type"] == "get_prompt":
                        try:
                            result = session.get_prompt(name=cmd["name"], arguments=cmd["args"])
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
                    elif cmd["type"] == "list_tools":
                        try:
                            result = session.list_tools()
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
                    elif cmd["type"] == "list_prompts":
                        try:
                            result = session.list_prompts()
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
                    elif cmd["type"] == "list_resources":
                        try:
                            result = session.list_resources()
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
                    elif cmd["type"] == "list_resource_templates":
                        try:
                            result = session.list_resource_templates()
                            cmd["future"].set_result(result)
                        except Exception as e:
                            cmd["future"].set_exception(e)
        except Exception as e:
            if self._shutdown_future and not self._shutdown_future.done():
                self._shutdown_future.set_exception(e)
            else:
                logger.exception("Exception in MCP actor task")
        finally:
            self._active = False
            self._actor_task = None

    def _sync_shutdown(self) -> None:
        if not self._active or self._actor_task is None:
            return
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            # No loop available — interpreter is likely shutting down
            return

        if loop.is_closed():
            return

        if loop.is_running():
            loop.create_task(self.close())
        else:
            loop.run_until_complete(self.close())

    def _to_config(self) -> McpSessionActorConfig:
        """
        Convert the adapter to its configuration representation.

        Returns:
            McpSessionConfig: The configuration of the adapter.
        """
        return McpSessionActorConfig(server_params=self.server_params)

    @classmethod
    def _from_config(cls, config: McpSessionActorConfig) -> Self:
        """
        Create an instance of McpSessionActor from its configuration.

        Args:
            config (McpSessionConfig): The configuration of the adapter.

        Returns:
            McpSessionActor: An instance of SseMcpToolAdapter.
        """
        return cls(server_params=config.server_params)


# From mcp/_stdio.py
class StdioMcpToolAdapterConfig(BaseModel):
    """Configuration for the MCP tool adapter."""

    server_params: StdioServerParams
    tool: Tool

# From mcp/_stdio.py
class StdioMcpToolAdapter(
    McpToolAdapter[StdioServerParams],
    Component[StdioMcpToolAdapterConfig],
):
    """Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.

    This adapter enables using MCP-compatible tools that communicate over standard input/output
    with AutoGen agents. Common use cases include wrapping command-line tools and local services
    that implement the Model Context Protocol (MCP).

    .. note::

        To use this class, you need to install `mcp` extra for the `autogen-ext` package.

        .. code-block:: bash

            pip install -U "autogen-ext[mcp]"


    Args:
        server_params (StdioServerParams): Parameters for the MCP server connection,
            including command to run and its arguments
        tool (Tool): The MCP tool to wrap
        session (ClientSession, optional): The MCP client session to use. If not provided,
            a new session will be created. This is useful for testing or when you want to
            manage the session lifecycle yourself.

    See :func:`~autogen_ext.tools.mcp.mcp_server_tools` for examples.
    """

    component_config_schema = StdioMcpToolAdapterConfig
    component_provider_override = "autogen_ext.tools.mcp.StdioMcpToolAdapter"

    def __init__(self, server_params: StdioServerParams, tool: Tool, session: ClientSession | None = None) -> None:
        super().__init__(server_params=server_params, tool=tool, session=session)

    def _to_config(self) -> StdioMcpToolAdapterConfig:
        """
        Convert the adapter to its configuration representation.

        Returns:
            StdioMcpToolAdapterConfig: The configuration of the adapter.
        """
        return StdioMcpToolAdapterConfig(server_params=self._server_params, tool=self._tool)

    @classmethod
    def _from_config(cls, config: StdioMcpToolAdapterConfig) -> Self:
        """
        Create an instance of StdioMcpToolAdapter from its configuration.

        Args:
            config (StdioMcpToolAdapterConfig): The configuration of the adapter.

        Returns:
            StdioMcpToolAdapter: An instance of StdioMcpToolAdapter.
        """
        return cls(server_params=config.server_params, tool=config.tool)

from mcp.client.session import SamplingFnT

from json_schema_to_pydantic import create_model

# From http/_http_tool.py
class HttpToolConfig(BaseModel):
    name: str
    """
    The name of the tool.
    """
    description: Optional[str]
    """
    A description of the tool.
    """
    scheme: Literal["http", "https"] = "http"
    """
    The scheme to use for the request.
    """
    host: str
    """
    The URL to send the request to.
    """
    port: int
    """
    The port to send the request to.
    """
    path: str = Field(default="/")
    """
    The path to send the request to. defaults to "/"
    The path can accept parameters, e.g. "/{param1}/{param2}".
    These parameters will be templated from the inputs args, any additional parameters will be added as query parameters or the body of the request.
    """
    method: Optional[Literal["GET", "POST", "PUT", "DELETE", "PATCH"]] = "POST"
    """
    The HTTP method to use, will default to POST if not provided.
    """
    headers: Optional[dict[str, Any]]
    """
    A dictionary of headers to send with the request.
    """
    json_schema: dict[str, Any]
    """
    A JSON Schema object defining the expected parameters for the tool.
    Path parameters MUST also be included in the json_schema. They must also MUST be set to string
    """
    return_type: Optional[Literal["text", "json"]] = "text"
    """
    The type of response to return from the tool.
    """
    timeout: float = DEFAULT_TIMEOUT_CONFIG
    """
    The timeout for the tool request in seconds.
    """

# From http/_http_tool.py
class HttpTool(BaseTool[BaseModel, Any], Component[HttpToolConfig]):
    """A wrapper for using an HTTP server as a tool.

    Args:
        name (str): The name of the tool.
        description (str, optional): A description of the tool.
        scheme (str): The scheme to use for the request. Must be either "http" or "https".
        host (str): The host to send the request to.
        port (int): The port to send the request to.
        path (str, optional): The path to send the request to. Defaults to "/".
            Can include path parameters like "/{param1}/{param2}" which will be templated from input args.
        method (str, optional): The HTTP method to use, will default to POST if not provided.
            Must be one of "GET", "POST", "PUT", "DELETE", "PATCH".
        headers (dict[str, Any], optional): A dictionary of headers to send with the request.
        json_schema (dict[str, Any]): A JSON Schema object defining the expected parameters for the tool.
            Path parameters must also be included in the schema and must be strings.
        return_type (Literal["text", "json"], optional): The type of response to return from the tool.
            Defaults to "text".
        timeout (float, optional): The timeout for HTTP requests in seconds.
            Defaults to 5.0.

    .. note::
        This tool requires the :code:`http-tool` extra for the :code:`autogen-ext` package.

        To install:

        .. code-block:: bash

            pip install -U "autogen-agentchat" "autogen-ext[http-tool]"

    Example:
        Simple use case::

          import asyncio

          from autogen_agentchat.agents import AssistantAgent
          from autogen_agentchat.messages import TextMessage
          from autogen_core import CancellationToken
          from autogen_ext.models.openai import OpenAIChatCompletionClient
          from autogen_ext.tools.http import HttpTool

          # Define a JSON schema for a base64 decode tool
          base64_schema = {
              "type": "object",
              "properties": {
                  "value": {"type": "string", "description": "The base64 value to decode"},
              },
              "required": ["value"],
          }

          # Create an HTTP tool for the httpbin API
          base64_tool = HttpTool(
              name="base64_decode",
              description="base64 decode a value",
              scheme="https",
              host="httpbin.org",
              port=443,
              path="/base64/{value}",
              method="GET",
              json_schema=base64_schema,
          )


          async def main():
              # Create an assistant with the base64 tool
              model = OpenAIChatCompletionClient(model="gpt-4")
              assistant = AssistantAgent("base64_assistant", model_client=model, tools=[base64_tool])

              # The assistant can now use the base64 tool to decode the string
              response = await assistant.on_messages(
                  [TextMessage(content="Can you base64 decode the value 'YWJjZGU=', please?", source="user")],
                  CancellationToken(),
              )
              print(response.chat_message)


          asyncio.run(main())
    """

    component_type = "tool"
    component_provider_override = "autogen_ext.tools.http.HttpTool"
    component_config_schema = HttpToolConfig

    def __init__(
        self,
        name: str,
        host: str,
        port: int,
        json_schema: dict[str, Any],
        headers: Optional[dict[str, Any]] = None,
        description: str = "HTTP tool",
        path: str = "/",
        scheme: Literal["http", "https"] = "http",
        method: Literal["GET", "POST", "PUT", "DELETE", "PATCH"] = "POST",
        return_type: Literal["text", "json"] = "text",
        timeout: float = DEFAULT_TIMEOUT_CONFIG,
    ) -> None:
        self.server_params = HttpToolConfig(
            name=name,
            description=description,
            host=host,
            port=port,
            path=path,
            scheme=scheme,
            method=method,
            headers=headers,
            json_schema=json_schema,
            return_type=return_type,
            timeout=timeout,
        )

        # Use regex to find all path parameters, we will need those later to template the path
        path_params = {match.group(1) for match in re.finditer(r"{([^}]*)}", path)}
        self._path_params = path_params

        # Create the input model from the modified schema
        input_model = create_model(json_schema)

        # Use Any as return type since HTTP responses can vary
        base_return_type: Type[Any] = object

        super().__init__(input_model, base_return_type, name, description)

    def _to_config(self) -> HttpToolConfig:
        copied_config = self.server_params.model_copy()
        return copied_config

    @classmethod
    def _from_config(cls, config: HttpToolConfig) -> Self:
        copied_config = config.model_copy().model_dump()
        return cls(**copied_config)

    async def run(self, args: BaseModel, cancellation_token: CancellationToken) -> Any:
        """Execute the HTTP tool with the given arguments.

        Args:
            args: The validated input arguments
            cancellation_token: Token for cancelling the operation

        Returns:
            The response body from the HTTP call in JSON format

        Raises:
            Exception: If tool execution fails
        """

        model_dump = args.model_dump()
        path_params = {k: v for k, v in model_dump.items() if k in self._path_params}
        # Remove path params from the model dump
        for k in self._path_params:
            model_dump.pop(k)

        path = self.server_params.path.format(**path_params)

        url = httpx.URL(
            scheme=self.server_params.scheme,
            host=self.server_params.host,
            port=self.server_params.port,
            path=path,
        )
        timeout_config = httpx.Timeout(timeout=self.server_params.timeout)
        async with httpx.AsyncClient(timeout=timeout_config) as client:
            match self.server_params.method:
                case "GET":
                    response = await client.get(url, headers=self.server_params.headers, params=model_dump)
                case "PUT":
                    response = await client.put(url, headers=self.server_params.headers, json=model_dump)
                case "DELETE":
                    response = await client.delete(url, headers=self.server_params.headers, params=model_dump)
                case "PATCH":
                    response = await client.patch(url, headers=self.server_params.headers, json=model_dump)
                case _:  # Default case POST
                    response = await client.post(url, headers=self.server_params.headers, json=model_dump)

        match self.server_params.return_type:
            case "text":
                return response.text
            case "json":
                return response.json()
            case _:
                raise ValueError(f"Invalid return type: {self.server_params.return_type}")

from azure.core.credentials import AzureKeyCredential
from azure.core.credentials_async import AsyncTokenCredential

# From azure/_config.py
class AzureAISearchConfig(BaseModel):
    """Configuration for Azure AI Search with validation.

    This class defines the configuration parameters for Azure AI Search tools, including
    authentication, search behavior, caching, and embedding settings.

    .. note::
        This class requires the ``azure`` extra for the ``autogen-ext`` package.

        .. code-block:: bash

            pip install -U "autogen-ext[azure]"

    .. note::
        **Prerequisites:**

        1. An Azure AI Search service must be created in your Azure subscription.
        2. The search index must be properly configured for your use case:

           - For vector search: Index must have vector fields
           - For semantic search: Index must have semantic configuration
           - For hybrid search: Both vector fields and text fields must be configured
        3. Required packages:

           - Base functionality: ``azure-search-documents>=11.4.0``
           - For Azure OpenAI embeddings: ``openai azure-identity``
           - For OpenAI embeddings: ``openai``

    Example Usage:
        .. code-block:: python

            from azure.core.credentials import AzureKeyCredential
            from autogen_ext.tools.azure import AzureAISearchConfig

            # Basic configuration for full-text search
            config = AzureAISearchConfig(
                name="doc-search",
                endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                index_name="<your-index>",  # Name of your search index
                credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                query_type="simple",
                search_fields=["content", "title"],  # Update with your searchable fields
                top=5,
            )

            # Configuration for vector search with Azure OpenAI embeddings
            vector_config = AzureAISearchConfig(
                name="vector-search",
                endpoint="https://your-search.search.windows.net",
                index_name="<your-index>",
                credential=AzureKeyCredential("<your-key>"),
                query_type="vector",
                vector_fields=["embedding"],  # Update with your vector field name
                embedding_provider="azure_openai",
                embedding_model="text-embedding-ada-002",
                openai_endpoint="https://your-openai.openai.azure.com",  # Your Azure OpenAI endpoint
                openai_api_key="<your-openai-key>",  # Your Azure OpenAI key
                top=5,
            )

            # Configuration for hybrid search with semantic ranking
            hybrid_config = AzureAISearchConfig(
                name="hybrid-search",
                endpoint="https://your-search.search.windows.net",
                index_name="<your-index>",
                credential=AzureKeyCredential("<your-key>"),
                query_type="semantic",
                semantic_config_name="<your-semantic-config>",  # Name of your semantic configuration
                search_fields=["content", "title"],  # Update with your search fields
                vector_fields=["embedding"],  # Update with your vector field name
                embedding_provider="openai",
                embedding_model="text-embedding-ada-002",
                openai_api_key="<your-openai-key>",  # Your OpenAI API key
                top=5,
            )
    """

    name: str = Field(description="The name of this tool instance")
    description: Optional[str] = Field(default=None, description="Description explaining the tool's purpose")
    endpoint: str = Field(description="The full URL of your Azure AI Search service")
    index_name: str = Field(description="Name of the search index to query")
    credential: Union[AzureKeyCredential, AsyncTokenCredential] = Field(
        description="Azure credential for authentication (API key or token)"
    )
    api_version: str = Field(
        default=DEFAULT_API_VERSION,
        description=f"Azure AI Search API version to use. Defaults to {DEFAULT_API_VERSION}.",
    )
    query_type: QueryTypeLiteral = Field(
        default="simple", description="Type of search to perform: simple, full, semantic, or vector"
    )
    search_fields: Optional[List[str]] = Field(default=None, description="Fields to search within documents")
    select_fields: Optional[List[str]] = Field(default=None, description="Fields to return in search results")
    vector_fields: Optional[List[str]] = Field(default=None, description="Fields to use for vector search")
    top: Optional[int] = Field(
        default=None, description="Maximum number of results to return. For vector searches, acts as k in k-NN."
    )
    filter: Optional[str] = Field(default=None, description="OData filter expression to refine search results")
    semantic_config_name: Optional[str] = Field(
        default=None, description="Semantic configuration name for enhanced results"
    )

    enable_caching: bool = Field(default=False, description="Whether to cache search results")
    cache_ttl_seconds: int = Field(default=300, description="How long to cache results in seconds")

    embedding_provider: Optional[str] = Field(
        default=None, description="Name of embedding provider for client-side embeddings"
    )
    embedding_model: Optional[str] = Field(default=None, description="Model name for client-side embeddings")
    openai_api_key: Optional[str] = Field(default=None, description="API key for OpenAI/Azure OpenAI embeddings")
    openai_api_version: Optional[str] = Field(default=None, description="API version for Azure OpenAI embeddings")
    openai_endpoint: Optional[str] = Field(default=None, description="Endpoint URL for Azure OpenAI embeddings")

    model_config = {"arbitrary_types_allowed": True}

    @field_validator("endpoint")
    def validate_endpoint(cls, v: str) -> str:
        """Validate that the endpoint is a valid URL."""
        if not v.startswith(("http://", "https://")):
            raise ValueError("endpoint must be a valid URL starting with http:// or https://")
        return v

    @field_validator("query_type")
    def normalize_query_type(cls, v: QueryTypeLiteral) -> QueryTypeLiteral:
        """Normalize query type to standard values."""
        if not v:
            return "simple"

        if isinstance(v, str) and v.lower() == "fulltext":
            return "full"

        return v

    @field_validator("top")
    def validate_top(cls, v: Optional[int]) -> Optional[int]:
        """Ensure top is a positive integer if provided."""
        if v is not None and v <= 0:
            raise ValueError("top must be a positive integer")
        return v

    @model_validator(mode="after")
    def validate_interdependent_fields(self) -> "AzureAISearchConfig":
        """Validate interdependent fields after all fields have been parsed."""
        if self.query_type == "semantic" and not self.semantic_config_name:
            raise ValueError("semantic_config_name must be provided when query_type is 'semantic'")

        if self.query_type == "vector" and not self.vector_fields:
            raise ValueError("vector_fields must be provided for vector search")

        if (
            self.embedding_provider
            and self.embedding_provider.lower() == "azure_openai"
            and self.embedding_model
            and not self.openai_endpoint
        ):
            raise ValueError("openai_endpoint must be provided for azure_openai embedding provider")

        return self

# From azure/_config.py
def validate_endpoint(cls, v: str) -> str:
        """Validate that the endpoint is a valid URL."""
        if not v.startswith(("http://", "https://")):
            raise ValueError("endpoint must be a valid URL starting with http:// or https://")
        return v

# From azure/_config.py
def normalize_query_type(cls, v: QueryTypeLiteral) -> QueryTypeLiteral:
        """Normalize query type to standard values."""
        if not v:
            return "simple"

        if isinstance(v, str) and v.lower() == "fulltext":
            return "full"

        return v

# From azure/_config.py
def validate_top(cls, v: Optional[int]) -> Optional[int]:
        """Ensure top is a positive integer if provided."""
        if v is not None and v <= 0:
            raise ValueError("top must be a positive integer")
        return v

# From azure/_config.py
def validate_interdependent_fields(self) -> "AzureAISearchConfig":
        """Validate interdependent fields after all fields have been parsed."""
        if self.query_type == "semantic" and not self.semantic_config_name:
            raise ValueError("semantic_config_name must be provided when query_type is 'semantic'")

        if self.query_type == "vector" and not self.vector_fields:
            raise ValueError("vector_fields must be provided for vector search")

        if (
            self.embedding_provider
            and self.embedding_provider.lower() == "azure_openai"
            and self.embedding_model
            and not self.openai_endpoint
        ):
            raise ValueError("openai_endpoint must be provided for azure_openai embedding provider")

        return self

from typing import TYPE_CHECKING
from azure.core.exceptions import HttpResponseError
from azure.core.exceptions import ResourceNotFoundError
from azure.search.documents.aio import SearchClient
from _config import DEFAULT_API_VERSION
from _config import AzureAISearchConfig
from azure.search.documents.aio import AsyncSearchItemPaged
from azure.search.documents.models import VectorizableTextQuery
from azure.search.documents.models import VectorizedQuery
from azure.search.documents.models import VectorQuery
from openai import AsyncAzureOpenAI
from openai import AsyncOpenAI

# From azure/_ai_search.py
class SearchQuery(BaseModel):
    """Search query parameters.

    This simplified interface only requires a search query string.
    All other parameters (top, filters, vector fields, etc.) are specified during tool creation
    rather than at query time, making it easier for language models to generate structured output.

    Args:
        query (str): The search query text.
    """

    query: str = Field(description="Search query text")

# From azure/_ai_search.py
class SearchResult(BaseModel):
    """Search result.

    Args:
        score (float): The search score.
        content (ContentDict): The document content.
        metadata (MetadataDict): Additional metadata about the document.
    """

    score: float = Field(description="The search score")
    content: ContentDict = Field(description="The document content")
    metadata: MetadataDict = Field(description="Additional metadata about the document")

# From azure/_ai_search.py
class SearchResults(BaseModel):
    """Container for search results.

    Args:
        results (List[SearchResult]): List of search results.
    """

    results: List[SearchResult] = Field(description="List of search results")

# From azure/_ai_search.py
class EmbeddingProvider(Protocol):
    """Protocol defining the interface for embedding generation."""

    async def _get_embedding(self, query: str) -> List[float]:
        """Generate embedding vector for the query text."""
        ...

# From azure/_ai_search.py
class EmbeddingProviderMixin:
    """Mixin class providing embedding generation functionality."""

    search_config: AzureAISearchConfig

    async def _get_embedding(self, query: str) -> List[float]:
        """Generate embedding vector for the query text."""
        if not hasattr(self, "search_config"):
            raise ValueError("Host class must have a search_config attribute")

        search_config = self.search_config
        embedding_provider = getattr(search_config, "embedding_provider", None)
        embedding_model = getattr(search_config, "embedding_model", None)

        if not embedding_provider or not embedding_model:
            raise ValueError(
                "Client-side embedding is not configured. `embedding_provider` and `embedding_model` must be set."
            ) from None

        if embedding_provider.lower() == "azure_openai":
            try:
                from openai import AsyncAzureOpenAI

                from azure.identity import DefaultAzureCredential
            except ImportError:
                raise ImportError(
                    "Azure OpenAI SDK is required for client-side embedding generation. "
                    "Please install it with: uv add openai azure-identity"
                ) from None

            api_key = getattr(search_config, "openai_api_key", None)
            api_version = getattr(search_config, "openai_api_version", "2023-11-01")
            endpoint = getattr(search_config, "openai_endpoint", None)

            if not endpoint:
                raise ValueError(
                    "Azure OpenAI endpoint (`openai_endpoint`) must be provided for client-side Azure OpenAI embeddings."
                ) from None

            if api_key:
                azure_client = AsyncAzureOpenAI(api_key=api_key, api_version=api_version, azure_endpoint=endpoint)
            else:

                def get_token() -> str:
                    credential = DefaultAzureCredential()
                    token = credential.get_token("https://cognitiveservices.azure.com/.default")
                    if not token or not token.token:
                        raise ValueError("Failed to acquire token using DefaultAzureCredential for Azure OpenAI.")
                    return token.token

                azure_client = AsyncAzureOpenAI(
                    azure_ad_token_provider=get_token, api_version=api_version, azure_endpoint=endpoint
                )

            try:
                response = await azure_client.embeddings.create(model=embedding_model, input=query)
                return response.data[0].embedding
            except Exception as e:
                raise ValueError(f"Failed to generate embeddings with Azure OpenAI: {str(e)}") from e

        elif embedding_provider.lower() == "openai":
            try:
                from openai import AsyncOpenAI
            except ImportError:
                raise ImportError(
                    "OpenAI SDK is required for client-side embedding generation. "
                    "Please install it with: uv add openai"
                ) from None

            api_key = getattr(search_config, "openai_api_key", None)
            openai_client = AsyncOpenAI(api_key=api_key)

            try:
                response = await openai_client.embeddings.create(model=embedding_model, input=query)
                return response.data[0].embedding
            except Exception as e:
                raise ValueError(f"Failed to generate embeddings with OpenAI: {str(e)}") from e
        else:
            raise ValueError(
                f"Unsupported client-side embedding provider: {embedding_provider}. "
                "Currently supported providers are 'azure_openai' and 'openai'."
            )

# From azure/_ai_search.py
class BaseAzureAISearchTool(
    BaseTool[SearchQuery, SearchResults], Component[AzureAISearchConfig], EmbeddingProvider, ABC
):
    """Abstract base class for Azure AI Search tools.

    This class defines the common interface and functionality for all Azure AI Search tools.
    It handles configuration management, client initialization, and the abstract methods
    that subclasses must implement.

    Attributes:
        search_config: Configuration parameters for the search service.

    Note:
        This is an abstract base class and should not be instantiated directly.
        Use concrete implementations or the factory methods in AzureAISearchTool.
    """

    component_config_schema = AzureAISearchConfig
    component_provider_override = "autogen_ext.tools.azure.BaseAzureAISearchTool"

    def __init__(
        self,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        description: Optional[str] = None,
        api_version: str = DEFAULT_API_VERSION,
        query_type: Literal["simple", "full", "semantic", "vector"] = "simple",
        search_fields: Optional[List[str]] = None,
        select_fields: Optional[List[str]] = None,
        vector_fields: Optional[List[str]] = None,
        top: Optional[int] = None,
        filter: Optional[str] = None,
        semantic_config_name: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
        embedding_provider: Optional[str] = None,
        embedding_model: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        openai_api_version: Optional[str] = None,
        openai_endpoint: Optional[str] = None,
    ):
        """Initialize the Azure AI Search tool.

        Args:
            name (str): The name of this tool instance
            endpoint (str): The full URL of your Azure AI Search service
            index_name (str): Name of the search index to query
            credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]): Azure credential for authentication
            description (Optional[str]): Optional description explaining the tool's purpose
            api_version (Optional[str]): Azure AI Search API version to use
            query_type (Literal["simple", "full", "semantic", "vector"]): Type of search to perform
            search_fields (Optional[List[str]]): Fields to search within documents
            select_fields (Optional[List[str]]): Fields to return in search results
            vector_fields (Optional[List[str]]): Fields to use for vector search
            top (Optional[int]): Maximum number of results to return
            filter (Optional[str]): OData filter expression to refine search results
            semantic_config_name (Optional[str]): Semantic configuration name for enhanced results
            enable_caching (bool): Whether to cache search results
            cache_ttl_seconds (int): How long to cache results in seconds
            embedding_provider (Optional[str]): Name of embedding provider for client-side embeddings
            embedding_model (Optional[str]): Model name for client-side embeddings
            openai_api_key (Optional[str]): API key for OpenAI/Azure OpenAI embeddings
            openai_api_version (Optional[str]): API version for Azure OpenAI embeddings
            openai_endpoint (Optional[str]): Endpoint URL for Azure OpenAI embeddings
        """
        if not has_azure_search:
            raise ImportError(
                "Azure Search SDK is required but not installed. "
                "Please install it with: pip install azure-search-documents>=11.4.0"
            )

        if description is None:
            description = (
                f"Search for information in the {index_name} index using Azure AI Search. "
                f"Supports full-text search with optional filters and semantic capabilities."
            )

        super().__init__(
            args_type=SearchQuery,
            return_type=SearchResults,
            name=name,
            description=description,
        )

        processed_credential = self._process_credential(credential)

        self.search_config: AzureAISearchConfig = AzureAISearchConfig(
            name=name,
            description=description,
            endpoint=endpoint,
            index_name=index_name,
            credential=processed_credential,
            api_version=api_version,
            query_type=query_type,
            search_fields=search_fields,
            select_fields=select_fields,
            vector_fields=vector_fields,
            top=top,
            filter=filter,
            semantic_config_name=semantic_config_name,
            enable_caching=enable_caching,
            cache_ttl_seconds=cache_ttl_seconds,
            embedding_provider=embedding_provider,
            embedding_model=embedding_model,
            openai_api_key=openai_api_key,
            openai_api_version=openai_api_version,
            openai_endpoint=openai_endpoint,
        )

        self._endpoint = endpoint
        self._index_name = index_name
        self._credential = processed_credential
        self._api_version = api_version

        self._client: Optional[SearchClient] = None
        self._cache: Dict[str, Dict[str, Any]] = {}

        if self.search_config.api_version == "2023-11-01" and self.search_config.vector_fields:
            warning_message = (
                f"When explicitly setting api_version='{self.search_config.api_version}' for vector search: "
                f"If client-side embedding is NOT configured (e.g., `embedding_model` is not set), "
                f"this tool defaults to service-side vectorization (VectorizableTextQuery), which may fail or have limitations with this API version. "
                f"If client-side embedding IS configured, the tool will use VectorizedQuery, which is generally compatible. "
                f"For robust vector search, consider omitting api_version (recommended to use SDK default) or use a newer API version."
            )
            logger.warning(warning_message)

    async def close(self) -> None:
        """Explicitly close the Azure SearchClient if needed (for cleanup)."""
        if self._client is not None:
            try:
                await self._client.close()
            except Exception:
                pass
            finally:
                self._client = None

    def _process_credential(
        self, credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]]
    ) -> Union[AzureKeyCredential, AsyncTokenCredential]:
        """Process credential to ensure it's the correct type for async SearchClient.

        Converts dictionary credentials with 'api_key' to AzureKeyCredential objects.

        Args:
            credential: The credential in either object or dictionary form

        Returns:
            A properly formatted credential object

        Raises:
            ValueError: If the credential dictionary doesn't contain an 'api_key'
            TypeError: If the credential is not of a supported type
        """
        if isinstance(credential, dict):
            if "api_key" in credential:
                return AzureKeyCredential(credential["api_key"])
            raise ValueError("If credential is a dict, it must contain an 'api_key' key")

        if isinstance(credential, (AzureKeyCredential, AsyncTokenCredential)):
            return credential

        raise TypeError("Credential must be AzureKeyCredential, AsyncTokenCredential, or a valid dict")

    async def _get_client(self) -> SearchClient:
        """Get the search client for the configured index.

        Returns:
            SearchClient: Initialized search client

        Raises:
            ValueError: If index doesn't exist or authentication fails
        """
        if self._client is not None:
            return self._client

        try:
            self._client = SearchClient(
                endpoint=self.search_config.endpoint,
                index_name=self.search_config.index_name,
                credential=self.search_config.credential,
                api_version=self.search_config.api_version,
            )
            return self._client
        except ResourceNotFoundError as e:
            raise ValueError(f"Index '{self.search_config.index_name}' not found in Azure AI Search service.") from e
        except HttpResponseError as e:
            if e.status_code == 401:
                raise ValueError("Authentication failed. Please check your credentials.") from e
            elif e.status_code == 403:
                raise ValueError("Permission denied to access this index.") from e
            else:
                raise ValueError(f"Error connecting to Azure AI Search: {str(e)}") from e
        except Exception as e:
            raise ValueError(f"Unexpected error initializing search client: {str(e)}") from e

    async def run(
        self, args: Union[str, Dict[str, Any], SearchQuery], cancellation_token: Optional[CancellationToken] = None
    ) -> SearchResults:
        """Execute a search against the Azure AI Search index.

        Args:
            args: Search query text or SearchQuery object
            cancellation_token: Optional token to cancel the operation

        Returns:
            SearchResults: Container with search results and metadata

        Raises:
            ValueError: If the search query is empty or invalid
            ValueError: If there is an authentication error or other search issue
            asyncio.CancelledError: If the operation is cancelled
        """
        if isinstance(args, str):
            if not args.strip():
                raise ValueError("Search query cannot be empty")
            search_query = SearchQuery(query=args)
        elif isinstance(args, dict) and "query" in args:
            search_query = SearchQuery(query=args["query"])
        elif isinstance(args, SearchQuery):
            search_query = args
        else:
            raise ValueError("Invalid search query format. Expected string, dict with 'query', or SearchQuery")

        if cancellation_token is not None and cancellation_token.is_cancelled():
            raise asyncio.CancelledError("Operation cancelled")

        cache_key = ""
        if self.search_config.enable_caching:
            cache_key_parts = [
                search_query.query,
                str(self.search_config.top),
                self.search_config.query_type,
                ",".join(sorted(self.search_config.search_fields or [])),
                ",".join(sorted(self.search_config.select_fields or [])),
                ",".join(sorted(self.search_config.vector_fields or [])),
                str(self.search_config.filter or ""),
                str(self.search_config.semantic_config_name or ""),
            ]
            cache_key = ":".join(filter(None, cache_key_parts))
            if cache_key in self._cache:
                cache_entry = self._cache[cache_key]
                cache_age = time.time() - cache_entry["timestamp"]
                if cache_age < self.search_config.cache_ttl_seconds:
                    logger.debug(f"Using cached results for query: {search_query.query}")
                    return SearchResults(
                        results=[
                            SearchResult(score=r.score, content=r.content, metadata=r.metadata)
                            for r in cache_entry["results"]
                        ]
                    )

        try:
            search_kwargs: Dict[str, Any] = {}

            if self.search_config.query_type != "vector":
                search_kwargs["search_text"] = search_query.query
                search_kwargs["query_type"] = self.search_config.query_type

                if self.search_config.search_fields:
                    search_kwargs["search_fields"] = self.search_config.search_fields  # type: ignore[assignment]

                if self.search_config.query_type == "semantic" and self.search_config.semantic_config_name:
                    search_kwargs["semantic_configuration_name"] = self.search_config.semantic_config_name

            if self.search_config.select_fields:
                search_kwargs["select"] = self.search_config.select_fields  # type: ignore[assignment]
            if self.search_config.filter:
                search_kwargs["filter"] = str(self.search_config.filter)
            if self.search_config.top is not None:
                search_kwargs["top"] = self.search_config.top  # type: ignore[assignment]

            if self.search_config.vector_fields and len(self.search_config.vector_fields) > 0:
                if not search_query.query:
                    raise ValueError("Query text cannot be empty for vector search operations")

                use_client_side_embeddings = bool(
                    self.search_config.embedding_model and self.search_config.embedding_provider
                )

                vector_queries: List[Union[VectorizedQuery, VectorizableTextQuery]] = []
                if use_client_side_embeddings:
                    from azure.search.documents.models import VectorizedQuery

                    embedding_vector: List[float] = await self._get_embedding(search_query.query)
                    for field_spec in self.search_config.vector_fields:
                        fields = field_spec if isinstance(field_spec, str) else ",".join(field_spec)
                        vector_queries.append(
                            VectorizedQuery(
                                vector=embedding_vector,
                                k_nearest_neighbors=self.search_config.top or 5,
                                fields=fields,
                                kind="vector",
                            )
                        )
                else:
                    from azure.search.documents.models import VectorizableTextQuery

                    for field in self.search_config.vector_fields:
                        fields = field if isinstance(field, str) else ",".join(field)
                        vector_queries.append(
                            VectorizableTextQuery(  # type: ignore
                                text=search_query.query,
                                k_nearest_neighbors=self.search_config.top or 5,
                                fields=fields,
                                kind="vectorizable",
                            )
                        )

                search_kwargs["vector_queries"] = vector_queries  # type: ignore[assignment]

            if cancellation_token is not None:
                dummy_task = asyncio.create_task(asyncio.sleep(60))
                cancellation_token.link_future(dummy_task)

                def is_cancelled() -> bool:
                    return cancellation_token.is_cancelled()
            else:

                def is_cancelled() -> bool:
                    return False

            client = await self._get_client()
            search_results: SearchResultsIterable = await client.search(**search_kwargs)  # type: ignore[arg-type]

            results: List[SearchResult] = []
            async for doc in search_results:
                if is_cancelled():
                    raise asyncio.CancelledError("Operation was cancelled")

                try:
                    metadata: Dict[str, Any] = {}
                    content: Dict[str, Any] = {}

                    for key, value in doc.items():
                        if isinstance(key, str) and key.startswith(("@", "_")):
                            metadata[key] = value
                        else:
                            content[str(key)] = value

                    score = float(metadata.get("@search.score", 0.0))
                    results.append(SearchResult(score=score, content=content, metadata=metadata))
                except Exception as e:
                    logger.warning(f"Error processing search document: {e}")
                    continue

            if self.search_config.enable_caching:
                self._cache[cache_key] = {"results": results, "timestamp": time.time()}

            return SearchResults(results=results)

        except asyncio.CancelledError:
            raise
        except Exception as e:
            error_msg = str(e)
            if isinstance(e, HttpResponseError):
                if hasattr(e, "message") and e.message:
                    error_msg = e.message

            if "not found" in error_msg.lower():
                raise ValueError(f"Index '{self.search_config.index_name}' not found.") from e
            elif "unauthorized" in error_msg.lower() or "401" in error_msg:
                raise ValueError(f"Authentication failed: {error_msg}") from e
            else:
                raise ValueError(f"Error from Azure AI Search: {error_msg}") from e

    def _to_config(self) -> AzureAISearchConfig:
        """Convert the current instance to a configuration object."""
        return self.search_config

    @property
    def schema(self) -> ToolSchema:
        """Return the schema for the tool."""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {"query": {"type": "string", "description": "Search query text"}},
                "required": ["query"],
                "additionalProperties": False,
            },
            "strict": True,
        }

    def return_value_as_string(self, value: SearchResults) -> str:
        """Convert the search results to a string representation."""
        if not value.results:
            return "No results found."

        result_strings: List[str] = []
        for i, result in enumerate(value.results, 1):
            content_items = [f"{k}: {str(v) if v is not None else 'None'}" for k, v in result.content.items()]
            content_str = ", ".join(content_items)
            result_strings.append(f"Result {i} (Score: {result.score:.2f}): {content_str}")

        return "\n".join(result_strings)

    @classmethod
    def _validate_config(
        cls, config_dict: Dict[str, Any], search_type: Literal["full_text", "vector", "hybrid"]
    ) -> None:
        """Validate configuration for specific search types."""
        credential = config_dict.get("credential")
        if isinstance(credential, str):
            raise TypeError("Credential must be AzureKeyCredential, AsyncTokenCredential, or a valid dict")
        if isinstance(credential, dict) and "api_key" not in credential:
            raise ValueError("If credential is a dict, it must contain an 'api_key' key")

        try:
            _ = AzureAISearchConfig(**config_dict)
        except Exception as e:
            raise ValueError(f"Invalid configuration: {str(e)}") from e

        if search_type == "vector":
            vector_fields = config_dict.get("vector_fields")
            if not vector_fields or len(vector_fields) == 0:
                raise ValueError("vector_fields must contain at least one field name for vector search")

        elif search_type == "hybrid":
            vector_fields = config_dict.get("vector_fields")
            search_fields = config_dict.get("search_fields")

            if not vector_fields or len(vector_fields) == 0:
                raise ValueError("vector_fields must contain at least one field name for hybrid search")

            if not search_fields or len(search_fields) == 0:
                raise ValueError("search_fields must contain at least one field name for hybrid search")

    @classmethod
    @abstractmethod
    def _from_config(cls, config: AzureAISearchConfig) -> "BaseAzureAISearchTool":
        """Create a tool instance from a configuration object.

        This is an abstract method that must be implemented by subclasses.
        """
        if cls is BaseAzureAISearchTool:
            raise NotImplementedError(
                "BaseAzureAISearchTool is an abstract base class and cannot be instantiated directly. "
                "Use a concrete implementation like AzureAISearchTool."
            )
        raise NotImplementedError("Subclasses must implement _from_config")

    @abstractmethod
    async def _get_embedding(self, query: str) -> List[float]:
        """Generate embedding vector for the query text."""
        raise NotImplementedError("Subclasses must implement _get_embedding")

# From azure/_ai_search.py
class AzureAISearchTool(EmbeddingProviderMixin, BaseAzureAISearchTool):
    """Azure AI Search tool for querying Azure search indexes.

    This tool provides a simplified interface for querying Azure AI Search indexes using
    various search methods. It's recommended to use the factory methods to create
    instances tailored for specific search types:

    1.  **Full-Text Search**: For traditional keyword-based searches, Lucene queries, or
        semantically re-ranked results.
        - Use `AzureAISearchTool.create_full_text_search()`
        - Supports `query_type`: "simple" (keyword), "full" (Lucene), "semantic".

    2.  **Vector Search**: For pure similarity searches based on vector embeddings.
        - Use `AzureAISearchTool.create_vector_search()`

    3.  **Hybrid Search**: For combining vector search with full-text or semantic search
        to get the benefits of both.
        - Use `AzureAISearchTool.create_hybrid_search()`
        - The text component can be "simple", "full", or "semantic" via the `query_type` parameter.

    Each factory method configures the tool with appropriate defaults and validations
    for the chosen search strategy.

    .. warning::
        If you set `query_type="semantic"`, you must also provide a valid `semantic_config_name`.
        This configuration must be set up in your Azure AI Search index beforehand.
    """

    component_provider_override = "autogen_ext.tools.azure.AzureAISearchTool"

    @classmethod
    def _from_config(cls, config: AzureAISearchConfig) -> "AzureAISearchTool":
        """Create a tool instance from a configuration object.

        Args:
            config: The configuration object with tool settings

        Returns:
            AzureAISearchTool: An initialized tool instance
        """
        token = _allow_private_constructor.set(True)
        try:
            instance = cls(
                name=config.name,
                description=config.description or "",
                endpoint=config.endpoint,
                index_name=config.index_name,
                credential=config.credential,
                api_version=config.api_version,
                query_type=config.query_type,
                search_fields=config.search_fields,
                select_fields=config.select_fields,
                vector_fields=config.vector_fields,
                top=config.top,
                filter=config.filter,
                semantic_config_name=config.semantic_config_name,
                enable_caching=config.enable_caching,
                cache_ttl_seconds=config.cache_ttl_seconds,
                embedding_provider=config.embedding_provider,
                embedding_model=config.embedding_model,
                openai_api_key=config.openai_api_key,
                openai_api_version=config.openai_api_version,
                openai_endpoint=config.openai_endpoint,
            )
            return instance
        finally:
            _allow_private_constructor.reset(token)

    @classmethod
    def _create_from_params(
        cls, config_dict: Dict[str, Any], search_type: Literal["full_text", "vector", "hybrid"]
    ) -> "AzureAISearchTool":
        """Private helper to create an instance from parameters after validation.

        Args:
            config_dict: Dictionary with configuration parameters
            search_type: Type of search for validation

        Returns:
            Configured AzureAISearchTool instance
        """
        cls._validate_config(config_dict, search_type)

        token = _allow_private_constructor.set(True)
        try:
            return cls(**config_dict)
        finally:
            _allow_private_constructor.reset(token)

    @classmethod
    def create_full_text_search(
        cls,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        description: Optional[str] = None,
        api_version: Optional[str] = None,
        query_type: Literal["simple", "full", "semantic"] = "simple",
        search_fields: Optional[List[str]] = None,
        select_fields: Optional[List[str]] = None,
        top: Optional[int] = 5,
        filter: Optional[str] = None,
        semantic_config_name: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
    ) -> "AzureAISearchTool":
        """Create a tool for traditional text-based searches.

        This factory method creates an AzureAISearchTool optimized for full-text search,
        supporting keyword matching, Lucene syntax, and semantic search capabilities.

        Args:
            name: The name of this tool instance
            endpoint: The full URL of your Azure AI Search service
            index_name: Name of the search index to query
            credential: Azure credential for authentication (API key or token)
            description: Optional description explaining the tool's purpose
            api_version: Azure AI Search API version to use
            query_type: Type of text search to perform:

                • **simple** : Basic keyword search that matches exact terms and their variations
                • **full**: Advanced search using Lucene query syntax for complex queries
                • **semantic**: AI-powered search that understands meaning and context, providing enhanced relevance ranking
            search_fields: Fields to search within documents
            select_fields: Fields to return in search results
            top: Maximum number of results to return (default: 5)
            filter: OData filter expression to refine search results
            semantic_config_name: Semantic configuration name (required for semantic query_type)
            enable_caching: Whether to cache search results
            cache_ttl_seconds: How long to cache results in seconds

        Returns:
            An initialized AzureAISearchTool for full-text search

        Example:
            .. code-block:: python

                from azure.core.credentials import AzureKeyCredential
                from autogen_ext.tools.azure import AzureAISearchTool

                # Basic keyword search
                tool = AzureAISearchTool.create_full_text_search(
                    name="doc-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    query_type="simple",  # Enable keyword search
                    search_fields=["content", "title"],  # Required: fields to search within
                    select_fields=["content", "title", "url"],  # Optional: fields to return
                    top=5,
                )

                # full text (Lucene query) search
                full_text_tool = AzureAISearchTool.create_full_text_search(
                    name="doc-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    query_type="full",  # Enable Lucene query syntax
                    search_fields=["content", "title"],  # Required: fields to search within
                    select_fields=["content", "title", "url"],  # Optional: fields to return
                    top=5,
                )

                # Semantic search with re-ranking
                # Note: Make sure your index has semantic configuration enabled
                semantic_tool = AzureAISearchTool.create_full_text_search(
                    name="semantic-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    query_type="semantic",  # Enable semantic ranking
                    semantic_config_name="<your-semantic-config>",  # Required for semantic search
                    search_fields=["content", "title"],  # Required: fields to search within
                    select_fields=["content", "title", "url"],  # Optional: fields to return
                    top=5,
                )

                # The search tool can be used with an Agent
                # assistant = Agent("assistant", tools=[semantic_tool])
        """
        if query_type == "semantic" and not semantic_config_name:
            raise ValueError("semantic_config_name is required when query_type is 'semantic'")

        config_dict = {
            "name": name,
            "endpoint": endpoint,
            "index_name": index_name,
            "credential": credential,
            "description": description,
            "api_version": api_version or DEFAULT_API_VERSION,
            "query_type": query_type,
            "search_fields": search_fields,
            "select_fields": select_fields,
            "top": top,
            "filter": filter,
            "semantic_config_name": semantic_config_name,
            "enable_caching": enable_caching,
            "cache_ttl_seconds": cache_ttl_seconds,
        }

        return cls._create_from_params(config_dict, "full_text")

    @classmethod
    def create_vector_search(
        cls,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        vector_fields: List[str],
        description: Optional[str] = None,
        api_version: Optional[str] = None,
        select_fields: Optional[List[str]] = None,
        top: int = 5,
        filter: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
        embedding_provider: Optional[str] = None,
        embedding_model: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        openai_api_version: Optional[str] = None,
        openai_endpoint: Optional[str] = None,
    ) -> "AzureAISearchTool":
        """Create a tool for pure vector/similarity search.

        This factory method creates an AzureAISearchTool optimized for vector search,
        allowing for semantic similarity-based matching using vector embeddings.

        Args:
            name: The name of this tool instance
            endpoint: The full URL of your Azure AI Search service
            index_name: Name of the search index to query
            credential: Azure credential for authentication (API key or token)
            vector_fields: Fields to use for vector search (required)
            description: Optional description explaining the tool's purpose
            api_version: Azure AI Search API version to use
            select_fields: Fields to return in search results
            top: Maximum number of results to return / k in k-NN (default: 5)
            filter: OData filter expression to refine search results
            enable_caching: Whether to cache search results
            cache_ttl_seconds: How long to cache results in seconds
            embedding_provider: Provider for client-side embeddings (e.g., 'azure_openai', 'openai')
            embedding_model: Model for client-side embeddings (e.g., 'text-embedding-ada-002')
            openai_api_key: API key for OpenAI/Azure OpenAI embeddings
            openai_api_version: API version for Azure OpenAI embeddings
            openai_endpoint: Endpoint URL for Azure OpenAI embeddings

        Returns:
            An initialized AzureAISearchTool for vector search

        Raises:
            ValueError: If vector_fields is empty
            ValueError: If embedding_provider is 'azure_openai' without openai_endpoint
            ValueError: If required parameters are missing or invalid

        Example Usage:
            .. code-block:: python

                from azure.core.credentials import AzureKeyCredential
                from autogen_ext.tools.azure import AzureAISearchTool

                # Vector search with service-side vectorization
                tool = AzureAISearchTool.create_vector_search(
                    name="vector-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    vector_fields=["content_vector"],  # Your vector field name
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    top=5,
                )

                # Vector search with Azure OpenAI embeddings
                azure_openai_tool = AzureAISearchTool.create_vector_search(
                    name="azure-openai-vector-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    vector_fields=["content_vector"],
                    embedding_provider="azure_openai",  # Use Azure OpenAI for embeddings
                    embedding_model="text-embedding-ada-002",  # Embedding model to use
                    openai_endpoint="https://your-openai.openai.azure.com",  # Your Azure OpenAI endpoint
                    openai_api_key="<your-openai-key>",  # Your Azure OpenAI key
                    openai_api_version="2024-02-15-preview",  # Azure OpenAI API version
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    top=5,
                )

                # Vector search with OpenAI embeddings
                openai_tool = AzureAISearchTool.create_vector_search(
                    name="openai-vector-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    vector_fields=["content_vector"],
                    embedding_provider="openai",  # Use OpenAI for embeddings
                    embedding_model="text-embedding-ada-002",  # Embedding model to use
                    openai_api_key="<your-openai-key>",  # Your OpenAI API key
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    top=5,
                )

                # Use the tool with an Agent
                # assistant = Agent("assistant", tools=[azure_openai_tool])
        """
        if embedding_provider == "azure_openai" and not openai_endpoint:
            raise ValueError("openai_endpoint is required when embedding_provider is 'azure_openai'")

        config_dict = {
            "name": name,
            "endpoint": endpoint,
            "index_name": index_name,
            "credential": credential,
            "description": description,
            "api_version": api_version or DEFAULT_API_VERSION,
            "query_type": "vector",
            "select_fields": select_fields,
            "vector_fields": vector_fields,
            "top": top,
            "filter": filter,
            "enable_caching": enable_caching,
            "cache_ttl_seconds": cache_ttl_seconds,
            "embedding_provider": embedding_provider,
            "embedding_model": embedding_model,
            "openai_api_key": openai_api_key,
            "openai_api_version": openai_api_version,
            "openai_endpoint": openai_endpoint,
        }

        return cls._create_from_params(config_dict, "vector")

    @classmethod
    def create_hybrid_search(
        cls,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        vector_fields: List[str],
        search_fields: List[str],
        description: Optional[str] = None,
        api_version: Optional[str] = None,
        query_type: Literal["simple", "full", "semantic"] = "simple",
        select_fields: Optional[List[str]] = None,
        top: int = 5,
        filter: Optional[str] = None,
        semantic_config_name: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
        embedding_provider: Optional[str] = None,
        embedding_model: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        openai_api_version: Optional[str] = None,
        openai_endpoint: Optional[str] = None,
    ) -> "AzureAISearchTool":
        """Create a tool that combines vector and text search capabilities.

        This factory method creates an AzureAISearchTool configured for hybrid search,
        which combines the benefits of vector similarity and traditional text search.

        Args:
            name: The name of this tool instance
            endpoint: The full URL of your Azure AI Search service
            index_name: Name of the search index to query
            credential: Azure credential for authentication (API key or token)
            vector_fields: Fields to use for vector search (required)
            search_fields: Fields to use for text search (required)
            description: Optional description explaining the tool's purpose
            api_version: Azure AI Search API version to use
            query_type: Type of text search to perform:

                • **simple**: Basic keyword search that matches exact terms and their variations
                • **full**: Advanced search using Lucene query syntax for complex queries
                • **semantic**: AI-powered search that understands meaning and context, providing enhanced relevance ranking
            select_fields: Fields to return in search results
            top: Maximum number of results to return (default: 5)
            filter: OData filter expression to refine search results
            semantic_config_name: Semantic configuration name (required if query_type="semantic")
            enable_caching: Whether to cache search results
            cache_ttl_seconds: How long to cache results in seconds
            embedding_provider: Provider for client-side embeddings (e.g., 'azure_openai', 'openai')
            embedding_model: Model for client-side embeddings (e.g., 'text-embedding-ada-002')
            openai_api_key: API key for OpenAI/Azure OpenAI embeddings
            openai_api_version: API version for Azure OpenAI embeddings
            openai_endpoint: Endpoint URL for Azure OpenAI embeddings

        Returns:
            An initialized AzureAISearchTool for hybrid search

        Raises:
            ValueError: If vector_fields or search_fields is empty
            ValueError: If query_type is "semantic" without semantic_config_name
            ValueError: If embedding_provider is 'azure_openai' without openai_endpoint
            ValueError: If required parameters are missing or invalid

        Example:
            .. code-block:: python

                from azure.core.credentials import AzureKeyCredential
                from autogen_ext.tools.azure import AzureAISearchTool

                # Basic hybrid search with service-side vectorization
                tool = AzureAISearchTool.create_hybrid_search(
                    name="hybrid-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    vector_fields=["content_vector"],  # Your vector field name
                    search_fields=["content", "title"],  # Your searchable fields
                    top=5,
                )

                # Hybrid search with semantic ranking and Azure OpenAI embeddings
                semantic_tool = AzureAISearchTool.create_hybrid_search(
                    name="semantic-hybrid-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    vector_fields=["content_vector"],
                    search_fields=["content", "title"],
                    query_type="semantic",  # Enable semantic ranking
                    semantic_config_name="<your-semantic-config>",  # Your semantic config name
                    embedding_provider="azure_openai",  # Use Azure OpenAI for embeddings
                    embedding_model="text-embedding-ada-002",  # Embedding model to use
                    openai_endpoint="https://your-openai.openai.azure.com",  # Your Azure OpenAI endpoint
                    openai_api_key="<your-openai-key>",  # Your Azure OpenAI key
                    openai_api_version="2024-02-15-preview",  # Azure OpenAI API version
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    filter="language eq 'en'",  # Optional OData filter
                    top=5,
                )

                # The search tool can be used with an Agent
                # assistant = Agent("assistant", tools=[semantic_tool])
        """
        if query_type == "semantic" and not semantic_config_name:
            raise ValueError("semantic_config_name is required when query_type is 'semantic'")

        if embedding_provider == "azure_openai" and not openai_endpoint:
            raise ValueError("openai_endpoint is required when embedding_provider is 'azure_openai'")

        config_dict = {
            "name": name,
            "endpoint": endpoint,
            "index_name": index_name,
            "credential": credential,
            "description": description,
            "api_version": api_version or DEFAULT_API_VERSION,
            "query_type": query_type,
            "search_fields": search_fields,
            "select_fields": select_fields,
            "vector_fields": vector_fields,
            "top": top,
            "filter": filter,
            "semantic_config_name": semantic_config_name,
            "enable_caching": enable_caching,
            "cache_ttl_seconds": cache_ttl_seconds,
            "embedding_provider": embedding_provider,
            "embedding_model": embedding_model,
            "openai_api_key": openai_api_key,
            "openai_api_version": openai_api_version,
            "openai_endpoint": openai_endpoint,
        }

        return cls._create_from_params(config_dict, "hybrid")

# From azure/_ai_search.py
class SearchClientProtocol(Protocol):
        async def search(self, **kwargs: Any) -> SearchResultsIterable: ...
        async def close(self) -> None: ...

# From azure/_ai_search.py
def schema(self) -> ToolSchema:
        """Return the schema for the tool."""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {"query": {"type": "string", "description": "Search query text"}},
                "required": ["query"],
                "additionalProperties": False,
            },
            "strict": True,
        }

# From azure/_ai_search.py
def create_full_text_search(
        cls,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        description: Optional[str] = None,
        api_version: Optional[str] = None,
        query_type: Literal["simple", "full", "semantic"] = "simple",
        search_fields: Optional[List[str]] = None,
        select_fields: Optional[List[str]] = None,
        top: Optional[int] = 5,
        filter: Optional[str] = None,
        semantic_config_name: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
    ) -> "AzureAISearchTool":
        """Create a tool for traditional text-based searches.

        This factory method creates an AzureAISearchTool optimized for full-text search,
        supporting keyword matching, Lucene syntax, and semantic search capabilities.

        Args:
            name: The name of this tool instance
            endpoint: The full URL of your Azure AI Search service
            index_name: Name of the search index to query
            credential: Azure credential for authentication (API key or token)
            description: Optional description explaining the tool's purpose
            api_version: Azure AI Search API version to use
            query_type: Type of text search to perform:

                • **simple** : Basic keyword search that matches exact terms and their variations
                • **full**: Advanced search using Lucene query syntax for complex queries
                • **semantic**: AI-powered search that understands meaning and context, providing enhanced relevance ranking
            search_fields: Fields to search within documents
            select_fields: Fields to return in search results
            top: Maximum number of results to return (default: 5)
            filter: OData filter expression to refine search results
            semantic_config_name: Semantic configuration name (required for semantic query_type)
            enable_caching: Whether to cache search results
            cache_ttl_seconds: How long to cache results in seconds

        Returns:
            An initialized AzureAISearchTool for full-text search

        Example:
            .. code-block:: python

                from azure.core.credentials import AzureKeyCredential
                from autogen_ext.tools.azure import AzureAISearchTool

                # Basic keyword search
                tool = AzureAISearchTool.create_full_text_search(
                    name="doc-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    query_type="simple",  # Enable keyword search
                    search_fields=["content", "title"],  # Required: fields to search within
                    select_fields=["content", "title", "url"],  # Optional: fields to return
                    top=5,
                )

                # full text (Lucene query) search
                full_text_tool = AzureAISearchTool.create_full_text_search(
                    name="doc-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    query_type="full",  # Enable Lucene query syntax
                    search_fields=["content", "title"],  # Required: fields to search within
                    select_fields=["content", "title", "url"],  # Optional: fields to return
                    top=5,
                )

                # Semantic search with re-ranking
                # Note: Make sure your index has semantic configuration enabled
                semantic_tool = AzureAISearchTool.create_full_text_search(
                    name="semantic-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    query_type="semantic",  # Enable semantic ranking
                    semantic_config_name="<your-semantic-config>",  # Required for semantic search
                    search_fields=["content", "title"],  # Required: fields to search within
                    select_fields=["content", "title", "url"],  # Optional: fields to return
                    top=5,
                )

                # The search tool can be used with an Agent
                # assistant = Agent("assistant", tools=[semantic_tool])
        """
        if query_type == "semantic" and not semantic_config_name:
            raise ValueError("semantic_config_name is required when query_type is 'semantic'")

        config_dict = {
            "name": name,
            "endpoint": endpoint,
            "index_name": index_name,
            "credential": credential,
            "description": description,
            "api_version": api_version or DEFAULT_API_VERSION,
            "query_type": query_type,
            "search_fields": search_fields,
            "select_fields": select_fields,
            "top": top,
            "filter": filter,
            "semantic_config_name": semantic_config_name,
            "enable_caching": enable_caching,
            "cache_ttl_seconds": cache_ttl_seconds,
        }

        return cls._create_from_params(config_dict, "full_text")

# From azure/_ai_search.py
def create_vector_search(
        cls,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        vector_fields: List[str],
        description: Optional[str] = None,
        api_version: Optional[str] = None,
        select_fields: Optional[List[str]] = None,
        top: int = 5,
        filter: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
        embedding_provider: Optional[str] = None,
        embedding_model: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        openai_api_version: Optional[str] = None,
        openai_endpoint: Optional[str] = None,
    ) -> "AzureAISearchTool":
        """Create a tool for pure vector/similarity search.

        This factory method creates an AzureAISearchTool optimized for vector search,
        allowing for semantic similarity-based matching using vector embeddings.

        Args:
            name: The name of this tool instance
            endpoint: The full URL of your Azure AI Search service
            index_name: Name of the search index to query
            credential: Azure credential for authentication (API key or token)
            vector_fields: Fields to use for vector search (required)
            description: Optional description explaining the tool's purpose
            api_version: Azure AI Search API version to use
            select_fields: Fields to return in search results
            top: Maximum number of results to return / k in k-NN (default: 5)
            filter: OData filter expression to refine search results
            enable_caching: Whether to cache search results
            cache_ttl_seconds: How long to cache results in seconds
            embedding_provider: Provider for client-side embeddings (e.g., 'azure_openai', 'openai')
            embedding_model: Model for client-side embeddings (e.g., 'text-embedding-ada-002')
            openai_api_key: API key for OpenAI/Azure OpenAI embeddings
            openai_api_version: API version for Azure OpenAI embeddings
            openai_endpoint: Endpoint URL for Azure OpenAI embeddings

        Returns:
            An initialized AzureAISearchTool for vector search

        Raises:
            ValueError: If vector_fields is empty
            ValueError: If embedding_provider is 'azure_openai' without openai_endpoint
            ValueError: If required parameters are missing or invalid

        Example Usage:
            .. code-block:: python

                from azure.core.credentials import AzureKeyCredential
                from autogen_ext.tools.azure import AzureAISearchTool

                # Vector search with service-side vectorization
                tool = AzureAISearchTool.create_vector_search(
                    name="vector-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    vector_fields=["content_vector"],  # Your vector field name
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    top=5,
                )

                # Vector search with Azure OpenAI embeddings
                azure_openai_tool = AzureAISearchTool.create_vector_search(
                    name="azure-openai-vector-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    vector_fields=["content_vector"],
                    embedding_provider="azure_openai",  # Use Azure OpenAI for embeddings
                    embedding_model="text-embedding-ada-002",  # Embedding model to use
                    openai_endpoint="https://your-openai.openai.azure.com",  # Your Azure OpenAI endpoint
                    openai_api_key="<your-openai-key>",  # Your Azure OpenAI key
                    openai_api_version="2024-02-15-preview",  # Azure OpenAI API version
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    top=5,
                )

                # Vector search with OpenAI embeddings
                openai_tool = AzureAISearchTool.create_vector_search(
                    name="openai-vector-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    vector_fields=["content_vector"],
                    embedding_provider="openai",  # Use OpenAI for embeddings
                    embedding_model="text-embedding-ada-002",  # Embedding model to use
                    openai_api_key="<your-openai-key>",  # Your OpenAI API key
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    top=5,
                )

                # Use the tool with an Agent
                # assistant = Agent("assistant", tools=[azure_openai_tool])
        """
        if embedding_provider == "azure_openai" and not openai_endpoint:
            raise ValueError("openai_endpoint is required when embedding_provider is 'azure_openai'")

        config_dict = {
            "name": name,
            "endpoint": endpoint,
            "index_name": index_name,
            "credential": credential,
            "description": description,
            "api_version": api_version or DEFAULT_API_VERSION,
            "query_type": "vector",
            "select_fields": select_fields,
            "vector_fields": vector_fields,
            "top": top,
            "filter": filter,
            "enable_caching": enable_caching,
            "cache_ttl_seconds": cache_ttl_seconds,
            "embedding_provider": embedding_provider,
            "embedding_model": embedding_model,
            "openai_api_key": openai_api_key,
            "openai_api_version": openai_api_version,
            "openai_endpoint": openai_endpoint,
        }

        return cls._create_from_params(config_dict, "vector")

# From azure/_ai_search.py
def create_hybrid_search(
        cls,
        name: str,
        endpoint: str,
        index_name: str,
        credential: Union[AzureKeyCredential, AsyncTokenCredential, Dict[str, str]],
        vector_fields: List[str],
        search_fields: List[str],
        description: Optional[str] = None,
        api_version: Optional[str] = None,
        query_type: Literal["simple", "full", "semantic"] = "simple",
        select_fields: Optional[List[str]] = None,
        top: int = 5,
        filter: Optional[str] = None,
        semantic_config_name: Optional[str] = None,
        enable_caching: bool = False,
        cache_ttl_seconds: int = 300,
        embedding_provider: Optional[str] = None,
        embedding_model: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        openai_api_version: Optional[str] = None,
        openai_endpoint: Optional[str] = None,
    ) -> "AzureAISearchTool":
        """Create a tool that combines vector and text search capabilities.

        This factory method creates an AzureAISearchTool configured for hybrid search,
        which combines the benefits of vector similarity and traditional text search.

        Args:
            name: The name of this tool instance
            endpoint: The full URL of your Azure AI Search service
            index_name: Name of the search index to query
            credential: Azure credential for authentication (API key or token)
            vector_fields: Fields to use for vector search (required)
            search_fields: Fields to use for text search (required)
            description: Optional description explaining the tool's purpose
            api_version: Azure AI Search API version to use
            query_type: Type of text search to perform:

                • **simple**: Basic keyword search that matches exact terms and their variations
                • **full**: Advanced search using Lucene query syntax for complex queries
                • **semantic**: AI-powered search that understands meaning and context, providing enhanced relevance ranking
            select_fields: Fields to return in search results
            top: Maximum number of results to return (default: 5)
            filter: OData filter expression to refine search results
            semantic_config_name: Semantic configuration name (required if query_type="semantic")
            enable_caching: Whether to cache search results
            cache_ttl_seconds: How long to cache results in seconds
            embedding_provider: Provider for client-side embeddings (e.g., 'azure_openai', 'openai')
            embedding_model: Model for client-side embeddings (e.g., 'text-embedding-ada-002')
            openai_api_key: API key for OpenAI/Azure OpenAI embeddings
            openai_api_version: API version for Azure OpenAI embeddings
            openai_endpoint: Endpoint URL for Azure OpenAI embeddings

        Returns:
            An initialized AzureAISearchTool for hybrid search

        Raises:
            ValueError: If vector_fields or search_fields is empty
            ValueError: If query_type is "semantic" without semantic_config_name
            ValueError: If embedding_provider is 'azure_openai' without openai_endpoint
            ValueError: If required parameters are missing or invalid

        Example:
            .. code-block:: python

                from azure.core.credentials import AzureKeyCredential
                from autogen_ext.tools.azure import AzureAISearchTool

                # Basic hybrid search with service-side vectorization
                tool = AzureAISearchTool.create_hybrid_search(
                    name="hybrid-search",
                    endpoint="https://your-search.search.windows.net",  # Your Azure AI Search endpoint
                    index_name="<your-index>",  # Name of your search index
                    credential=AzureKeyCredential("<your-key>"),  # Your Azure AI Search admin key
                    vector_fields=["content_vector"],  # Your vector field name
                    search_fields=["content", "title"],  # Your searchable fields
                    top=5,
                )

                # Hybrid search with semantic ranking and Azure OpenAI embeddings
                semantic_tool = AzureAISearchTool.create_hybrid_search(
                    name="semantic-hybrid-search",
                    endpoint="https://your-search.search.windows.net",
                    index_name="<your-index>",
                    credential=AzureKeyCredential("<your-key>"),
                    vector_fields=["content_vector"],
                    search_fields=["content", "title"],
                    query_type="semantic",  # Enable semantic ranking
                    semantic_config_name="<your-semantic-config>",  # Your semantic config name
                    embedding_provider="azure_openai",  # Use Azure OpenAI for embeddings
                    embedding_model="text-embedding-ada-002",  # Embedding model to use
                    openai_endpoint="https://your-openai.openai.azure.com",  # Your Azure OpenAI endpoint
                    openai_api_key="<your-openai-key>",  # Your Azure OpenAI key
                    openai_api_version="2024-02-15-preview",  # Azure OpenAI API version
                    select_fields=["content", "title", "url"],  # Fields to return in results
                    filter="language eq 'en'",  # Optional OData filter
                    top=5,
                )

                # The search tool can be used with an Agent
                # assistant = Agent("assistant", tools=[semantic_tool])
        """
        if query_type == "semantic" and not semantic_config_name:
            raise ValueError("semantic_config_name is required when query_type is 'semantic'")

        if embedding_provider == "azure_openai" and not openai_endpoint:
            raise ValueError("openai_endpoint is required when embedding_provider is 'azure_openai'")

        config_dict = {
            "name": name,
            "endpoint": endpoint,
            "index_name": index_name,
            "credential": credential,
            "description": description,
            "api_version": api_version or DEFAULT_API_VERSION,
            "query_type": query_type,
            "search_fields": search_fields,
            "select_fields": select_fields,
            "vector_fields": vector_fields,
            "top": top,
            "filter": filter,
            "semantic_config_name": semantic_config_name,
            "enable_caching": enable_caching,
            "cache_ttl_seconds": cache_ttl_seconds,
            "embedding_provider": embedding_provider,
            "embedding_model": embedding_model,
            "openai_api_key": openai_api_key,
            "openai_api_version": openai_api_version,
            "openai_endpoint": openai_endpoint,
        }

        return cls._create_from_params(config_dict, "hybrid")

# From azure/_ai_search.py
def get_token() -> str:
                    credential = DefaultAzureCredential()
                    token = credential.get_token("https://cognitiveservices.azure.com/.default")
                    if not token or not token.token:
                        raise ValueError("Failed to acquire token using DefaultAzureCredential for Azure OpenAI.")
                    return token.token

import tiktoken
import graphrag.config.defaults
from graphrag.config.load_config import load_config
from graphrag.language_model.manager import ModelManager
from graphrag.language_model.protocol import ChatModel
from graphrag.language_model.protocol import EmbeddingModel
from graphrag.query.indexer_adapters import read_indexer_entities
from graphrag.query.indexer_adapters import read_indexer_relationships
from graphrag.query.indexer_adapters import read_indexer_text_units
from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
from _config import LocalContextConfig
from _config import SearchConfig
from _config import LocalDataConfig

# From graphrag/_local_search.py
class LocalSearchToolArgs(BaseModel):
    query: str = Field(..., description="The user query to perform local search on.")

# From graphrag/_local_search.py
class LocalSearchToolReturn(BaseModel):
    answer: str = Field(..., description="The answer to the user query.")

# From graphrag/_local_search.py
class LocalSearchTool(BaseTool[LocalSearchToolArgs, LocalSearchToolReturn]):
    """Enables running GraphRAG local search queries as an AutoGen tool.

    This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
    The search combines local document context with semantic embeddings to find relevant information.

    .. note::
        This tool requires the :code:`graphrag` extra for the :code:`autogen-ext` package.
        To install:

        .. code-block:: bash

            pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

        Before using this tool, you must complete the GraphRAG setup and indexing process:

        1. Follow the GraphRAG documentation to initialize your project and settings
        2. Configure and tune your prompts for the specific use case
        3. Run the indexing process to generate the required data files
        4. Ensure you have the settings.yaml file from the setup process

        Please refer to the [GraphRAG documentation](https://microsoft.github.io/graphrag/)
        for detailed instructions on completing these prerequisite steps.

    Example usage with AssistantAgent:

    .. code-block:: python

        import asyncio
        from pathlib import Path
        from autogen_ext.models.openai import OpenAIChatCompletionClient
        from autogen_agentchat.ui import Console
        from autogen_ext.tools.graphrag import LocalSearchTool
        from autogen_agentchat.agents import AssistantAgent


        async def main():
            # Initialize the OpenAI client
            openai_client = OpenAIChatCompletionClient(
                model="gpt-4o-mini",
                api_key="<api-key>",
            )

            # Set up local search tool
            local_tool = LocalSearchTool.from_settings(root_dir=Path("./"), config_filepath=Path("./settings.yaml"))

            # Create assistant agent with the local search tool
            assistant_agent = AssistantAgent(
                name="search_assistant",
                tools=[local_tool],
                model_client=openai_client,
                system_message=(
                    "You are a tool selector AI assistant using the GraphRAG framework. "
                    "Your primary task is to determine the appropriate search tool to call based on the user's query. "
                    "For specific, detailed information about particular entities or relationships, call the 'local_search' function."
                ),
            )

            # Run a sample query
            query = "What does the station-master say about Dr. Becher?"
            await Console(assistant_agent.run_stream(task=query))


        if __name__ == "__main__":
            asyncio.run(main())


    Args:
        token_encoder (tiktoken.Encoding): The tokenizer used for text encoding
        model: The chat model to use for search (GraphRAG ChatModel)
        embedder: The text embedding model to use (GraphRAG EmbeddingModel)
        data_config (DataConfig): Configuration for data source locations and settings
        context_config (LocalContextConfig, optional): Configuration for context building. Defaults to default config.
        search_config (SearchConfig, optional): Configuration for search operations. Defaults to default config.
    """

    def __init__(
        self,
        token_encoder: tiktoken.Encoding,
        model: ChatModel,  # ChatModel from GraphRAG
        embedder: EmbeddingModel,  # EmbeddingModel from GraphRAG
        data_config: DataConfig,
        context_config: LocalContextConfig = _default_context_config,
        search_config: SearchConfig = _default_search_config,
    ):
        super().__init__(
            args_type=LocalSearchToolArgs,
            return_type=LocalSearchToolReturn,
            name="local_search_tool",
            description="Perform a local search with given parameters using graphrag.",
        )
        # Use the provided models
        self._model = model
        self._embedder = embedder

        # Load parquet files
        entity_df: pd.DataFrame = pd.read_parquet(f"{data_config.input_dir}/{data_config.entity_table}.parquet")  # type: ignore
        relationship_df: pd.DataFrame = pd.read_parquet(  # type: ignore
            f"{data_config.input_dir}/{data_config.relationship_table}.parquet"
        )
        text_unit_df: pd.DataFrame = pd.read_parquet(f"{data_config.input_dir}/{data_config.text_unit_table}.parquet")  # type: ignore
        community_df: pd.DataFrame = pd.read_parquet(f"{data_config.input_dir}/{data_config.community_table}.parquet")  # type: ignore

        # Read data using indexer adapters
        entities = read_indexer_entities(entity_df, community_df, data_config.community_level)
        relationships = read_indexer_relationships(relationship_df)
        text_units = read_indexer_text_units(text_unit_df)
        # Set up vector store for entity embeddings
        description_embedding_store = LanceDBVectorStore(
            collection_name="default-entity-description",
        )
        description_embedding_store.connect(db_uri=f"{data_config.input_dir}/lancedb")

        # Set up context builder
        context_builder = LocalSearchMixedContext(
            entities=entities,
            entity_text_embeddings=description_embedding_store,
            text_embedder=self._embedder,
            text_units=text_units,
            relationships=relationships,
            token_encoder=token_encoder,
        )

        context_builder_params = {
            "text_unit_prop": context_config.text_unit_prop,
            "community_prop": context_config.community_prop,
            "include_entity_rank": context_config.include_entity_rank,
            "rank_description": context_config.rank_description,
            "include_relationship_weight": context_config.include_relationship_weight,
            "relationship_ranking_attribute": context_config.relationship_ranking_attribute,
            "max_tokens": context_config.max_data_tokens,
        }

        llm_params = {
            "max_tokens": search_config.max_tokens,
            "temperature": search_config.temperature,
        }

        self._search_engine = LocalSearch(
            model=self._model,
            context_builder=context_builder,
            token_encoder=token_encoder,
            response_type=search_config.response_type,
            context_builder_params=context_builder_params,
            model_params=llm_params,
        )

    async def run(self, args: LocalSearchToolArgs, cancellation_token: CancellationToken) -> LocalSearchToolReturn:
        search_result = await self._search_engine.search(args.query)  # type: ignore[reportUnknownMemberType]
        assert isinstance(search_result.response, str), "Expected response to be a string"
        return LocalSearchToolReturn(answer=search_result.response)

    @classmethod
    def from_settings(cls, root_dir: Path, config_filepath: Path | None = None) -> "LocalSearchTool":
        """Create a LocalSearchTool instance from GraphRAG settings file.

        Args:
            root_dir: Path to the GraphRAG root directory
            config_filepath: Path to the GraphRAG settings file (optional)

        Returns:
            An initialized LocalSearchTool instance
        """
        # Load GraphRAG config
        config = load_config(root_dir=root_dir, config_filepath=config_filepath)

        # Get the language model configurations from the models section
        chat_model_config = config.models.get(defs.DEFAULT_CHAT_MODEL_ID)
        embedding_model_config = config.models.get(defs.DEFAULT_EMBEDDING_MODEL_ID)

        if chat_model_config is None:
            raise ValueError("default_chat_model not found in config.models")
        if embedding_model_config is None:
            raise ValueError("default_embedding_model not found in config.models")

        # Initialize token encoder based on the model being used
        try:
            token_encoder = tiktoken.encoding_for_model(chat_model_config.model)
        except KeyError:
            # Fallback to cl100k_base if model is not recognized by tiktoken
            token_encoder = tiktoken.get_encoding("cl100k_base")

        # Create the models using ModelManager
        model = ModelManager().get_or_create_chat_model(
            name="local_search_model",
            model_type=chat_model_config.type,
            config=chat_model_config,
        )

        embedder = ModelManager().get_or_create_embedding_model(
            name="local_search_embedder",
            model_type=embedding_model_config.type,
            config=embedding_model_config,
        )

        # Create data config from storage paths
        data_config = DataConfig(
            input_dir=str(config.output.base_dir),
        )

        return cls(
            token_encoder=token_encoder,
            model=model,
            embedder=embedder,
            data_config=data_config,
            context_config=_default_context_config,
            search_config=_default_search_config,
        )

# From graphrag/_local_search.py
def from_settings(cls, root_dir: Path, config_filepath: Path | None = None) -> "LocalSearchTool":
        """Create a LocalSearchTool instance from GraphRAG settings file.

        Args:
            root_dir: Path to the GraphRAG root directory
            config_filepath: Path to the GraphRAG settings file (optional)

        Returns:
            An initialized LocalSearchTool instance
        """
        # Load GraphRAG config
        config = load_config(root_dir=root_dir, config_filepath=config_filepath)

        # Get the language model configurations from the models section
        chat_model_config = config.models.get(defs.DEFAULT_CHAT_MODEL_ID)
        embedding_model_config = config.models.get(defs.DEFAULT_EMBEDDING_MODEL_ID)

        if chat_model_config is None:
            raise ValueError("default_chat_model not found in config.models")
        if embedding_model_config is None:
            raise ValueError("default_embedding_model not found in config.models")

        # Initialize token encoder based on the model being used
        try:
            token_encoder = tiktoken.encoding_for_model(chat_model_config.model)
        except KeyError:
            # Fallback to cl100k_base if model is not recognized by tiktoken
            token_encoder = tiktoken.get_encoding("cl100k_base")

        # Create the models using ModelManager
        model = ModelManager().get_or_create_chat_model(
            name="local_search_model",
            model_type=chat_model_config.type,
            config=chat_model_config,
        )

        embedder = ModelManager().get_or_create_embedding_model(
            name="local_search_embedder",
            model_type=embedding_model_config.type,
            config=embedding_model_config,
        )

        # Create data config from storage paths
        data_config = DataConfig(
            input_dir=str(config.output.base_dir),
        )

        return cls(
            token_encoder=token_encoder,
            model=model,
            embedder=embedder,
            data_config=data_config,
            context_config=_default_context_config,
            search_config=_default_search_config,
        )

from graphrag.query.indexer_adapters import read_indexer_communities
from graphrag.query.indexer_adapters import read_indexer_reports
from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext
from graphrag.query.structured_search.global_search.search import GlobalSearch
from _config import GlobalContextConfig
from _config import GlobalDataConfig
from _config import MapReduceConfig

# From graphrag/_global_search.py
class GlobalSearchToolArgs(BaseModel):
    query: str = Field(..., description="The user query to perform global search on.")

# From graphrag/_global_search.py
class GlobalSearchToolReturn(BaseModel):
    answer: str

# From graphrag/_global_search.py
class GlobalSearchTool(BaseTool[GlobalSearchToolArgs, GlobalSearchToolReturn]):
    """Enables running GraphRAG global search queries as an AutoGen tool.

    This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
    The search combines graph-based document relationships with semantic embeddings to find relevant information.

    .. note::
        This tool requires the :code:`graphrag` extra for the :code:`autogen-ext` package.

        To install:

        .. code-block:: bash

            pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

        Before using this tool, you must complete the GraphRAG setup and indexing process:

        1. Follow the GraphRAG documentation to initialize your project and settings
        2. Configure and tune your prompts for the specific use case
        3. Run the indexing process to generate the required data files
        4. Ensure you have the settings.yaml file from the setup process

        Please refer to the [GraphRAG documentation](https://microsoft.github.io/graphrag/)
        for detailed instructions on completing these prerequisite steps.

    Example usage with AssistantAgent:

    .. code-block:: python

        import asyncio
        from pathlib import Path
        from autogen_ext.models.openai import OpenAIChatCompletionClient
        from autogen_agentchat.ui import Console
        from autogen_ext.tools.graphrag import GlobalSearchTool
        from autogen_agentchat.agents import AssistantAgent


        async def main():
            # Initialize the OpenAI client
            openai_client = OpenAIChatCompletionClient(
                model="gpt-4o-mini",
                api_key="<api-key>",
            )

            # Set up global search tool
            global_tool = GlobalSearchTool.from_settings(root_dir=Path("./"), config_filepath=Path("./settings.yaml"))

            # Create assistant agent with the global search tool
            assistant_agent = AssistantAgent(
                name="search_assistant",
                tools=[global_tool],
                model_client=openai_client,
                system_message=(
                    "You are a tool selector AI assistant using the GraphRAG framework. "
                    "Your primary task is to determine the appropriate search tool to call based on the user's query. "
                    "For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function."
                ),
            )

            # Run a sample query
            query = "What is the overall sentiment of the community reports?"
            await Console(assistant_agent.run_stream(task=query))


        if __name__ == "__main__":
            asyncio.run(main())
    """

    def __init__(
        self,
        token_encoder: tiktoken.Encoding,
        model: ChatModel,
        data_config: DataConfig,
        context_config: ContextConfig = _default_context_config,
        mapreduce_config: MapReduceConfig = _default_mapreduce_config,
    ):
        super().__init__(
            args_type=GlobalSearchToolArgs,
            return_type=GlobalSearchToolReturn,
            name="global_search_tool",
            description="Perform a global search with given parameters using graphrag.",
        )
        # Use the provided model
        self._model = model

        # Load parquet files
        community_df: pd.DataFrame = pd.read_parquet(f"{data_config.input_dir}/{data_config.community_table}.parquet")  # type: ignore
        entity_df: pd.DataFrame = pd.read_parquet(f"{data_config.input_dir}/{data_config.entity_table}.parquet")  # type: ignore
        report_df: pd.DataFrame = pd.read_parquet(  # type: ignore
            f"{data_config.input_dir}/{data_config.community_report_table}.parquet"
        )

        # Fix: Use correct argument order and types for GraphRAG API
        communities = read_indexer_communities(community_df, report_df)
        reports = read_indexer_reports(report_df, community_df, data_config.community_level)
        entities = read_indexer_entities(entity_df, community_df, data_config.community_level)

        context_builder = GlobalCommunityContext(
            community_reports=reports,
            communities=communities,
            entities=entities,
            token_encoder=token_encoder,
        )

        context_builder_params = {
            "use_community_summary": context_config.use_community_summary,
            "shuffle_data": context_config.shuffle_data,
            "include_community_rank": context_config.include_community_rank,
            "min_community_rank": context_config.min_community_rank,
            "community_rank_name": context_config.community_rank_name,
            "include_community_weight": context_config.include_community_weight,
            "community_weight_name": context_config.community_weight_name,
            "normalize_community_weight": context_config.normalize_community_weight,
            "max_tokens": context_config.max_data_tokens,
            "context_name": "Reports",
        }

        map_llm_params = {
            "max_tokens": mapreduce_config.map_max_tokens,
            "temperature": mapreduce_config.map_temperature,
            "response_format": {"type": "json_object"},
        }

        reduce_llm_params = {
            "max_tokens": mapreduce_config.reduce_max_tokens,
            "temperature": mapreduce_config.reduce_temperature,
        }

        self._search_engine = GlobalSearch(
            model=self._model,
            context_builder=context_builder,
            token_encoder=token_encoder,
            max_data_tokens=context_config.max_data_tokens,
            map_llm_params=map_llm_params,
            reduce_llm_params=reduce_llm_params,
            allow_general_knowledge=mapreduce_config.allow_general_knowledge,
            json_mode=mapreduce_config.json_mode,
            context_builder_params=context_builder_params,
            concurrent_coroutines=32,
            response_type=mapreduce_config.response_type,
        )

    async def run(self, args: GlobalSearchToolArgs, cancellation_token: CancellationToken) -> GlobalSearchToolReturn:
        search_result = await self._search_engine.search(args.query)
        assert isinstance(search_result.response, str), "Expected response to be a string"
        return GlobalSearchToolReturn(answer=search_result.response)

    @classmethod
    def from_settings(cls, root_dir: str | Path, config_filepath: str | Path | None = None) -> "GlobalSearchTool":
        """Create a GlobalSearchTool instance from GraphRAG settings file.

        Args:
            root_dir: Path to the GraphRAG root directory
            config_filepath: Path to the GraphRAG settings file (optional)

        Returns:
            An initialized GlobalSearchTool instance
        """
        # Load GraphRAG config
        if isinstance(root_dir, str):
            root_dir = Path(root_dir)
        if isinstance(config_filepath, str):
            config_filepath = Path(config_filepath)
        config = load_config(root_dir=root_dir, config_filepath=config_filepath)

        # Get the language model configuration from the models section
        chat_model_config = config.models.get(defs.DEFAULT_CHAT_MODEL_ID)

        if chat_model_config is None:
            raise ValueError("default_chat_model not found in config.models")

        # Initialize token encoder based on the model being used
        try:
            token_encoder = tiktoken.encoding_for_model(chat_model_config.model)
        except KeyError:
            # Fallback to cl100k_base if model is not recognized by tiktoken
            token_encoder = tiktoken.get_encoding("cl100k_base")

        # Create the LLM using ModelManager
        model = ModelManager().get_or_create_chat_model(
            name="global_search_model",
            model_type=chat_model_config.type,
            config=chat_model_config,
        )

        # Create data config from storage paths
        data_config = DataConfig(
            input_dir=str(config.output.base_dir),
        )

        return cls(
            token_encoder=token_encoder,
            model=model,
            data_config=data_config,
            context_config=_default_context_config,
            mapreduce_config=_default_mapreduce_config,
        )


# From graphrag/_config.py
class DataConfig(BaseModel):
    input_dir: str
    entity_table: str = "entities"
    entity_embedding_table: str = "entities"
    community_table: str = "communities"
    community_level: int = 2

# From graphrag/_config.py
class GlobalDataConfig(DataConfig):
    community_report_table: str = "community_reports"

# From graphrag/_config.py
class LocalDataConfig(DataConfig):
    relationship_table: str = "relationships"
    text_unit_table: str = "text_units"

# From graphrag/_config.py
class ContextConfig(BaseModel):
    max_data_tokens: int = 8000

# From graphrag/_config.py
class GlobalContextConfig(ContextConfig):
    use_community_summary: bool = False
    shuffle_data: bool = True
    include_community_rank: bool = True
    min_community_rank: int = 0
    community_rank_name: str = "rank"
    include_community_weight: bool = True
    community_weight_name: str = "occurrence weight"
    normalize_community_weight: bool = True
    max_data_tokens: int = 12000

# From graphrag/_config.py
class LocalContextConfig(ContextConfig):
    text_unit_prop: float = 0.5
    community_prop: float = 0.25
    include_entity_rank: bool = True
    rank_description: str = "number of relationships"
    include_relationship_weight: bool = True
    relationship_ranking_attribute: str = "rank"

# From graphrag/_config.py
class MapReduceConfig(BaseModel):
    map_max_tokens: int = 1000
    map_temperature: float = 0.0
    reduce_max_tokens: int = 2000
    reduce_temperature: float = 0.0
    allow_general_knowledge: bool = False
    json_mode: bool = False
    response_type: str = "multiple paragraphs"

# From graphrag/_config.py
class SearchConfig(BaseModel):
    max_tokens: int = 1500
    temperature: float = 0.0
    response_type: str = "multiple paragraphs"

from langchain_core.tools import BaseTool

# From langchain/_langchain_adapter.py
class LangChainToolAdapter(BaseTool[BaseModel, Any]):
    """Allows you to wrap a LangChain tool and make it available to AutoGen.

    .. note::

        This class requires the :code:`langchain` extra for the :code:`autogen-ext` package.

        .. code-block:: bash

            pip install -U "autogen-ext[langchain]"


    Args:
        langchain_tool (LangChainTool): A LangChain tool to wrap

    Examples:

        Use the `PythonAstREPLTool` from the `langchain_experimental` package to
        create a tool that allows you to interact with a Pandas DataFrame.

        .. code-block:: python

            import asyncio
            import pandas as pd
            from langchain_experimental.tools.python.tool import PythonAstREPLTool
            from autogen_ext.tools.langchain import LangChainToolAdapter
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_agentchat.messages import TextMessage
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.ui import Console
            from autogen_core import CancellationToken


            async def main() -> None:
                df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")  # type: ignore
                tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
                model_client = OpenAIChatCompletionClient(model="gpt-4o")
                agent = AssistantAgent(
                    "assistant",
                    tools=[tool],
                    model_client=model_client,
                    system_message="Use the `df` variable to access the dataset.",
                )
                await Console(
                    agent.on_messages_stream(
                        [TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
                    )
                )


            asyncio.run(main())

        This example demonstrates how to use the `SQLDatabaseToolkit` from the `langchain_community`
        package to interact with an SQLite database.
        It uses the :class:`~autogen_agentchat.team.RoundRobinGroupChat` to iterate the single agent over multiple steps.
        If you want to one step at a time, you can just call `run_stream` method of the
        :class:`~autogen_agentchat.agents.AssistantAgent` class directly.

        .. code-block:: python

            import asyncio
            import sqlite3

            import requests
            from autogen_agentchat.agents import AssistantAgent
            from autogen_agentchat.conditions import TextMentionTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_agentchat.ui import Console
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.tools.langchain import LangChainToolAdapter
            from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
            from langchain_community.utilities.sql_database import SQLDatabase
            from langchain_openai import ChatOpenAI
            from sqlalchemy import Engine, create_engine
            from sqlalchemy.pool import StaticPool


            def get_engine_for_chinook_db() -> Engine:
                url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
                response = requests.get(url)
                sql_script = response.text
                connection = sqlite3.connect(":memory:", check_same_thread=False)
                connection.executescript(sql_script)
                return create_engine(
                    "sqlite://",
                    creator=lambda: connection,
                    poolclass=StaticPool,
                    connect_args={"check_same_thread": False},
                )


            async def main() -> None:
                # Create the engine and database wrapper.
                engine = get_engine_for_chinook_db()
                db = SQLDatabase(engine)

                # Create the toolkit.
                llm = ChatOpenAI(temperature=0)
                toolkit = SQLDatabaseToolkit(db=db, llm=llm)

                # Create the LangChain tool adapter for every tool in the toolkit.
                tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]

                # Create the chat completion client.
                model_client = OpenAIChatCompletionClient(model="gpt-4o")

                # Create the assistant agent.
                agent = AssistantAgent(
                    "assistant",
                    model_client=model_client,
                    tools=tools,  # type: ignore
                    model_client_stream=True,
                    system_message="Respond with 'TERMINATE' if the task is completed.",
                )

                # Create termination condition.
                termination = TextMentionTermination("TERMINATE")

                # Create a round-robin group chat to iterate the single agent over multiple steps.
                chat = RoundRobinGroupChat([agent], termination_condition=termination)

                # Run the chat.
                await Console(chat.run_stream(task="Show some tables in the database"))


            if __name__ == "__main__":
                asyncio.run(main())

    """

    def __init__(self, langchain_tool: LangChainTool):
        self._langchain_tool: LangChainTool = langchain_tool

        # Extract name and description
        name = self._langchain_tool.name
        description = self._langchain_tool.description or ""

        # Determine the callable method
        if hasattr(self._langchain_tool, "func") and callable(self._langchain_tool.func):  # type: ignore
            assert self._langchain_tool.func is not None  # type: ignore
            self._callable: Callable[..., Any] = self._langchain_tool.func  # type: ignore
        elif hasattr(self._langchain_tool, "_run") and callable(self._langchain_tool._run):  # type: ignore
            self._callable: Callable[..., Any] = self._langchain_tool._run  # type: ignore
        else:
            raise AttributeError(
                f"The provided LangChain tool '{name}' does not have a callable 'func' or '_run' method."
            )

        # Determine args_type
        if self._langchain_tool.args_schema:  # pyright: ignore
            args_type = self._langchain_tool.args_schema  # pyright: ignore
        else:
            # Infer args_type from the callable's signature
            sig = inspect.signature(cast(Callable[..., Any], self._callable))  # type: ignore
            fields = {
                k: (v.annotation, Field(...))
                for k, v in sig.parameters.items()
                if k != "self" and v.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)
            }
            args_type = create_model(f"{name}Args", **fields)  # type: ignore
            # Note: type ignore is used due to a LangChain typing limitation

        # Ensure args_type is a subclass of BaseModel
        if not issubclass(args_type, BaseModel):
            raise ValueError(f"Failed to create a valid Pydantic v2 model for {name}")

        # Assume return_type as Any if not specified
        return_type: Type[Any] = object

        super().__init__(args_type, return_type, name, description)

    async def run(self, args: BaseModel, cancellation_token: CancellationToken) -> Any:
        # Prepare arguments
        kwargs = args.model_dump()

        # Determine if the callable is asynchronous
        if inspect.iscoroutinefunction(self._callable):
            return await self._callable(**kwargs)
        else:
            # Run in a thread to avoid blocking the event loop
            return await asyncio.to_thread(self._call_sync, kwargs)

    def _call_sync(self, kwargs: Dict[str, Any]) -> Any:
        return self._callable(**kwargs)

import shlex
from concurrent.futures import Future
from hashlib import sha256
from typing import ParamSpec
from autogen_core.code_executor import CodeBlock
from autogen_core.code_executor import CodeExecutor
from docker.types import DeviceRequest
from _common import CommandLineCodeResult
from _common import build_python_functions_file
from _common import get_file_name_from_content
from _common import lang_to_cmd
from _common import silence_pip
from typing import Self
import asyncio_atexit
from docker.errors import NotFound
from docker.models.containers import Container

# From docker/_docker_code_executor.py
class DockerCommandLineCodeExecutorConfig(BaseModel):
    """Configuration for DockerCommandLineCodeExecutor"""

    image: str = "python:3-slim"
    container_name: Optional[str] = None
    timeout: int = 60
    work_dir: Optional[str] = None
    bind_dir: Optional[str] = None
    auto_remove: bool = True
    stop_container: bool = True
    functions_module: str = "functions"
    extra_volumes: Dict[str, Dict[str, str]] = {}
    extra_hosts: Dict[str, str] = {}
    init_command: Optional[str] = None
    delete_tmp_files: bool = False

# From docker/_docker_code_executor.py
class DockerCommandLineCodeExecutor(CodeExecutor, Component[DockerCommandLineCodeExecutorConfig]):
    """Executes code through a command line environment in a Docker container.

    .. note::

        This class requires the :code:`docker` extra for the :code:`autogen-ext` package:

        .. code-block:: bash

            pip install "autogen-ext[docker]"


    The executor first saves each code block in a file in the working
    directory, and then executes the code file in the container.
    The executor executes the code blocks in the order they are received.
    Currently, the executor only supports Python and shell scripts.
    For Python code, use the language "python" for the code block.
    For shell scripts, use the language "bash", "shell", "sh", "pwsh", "powershell", or "ps1" for the code block.

    Args:
        image (_type_, optional): Docker image to use for code execution.
            Defaults to "python:3-slim".
        container_name (Optional[str], optional): Name of the Docker container
            which is created. If None, will autogenerate a name. Defaults to None.
        timeout (int, optional): The timeout for code execution. Defaults to 60.
        work_dir (Union[Path, str], optional): The working directory for the code
            execution. Defaults to temporary directory.
        bind_dir (Union[Path, str], optional): The directory that will be bound
        to the code executor container. Useful for cases where you want to spawn
        the container from within a container. Defaults to work_dir.
        auto_remove (bool, optional): If true, will automatically remove the Docker
            container when it is stopped. Defaults to True.
        stop_container (bool, optional): If true, will automatically stop the
            container when stop is called, when the context manager exits or when
            the Python process exits with atext. Defaults to True.
        device_requests (Optional[List[DeviceRequest]], optional): A list of device request instances to add to the container for exposing GPUs (e.g., [docker.types.DeviceRequest(count=-1, capabilities=[['gpu']])]). Defaults to None.
        functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]): A list of functions that are available to the code executor. Default is an empty list.
        functions_module (str, optional): The name of the module that will be created to store the functions. Defaults to "functions".
        extra_volumes (Optional[Dict[str, Dict[str, str]]], optional): A dictionary of extra volumes (beyond the work_dir) to mount to the container;
            key is host source path and value 'bind' is the container path. See  Defaults to None.
            Example: extra_volumes = {'/home/user1/': {'bind': '/mnt/vol2', 'mode': 'rw'}, '/var/www': {'bind': '/mnt/vol1', 'mode': 'ro'}}
        extra_hosts (Optional[Dict[str, str]], optional): A dictionary of host mappings to add to the container. (See Docker docs on extra_hosts) Defaults to None.
            Example: extra_hosts = {"kubernetes.docker.internal": "host-gateway"}
        init_command (Optional[str], optional): A shell command to run before each shell operation execution. Defaults to None.
            Example: init_command="kubectl config use-context docker-hub"
        delete_tmp_files (bool, optional): If true, will delete temporary files after execution. Defaults to False.

    .. note::
        Using the current directory (".") as working directory is deprecated. Using it will raise a deprecation warning.

    """

    component_config_schema = DockerCommandLineCodeExecutorConfig
    component_provider_override = "autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor"

    SUPPORTED_LANGUAGES: ClassVar[List[str]] = [
        "bash",
        "shell",
        "sh",
        "pwsh",
        "powershell",
        "ps1",
        "python",
    ]

    FUNCTION_PROMPT_TEMPLATE: ClassVar[
        str
    ] = """You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.

For example, if there was a function called `foo` you could import it by writing `from $module_name import foo`

$functions"""

    def __init__(
        self,
        image: str = "python:3-slim",
        container_name: Optional[str] = None,
        *,
        timeout: int = 60,
        work_dir: Union[Path, str, None] = None,
        bind_dir: Optional[Union[Path, str]] = None,
        auto_remove: bool = True,
        stop_container: bool = True,
        device_requests: Optional[List[DeviceRequest]] = None,
        functions: Sequence[
            Union[
                FunctionWithRequirements[Any, A],
                Callable[..., Any],
                FunctionWithRequirementsStr,
            ]
        ] = [],
        functions_module: str = "functions",
        extra_volumes: Optional[Dict[str, Dict[str, str]]] = None,
        extra_hosts: Optional[Dict[str, str]] = None,
        init_command: Optional[str] = None,
        delete_tmp_files: bool = False,
    ):
        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        # Handle working directory logic
        if work_dir is None:
            self._work_dir = None
        else:
            if isinstance(work_dir, str):
                work_dir = Path(work_dir)
            # Emit a deprecation warning if the user is using the current directory as working directory
            if work_dir.resolve() == Path.cwd().resolve():
                warnings.warn(
                    "Using the current directory as work_dir is deprecated.",
                    DeprecationWarning,
                    stacklevel=2,
                )
            self._work_dir = work_dir
            # Create the working directory if it doesn't exist
            self._work_dir.mkdir(exist_ok=True, parents=True)

        if container_name is None:
            self.container_name = f"autogen-code-exec-{uuid.uuid4()}"
        else:
            self.container_name = container_name

        self._timeout = timeout

        # Handle bind_dir
        self._bind_dir: Optional[Path] = None
        if bind_dir is not None:
            self._bind_dir = Path(bind_dir) if isinstance(bind_dir, str) else bind_dir
        else:
            self._bind_dir = self._work_dir  # Default to work_dir if not provided

        # Track temporary directory
        self._temp_dir: Optional[tempfile.TemporaryDirectory[str]] = None
        self._temp_dir_path: Optional[Path] = None

        self._started = False

        self._auto_remove = auto_remove
        self._stop_container = stop_container
        self._image = image

        if not functions_module.isidentifier():
            raise ValueError("Module name must be a valid Python identifier")

        self._functions_module = functions_module
        self._functions = functions
        self._extra_volumes = extra_volumes if extra_volumes is not None else {}
        self._extra_hosts = extra_hosts if extra_hosts is not None else {}
        self._init_command = init_command
        self._delete_tmp_files = delete_tmp_files
        self._device_requests = device_requests

        # Setup could take some time so we intentionally wait for the first code block to do it.
        if len(functions) > 0:
            self._setup_functions_complete = False
        else:
            self._setup_functions_complete = True

        self._container: Container | None = None
        self._running = False

        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._cancellation_futures: List[ConcurrentFuture[None]] = []

    @property
    def timeout(self) -> int:
        """(Experimental) The timeout for code execution."""
        return self._timeout

    async def _setup_functions(self, cancellation_token: CancellationToken) -> None:
        func_file_content = build_python_functions_file(self._functions)
        func_file = self.work_dir / f"{self._functions_module}.py"
        func_file.write_text(func_file_content)

        # Collect requirements
        lists_of_packages = [x.python_packages for x in self._functions if isinstance(x, FunctionWithRequirements)]
        flattened_packages = [item for sublist in lists_of_packages for item in sublist]
        required_packages = list(set(flattened_packages))
        if len(required_packages) > 0:
            logging.info("Ensuring packages are installed in executor.")

            packages = shlex.join(required_packages)

            result = await self._execute_code_dont_check_setup(
                [CodeBlock(code=f"python -m pip install {packages}", language="sh")], cancellation_token
            )

            if result.exit_code != 0:
                stdout = result.output
                stderr = result.output
                raise ValueError(f"Pip install failed. {stdout}, {stderr}")

        # Attempt to load the function file to check for syntax errors, imports etc.
        exec_result = await self._execute_code_dont_check_setup(
            [CodeBlock(code=func_file_content, language="python")], cancellation_token
        )

        if exec_result.exit_code != 0:
            raise ValueError(f"Functions failed to load: {exec_result.output}")

        self._setup_functions_complete = True

    async def _kill_running_command(self, command: List[str]) -> None:
        if self._container is None or not self._running:
            return
        await asyncio.to_thread(self._container.exec_run, ["pkill", "-f", " ".join(command)])

    async def _execute_command(self, command: List[str], cancellation_token: CancellationToken) -> Tuple[str, int]:
        if self._container is None or not self._running:
            raise ValueError("Container is not running. Must first be started with either start or a context manager.")

        exec_task = asyncio.create_task(asyncio.to_thread(self._container.exec_run, command))
        cancellation_token.link_future(exec_task)

        # Wait for the exec task to finish.
        try:
            result = await exec_task
            exit_code = result.exit_code
            output = result.output.decode("utf-8")
            if exit_code == 124:
                output += "\n Timeout"
            return output, exit_code
        except asyncio.CancelledError:
            # Schedule a task to kill the running command in the background.
            if self._loop and not self._loop.is_closed():
                try:
                    logging.debug(f"Scheduling kill command via run_coroutine_threadsafe on loop {self._loop!r}")
                    future: ConcurrentFuture[None] = asyncio.run_coroutine_threadsafe(
                        self._kill_running_command(command), self._loop
                    )
                    self._cancellation_futures.append(future)
                    logging.debug(f"Kill command scheduled, future: {future!r}")
                except RuntimeError as e:
                    logging.error(f"Failed to schedule kill command on loop {self._loop!r}: {e}")
                except Exception as e:
                    logging.exception(f"Unexpected error scheduling kill command: {e}")
            else:
                logging.warning(
                    f"Cannot schedule kill command: Executor loop is not available or closed (loop: {self._loop!r})."
                )
            return "Code execution was cancelled.", 1

    async def _execute_code_dont_check_setup(
        self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken
    ) -> CommandLineCodeResult:
        if self._container is None or not self._running:
            raise ValueError("Container is not running. Must first be started with either start or a context manager.")

        if len(code_blocks) == 0:
            raise ValueError("No code blocks to execute.")

        outputs: List[str] = []
        files: List[Path] = []
        last_exit_code = 0
        try:
            for code_block in code_blocks:
                lang = code_block.language.lower()
                code = silence_pip(code_block.code, lang)

                # Check if there is a filename comment
                try:
                    filename = get_file_name_from_content(code, self.work_dir)
                except ValueError:
                    outputs.append("Filename is not in the workspace")
                    last_exit_code = 1
                    break

                if not filename:
                    filename = f"tmp_code_{sha256(code.encode()).hexdigest()}.{lang}"

                code_path = self.work_dir / filename
                with code_path.open("w", encoding="utf-8") as fout:
                    fout.write(code)
                files.append(code_path)

                command = ["timeout", str(self._timeout), lang_to_cmd(lang), filename]

                output, exit_code = await self._execute_command(command, cancellation_token)
                outputs.append(output)
                last_exit_code = exit_code
                if exit_code != 0:
                    break
        finally:
            if self._delete_tmp_files:
                for file in files:
                    try:
                        file.unlink()
                    except (OSError, FileNotFoundError):
                        pass

        code_file = str(files[0]) if files else None
        return CommandLineCodeResult(exit_code=last_exit_code, output="".join(outputs), code_file=code_file)

    @property
    def work_dir(self) -> Path:
        # If a user specifies a working directory, use that
        if self._work_dir is not None:
            # If a user specifies the current directory, warn them that this is deprecated
            if self._work_dir == Path("."):
                warnings.warn(
                    "Using the current directory as work_dir is deprecated.",
                    DeprecationWarning,
                    stacklevel=2,
                )
            return self._work_dir
        # If a user does not specify a working directory, use the default directory (tempfile.TemporaryDirectory)
        elif self._temp_dir is not None:
            return Path(self._temp_dir.name)
        else:
            raise RuntimeError("Working directory not properly initialized")

    @property
    def bind_dir(self) -> Path:
        # If the user specified a bind directory, return it
        if self._bind_dir is not None:
            return self._bind_dir
        # Otherwise bind_dir is set to the current work_dir as default
        else:
            return self.work_dir

    async def execute_code_blocks(
        self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken
    ) -> CommandLineCodeResult:
        """(Experimental) Execute the code blocks and return the result.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.

        Returns:
            CommandlineCodeResult: The result of the code execution."""

        if not self._setup_functions_complete:
            await self._setup_functions(cancellation_token)

        return await self._execute_code_dont_check_setup(code_blocks, cancellation_token)

    async def restart(self) -> None:
        """(Experimental) Restart the Docker container code executor."""
        if self._container is None or not self._running:
            raise ValueError("Container is not running. Must first be started with either start or a context manager.")

        await asyncio.to_thread(self._container.restart)  # type: ignore
        if self._container.status != "running":
            self._running = False
            logs_str = self._container.logs().decode("utf-8")
            raise ValueError(f"Failed to restart container. Logs: {logs_str}")

    async def stop(self) -> None:
        """(Experimental) Stop the code executor.

        Stops the Docker container and cleans up any temporary files (if they were created), along with the temporary directory.
        The method first waits for all cancellation tasks to finish before stopping the container. Finally it marks the executor as not running.
        If the container is not running, the method does nothing.
        """
        if not self._running:
            return

        if self._temp_dir is not None:
            self._temp_dir.cleanup()
            self._temp_dir = None

        client = docker.from_env()
        try:
            try:
                container = await asyncio.to_thread(client.containers.get, self.container_name)
            except NotFound:
                logging.debug(f"Container {self.container_name} not found during stop...")
                self._running = False
                self._cancellation_futures.clear()
                return

            if self._cancellation_futures:
                if not self._loop or self._loop.is_closed():
                    logging.warning(
                        f"Executor loop ({self._loop!r}) is closed or unavailable. Cannot reliably wait for "
                        f"{len(self._cancellation_futures)} cancellation futures."
                    )
                    self._cancellation_futures.clear()
                else:
                    # concurrent.futures.Future -> asyncio.Future
                    asyncio_futures = [asyncio.wrap_future(f, loop=self._loop) for f in self._cancellation_futures]

                    if asyncio_futures:
                        logging.debug(
                            f"Waiting for {len(asyncio_futures)} cancellation futures to complete on loop {self._loop!r}..."
                        )
                        results = await asyncio.gather(*asyncio_futures, return_exceptions=True)
                        for i, result in enumerate(results):
                            original_future = self._cancellation_futures[i]
                            if isinstance(result, Exception):
                                logging.warning(f"Cancellation future {original_future!r} failed: {result}")
                            else:
                                logging.debug(f"Cancellation future {original_future!r} completed successfully.")
                    else:
                        logging.debug("No valid cancellation futures to await.")

                    self._cancellation_futures.clear()

            logging.debug(f"Stopping container {self.container_name}...")
            await asyncio.to_thread(container.stop)
            logging.debug(f"Container {self.container_name} stopped.")

        except DockerException as e:
            logging.error(f"Docker error while stopping container {self.container_name}: {e}")
        except Exception as e:
            logging.exception(f"Unexpected error during stop operation for container {self.container_name}: {e}")
        finally:
            self._running = False
            self._cancellation_futures.clear()

    async def start(self) -> None:
        """(Experimental) Start the code executor.

        This method sets the working environment variables, connects to Docker and starts the code executor.
        If no working directory was provided to the code executor, it creates a temporary directory and sets it as the code executor working directory.
        """

        if self._work_dir is None and self._temp_dir is None:
            self._temp_dir = tempfile.TemporaryDirectory()
            self._temp_dir_path = Path(self._temp_dir.name)
            self._temp_dir_path.mkdir(exist_ok=True)

        # Start a container from the image, read to exec commands later
        try:
            client = docker.from_env()
        except DockerException as e:
            if "FileNotFoundError" in str(e):
                raise RuntimeError("Failed to connect to Docker. Please ensure Docker is installed and running.") from e
            raise
        except Exception as e:
            raise RuntimeError(f"Unexpected error while connecting to Docker: {str(e)}") from e

        # Check if the image exists
        try:
            await asyncio.to_thread(client.images.get, self._image)
        except ImageNotFound:
            # TODO logger
            logging.info(f"Pulling image {self._image}...")
            # Let the docker exception escape if this fails.
            await asyncio.to_thread(client.images.pull, self._image)

        # Prepare the command (if needed)
        shell_command = "/bin/sh"
        command = ["-c", f"{(self._init_command)};exec {shell_command}"] if self._init_command else None

        # Check if a container with the same name already exists and remove it
        try:
            existing_container = await asyncio.to_thread(client.containers.get, self.container_name)
            await asyncio.to_thread(existing_container.remove, force=True)
        except NotFound:
            pass

        self._container = await asyncio.to_thread(
            client.containers.create,
            self._image,
            name=self.container_name,
            entrypoint=shell_command,
            command=command,
            tty=True,
            detach=True,
            auto_remove=self._auto_remove,
            volumes={str(self.bind_dir.resolve()): {"bind": "/workspace", "mode": "rw"}, **self._extra_volumes},
            working_dir="/workspace",
            extra_hosts=self._extra_hosts,
            device_requests=self._device_requests,
        )
        await asyncio.to_thread(self._container.start)

        await _wait_for_ready(self._container)

        async def cleanup() -> None:
            await self.stop()
            asyncio_atexit.unregister(cleanup)  # type: ignore

        if self._stop_container:
            asyncio_atexit.register(cleanup)  # type: ignore

        # Check if the container is running
        if self._container.status != "running":
            logs_str = self._container.logs().decode("utf-8")
            raise ValueError(f"Failed to start container from image {self._image}. Logs: {logs_str}")

        self._loop = asyncio.get_running_loop()
        self._cancellation_futures = []
        logging.debug(f"Executor started, associated with event loop: {self._loop!r}")

        self._running = True

    def _to_config(self) -> DockerCommandLineCodeExecutorConfig:
        """(Experimental) Convert the component to a config object."""
        if self._functions:
            logging.info("Functions will not be included in serialized configuration")

        return DockerCommandLineCodeExecutorConfig(
            image=self._image,
            container_name=self.container_name,
            timeout=self._timeout,
            work_dir=str(self._work_dir) if self._work_dir else None,
            bind_dir=str(self._bind_dir) if self._bind_dir else None,
            auto_remove=self._auto_remove,
            stop_container=self._stop_container,
            functions_module=self._functions_module,
            extra_volumes=self._extra_volumes,
            extra_hosts=self._extra_hosts,
            init_command=self._init_command,
            delete_tmp_files=self._delete_tmp_files,
        )

    @classmethod
    def _from_config(cls, config: DockerCommandLineCodeExecutorConfig) -> Self:
        """(Experimental) Create a component from a config object."""

        return cls(
            image=config.image,
            container_name=config.container_name,
            timeout=config.timeout,
            work_dir=Path(config.work_dir) if config.work_dir else None,
            bind_dir=Path(config.bind_dir) if config.bind_dir else None,
            auto_remove=config.auto_remove,
            stop_container=config.stop_container,
            functions=[],  # Functions not restored from config
            functions_module=config.functions_module,
            extra_volumes=config.extra_volumes,
            extra_hosts=config.extra_hosts,
            init_command=config.init_command,
            delete_tmp_files=config.delete_tmp_files,
        )

# From docker/_docker_code_executor.py
def timeout(self) -> int:
        """(Experimental) The timeout for code execution."""
        return self._timeout

# From docker/_docker_code_executor.py
def work_dir(self) -> Path:
        # If a user specifies a working directory, use that
        if self._work_dir is not None:
            # If a user specifies the current directory, warn them that this is deprecated
            if self._work_dir == Path("."):
                warnings.warn(
                    "Using the current directory as work_dir is deprecated.",
                    DeprecationWarning,
                    stacklevel=2,
                )
            return self._work_dir
        # If a user does not specify a working directory, use the default directory (tempfile.TemporaryDirectory)
        elif self._temp_dir is not None:
            return Path(self._temp_dir.name)
        else:
            raise RuntimeError("Working directory not properly initialized")

# From docker/_docker_code_executor.py
def bind_dir(self) -> Path:
        # If the user specified a bind directory, return it
        if self._bind_dir is not None:
            return self._bind_dir
        # Otherwise bind_dir is set to the current work_dir as default
        else:
            return self.work_dir

from autogen_ext.code_executors._common import silence_pip
from _jupyter_server import JupyterClient
from _jupyter_server import JupyterConnectable
from _jupyter_server import JupyterConnectionInfo
from _jupyter_server import JupyterKernelClient

# From docker_jupyter/_docker_jupyter.py
class DockerJupyterCodeResult(CodeResult):
    """(Experimental) A code result class for IPython code executor."""

    output_files: list[Path]

# From docker_jupyter/_docker_jupyter.py
class DockerJupyterCodeExecutorConfig(BaseModel):
    """Configuration for JupyterCodeExecutor"""

    jupyter_server: Union[JupyterConnectable, JupyterConnectionInfo]
    kernel_name: str = "python3"
    timeout: int = 60
    output_dir: Optional[Union[Path, str]] = None

    class Config:
        arbitrary_types_allowed = True

# From docker_jupyter/_docker_jupyter.py
class DockerJupyterCodeExecutor(CodeExecutor, Component[DockerJupyterCodeExecutorConfig]):
    """(Experimental) A code executor class that executes code statefully using
    a Jupyter server supplied to this class.

    Each execution is stateful and can access variables created from previous
    executions in the same session.

    To use this, you need to install the following dependencies:

    .. code-block:: shell

        pip install "autogen-ext[docker-jupyter-executor]"

    Args:
        jupyter_server (Union[JupyterConnectable, JupyterConnectionInfo]): The Jupyter server to use.
        kernel_name (str): The kernel name to use. Make sure it is installed.
            By default, it is "python3".
        timeout (int): The timeout for code execution, by default 60.
        output_dir (str): The directory to save output files, by default None.

    Example of using it directly:

    .. code-block:: python

        import asyncio
        from autogen_core import CancellationToken
        from autogen_core.code_executor import CodeBlock
        from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


        async def main() -> None:
            async with DockerJupyterServer() as jupyter_server:
                async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
                    code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
                    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
                    print(code_result)


        asyncio.run(main())

    Example of using it with your own jupyter image:

    .. code-block:: python

        import asyncio
        from autogen_core import CancellationToken
        from autogen_core.code_executor import CodeBlock
        from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


        async def main() -> None:
            async with DockerJupyterServer(custom_image_name="your_custom_images_name", expose_port=8888) as jupyter_server:
                async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
                    code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
                    code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
                    print(code_result)


        asyncio.run(main())

    Example of using it with :class:`~autogen_ext.tools.code_execution.PythonCodeExecutionTool`:

    .. code-block:: python

        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
        from autogen_ext.models.openai import OpenAIChatCompletionClient
        from autogen_ext.tools.code_execution import PythonCodeExecutionTool


        async def main() -> None:
            async with DockerJupyterServer() as jupyter_server:
                async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
                    tool = PythonCodeExecutionTool(executor)
                    model_client = OpenAIChatCompletionClient(model="gpt-4o")
                    agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
                    result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
                    print(result)


        asyncio.run(main())

    Example of using it inside a :class:`~autogen_agentchat.agents._code_executor_agent.CodeExecutorAgent`:

    .. code-block:: python

        import asyncio
        from autogen_agentchat.agents import CodeExecutorAgent
        from autogen_agentchat.messages import TextMessage
        from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
        from autogen_core import CancellationToken


        async def main() -> None:
            async with DockerJupyterServer() as jupyter_server:
                async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
                    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
                    task = TextMessage(
                        content='''Here is some code
                ```python
                print('Hello world')
                ```
                ''',
                        source="user",
                    )
                    response = await code_executor_agent.on_messages([task], CancellationToken())
                    print(response.chat_message)


        asyncio.run(main())

    """

    component_config_schema = DockerJupyterCodeExecutorConfig
    component_provider_override = "autogen_ext.code_executors.docker_jupyter.DockerJupyterCodeExecutor"

    def __init__(
        self,
        jupyter_server: Union[JupyterConnectable, JupyterConnectionInfo],
        kernel_name: str = "python3",
        timeout: int = 60,
        output_dir: Path | None = None,
    ):
        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        if isinstance(jupyter_server, JupyterConnectable):
            self._connection_info = jupyter_server.connection_info
        elif isinstance(jupyter_server, JupyterConnectionInfo):
            self._connection_info = jupyter_server
        else:
            raise ValueError("jupyter_server must be a JupyterConnectable or JupyterConnectionInfo.")

        self._output_dir = output_dir or getattr(jupyter_server, "_bind_dir", None)
        if not self._output_dir:
            with tempfile.TemporaryDirectory() as temp_dir:
                self._output_dir = Path(temp_dir)
                self._output_dir.mkdir(exist_ok=True)

        self._jupyter_client = JupyterClient(self._connection_info)

        self._kernel_name = kernel_name
        self._timeout = timeout
        self._async_jupyter_kernel_client: Optional[JupyterKernelClient] = None
        self._kernel_id: Optional[str] = None

    async def _ensure_async_kernel_client(self) -> JupyterKernelClient:
        """Ensure that an async kernel client exists and return it."""
        if self._kernel_id is None:
            await self.start()
            assert self._kernel_id is not None
        if self._async_jupyter_kernel_client is None:
            self._async_jupyter_kernel_client = await self._jupyter_client.get_kernel_client(self._kernel_id)
        return self._async_jupyter_kernel_client

    async def execute_code_blocks(
        self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken
    ) -> DockerJupyterCodeResult:
        """(Experimental) Execute a list of code blocks and return the result.

        This method executes a list of code blocks as cells in the Jupyter kernel.
        See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
        for the message protocol.

        Args:
            code_blocks (List[CodeBlock]): A list of code blocks to execute.

        Returns:
            DockerJupyterCodeResult: The result of the code execution.
        """
        kernel_client = await self._ensure_async_kernel_client()
        # Wait for kernel to be ready using async client
        is_ready = await kernel_client.wait_for_ready(timeout_seconds=self._timeout)
        if not is_ready:
            return DockerJupyterCodeResult(exit_code=1, output="ERROR: Kernel not ready", output_files=[])

        outputs: List[str] = []
        output_files: List[Path] = []
        for code_block in code_blocks:
            code = silence_pip(code_block.code, code_block.language)
            # Execute code using async client
            exec_task = asyncio.create_task(kernel_client.execute(code, timeout_seconds=self._timeout))
            cancellation_token.link_future(exec_task)
            result = await exec_task
            if result.is_ok:
                outputs.append(result.output)
                for data in result.data_items:
                    if data.mime_type == "image/png":
                        path = self._save_image(data.data)
                        outputs.append(path)
                        output_files.append(Path(path))
                    elif data.mime_type == "text/html":
                        path = self._save_html(data.data)
                        outputs.append(path)
                        output_files.append(Path(path))
                    else:
                        outputs.append(json.dumps(data.data))
            else:
                existing_output = "\n".join([str(output) for output in outputs])
                return DockerJupyterCodeResult(
                    exit_code=1, output=existing_output + "\nERROR: " + result.output, output_files=output_files
                )
        return DockerJupyterCodeResult(
            exit_code=0, output="\n".join([str(output) for output in outputs]), output_files=output_files
        )

    async def restart(self) -> None:
        """(Experimental) Restart a new session."""
        # Use async client to restart kernel
        if self._kernel_id is not None:
            await self._jupyter_client.restart_kernel(self._kernel_id)
        # Reset the clients to force recreation
        if self._async_jupyter_kernel_client is not None:
            await self._async_jupyter_kernel_client.stop()
            self._async_jupyter_kernel_client = None

    async def start(self) -> None:
        """(Experimental) Start a new session."""
        available_kernels = await self._jupyter_client.list_kernel_specs()
        if self._kernel_name not in available_kernels["kernelspecs"]:
            raise ValueError(f"Kernel {self._kernel_name} is not installed.")
        self._kernel_id = await self._jupyter_client.start_kernel(self._kernel_name)

    def _save_image(self, image_data_base64: str) -> str:
        """Save image data to a file."""
        image_data = base64.b64decode(image_data_base64)
        filename = f"{uuid.uuid4().hex}.png"
        path = os.path.join(str(self._output_dir), filename)
        with open(path, "wb") as f:
            f.write(image_data)
        return os.path.abspath(path)

    def _save_html(self, html_data: str) -> str:
        """Save html data to a file."""
        filename = f"{uuid.uuid4().hex}.html"
        path = os.path.join(str(self._output_dir), filename)
        with open(path, "w") as f:
            f.write(html_data)
        return os.path.abspath(path)

    async def stop(self) -> None:
        """Stop the kernel."""
        if self._kernel_id is not None:
            await self._jupyter_client.delete_kernel(self._kernel_id)
        if self._async_jupyter_kernel_client is not None:
            await self._async_jupyter_kernel_client.stop()
            self._async_jupyter_kernel_client = None
        await self._jupyter_client.close()

    async def __aenter__(self) -> Self:
        await self.start()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: TracebackType | None,
    ) -> None:
        await self.stop()

# From docker_jupyter/_docker_jupyter.py
class Config:
        arbitrary_types_allowed = True

from string import Template
import aiohttp
from anyio import open_file
from _common import get_required_packages
from _common import to_stub
from azure.core.credentials import AccessToken

# From azure/_azure_container_code_executor.py
class TokenProvider(Protocol):
    def get_token(
        self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None, **kwargs: Any
    ) -> AccessToken: ...

# From azure/_azure_container_code_executor.py
class ACADynamicSessionsCodeExecutor(CodeExecutor):
    """(Experimental) A code executor class that executes code through a an Azure
    Container Apps Dynamic Sessions instance.

    .. note::

        This class requires the :code:`azure` extra for the :code:`autogen-ext` package:

        .. code-block:: bash

            pip install "autogen-ext[azure]"

    .. caution::

        **This will execute LLM generated code on an Azure dynamic code container.**

    The execution environment is similar to that of a jupyter notebook which allows for incremental code execution. The parameter functions are executed in order once at the beginning of each session. Each code block is then executed serially and in the order they are received. Each environment has a statically defined set of available packages which cannot be changed.
    Currently, attempting to use packages beyond what is available on the environment will result in an error. To get the list of supported packages, call the `get_available_packages` function.
    Currently the only supported language is Python.
    For Python code, use the language "python" for the code block.

    Args:
        pool_management_endpoint (str): The azure container apps dynamic sessions endpoint.
        credential (TokenProvider): An object that implements the get_token function.
        timeout (int): The timeout for the execution of any single code block. Default is 60.
        work_dir (str): The working directory for the code execution. If None,
            a default working directory will be used. The default working
            directory is a temporal directory.
        functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]): A list of functions that are available to the code executor. Default is an empty list.
        suppress_result_output bool: By default the executor will attach any result info in the execution response to the result outpu. Set this to True to prevent this.
        session_id (str): The session id for the code execution (passed to Dynamic Sessions). If None, a new session id will be generated. Default is None. Note this value will be reset when calling `restart`

    .. note::
        Using the current directory (".") as working directory is deprecated. Using it will raise a deprecation warning.
    """

    SUPPORTED_LANGUAGES: ClassVar[List[str]] = [
        "python",
    ]
    FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = """You have access to the following user defined functions.

$functions"""

    _AZURE_API_VER = "2024-02-02-preview"

    def __init__(
        self,
        pool_management_endpoint: str,
        credential: TokenProvider,
        timeout: int = 60,
        work_dir: Union[Path, str, None] = None,
        functions: Sequence[
            Union[
                FunctionWithRequirements[Any, A],
                Callable[..., Any],
                FunctionWithRequirementsStr,
            ]
        ] = [],
        functions_module: str = "functions",
        suppress_result_output: bool = False,
        session_id: Optional[str] = None,
    ):
        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        self._work_dir: Optional[Path] = None
        self._temp_dir: Optional[tempfile.TemporaryDirectory[str]] = None

        # If a user specifies a working directory, use that
        if work_dir is not None:
            if isinstance(work_dir, str):
                self._work_dir = Path(work_dir)
            else:
                self._work_dir = work_dir
            # Create the directory if it doesn't exist
            self._work_dir.mkdir(exist_ok=True, parents=True)
        # If a user does not specify a working directory, use the default directory (tempfile.TemporaryDirectory)
        else:
            self._temp_dir = tempfile.TemporaryDirectory()
            temp_dir_path = Path(self._temp_dir.name)
            temp_dir_path.mkdir(exist_ok=True, parents=True)

        self._started = False

        # Rest of initialization remains the same
        self._functions_module = functions_module
        self._timeout = timeout
        self._functions = functions
        self._func_code: Optional[str] = None

        # Setup could take some time so we intentionally wait for the first code block to do it.
        if len(functions) > 0:
            self._setup_functions_complete = False
        else:
            self._setup_functions_complete = True

        self._suppress_result_output = suppress_result_output

        self._pool_management_endpoint = pool_management_endpoint
        self._access_token: str | None = None
        self._session_id: str = session_id or str(uuid4())
        self._available_packages: set[str] | None = None
        self._credential: TokenProvider = credential
        # cwd needs to be set to /mnt/data to properly read uploaded files and download written files
        self._setup_cwd_complete = False

    # TODO: expiration?
    def _ensure_access_token(self) -> None:
        if not self._access_token:
            scope = "https://dynamicsessions.io/.default"
            self._access_token = self._credential.get_token(scope).token

    def format_functions_for_prompt(self, prompt_template: str = FUNCTION_PROMPT_TEMPLATE) -> str:
        """(Experimental) Format the functions for a prompt.

        The template includes one variable:
        - `$functions`: The functions formatted as stubs with two newlines between each function.

        Args:
            prompt_template (str): The prompt template. Default is the class default.

        Returns:
            str: The formatted prompt.
        """

        template = Template(prompt_template)
        return template.substitute(
            functions="\n\n".join([to_stub(func) for func in self._functions]),
        )

    @property
    def functions_module(self) -> str:
        """(Experimental) The module name for the functions."""
        return self._functions_module

    @property
    def functions(self) -> List[str]:
        raise NotImplementedError

    @property
    def timeout(self) -> int:
        """(Experimental) The timeout for code execution."""
        return self._timeout

    @property
    def work_dir(self) -> Path:
        # If a user specifies a working directory, use that
        if self._work_dir is not None:
            # If a user specifies the current directory, warn them that this is deprecated
            if self._work_dir == Path("."):
                warnings.warn(
                    "Using the current directory as work_dir is deprecated",
                    DeprecationWarning,
                    stacklevel=2,
                )
            return self._work_dir
        # If a user does not specify a working directory, use the default directory (tempfile.TemporaryDirectory)
        elif self._temp_dir is not None:
            return Path(self._temp_dir.name)
        else:
            raise RuntimeError("Working directory not properly initialized")

    def _construct_url(self, path: str) -> str:
        endpoint = self._pool_management_endpoint
        if not endpoint.endswith("/"):
            endpoint += "/"
        url = endpoint + f"{path}?api-version={self._AZURE_API_VER}&identifier={self._session_id}"
        return url

    async def get_available_packages(self, cancellation_token: CancellationToken) -> set[str]:
        if self._available_packages is not None:
            return self._available_packages
        avail_pkgs = """
import pkg_resources\n[d.project_name for d in pkg_resources.working_set]
"""
        ret = await self._execute_code_dont_check_setup(
            [CodeBlock(code=avail_pkgs, language="python")], cancellation_token
        )
        if ret.exit_code != 0:
            raise ValueError(f"Failed to get list of available packages: {ret.output.strip()}")
        pkgs = ret.output.strip("[]")
        pkglist = pkgs.split(",\n")
        return {pkg.strip(" '") for pkg in pkglist}

    async def _populate_available_packages(self, cancellation_token: CancellationToken) -> None:
        self._available_packages = await self.get_available_packages(cancellation_token)

    async def _setup_functions(self, cancellation_token: CancellationToken) -> None:
        if not self._func_code:
            self._func_code = build_python_functions_file(self._functions)

            # Check required function imports and packages
            lists_of_packages = [x.python_packages for x in self._functions if isinstance(x, FunctionWithRequirements)]
            # Should we also be checking the imports?

            flattened_packages = [item for sublist in lists_of_packages for item in sublist]
            required_packages = set(flattened_packages)

            if self._available_packages is None:
                await self._populate_available_packages(cancellation_token)

            if self._available_packages is not None:
                missing_pkgs = set(required_packages - self._available_packages)
                if len(missing_pkgs) > 0:
                    raise ValueError(f"Packages unavailable in environment: {missing_pkgs}")

        func_file = self.work_dir / f"{self._functions_module}.py"
        func_file.write_text(self._func_code)

        # Attempt to load the function file to check for syntax errors, imports etc.
        exec_result = await self._execute_code_dont_check_setup(
            [CodeBlock(code=self._func_code, language="python")], cancellation_token
        )

        if exec_result.exit_code != 0:
            raise ValueError(f"Functions failed to load: {exec_result.output.strip()}")

        self._setup_functions_complete = True

    async def _setup_cwd(self, cancellation_token: CancellationToken) -> None:
        # Change the cwd to /mnt/data to properly have access to uploaded files
        exec_result = await self._execute_code_dont_check_setup(
            [CodeBlock(code="import os; os.chdir('/mnt/data')", language="python")], cancellation_token
        )

        if exec_result.exit_code != 0:
            raise ValueError("Failed to set up Azure container working directory")
        self._setup_cwd_complete = True

    async def get_file_list(self, cancellation_token: CancellationToken) -> List[str]:
        self._ensure_access_token()
        timeout = aiohttp.ClientTimeout(total=float(self._timeout))
        headers = {
            "Authorization": f"Bearer {self._access_token}",
        }
        url = self._construct_url("files")
        async with aiohttp.ClientSession(timeout=timeout) as client:
            task = asyncio.create_task(
                client.get(
                    url,
                    headers=headers,
                )
            )
            cancellation_token.link_future(task)
            try:
                resp = await task
                resp.raise_for_status()
                data = await resp.json()
            except asyncio.TimeoutError as e:
                # e.add_note is only in py 3.11+
                raise asyncio.TimeoutError("Timeout getting file list") from e
            except asyncio.CancelledError as e:
                # e.add_note is only in py 3.11+
                raise asyncio.CancelledError("File list retrieval cancelled") from e
            except aiohttp.ClientResponseError as e:
                raise ConnectionError("Error while getting file list") from e

        values = data["value"]
        file_info_list: List[str] = []
        for value in values:
            file = value["properties"]
            file_info_list.append(file["filename"])
        return file_info_list

    async def upload_files(self, files: List[Union[Path, str]], cancellation_token: CancellationToken) -> None:
        self._ensure_access_token()
        # TODO: Better to use the client auth system rather than headers
        headers = {"Authorization": f"Bearer {self._access_token}"}
        url = self._construct_url("files/upload")
        timeout = aiohttp.ClientTimeout(total=float(self._timeout))
        async with aiohttp.ClientSession(timeout=timeout) as client:
            for file in files:
                file_path = self.work_dir / file
                if not file_path.is_file():
                    # TODO: what to do here?
                    raise FileNotFoundError(f"{file} does not exist")

                data = aiohttp.FormData()
                async with await open_file(file_path, "rb") as f:
                    data.add_field(
                        "file",
                        f,
                        filename=os.path.basename(file_path),
                        content_type="application/octet-stream",
                    )

                    task = asyncio.create_task(
                        client.post(
                            url,
                            headers=headers,
                            data=data,
                        )
                    )

                    cancellation_token.link_future(task)
                    try:
                        resp = await task
                        resp.raise_for_status()

                    except asyncio.TimeoutError as e:
                        # e.add_note is only in py 3.11+
                        raise asyncio.TimeoutError("Timeout uploading files") from e
                    except asyncio.CancelledError as e:
                        # e.add_note is only in py 3.11+
                        raise asyncio.CancelledError("Uploading files cancelled") from e
                    except aiohttp.ClientResponseError as e:
                        raise ConnectionError("Error while uploading files") from e

    async def download_files(self, files: List[Union[Path, str]], cancellation_token: CancellationToken) -> List[str]:
        self._ensure_access_token()
        available_files = await self.get_file_list(cancellation_token)
        # TODO: Better to use the client auth system rather than headers
        headers = {"Authorization": f"Bearer {self._access_token}"}
        timeout = aiohttp.ClientTimeout(total=float(self._timeout))
        local_paths: List[str] = []
        async with aiohttp.ClientSession(timeout=timeout) as client:
            for file in files:
                if file not in available_files:
                    # TODO: what's the right thing to do here?
                    raise FileNotFoundError(f"{file} does not exist")

                url = self._construct_url(f"files/content/{file}")

                task = asyncio.create_task(
                    client.get(
                        url,
                        headers=headers,
                    )
                )
                cancellation_token.link_future(task)
                try:
                    resp = await task
                    resp.raise_for_status()
                    local_path = self.work_dir / file
                    local_paths.append(str(local_path))
                    async with await open_file(local_path, "wb") as f:
                        await f.write(await resp.read())
                except asyncio.TimeoutError as e:
                    # e.add_note is only in py 3.11+
                    raise asyncio.TimeoutError("Timeout downloading files") from e
                except asyncio.CancelledError as e:
                    # e.add_note is only in py 3.11+
                    raise asyncio.CancelledError("Downloading files cancelled") from e
                except aiohttp.ClientResponseError as e:
                    raise ConnectionError("Error while downloading files") from e
        return local_paths

    async def execute_code_blocks(
        self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken
    ) -> CodeResult:
        """(Experimental) Execute the code blocks and return the result.

        Args:
            code_blocks (List[CodeBlock]): The code blocks to execute.
            cancellation_token (CancellationToken): a token to cancel the operation
            input_files (Optional[Union[Path, str]]): Any files the code blocks will need to access

        Returns:
            CodeResult: The result of the code execution."""

        self._ensure_access_token()
        if self._available_packages is None:
            await self._populate_available_packages(cancellation_token)
        if not self._setup_functions_complete:
            await self._setup_functions(cancellation_token)
        if not self._setup_cwd_complete:
            await self._setup_cwd(cancellation_token)

        return await self._execute_code_dont_check_setup(code_blocks, cancellation_token)

    # The http call here should be replaced by an actual Azure client call once its available
    async def _execute_code_dont_check_setup(
        self, code_blocks: List[CodeBlock], cancellation_token: CancellationToken
    ) -> CodeResult:
        logs_all = ""
        exitcode = 0

        # TODO: Better to use the client auth system rather than headers
        assert self._access_token is not None
        headers = {
            "Authorization": f"Bearer {self._access_token}",
            "Content-Type": "application/json",
        }
        properties = {
            "codeInputType": "inline",
            "executionType": "synchronous",
            "code": "",  # Filled in later
        }
        url = self._construct_url("code/execute")
        timeout = aiohttp.ClientTimeout(total=float(self._timeout))
        async with aiohttp.ClientSession(timeout=timeout) as client:
            for code_block in code_blocks:
                lang, code = code_block.language, code_block.code
                lang = lang.lower()

                if lang in PYTHON_VARIANTS:
                    lang = "python"

                if lang not in self.SUPPORTED_LANGUAGES:
                    # In case the language is not supported, we return an error message.
                    exitcode = 1
                    logs_all += "\n" + f"unknown language {lang}"
                    break

                if self._available_packages is not None:
                    req_pkgs = get_required_packages(code, lang)
                    missing_pkgs = set(req_pkgs - self._available_packages)
                    if len(missing_pkgs) > 0:
                        # In case the code requires packages that are not available in the environment
                        exitcode = 1
                        logs_all += "\n" + f"Python packages unavailable in environment: {missing_pkgs}"
                        break

                properties["code"] = code_block.code

                task = asyncio.create_task(
                    client.post(
                        url,
                        headers=headers,
                        json={"properties": properties},
                    )
                )

                cancellation_token.link_future(task)
                try:
                    response = await task
                    response.raise_for_status()
                    data = await response.json()
                    data = data["properties"]
                    logs_all += data.get("stderr", "") + data.get("stdout", "")
                    if "Success" in data["status"]:
                        if not self._suppress_result_output:
                            logs_all += str(data["result"])
                    elif "Failure" in data["status"]:
                        exitcode = 1

                except asyncio.TimeoutError as e:
                    logs_all += "\n Timeout"
                    # e.add_note is only in py 3.11+
                    raise asyncio.TimeoutError(logs_all) from e
                except asyncio.CancelledError as e:
                    logs_all += "\n Cancelled"
                    # e.add_note is only in py 3.11+
                    raise asyncio.CancelledError(logs_all) from e
                except aiohttp.ClientResponseError as e:
                    logs_all += "\nError while sending code block to endpoint"
                    raise ConnectionError(logs_all) from e

        return CodeResult(exit_code=exitcode, output=logs_all)

    async def restart(self) -> None:
        """(Experimental) Restart the code executor.

        Resets the internal state of the executor by generating a new session ID and resetting the setup variables.
        This causes the next code execution to reinitialize the environment and re-run any setup code.
        """
        self._session_id = str(uuid4())
        self._setup_functions_complete = False
        self._access_token = None
        self._available_packages = None
        self._setup_cwd_complete = False

    async def start(self) -> None:
        """(Experimental) Start the code executor.

        Marks the code executor as started."""
        # No setup needed for this executor
        self._started = True

    async def stop(self) -> None:
        """(Experimental) Stop the code executor.

        Stops the code executor after cleaning up the temporary working directory (if it was created)."""
        if self._temp_dir is not None:
            self._temp_dir.cleanup()
            self._temp_dir = None
        self._started = False

# From azure/_azure_container_code_executor.py
def format_functions_for_prompt(self, prompt_template: str = FUNCTION_PROMPT_TEMPLATE) -> str:
        """(Experimental) Format the functions for a prompt.

        The template includes one variable:
        - `$functions`: The functions formatted as stubs with two newlines between each function.

        Args:
            prompt_template (str): The prompt template. Default is the class default.

        Returns:
            str: The formatted prompt.
        """

        template = Template(prompt_template)
        return template.substitute(
            functions="\n\n".join([to_stub(func) for func in self._functions]),
        )

# From azure/_azure_container_code_executor.py
def functions_module(self) -> str:
        """(Experimental) The module name for the functions."""
        return self._functions_module

# From azure/_azure_container_code_executor.py
def functions(self) -> List[str]:
        raise NotImplementedError

from contextlib import AbstractAsyncContextManager
from nbclient import NotebookClient
from nbformat import NotebookNode
from nbformat import v4

# From jupyter/_jupyter_code_executor.py
class JupyterCodeResult(CodeResult):
    """A code result class for Jupyter code executor."""

    output_files: list[Path]

# From jupyter/_jupyter_code_executor.py
class JupyterCodeExecutorConfig(BaseModel):
    """Configuration for JupyterCodeExecutor"""

    kernel_name: str = "python3"
    timeout: int = 60
    output_dir: Optional[str] = None

# From jupyter/_jupyter_code_executor.py
class JupyterCodeExecutor(CodeExecutor, Component[JupyterCodeExecutorConfig]):
    """A code executor class that executes code statefully using [nbclient](https://github.com/jupyter/nbclient).

    .. danger::

        This will execute code on the local machine. If being used with LLM generated code, caution should be used.

    Example of using it directly:

    .. code-block:: python

        import asyncio
        from autogen_core import CancellationToken
        from autogen_core.code_executor import CodeBlock
        from autogen_ext.code_executors.jupyter import JupyterCodeExecutor


        async def main() -> None:
            async with JupyterCodeExecutor() as executor:
                cancel_token = CancellationToken()
                code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
                code_result = await executor.execute_code_blocks(code_blocks, cancel_token)
                print(code_result)


        asyncio.run(main())

    Example of using it with :class:`~autogen_ext.tools.code_execution.PythonCodeExecutionTool`:

    .. code-block:: python

        import asyncio
        from autogen_agentchat.agents import AssistantAgent
        from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
        from autogen_ext.models.openai import OpenAIChatCompletionClient
        from autogen_ext.tools.code_execution import PythonCodeExecutionTool


        async def main() -> None:
            async with JupyterCodeExecutor() as executor:
                tool = PythonCodeExecutionTool(executor)
                model_client = OpenAIChatCompletionClient(model="gpt-4o")
                agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
                result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
                print(result)


        asyncio.run(main())

    Example of using it inside a :class:`~autogen_agentchat.agents._code_executor_agent.CodeExecutorAgent`:

    .. code-block:: python

        import asyncio
        from autogen_agentchat.agents import CodeExecutorAgent
        from autogen_agentchat.messages import TextMessage
        from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
        from autogen_core import CancellationToken


        async def main() -> None:
            async with JupyterCodeExecutor() as executor:
                code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
                task = TextMessage(
                    content='''Here is some code
            ```python
            print('Hello world')
            ```
            ''',
                    source="user",
                )
                response = await code_executor_agent.on_messages([task], CancellationToken())
                print(response.chat_message)


        asyncio.run(main())


    Args:
        kernel_name (str): The kernel name to use. By default, "python3".
        timeout (int): The timeout for code execution, by default 60.
        output_dir (Path): The directory to save output files, by default a temporary directory.


    .. note::
        Using the current directory (".") as output directory is deprecated. Using it will raise a deprecation warning.
    """

    component_config_schema = JupyterCodeExecutorConfig
    component_provider_override = "autogen_ext.code_executors.jupyter.JupyterCodeExecutor"

    def __init__(
        self,
        kernel_name: str = "python3",
        timeout: int = 60,
        output_dir: Optional[Union[Path, str]] = None,
    ):
        if timeout < 1:
            raise ValueError("Timeout must be greater than or equal to 1.")

        self._output_dir: Path = Path(tempfile.mkdtemp()) if output_dir is None else Path(output_dir)
        self._output_dir.mkdir(exist_ok=True, parents=True)

        self._temp_dir: Optional[tempfile.TemporaryDirectory[str]] = None
        self._temp_dir_path: Optional[Path] = None

        self._started = False

        self._kernel_name = kernel_name
        self._timeout = timeout

        self._client: Optional[NotebookClient] = None
        self.kernel_context: Optional[AbstractAsyncContextManager[None]] = None

    async def execute_code_blocks(
        self, code_blocks: list[CodeBlock], cancellation_token: CancellationToken
    ) -> JupyterCodeResult:
        """Execute code blocks and return the result.

        Args:
            code_blocks (list[CodeBlock]): The code blocks to execute.

        Returns:
            JupyterCodeResult: The result of the code execution.
        """
        outputs: list[str] = []
        output_files: list[Path] = []
        exit_code = 0

        for code_block in code_blocks:
            result = await self._execute_code_block(code_block, cancellation_token)
            exit_code = result.exit_code
            outputs.append(result.output)
            output_files.extend(result.output_files)

            # Stop execution if one code block fails
            if exit_code != 0:
                break

        return JupyterCodeResult(exit_code=exit_code, output="\n".join(outputs), output_files=output_files)

    async def _execute_code_block(
        self, code_block: CodeBlock, cancellation_token: CancellationToken
    ) -> JupyterCodeResult:
        """Execute single code block and return the result.

        Args:
            code_block (CodeBlock): The code block to execute.

        Returns:
            JupyterCodeResult: The result of the code execution.
        """
        execute_task = asyncio.create_task(
            self._execute_cell(
                nbformat.new_code_cell(silence_pip(code_block.code, code_block.language))  # type: ignore
            )
        )

        cancellation_token.link_future(execute_task)
        output_cell = await asyncio.wait_for(asyncio.shield(execute_task), timeout=self._timeout)

        outputs: list[str] = []
        output_files: list[Path] = []
        exit_code = 0

        for output in output_cell.get("outputs", []):
            match output.get("output_type"):
                case "stream":
                    outputs.append(output.get("text", ""))
                case "error":
                    traceback = re.sub(r"\x1b\[[0-9;]*[A-Za-z]", "", "\n".join(output["traceback"]))
                    outputs.append(traceback)
                    exit_code = 1
                case "execute_result" | "display_data":
                    data = output.get("data", {})
                    for mime, content in data.items():
                        match mime:
                            case "text/plain":
                                outputs.append(content)
                            case "image/png":
                                path = self._save_image(content)
                                output_files.append(path)
                            case "image/jpeg":
                                # TODO: Should this also be encoded? Images are encoded as both png and jpg
                                pass
                            case "text/html":
                                path = self._save_html(content)
                                output_files.append(path)
                            case _:
                                outputs.append(json.dumps(content))
                case _:
                    pass

        return JupyterCodeResult(exit_code=exit_code, output="\n".join(outputs), output_files=output_files)

    async def _execute_cell(self, cell: NotebookNode) -> NotebookNode:
        # Temporary push cell to nb as async_execute_cell expects it. But then we want to remove it again as cells can take up significant amount of memory (especially with images)
        if not self._client:
            raise RuntimeError("Executor must be started before executing cells")
        self._client.nb.cells.append(cell)
        output = await self._client.async_execute_cell(
            cell,
            cell_index=0,
        )
        self._client.nb.cells.pop()
        return output

    def _save_image(self, image_data_base64: str) -> Path:
        """Save image data to a file."""
        image_data = base64.b64decode(image_data_base64)
        path = self._output_dir / f"{uuid.uuid4().hex}.png"
        path.write_bytes(image_data)
        return path.absolute()

    def _save_html(self, html_data: str) -> Path:
        """Save HTML data to a file."""
        path = self._output_dir / f"{uuid.uuid4().hex}.html"
        path.write_text(html_data)
        return path.absolute()

    async def restart(self) -> None:
        """Restart the code executor."""
        await self.stop()
        await self.start()

    async def start(self) -> None:
        """(Experimental) Start the code executor.

        Initializes the Jupyter Notebook execution environment by creating a new notebook and setting it up with the specified Jupyter Kernel.
        Marks the executor as started, allowing for code execution.
        This method should be called before executing any code blocks.
        """
        if self._started:
            return

        notebook: NotebookNode = nbformat.new_notebook()  # type: ignore

        self._client = NotebookClient(
            nb=notebook,
            kernel_name=self._kernel_name,
            timeout=self._timeout,
            allow_errors=True,
        )

        self.kernel_context = self._client.async_setup_kernel()
        await self.kernel_context.__aenter__()

        self._started = True

    async def stop(self) -> None:
        """(Experimental) Stop the code executor.

        Terminates the Jupyter Notebook execution by exiting the kernel context and cleaning up the associated resources."""
        if not self._started:
            return

        if self.kernel_context is not None:
            await self.kernel_context.__aexit__(None, None, None)
            self.kernel_context = None

        self._client = None
        self._started = False

    def _to_config(self) -> JupyterCodeExecutorConfig:
        """Convert current instance to config object"""
        return JupyterCodeExecutorConfig(
            kernel_name=self._kernel_name, timeout=self._timeout, output_dir=str(self.output_dir)
        )

    @property
    def output_dir(self) -> Path:
        # If a user specifies the current directory, warn them that this is deprecated
        if self._output_dir == Path("."):
            warnings.warn(
                "Using the current directory as output_dir is deprecated",
                DeprecationWarning,
                stacklevel=2,
            )
        return self._output_dir

    @classmethod
    def _from_config(cls, config: JupyterCodeExecutorConfig) -> Self:
        """Create instance from config object"""
        return cls(
            kernel_name=config.kernel_name,
            timeout=config.timeout,
            output_dir=Path(config.output_dir) if config.output_dir else None,
        )

# From jupyter/_jupyter_code_executor.py
def output_dir(self) -> Path:
        # If a user specifies the current directory, warn them that this is deprecated
        if self._output_dir == Path("."):
            warnings.warn(
                "Using the current directory as output_dir is deprecated",
                DeprecationWarning,
                stacklevel=2,
            )
        return self._output_dir


from autogen_core._subscription import Subscription
from autogen_core._type_prefix_subscription import TypePrefixSubscription
from autogen_core._type_subscription import TypeSubscription
from protos import agent_worker_pb2

# From grpc/_utils.py
def subscription_to_proto(subscription: Subscription) -> agent_worker_pb2.Subscription:
    match subscription:
        case TypeSubscription(topic_type=topic_type, agent_type=agent_type, id=id):
            return agent_worker_pb2.Subscription(
                id=id,
                typeSubscription=agent_worker_pb2.TypeSubscription(topic_type=topic_type, agent_type=agent_type),
            )
        case TypePrefixSubscription(topic_type_prefix=topic_type_prefix, agent_type=agent_type, id=id):
            return agent_worker_pb2.Subscription(
                id=id,
                typePrefixSubscription=agent_worker_pb2.TypePrefixSubscription(
                    topic_type_prefix=topic_type_prefix, agent_type=agent_type
                ),
            )
        case _:
            raise ValueError("Unsupported subscription type.")

# From grpc/_utils.py
def subscription_from_proto(subscription: agent_worker_pb2.Subscription) -> Subscription:
    oneofcase = subscription.WhichOneof("subscription")
    match oneofcase:
        case "typeSubscription":
            type_subscription_msg: agent_worker_pb2.TypeSubscription = subscription.typeSubscription
            return TypeSubscription(
                topic_type=type_subscription_msg.topic_type,
                agent_type=type_subscription_msg.agent_type,
                id=subscription.id,
            )

        case "typePrefixSubscription":
            type_prefix_subscription_msg: agent_worker_pb2.TypePrefixSubscription = subscription.typePrefixSubscription
            return TypePrefixSubscription(
                topic_type_prefix=type_prefix_subscription_msg.topic_type_prefix,
                agent_type=type_prefix_subscription_msg.agent_type,
                id=subscription.id,
            )
        case None:
            raise ValueError("Invalid subscription message.")


from google.protobuf import timestamp_pb2


from tools import extract_audio
from tools import get_screenshot_at
from tools import get_video_length
from tools import save_screenshot
from tools import transcribe_audio_with_timestamps
from tools import transcribe_video_screenshot

# From video_surfer/_video_surfer.py
class VideoSurfer(AssistantAgent):
    """
    VideoSurfer is a specialized agent designed to answer questions about a local video file.

    Installation:

    .. code-block:: bash

        pip install "autogen-ext[video-surfer]"

    This agent utilizes various tools to extract information from the video, such as its length, screenshots at specific timestamps, and audio transcriptions. It processes these elements to provide detailed answers to user queries.

    Available tools:

    - :func:`~autogen_ext.agents.video_surfer.tools.extract_audio`
    - :func:`~autogen_ext.agents.video_surfer.tools.get_video_length`
    - :func:`~autogen_ext.agents.video_surfer.tools.transcribe_audio_with_timestamps`
    - :func:`~autogen_ext.agents.video_surfer.tools.get_screenshot_at`
    - :func:`~autogen_ext.agents.video_surfer.tools.save_screenshot`
    - :func:`~autogen_ext.agents.video_surfer.tools.transcribe_video_screenshot`

    Args:
        name (str): The name of the agent.
        model_client (ChatCompletionClient): The model client used for generating responses.
        tools (List[BaseTool[BaseModel, BaseModel]  | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional):
            A list of tools or functions the agent can use. If not provided, defaults to all video tools from the action space.
        description (str, optional): A brief description of the agent. Defaults to "An agent that can answer questions about a local video.".
        system_message (str | None, optional): The system message guiding the agent's behavior. Defaults to a predefined message.

    Example usage:

    The following example demonstrates how to create an video surfing agent with
    a model client and generate a response to a simple query about a local video
    called video.mp4.

        .. code-block:: python


            import asyncio
            from autogen_agentchat.ui import Console
            from autogen_agentchat.conditions import TextMentionTermination
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.agents.video_surfer import VideoSurfer

            async def main() -> None:
                \"\"\"
                Main function to run the video agent.
                \"\"\"
                # Define an agent
                video_agent = VideoSurfer(
                    name="VideoSurfer",
                    model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
                    )

                # Define termination condition
                termination = TextMentionTermination("TERMINATE")

                # Define a team
                agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)

                # Run the team and stream messages to the console
                stream = agent_team.run_stream(task="How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?")
                await Console(stream)

            asyncio.run(main())

    The following example demonstrates how to create and use a VideoSurfer and UserProxyAgent with MagenticOneGroupChat.

        .. code-block:: python

            import asyncio

            from autogen_agentchat.ui import Console
            from autogen_agentchat.teams import MagenticOneGroupChat
            from autogen_agentchat.agents import UserProxyAgent
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.agents.video_surfer import VideoSurfer

            async def main() -> None:
                \"\"\"
                Main function to run the video agent.
                \"\"\"

                model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

                # Define an agent
                video_agent = VideoSurfer(
                    name="VideoSurfer",
                    model_client=model_client
                    )

                web_surfer_agent = UserProxyAgent(
                    name="User"
                )

                # Define a team
                agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client,)

                # Run the team and stream messages to the console
                stream = agent_team.run_stream(task="Find a latest video about magentic one on youtube and extract quotes from it that make sense.")
                await Console(stream)

            asyncio.run(main())
    """

    DEFAULT_DESCRIPTION = "An agent that can answer questions about a local video."

    DEFAULT_SYSTEM_MESSAGE = """
    You are a helpful agent that is an expert at answering questions from a video.
    When asked to answer a question about a video, you should:
    1. Check if that video is available locally.
    2. Use the transcription to find which part of the video the question is referring to.
    3. Optionally use screenshots from those timestamps
    4. Provide a detailed answer to the question.
    Reply with TERMINATE when the task has been completed.
    """

    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        *,
        tools: List[BaseTool[BaseModel, BaseModel] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None = None,
        description: Optional[str] = None,
        system_message: Optional[str] = None,
    ):
        super().__init__(
            name=name,
            model_client=model_client,
            tools=tools
            or [
                get_video_length,
                get_screenshot_at,
                save_screenshot,
                self.vs_transribe_video_screenshot,
                extract_audio,
                transcribe_audio_with_timestamps,
            ],
            description=description or self.DEFAULT_DESCRIPTION,
            system_message=system_message or self.DEFAULT_SYSTEM_MESSAGE,
        )

    async def vs_transribe_video_screenshot(self, video_path: str, timestamp: float) -> str:
        """
        Transcribes the video screenshot at a specific timestamp.

        Args:
            video_path (str): Path to the video file.
            timestamp (float): Timestamp to take the screenshot.

        Returns:
            str: Transcription of the video screenshot.
        """
        return await transcribe_video_screenshot(video_path, timestamp, self._model_client)


# From web_surfer/_events.py
class WebSurferEvent:
    source: str
    message: str
    url: str
    action: str | None = None
    arguments: Dict[str, Any] | None = None

from typing import BinaryIO
from PIL import ImageDraw
from PIL import ImageFont
from _types import DOMRectangle
from _types import InteractiveRegion

# From web_surfer/_set_of_mark.py
def add_set_of_mark(
    screenshot: bytes | Image.Image | io.BufferedIOBase, ROIs: Dict[str, InteractiveRegion]
) -> Tuple[Image.Image, List[str], List[str], List[str]]:
    if isinstance(screenshot, Image.Image):
        return _add_set_of_mark(screenshot, ROIs)

    if isinstance(screenshot, bytes):
        screenshot = io.BytesIO(screenshot)

    # TODO: Not sure why this cast was needed, but by this point screenshot is a binary file-like object
    image = Image.open(cast(BinaryIO, screenshot))
    comp, visible_rects, rects_above, rects_below = _add_set_of_mark(image, ROIs)
    image.close()
    return comp, visible_rects, rects_above, rects_below

from autogen_core.tools._base import ParametersSchema
from autogen_core.tools._base import ToolSchema

from urllib.parse import quote_plus
import PIL.Image
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.utils import remove_images
from playwright.async_api import BrowserContext
from playwright.async_api import Download
from playwright.async_api import Page
from playwright.async_api import Playwright
from playwright.async_api import async_playwright
from _events import WebSurferEvent
from _prompts import WEB_SURFER_QA_PROMPT
from _prompts import WEB_SURFER_QA_SYSTEM_MESSAGE
from _prompts import WEB_SURFER_TOOL_PROMPT_MM
from _prompts import WEB_SURFER_TOOL_PROMPT_TEXT
from _set_of_mark import add_set_of_mark
from _tool_definitions import TOOL_CLICK
from _tool_definitions import TOOL_HISTORY_BACK
from _tool_definitions import TOOL_HOVER
from _tool_definitions import TOOL_READ_PAGE_AND_ANSWER
from _tool_definitions import TOOL_SCROLL_DOWN
from _tool_definitions import TOOL_SCROLL_UP
from _tool_definitions import TOOL_SLEEP
from _tool_definitions import TOOL_SUMMARIZE_PAGE
from _tool_definitions import TOOL_TYPE
from _tool_definitions import TOOL_VISIT_URL
from _tool_definitions import TOOL_WEB_SEARCH
from _types import UserContent
from playwright_controller import PlaywrightController

# From web_surfer/_multimodal_web_surfer.py
class MultimodalWebSurferConfig(BaseModel):
    name: str
    model_client: ComponentModel
    downloads_folder: str | None = None
    description: str | None = None
    debug_dir: str | None = None
    headless: bool = True
    start_page: str | None = "https://www.bing.com/"
    animate_actions: bool = False
    to_save_screenshots: bool = False
    use_ocr: bool = False
    browser_channel: str | None = None
    browser_data_dir: str | None = None
    to_resize_viewport: bool = True

# From web_surfer/_multimodal_web_surfer.py
class MultimodalWebSurfer(BaseChatAgent, Component[MultimodalWebSurferConfig]):
    """
    MultimodalWebSurfer is a multimodal agent that acts as a web surfer that can search the web and visit web pages.

    Installation:

    .. code-block:: bash

        pip install "autogen-ext[web-surfer]"

    It launches a chromium browser and allows the playwright to interact with the web browser and can perform a variety of actions. The browser is launched on the first call to the agent and is reused for subsequent calls.

    It must be used with a multimodal model client that supports function/tool calling, ideally GPT-4o currently.


    When :meth:`on_messages` or :meth:`on_messages_stream` is called, the following occurs:
        1) If this is the first call, the browser is initialized and the page is loaded. This is done in :meth:`_lazy_init`. The browser is only closed when :meth:`close` is called.
        2) The method :meth:`_generate_reply` is called, which then creates the final response as below.
        3) The agent takes a screenshot of the page, extracts the interactive elements, and prepares a set-of-mark screenshot with bounding boxes around the interactive elements.
        4) The agent makes a call to the :attr:`model_client` with the SOM screenshot, history of messages, and the list of available tools.
            - If the model returns a string, the agent returns the string as the final response.
            - If the model returns a list of tool calls, the agent executes the tool calls with :meth:`_execute_tool` using :attr:`_playwright_controller`.
            - The agent returns a final response which includes a screenshot of the page, page metadata, description of the action taken and the inner text of the webpage.
        5) If at any point the agent encounters an error, it returns the error message as the final response.


    .. note::
        Please note that using the MultimodalWebSurfer involves interacting with a digital world designed for humans, which carries inherent risks.
        Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences.
        Moreover, be cautious that MultimodalWebSurfer may be susceptible to prompt injection attacks from webpages.

    .. note::

        On Windows, the event loop policy must be set to `WindowsProactorEventLoopPolicy` to avoid issues with subprocesses.

        .. code-block:: python

            import sys
            import asyncio

            if sys.platform == "win32":
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    Args:
        name (str): The name of the agent.
        model_client (ChatCompletionClient): The model client used by the agent. Must be multimodal and support function calling.
        downloads_folder (str, optional): The folder where downloads are saved. Defaults to None, no downloads are saved.
        description (str, optional): The description of the agent. Defaults to MultimodalWebSurfer.DEFAULT_DESCRIPTION.
        debug_dir (str, optional): The directory where debug information is saved. Defaults to None.
        headless (bool, optional): Whether the browser should be headless. Defaults to True.
        start_page (str, optional): The start page for the browser. Defaults to MultimodalWebSurfer.DEFAULT_START_PAGE.
        animate_actions (bool, optional): Whether to animate actions. Defaults to False.
        to_save_screenshots (bool, optional): Whether to save screenshots. Defaults to False.
        use_ocr (bool, optional): Whether to use OCR. Defaults to False.
        browser_channel (str, optional): The browser channel. Defaults to None.
        browser_data_dir (str, optional): The browser data directory. Defaults to None.
        to_resize_viewport (bool, optional): Whether to resize the viewport. Defaults to True.
        playwright (Playwright, optional): The playwright instance. Defaults to None.
        context (BrowserContext, optional): The browser context. Defaults to None.




    Example usage:

    The following example demonstrates how to create a web surfing agent with
    a model client and run it for multiple turns.

        .. code-block:: python


            import asyncio
            from autogen_agentchat.ui import Console
            from autogen_agentchat.teams import RoundRobinGroupChat
            from autogen_ext.models.openai import OpenAIChatCompletionClient
            from autogen_ext.agents.web_surfer import MultimodalWebSurfer


            async def main() -> None:
                # Define an agent
                web_surfer_agent = MultimodalWebSurfer(
                    name="MultimodalWebSurfer",
                    model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"),
                )

                # Define a team
                agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)

                # Run the team and stream messages to the console
                stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
                await Console(stream)
                # Close the browser controlled by the agent
                await web_surfer_agent.close()


            asyncio.run(main())
    """

    component_type = "agent"
    component_config_schema = MultimodalWebSurferConfig
    component_provider_override = "autogen_ext.agents.web_surfer.MultimodalWebSurfer"

    DEFAULT_DESCRIPTION = """
    A helpful assistant with access to a web browser.
    Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.).
    It can also summarize the entire page, or answer questions based on the content of the page.
    It can also be asked to sleep and wait for pages to load, in cases where the page seems not yet fully loaded.
    """
    DEFAULT_START_PAGE = "https://www.bing.com/"

    # Viewport dimensions
    VIEWPORT_HEIGHT = 900
    VIEWPORT_WIDTH = 1440

    # Size of the image we send to the MLM
    # Current values represent a 0.85 scaling to fit within the GPT-4v short-edge constraints (768px)
    MLM_HEIGHT = 765
    MLM_WIDTH = 1224

    SCREENSHOT_TOKENS = 1105

    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        downloads_folder: str | None = None,
        description: str = DEFAULT_DESCRIPTION,
        debug_dir: str | None = None,
        headless: bool = True,
        start_page: str | None = DEFAULT_START_PAGE,
        animate_actions: bool = False,
        to_save_screenshots: bool = False,
        use_ocr: bool = False,
        browser_channel: str | None = None,
        browser_data_dir: str | None = None,
        to_resize_viewport: bool = True,
        playwright: Playwright | None = None,
        context: BrowserContext | None = None,
    ):
        """
        Initialize the MultimodalWebSurfer.
        """
        super().__init__(name, description)
        if debug_dir is None and to_save_screenshots:
            raise ValueError(
                "Cannot save screenshots without a debug directory. Set it using the 'debug_dir' parameter. The debug directory is created if it does not exist."
            )
        if model_client.model_info["function_calling"] is False:
            raise ValueError(
                "The model does not support function calling. MultimodalWebSurfer requires a model that supports function calling."
            )

        self._model_client = model_client
        self.headless = headless
        self.browser_channel = browser_channel
        self.browser_data_dir = browser_data_dir
        self.start_page = start_page or self.DEFAULT_START_PAGE
        self.downloads_folder = downloads_folder
        self.debug_dir = debug_dir
        self.to_save_screenshots = to_save_screenshots
        self.use_ocr = use_ocr
        self.to_resize_viewport = to_resize_viewport
        self.animate_actions = animate_actions

        # Call init to set these in case not set
        self._playwright: Playwright | None = playwright
        self._context: BrowserContext | None = context
        self._page: Page | None = None
        self._last_download: Download | None = None
        self._prior_metadata_hash: str | None = None
        self.logger = logging.getLogger(EVENT_LOGGER_NAME + f".{self.name}.MultimodalWebSurfer")
        self._chat_history: List[LLMMessage] = []

        # Define the download handler
        def _download_handler(download: Download) -> None:
            self._last_download = download

        self._download_handler = _download_handler

        # Define the Playwright controller that handles the browser interactions
        self._playwright_controller = PlaywrightController(
            animate_actions=self.animate_actions,
            downloads_folder=self.downloads_folder,
            viewport_width=self.VIEWPORT_WIDTH,
            viewport_height=self.VIEWPORT_HEIGHT,
            _download_handler=self._download_handler,
            to_resize_viewport=self.to_resize_viewport,
        )
        self.default_tools = [
            TOOL_VISIT_URL,
            TOOL_WEB_SEARCH,
            TOOL_HISTORY_BACK,
            TOOL_CLICK,
            TOOL_TYPE,
            TOOL_READ_PAGE_AND_ANSWER,
            TOOL_SUMMARIZE_PAGE,
            TOOL_SLEEP,
            TOOL_HOVER,
        ]
        self.did_lazy_init = False  # flag to check if we have initialized the browser

    async def _lazy_init(
        self,
    ) -> None:
        """
        On the first call, we initialize the browser and the page.
        """

        # Check the current event loop policy if on windows.
        if sys.platform == "win32":
            current_policy = asyncio.get_event_loop_policy()
            if hasattr(asyncio, "WindowsProactorEventLoopPolicy") and not isinstance(
                current_policy, asyncio.WindowsProactorEventLoopPolicy
            ):
                warnings.warn(
                    "The current event loop policy is not WindowsProactorEventLoopPolicy. "
                    "This may cause issues with subprocesses. "
                    "Try setting the event loop policy to WindowsProactorEventLoopPolicy. "
                    "For example: `asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())`. "
                    "See https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop.",
                    stacklevel=2,
                )

        self._last_download = None
        self._prior_metadata_hash = None

        # Create the playwright self
        launch_args: Dict[str, Any] = {"headless": self.headless}
        if self.browser_channel is not None:
            launch_args["channel"] = self.browser_channel
        if self._playwright is None:
            self._playwright = await async_playwright().start()

        # Create the context -- are we launching persistent?
        if self._context is None:
            if self.browser_data_dir is None:
                browser = await self._playwright.chromium.launch(**launch_args)
                self._context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0"
                )
            else:
                self._context = await self._playwright.chromium.launch_persistent_context(
                    self.browser_data_dir, **launch_args
                )

        # Create the page
        self._context.set_default_timeout(60000)  # One minute
        self._page = await self._context.new_page()
        assert self._page is not None
        # self._page.route(lambda x: True, self._route_handler)
        self._page.on("download", self._download_handler)
        if self.to_resize_viewport:
            await self._page.set_viewport_size({"width": self.VIEWPORT_WIDTH, "height": self.VIEWPORT_HEIGHT})
        await self._page.add_init_script(
            path=os.path.join(os.path.abspath(os.path.dirname(__file__)), "page_script.js")
        )
        await self._page.goto(self.start_page)
        await self._page.wait_for_load_state()

        # Prepare the debug directory -- which stores the screenshots generated throughout the process
        await self._set_debug_dir(self.debug_dir)
        self.did_lazy_init = True

    async def close(self) -> None:
        """
        Close the browser and the page.
        Should be called when the agent is no longer needed.
        """
        if self._page is not None:
            await self._page.close()
            self._page = None
        if self._context is not None:
            await self._context.close()
            self._context = None
        if self._playwright is not None:
            await self._playwright.stop()
            self._playwright = None

    async def _set_debug_dir(self, debug_dir: str | None) -> None:
        assert self._page is not None
        if self.debug_dir is None:
            return

        if not os.path.isdir(self.debug_dir):
            os.mkdir(self.debug_dir)

        if self.to_save_screenshots:
            current_timestamp = "_" + int(time.time()).__str__()
            screenshot_png_name = "screenshot" + current_timestamp + ".png"

            await self._page.screenshot(path=os.path.join(self.debug_dir, screenshot_png_name))  # type: ignore
            self.logger.info(
                WebSurferEvent(
                    source=self.name,
                    url=self._page.url,
                    message="Screenshot: " + screenshot_png_name,
                )
            )

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (MultiModalMessage,)

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        if not self.did_lazy_init:
            return
        assert self._page is not None

        self._chat_history.clear()
        reset_prior_metadata, reset_last_download = await self._playwright_controller.visit_page(
            self._page, self.start_page
        )
        if reset_last_download and self._last_download is not None:
            self._last_download = None
        if reset_prior_metadata and self._prior_metadata_hash is not None:
            self._prior_metadata_hash = None
        if self.to_save_screenshots:
            current_timestamp = "_" + int(time.time()).__str__()
            screenshot_png_name = "screenshot" + current_timestamp + ".png"

            await self._page.screenshot(path=os.path.join(self.debug_dir, screenshot_png_name))  # type: ignore
            self.logger.info(
                WebSurferEvent(
                    source=self.name,
                    url=self._page.url,
                    message="Screenshot: " + screenshot_png_name,
                )
            )

        self.logger.info(
            WebSurferEvent(
                source=self.name,
                url=self._page.url,
                message="Resetting browser.",
            )
        )

    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                return message
        raise AssertionError("The stream should have returned the final result.")

    async def on_messages_stream(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        for chat_message in messages:
            self._chat_history.append(chat_message.to_model_message())

        self.inner_messages: List[BaseAgentEvent | BaseChatMessage] = []
        self.model_usage: List[RequestUsage] = []
        try:
            content = await self._generate_reply(cancellation_token=cancellation_token)
            self._chat_history.append(AssistantMessage(content=content_to_str(content), source=self.name))
            final_usage = RequestUsage(
                prompt_tokens=sum([u.prompt_tokens for u in self.model_usage]),
                completion_tokens=sum([u.completion_tokens for u in self.model_usage]),
            )
            if isinstance(content, str):
                yield Response(
                    chat_message=TextMessage(content=content, source=self.name, models_usage=final_usage),
                    inner_messages=self.inner_messages,
                )
            else:
                yield Response(
                    chat_message=MultiModalMessage(content=content, source=self.name, models_usage=final_usage),
                    inner_messages=self.inner_messages,
                )

        except BaseException:
            content = f"Web surfing error:\n\n{traceback.format_exc()}"
            self._chat_history.append(AssistantMessage(content=content, source=self.name))
            yield Response(chat_message=TextMessage(content=content, source=self.name))

    async def _generate_reply(self, cancellation_token: CancellationToken) -> UserContent:
        """Generates the actual reply. First calls the LLM to figure out which tool to use, then executes the tool."""

        # Lazy init, initialize the browser and the page on the first generate reply only
        if not self.did_lazy_init:
            await self._lazy_init()

        assert self._page is not None

        # Clone the messages, removing old screenshots
        history: List[LLMMessage] = remove_images(self._chat_history)

        # Split the history, removing the last message
        if len(history):
            user_request = history.pop()
        else:
            user_request = UserMessage(content="Empty request.", source="user")

        # Truncate the history for smaller models
        if self._model_client.model_info["family"] not in [
            ModelFamily.GPT_4O,
            ModelFamily.O1,
            ModelFamily.O3,
            ModelFamily.GPT_4,
            ModelFamily.GPT_35,
        ]:
            history = []

        # Ask the page for interactive elements, then prepare the state-of-mark screenshot
        rects = await self._playwright_controller.get_interactive_rects(self._page)
        viewport = await self._playwright_controller.get_visual_viewport(self._page)
        screenshot = await self._page.screenshot()
        som_screenshot, visible_rects, rects_above, rects_below = add_set_of_mark(screenshot, rects)

        if self.to_save_screenshots:
            current_timestamp = "_" + int(time.time()).__str__()
            screenshot_png_name = "screenshot_som" + current_timestamp + ".png"
            som_screenshot.save(os.path.join(self.debug_dir, screenshot_png_name))  # type: ignore
            self.logger.info(
                WebSurferEvent(
                    source=self.name,
                    url=self._page.url,
                    message="Screenshot: " + screenshot_png_name,
                )
            )
        # What tools are available?
        tools = self.default_tools.copy()

        # We can scroll up
        if viewport["pageTop"] > 5:
            tools.append(TOOL_SCROLL_UP)

        # Can scroll down
        if (viewport["pageTop"] + viewport["height"] + 5) < viewport["scrollHeight"]:
            tools.append(TOOL_SCROLL_DOWN)

        # Focus hint
        focused = await self._playwright_controller.get_focused_rect_id(self._page)
        focused_hint = ""
        if focused:
            name = self._target_name(focused, rects)
            if name:
                name = f"(and name '{name}') "
            else:
                name = ""

            role = "control"
            try:
                role = rects[focused]["role"]
            except KeyError:
                pass

            focused_hint = f"\nThe {role} with ID {focused} {name}currently has the input focus.\n\n"

        # Everything visible
        visible_targets = "\n".join(self._format_target_list(visible_rects, rects)) + "\n\n"

        # Everything else
        other_targets: List[str] = []
        other_targets.extend(self._format_target_list(rects_above, rects))
        other_targets.extend(self._format_target_list(rects_below, rects))

        if len(other_targets) > 0:
            if len(other_targets) > 30:
                other_targets = other_targets[0:30]
                other_targets.append("...")
            other_targets_str = (
                "Additional valid interaction targets include (but are not limited to):\n"
                + "\n".join(other_targets)
                + "\n\n"
            )
        else:
            other_targets_str = ""

        state_description = "Your " + await self._get_state_description()
        tool_names = "\n".join([t["name"] for t in tools])
        page_title = await self._page.title()

        prompt_message = None
        if self._model_client.model_info["vision"]:
            text_prompt = WEB_SURFER_TOOL_PROMPT_MM.format(
                state_description=state_description,
                visible_targets=visible_targets,
                other_targets_str=other_targets_str,
                focused_hint=focused_hint,
                tool_names=tool_names,
                title=page_title,
                url=self._page.url,
            ).strip()

            # Scale the screenshot for the MLM, and close the original
            scaled_screenshot = som_screenshot.resize((self.MLM_WIDTH, self.MLM_HEIGHT))
            som_screenshot.close()
            if self.to_save_screenshots:
                scaled_screenshot.save(os.path.join(self.debug_dir, "screenshot_scaled.png"))  # type: ignore

            # Create the message
            prompt_message = UserMessage(
                content=[re.sub(r"(\n\s*){3,}", "\n\n", text_prompt), AGImage.from_pil(scaled_screenshot)],
                source=self.name,
            )
        else:
            text_prompt = WEB_SURFER_TOOL_PROMPT_TEXT.format(
                state_description=state_description,
                visible_targets=visible_targets,
                other_targets_str=other_targets_str,
                focused_hint=focused_hint,
                tool_names=tool_names,
                title=page_title,
                url=self._page.url,
            ).strip()

            # Create the message
            prompt_message = UserMessage(content=re.sub(r"(\n\s*){3,}", "\n\n", text_prompt), source=self.name)

        history.append(prompt_message)
        history.append(user_request)

        # {history[-2].content if isinstance(history[-2].content, str) else history[-2].content[0]}
        # print(f"""
        # ================={len(history)}=================
        # {history[-2].content}
        # =====
        # {history[-1].content}
        # ===================================================
        # """)

        # Make the request
        response = await self._model_client.create(
            history, tools=tools, extra_create_args={"tool_choice": "auto"}, cancellation_token=cancellation_token
        )  # , "parallel_tool_calls": False})

        self.model_usage.append(response.usage)
        message = response.content
        self._last_download = None
        if isinstance(message, str):
            # Answer directly
            self.inner_messages.append(TextMessage(content=message, source=self.name))
            return message
        elif isinstance(message, list):
            # Take an action
            return await self._execute_tool(message, rects, tool_names, cancellation_token=cancellation_token)
        else:
            # Not sure what happened here
            raise AssertionError(f"Unknown response format '{message}'")

    async def _execute_tool(
        self,
        message: List[FunctionCall],
        rects: Dict[str, InteractiveRegion],
        tool_names: str,
        cancellation_token: Optional[CancellationToken] = None,
    ) -> UserContent:
        # Execute the tool
        name = message[0].name
        args = json.loads(message[0].arguments)
        action_description = ""
        assert self._page is not None
        self.logger.info(
            WebSurferEvent(
                source=self.name,
                url=self._page.url,
                action=name,
                arguments=args,
                message=f"{name}( {json.dumps(args)} )",
            )
        )
        self.inner_messages.append(TextMessage(content=f"{name}( {json.dumps(args)} )", source=self.name))

        if name == "visit_url":
            url = args.get("url")
            action_description = f"I typed '{url}' into the browser address bar."
            # Check if the argument starts with a known protocol
            if url.startswith(("https://", "http://", "file://", "about:")):
                reset_prior_metadata, reset_last_download = await self._playwright_controller.visit_page(
                    self._page, url
                )
            # If the argument contains a space, treat it as a search query
            elif " " in url:
                reset_prior_metadata, reset_last_download = await self._playwright_controller.visit_page(
                    self._page, f"https://www.bing.com/search?q={quote_plus(url)}&FORM=QBLH"
                )
            # Otherwise, prefix with https://
            else:
                reset_prior_metadata, reset_last_download = await self._playwright_controller.visit_page(
                    self._page, "https://" + url
                )
            if reset_last_download and self._last_download is not None:
                self._last_download = None
            if reset_prior_metadata and self._prior_metadata_hash is not None:
                self._prior_metadata_hash = None
        elif name == "history_back":
            action_description = "I clicked the browser back button."
            await self._playwright_controller.back(self._page)

        elif name == "web_search":
            query = args.get("query")
            action_description = f"I typed '{query}' into the browser search bar."
            reset_prior_metadata, reset_last_download = await self._playwright_controller.visit_page(
                self._page, f"https://www.bing.com/search?q={quote_plus(query)}&FORM=QBLH"
            )
            if reset_last_download and self._last_download is not None:
                self._last_download = None
            if reset_prior_metadata and self._prior_metadata_hash is not None:
                self._prior_metadata_hash = None
        elif name == "scroll_up":
            action_description = "I scrolled up one page in the browser."
            await self._playwright_controller.page_up(self._page)
        elif name == "scroll_down":
            action_description = "I scrolled down one page in the browser."
            await self._playwright_controller.page_down(self._page)

        elif name == "click":
            target_id = str(args.get("target_id"))
            target_name = self._target_name(target_id, rects)
            if target_name:
                action_description = f"I clicked '{target_name}'."
            else:
                action_description = "I clicked the control."
            new_page_tentative = await self._playwright_controller.click_id(self._page, target_id)
            if new_page_tentative is not None:
                self._page = new_page_tentative
                self._prior_metadata_hash = None
                self.logger.info(
                    WebSurferEvent(
                        source=self.name,
                        url=self._page.url,
                        message="New tab or window.",
                    )
                )
        elif name == "input_text":
            input_field_id = str(args.get("input_field_id"))
            text_value = str(args.get("text_value"))
            input_field_name = self._target_name(input_field_id, rects)
            if input_field_name:
                action_description = f"I typed '{text_value}' into '{input_field_name}'."
            else:
                action_description = f"I input '{text_value}'."
            await self._playwright_controller.fill_id(self._page, input_field_id, text_value)

        elif name == "scroll_element_up":
            target_id = str(args.get("target_id"))
            target_name = self._target_name(target_id, rects)

            if target_name:
                action_description = f"I scrolled '{target_name}' up."
            else:
                action_description = "I scrolled the control up."

            await self._playwright_controller.scroll_id(self._page, target_id, "up")

        elif name == "scroll_element_down":
            target_id = str(args.get("target_id"))
            target_name = self._target_name(target_id, rects)

            if target_name:
                action_description = f"I scrolled '{target_name}' down."
            else:
                action_description = "I scrolled the control down."

            await self._playwright_controller.scroll_id(self._page, target_id, "down")

        elif name == "answer_question":
            question = str(args.get("question"))
            action_description = f"I answered the following question '{question}' based on the web page."
            # Do Q&A on the DOM. No need to take further action. Browser state does not change.
            return await self._summarize_page(question=question, cancellation_token=cancellation_token)
        elif name == "summarize_page":
            # Summarize the DOM. No need to take further action. Browser state does not change.
            action_description = "I summarized the current web page"
            return await self._summarize_page(cancellation_token=cancellation_token)

        elif name == "hover":
            target_id = str(args.get("target_id"))
            target_name = self._target_name(target_id, rects)
            if target_name:
                action_description = f"I hovered over '{target_name}'."
            else:
                action_description = "I hovered over the control."
            await self._playwright_controller.hover_id(self._page, target_id)

        elif name == "sleep":
            action_description = "I am waiting a short period of time before taking further action."
            await self._playwright_controller.sleep(self._page, 3)

        else:
            raise ValueError(f"Unknown tool '{name}'. Please choose from:\n\n{tool_names}")

        await self._page.wait_for_load_state()
        await self._playwright_controller.sleep(self._page, 3)

        # Handle downloads
        if self._last_download is not None and self.downloads_folder is not None:
            fname = os.path.join(self.downloads_folder, self._last_download.suggested_filename)
            await self._last_download.save_as(fname)  # type: ignore
            page_body = f"<html><head><title>Download Successful</title></head><body style=\"margin: 20px;\"><h1>Successfully downloaded '{self._last_download.suggested_filename}' to local path:<br><br>{fname}</h1></body></html>"
            await self._page.goto(
                "data:text/html;base64," + base64.b64encode(page_body.encode("utf-8")).decode("utf-8")
            )
            await self._page.wait_for_load_state()

        # Handle metadata
        page_metadata = json.dumps(await self._playwright_controller.get_page_metadata(self._page), indent=4)
        metadata_hash = hashlib.md5(page_metadata.encode("utf-8")).hexdigest()
        if metadata_hash != self._prior_metadata_hash:
            page_metadata = (
                "\n\nThe following metadata was extracted from the webpage:\n\n" + page_metadata.strip() + "\n"
            )
        else:
            page_metadata = ""
        self._prior_metadata_hash = metadata_hash

        new_screenshot = await self._page.screenshot()
        if self.to_save_screenshots:
            current_timestamp = "_" + int(time.time()).__str__()
            screenshot_png_name = "screenshot" + current_timestamp + ".png"

            async with aiofiles.open(os.path.join(self.debug_dir, screenshot_png_name), "wb") as file:  # type: ignore
                await file.write(new_screenshot)  # type: ignore
            self.logger.info(
                WebSurferEvent(
                    source=self.name,
                    url=self._page.url,
                    message="Screenshot: " + screenshot_png_name,
                )
            )

        # Return the complete observation
        state_description = "The " + await self._get_state_description()
        message_content = (
            f"{action_description}\n\n" + state_description + page_metadata + "\nHere is a screenshot of the page."
        )

        return [
            re.sub(r"(\n\s*){3,}", "\n\n", message_content),  # Removing blank lines
            AGImage.from_pil(PIL.Image.open(io.BytesIO(new_screenshot))),
        ]

    async def _get_state_description(self) -> str:
        assert self._playwright_controller is not None
        assert self._page is not None

        # Describe the viewport of the new page in words
        viewport = await self._playwright_controller.get_visual_viewport(self._page)
        percent_visible = int(viewport["height"] * 100 / viewport["scrollHeight"])
        percent_scrolled = int(viewport["pageTop"] * 100 / viewport["scrollHeight"])
        if percent_scrolled < 1:  # Allow some rounding error
            position_text = "at the top of the page"
        elif percent_scrolled + percent_visible >= 99:  # Allow some rounding error
            position_text = "at the bottom of the page"
        else:
            position_text = str(percent_scrolled) + "% down from the top of the page"

        visible_text = await self._playwright_controller.get_visible_text(self._page)

        # Return the complete observation
        page_title = await self._page.title()
        message_content = f"web browser is open to the page [{page_title}]({self._page.url}).\nThe viewport shows {percent_visible}% of the webpage, and is positioned {position_text}\n"
        message_content += f"The following text is visible in the viewport:\n\n{visible_text}"
        return message_content

    def _target_name(self, target: str, rects: Dict[str, InteractiveRegion]) -> str | None:
        try:
            return rects[target]["aria_name"].strip()
        except KeyError:
            return None

    def _format_target_list(self, ids: List[str], rects: Dict[str, InteractiveRegion]) -> List[str]:
        """
        Format the list of targets in the webpage as a string to be used in the agent's prompt.
        """
        targets: List[str] = []
        for r in list(set(ids)):
            if r in rects:
                # Get the role
                aria_role = rects[r].get("role", "").strip()
                if len(aria_role) == 0:
                    aria_role = rects[r].get("tag_name", "").strip()

                # Get the name
                aria_name = re.sub(r"[\n\r]+", " ", rects[r].get("aria_name", "")).strip()

                # What are the actions?
                actions = ['"click", "hover"']
                if rects[r]["role"] in ["textbox", "searchbox", "search"]:
                    actions = ['"input_text"']
                actions_str = "[" + ",".join(actions) + "]"

                targets.append(f'{{"id": {r}, "name": "{aria_name}", "role": "{aria_role}", "tools": {actions_str} }}')

        return targets

    async def _summarize_page(
        self,
        question: str | None = None,
        cancellation_token: Optional[CancellationToken] = None,
    ) -> str:
        assert self._page is not None

        page_markdown: str = await self._playwright_controller.get_page_markdown(self._page)

        title: str = self._page.url
        try:
            title = await self._page.title()
        except Exception:
            pass

        # Take a screenshot and scale it
        screenshot = Image.open(io.BytesIO(await self._page.screenshot()))
        scaled_screenshot = screenshot.resize((self.MLM_WIDTH, self.MLM_HEIGHT))
        screenshot.close()
        ag_image = AGImage.from_pil(scaled_screenshot)

        # Prepare the system prompt
        messages: List[LLMMessage] = []
        messages.append(SystemMessage(content=WEB_SURFER_QA_SYSTEM_MESSAGE))
        prompt = WEB_SURFER_QA_PROMPT(title, question)
        # Grow the buffer (which is added to the prompt) until we overflow the context window or run out of lines
        buffer = ""
        # for line in re.split(r"([\r\n]+)", page_markdown):
        for line in page_markdown.splitlines():
            trial_message = UserMessage(
                content=prompt + buffer + line,
                source=self.name,
            )

            try:
                remaining = self._model_client.remaining_tokens(messages + [trial_message])
            except KeyError:
                # Use the default if the model isn't found
                remaining = DEFAULT_CONTEXT_SIZE - self._model_client.count_tokens(messages + [trial_message])

            if self._model_client.model_info["vision"] and remaining <= 0:
                break

            if self._model_client.model_info["vision"] and remaining <= self.SCREENSHOT_TOKENS:
                break

            buffer += line

        # Nothing to do
        buffer = buffer.strip()
        if len(buffer) == 0:
            return "Nothing to summarize."

        # Append the message
        if self._model_client.model_info["vision"]:
            # Multimodal
            messages.append(
                UserMessage(
                    content=[
                        prompt + buffer,
                        ag_image,
                    ],
                    source=self.name,
                )
            )
        else:
            # Text only
            messages.append(
                UserMessage(
                    content=prompt + buffer,
                    source=self.name,
                )
            )

        # Generate the response
        response = await self._model_client.create(messages, cancellation_token=cancellation_token)
        self.model_usage.append(response.usage)
        scaled_screenshot.close()
        assert isinstance(response.content, str)
        return response.content

    def _to_config(self) -> MultimodalWebSurferConfig:
        return MultimodalWebSurferConfig(
            name=self.name,
            model_client=self._model_client.dump_component(),
            downloads_folder=self.downloads_folder,
            description=self.description,
            debug_dir=self.debug_dir,
            headless=self.headless,
            start_page=self.start_page,
            animate_actions=self.animate_actions,
            to_save_screenshots=self.to_save_screenshots,
            use_ocr=self.use_ocr,
            browser_channel=self.browser_channel,
            browser_data_dir=self.browser_data_dir,
            to_resize_viewport=self.to_resize_viewport,
        )

    @classmethod
    def _from_config(cls, config: MultimodalWebSurferConfig) -> Self:
        return cls(
            name=config.name,
            model_client=ChatCompletionClient.load_component(config.model_client),
            downloads_folder=config.downloads_folder,
            description=config.description or cls.DEFAULT_DESCRIPTION,
            debug_dir=config.debug_dir,
            headless=config.headless,
            start_page=config.start_page or cls.DEFAULT_START_PAGE,
            animate_actions=config.animate_actions,
            to_save_screenshots=config.to_save_screenshots,
            use_ocr=config.use_ocr,
            browser_channel=config.browser_channel,
            browser_data_dir=config.browser_data_dir,
            to_resize_viewport=config.to_resize_viewport,
        )

# From web_surfer/_multimodal_web_surfer.py
def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (MultiModalMessage,)


# From web_surfer/_prompts.py
def WEB_SURFER_QA_PROMPT(title: str, question: str | None = None) -> str:
    base_prompt = f"We are visiting the webpage '{title}'. Its full-text content are pasted below, along with a screenshot of the page's current viewport."
    if question is not None:
        return (
            f"{base_prompt} Please summarize the webpage into one or two paragraphs with respect to '{question}':\n\n"
        )
    else:
        return f"{base_prompt} Please summarize the webpage into one or two paragraphs:\n\n"


# From web_surfer/_types.py
class DOMRectangle(TypedDict):
    x: Union[int, float]
    y: Union[int, float]
    width: Union[int, float]
    height: Union[int, float]
    top: Union[int, float]
    right: Union[int, float]
    bottom: Union[int, float]
    left: Union[int, float]

# From web_surfer/_types.py
class VisualViewport(TypedDict):
    height: Union[int, float]
    width: Union[int, float]
    offsetLeft: Union[int, float]
    offsetTop: Union[int, float]
    pageLeft: Union[int, float]
    pageTop: Union[int, float]
    scale: Union[int, float]
    clientWidth: Union[int, float]
    clientHeight: Union[int, float]
    scrollWidth: Union[int, float]
    scrollHeight: Union[int, float]

# From web_surfer/_types.py
class InteractiveRegion(TypedDict):
    tag_name: str
    role: str
    aria_name: str
    v_scrollable: bool
    rects: List[DOMRectangle]

# From web_surfer/_types.py
def domrectangle_from_dict(rect: Dict[str, Any]) -> DOMRectangle:
    return DOMRectangle(
        x=_get_number(rect, "x"),
        y=_get_number(rect, "y"),
        width=_get_number(rect, "width"),
        height=_get_number(rect, "height"),
        top=_get_number(rect, "top"),
        right=_get_number(rect, "right"),
        bottom=_get_number(rect, "bottom"),
        left=_get_number(rect, "left"),
    )

# From web_surfer/_types.py
def interactiveregion_from_dict(region: Dict[str, Any]) -> InteractiveRegion:
    typed_rects: List[DOMRectangle] = []
    for rect in region["rects"]:
        typed_rects.append(domrectangle_from_dict(rect))

    return InteractiveRegion(
        tag_name=_get_str(region, "tag_name"),
        role=_get_str(region, "role"),
        aria_name=_get_str(region, "aria-name"),
        v_scrollable=_get_bool(region, "v-scrollable"),
        rects=typed_rects,
    )

# From web_surfer/_types.py
def visualviewport_from_dict(viewport: Dict[str, Any]) -> VisualViewport:
    return VisualViewport(
        height=_get_number(viewport, "height"),
        width=_get_number(viewport, "width"),
        offsetLeft=_get_number(viewport, "offsetLeft"),
        offsetTop=_get_number(viewport, "offsetTop"),
        pageLeft=_get_number(viewport, "pageLeft"),
        pageTop=_get_number(viewport, "pageTop"),
        scale=_get_number(viewport, "scale"),
        clientWidth=_get_number(viewport, "clientWidth"),
        clientHeight=_get_number(viewport, "clientHeight"),
        scrollWidth=_get_number(viewport, "scrollWidth"),
        scrollHeight=_get_number(viewport, "scrollHeight"),
    )

from playwright._impl._errors import Error
from playwright._impl._errors import TimeoutError
from _types import VisualViewport
from _types import interactiveregion_from_dict
from _types import visualviewport_from_dict
import markitdown

# From web_surfer/playwright_controller.py
class PlaywrightController:
    """
    A helper class to allow Playwright to interact with web pages to perform actions such as clicking, filling, and scrolling.

    Args:
        downloads_folder (str | None): The folder to save downloads to. If None, downloads are not saved.
        animate_actions (bool): Whether to animate the actions (create fake cursor to click).
        viewport_width (int): The width of the viewport.
        viewport_height (int): The height of the viewport.
        _download_handler (Optional[Callable[[Download], None]]): A function to handle downloads.
        to_resize_viewport (bool): Whether to resize the viewport
    """

    def __init__(
        self,
        downloads_folder: str | None = None,
        animate_actions: bool = False,
        viewport_width: int = 1440,
        viewport_height: int = 900,
        _download_handler: Optional[Callable[[Download], None]] = None,
        to_resize_viewport: bool = True,
    ) -> None:
        """
        Initialize the PlaywrightController.
        """
        assert isinstance(animate_actions, bool)
        assert isinstance(viewport_width, int)
        assert isinstance(viewport_height, int)
        assert viewport_height > 0
        assert viewport_width > 0

        self.animate_actions = animate_actions
        self.downloads_folder = downloads_folder
        self.viewport_width = viewport_width
        self.viewport_height = viewport_height
        self._download_handler = _download_handler
        self.to_resize_viewport = to_resize_viewport
        self._page_script: str = ""
        self.last_cursor_position: Tuple[float, float] = (0.0, 0.0)
        self._markdown_converter: Optional[Any] | None = None

        # Read page_script
        with open(
            os.path.join(os.path.abspath(os.path.dirname(__file__)), "page_script.js"), "rt", encoding="utf-8"
        ) as fh:
            self._page_script = fh.read()

    async def sleep(self, page: Page, duration: Union[int, float]) -> None:
        """
        Pause the execution for a specified duration.

        Args:
            page (Page): The Playwright page object.
            duration (Union[int, float]): The duration to sleep in milliseconds.
        """
        assert page is not None
        await page.wait_for_timeout(duration * 1000)

    async def get_interactive_rects(self, page: Page) -> Dict[str, InteractiveRegion]:
        """
        Retrieve interactive regions from the web page.

        Args:
            page (Page): The Playwright page object.

        Returns:
            Dict[str, InteractiveRegion]: A dictionary of interactive regions.
        """
        assert page is not None
        # Read the regions from the DOM
        try:
            await page.evaluate(self._page_script)
        except Exception:
            pass
        result = cast(Dict[str, Dict[str, Any]], await page.evaluate("MultimodalWebSurfer.getInteractiveRects();"))

        # Convert the results into appropriate types
        assert isinstance(result, dict)
        typed_results: Dict[str, InteractiveRegion] = {}
        for k in result:
            assert isinstance(k, str)
            typed_results[k] = interactiveregion_from_dict(result[k])

        return typed_results

    async def get_visual_viewport(self, page: Page) -> VisualViewport:
        """
        Retrieve the visual viewport of the web page.

        Args:
            page (Page): The Playwright page object.

        Returns:
            VisualViewport: The visual viewport of the page.
        """
        assert page is not None
        try:
            await page.evaluate(self._page_script)
        except Exception:
            pass
        return visualviewport_from_dict(await page.evaluate("MultimodalWebSurfer.getVisualViewport();"))

    async def get_focused_rect_id(self, page: Page) -> str | None:
        """
        Retrieve the ID of the currently focused element.

        Args:
            page (Page): The Playwright page object.

        Returns:
            str: The ID of the focused element or None if no control has focus.
        """
        assert page is not None
        try:
            await page.evaluate(self._page_script)
        except Exception:
            pass
        result = await page.evaluate("MultimodalWebSurfer.getFocusedElementId();")
        return None if result is None else str(result)

    async def get_page_metadata(self, page: Page) -> Dict[str, Any]:
        """
        Retrieve metadata from the web page.

        Args:
            page (Page): The Playwright page object.

        Returns:
            Dict[str, Any]: A dictionary of page metadata.
        """
        assert page is not None
        try:
            await page.evaluate(self._page_script)
        except Exception:
            pass
        result = await page.evaluate("MultimodalWebSurfer.getPageMetadata();")
        assert isinstance(result, dict)
        return cast(Dict[str, Any], result)

    async def on_new_page(self, page: Page) -> None:
        """
        Handle actions to perform on a new page.

        Args:
            page (Page): The Playwright page object.
        """
        assert page is not None
        page.on("download", self._download_handler)  # type: ignore
        if self.to_resize_viewport and self.viewport_width and self.viewport_height:
            await page.set_viewport_size({"width": self.viewport_width, "height": self.viewport_height})
        await self.sleep(page, 0.2)
        await page.add_init_script(path=os.path.join(os.path.abspath(os.path.dirname(__file__)), "page_script.js"))
        await page.wait_for_load_state()

    async def back(self, page: Page) -> None:
        """
        Navigate back to the previous page.

        Args:
            page (Page): The Playwright page object.
        """
        assert page is not None
        await page.go_back()

    async def visit_page(self, page: Page, url: str) -> Tuple[bool, bool]:
        """
        Visit a specified URL.

        Args:
            page (Page): The Playwright page object.
            url (str): The URL to visit.

        Returns:
            Tuple[bool, bool]: A tuple indicating whether to reset prior metadata hash and last download.
        """
        assert page is not None
        reset_prior_metadata_hash = False
        reset_last_download = False
        try:
            # Regular webpage
            await page.goto(url)
            await page.wait_for_load_state()
            reset_prior_metadata_hash = True
        except Exception as e_outer:
            # Downloaded file
            if self.downloads_folder and "net::ERR_ABORTED" in str(e_outer):
                async with page.expect_download() as download_info:
                    try:
                        await page.goto(url)
                    except Exception as e_inner:
                        if "net::ERR_ABORTED" in str(e_inner):
                            pass
                        else:
                            raise e_inner
                    download = await download_info.value
                    fname = os.path.join(self.downloads_folder, download.suggested_filename)
                    await download.save_as(fname)
                    message = f"<body style=\"margin: 20px;\"><h1>Successfully downloaded '{download.suggested_filename}' to local path:<br><br>{fname}</h1></body>"
                    await page.goto(
                        "data:text/html;base64," + base64.b64encode(message.encode("utf-8")).decode("utf-8")
                    )
                    reset_last_download = True
            else:
                raise e_outer
        return reset_prior_metadata_hash, reset_last_download

    async def page_down(self, page: Page) -> None:
        """
        Scroll the page down by one viewport height minus 50 pixels.

        Args:
            page (Page): The Playwright page object.
        """
        assert page is not None
        await page.evaluate(f"window.scrollBy(0, {self.viewport_height-50});")

    async def page_up(self, page: Page) -> None:
        """
        Scroll the page up by one viewport height minus 50 pixels.

        Args:
            page (Page): The Playwright page object.
        """
        assert page is not None
        await page.evaluate(f"window.scrollBy(0, -{self.viewport_height-50});")

    async def gradual_cursor_animation(
        self, page: Page, start_x: float, start_y: float, end_x: float, end_y: float
    ) -> None:
        """
        Animate the cursor movement gradually from start to end coordinates.

        Args:
            page (Page): The Playwright page object.
            start_x (float): The starting x-coordinate.
            start_y (float): The starting y-coordinate.
            end_x (float): The ending x-coordinate.
            end_y (float): The ending y-coordinate.
        """
        # animation helper
        steps = 20
        for step in range(steps):
            x = start_x + (end_x - start_x) * (step / steps)
            y = start_y + (end_y - start_y) * (step / steps)
            # await page.mouse.move(x, y, steps=1)
            await page.evaluate(f"""
                (function() {{
                    let cursor = document.getElementById('red-cursor');
                    cursor.style.left = '{x}px';
                    cursor.style.top = '{y}px';
                }})();
            """)
            await asyncio.sleep(0.05)

        self.last_cursor_position = (end_x, end_y)

    async def add_cursor_box(self, page: Page, identifier: str) -> None:
        """
        Add a red cursor box around the element with the given identifier.

        Args:
            page (Page): The Playwright page object.
            identifier (str): The element identifier.
        """
        # animation helper
        await page.evaluate(f"""
            (function() {{
                let elm = document.querySelector("[__elementId='{identifier}']");
                if (elm) {{
                    elm.style.transition = 'border 0.3s ease-in-out';
                    elm.style.border = '2px solid red';
                }}
            }})();
        """)
        await asyncio.sleep(0.3)

        # Create a red cursor
        await page.evaluate("""
            (function() {
                let cursor = document.createElement('div');
                cursor.id = 'red-cursor';
                cursor.style.width = '10px';
                cursor.style.height = '10px';
                cursor.style.backgroundColor = 'red';
                cursor.style.position = 'absolute';
                cursor.style.borderRadius = '50%';
                cursor.style.zIndex = '10000';
                document.body.appendChild(cursor);
            })();
        """)

    async def remove_cursor_box(self, page: Page, identifier: str) -> None:
        """
        Remove the red cursor box around the element with the given identifier.

        Args:
            page (Page): The Playwright page object.
            identifier (str): The element identifier.
        """
        # Remove the highlight and cursor
        await page.evaluate(f"""
            (function() {{
                let elm = document.querySelector("[__elementId='{identifier}']");
                if (elm) {{
                    elm.style.border = '';
                }}
                let cursor = document.getElementById('red-cursor');
                if (cursor) {{
                    cursor.remove();
                }}
            }})();
        """)

    async def click_id(self, page: Page, identifier: str) -> Page | None:
        """
        Click the element with the given identifier.

        Args:
            page (Page): The Playwright page object.
            identifier (str): The element identifier.

        Returns:
            Page | None: The new page if a new page is opened, otherwise None.
        """
        new_page: Page | None = None
        assert page is not None
        target = page.locator(f"[__elementId='{identifier}']")

        # See if it exists
        try:
            await target.wait_for(timeout=5000)
        except TimeoutError:
            raise ValueError("No such element.") from None

        # Click it
        await target.scroll_into_view_if_needed()
        await asyncio.sleep(0.3)

        box = cast(Dict[str, Union[int, float]], await target.bounding_box())

        if self.animate_actions:
            await self.add_cursor_box(page, identifier)
            # Move cursor to the box slowly
            start_x, start_y = self.last_cursor_position
            end_x, end_y = box["x"] + box["width"] / 2, box["y"] + box["height"] / 2
            await self.gradual_cursor_animation(page, start_x, start_y, end_x, end_y)
            await asyncio.sleep(0.1)

            try:
                # Give it a chance to open a new page
                async with page.expect_event("popup", timeout=1000) as page_info:  # type: ignore
                    await page.mouse.click(end_x, end_y, delay=10)
                    new_page = await page_info.value  # type: ignore
                    assert isinstance(new_page, Page)
                    await self.on_new_page(new_page)
            except TimeoutError:
                pass
            await self.remove_cursor_box(page, identifier)

        else:
            try:
                # Give it a chance to open a new page
                async with page.expect_event("popup", timeout=1000) as page_info:  # type: ignore
                    await page.mouse.click(box["x"] + box["width"] / 2, box["y"] + box["height"] / 2, delay=10)
                    new_page = await page_info.value  # type: ignore
                    assert isinstance(new_page, Page)
                    await self.on_new_page(new_page)
            except TimeoutError:
                pass
        return new_page  # type: ignore

    async def hover_id(self, page: Page, identifier: str) -> None:
        """
        Hover the mouse over the element with the given identifier.

        Args:
            page (Page): The Playwright page object.
            identifier (str): The element identifier.
        """
        assert page is not None
        target = page.locator(f"[__elementId='{identifier}']")

        # See if it exists
        try:
            await target.wait_for(timeout=5000)
        except TimeoutError:
            raise ValueError("No such element.") from None

        # Hover over it
        await target.scroll_into_view_if_needed()
        await asyncio.sleep(0.3)

        box = cast(Dict[str, Union[int, float]], await target.bounding_box())

        if self.animate_actions:
            await self.add_cursor_box(page, identifier)
            # Move cursor to the box slowly
            start_x, start_y = self.last_cursor_position
            end_x, end_y = box["x"] + box["width"] / 2, box["y"] + box["height"] / 2
            await self.gradual_cursor_animation(page, start_x, start_y, end_x, end_y)
            await asyncio.sleep(0.1)
            await page.mouse.move(box["x"] + box["width"] / 2, box["y"] + box["height"] / 2)

            await self.remove_cursor_box(page, identifier)
        else:
            await page.mouse.move(box["x"] + box["width"] / 2, box["y"] + box["height"] / 2)

    async def fill_id(self, page: Page, identifier: str, value: str, press_enter: bool = True) -> None:
        """
        Fill the element with the given identifier with the specified value.

        Args:
            page (Page): The Playwright page object.
            identifier (str): The element identifier.
            value (str): The value to fill.
        """
        assert page is not None
        target = page.locator(f"[__elementId='{identifier}']")

        # See if it exists
        try:
            await target.wait_for(timeout=5000)
        except TimeoutError:
            raise ValueError("No such element.") from None

        # Fill it
        await target.scroll_into_view_if_needed()
        box = cast(Dict[str, Union[int, float]], await target.bounding_box())

        if self.animate_actions:
            await self.add_cursor_box(page, identifier)
            # Move cursor to the box slowly
            start_x, start_y = self.last_cursor_position
            end_x, end_y = box["x"] + box["width"] / 2, box["y"] + box["height"] / 2
            await self.gradual_cursor_animation(page, start_x, start_y, end_x, end_y)
            await asyncio.sleep(0.1)

        # Focus on the element
        await target.focus()
        if self.animate_actions:
            # fill char by char to mimic human speed for short text and type fast for long text
            if len(value) < 100:
                delay_typing_speed = 50 + 100 * random.random()
            else:
                delay_typing_speed = 10
            await target.press_sequentially(value, delay=delay_typing_speed)
        else:
            try:
                await target.fill(value)
            except PlaywrightError:
                await target.press_sequentially(value)
        if press_enter:
            await target.press("Enter")

        if self.animate_actions:
            await self.remove_cursor_box(page, identifier)

    async def scroll_id(self, page: Page, identifier: str, direction: str) -> None:
        """
        Scroll the element with the given identifier in the specified direction.

        Args:
            page (Page): The Playwright page object.
            identifier (str): The element identifier.
            direction (str): The direction to scroll ("up" or "down").
        """
        assert page is not None
        await page.evaluate(
            f"""
        (function() {{
            let elm = document.querySelector("[__elementId='{identifier}']");
            if (elm) {{
                if ("{direction}" == "up") {{
                    elm.scrollTop = Math.max(0, elm.scrollTop - elm.clientHeight);
                }}
                else {{
                    elm.scrollTop = Math.min(elm.scrollHeight - elm.clientHeight, elm.scrollTop + elm.clientHeight);
                }}
            }}
        }})();
    """
        )

    async def get_webpage_text(self, page: Page, n_lines: int = 50) -> str:
        """
        Retrieve the text content of the web page.

        Args:
            page (Page): The Playwright page object.
            n_lines (int): The number of lines to return from the page inner text.

        Returns:
            str: The text content of the page.
        """
        assert page is not None
        try:
            text_in_viewport = await page.evaluate("""() => {
                return document.body.innerText;
            }""")
            text_in_viewport = "\n".join(text_in_viewport.split("\n")[:n_lines])
            # remove empty lines
            text_in_viewport = "\n".join([line for line in text_in_viewport.split("\n") if line.strip()])
            assert isinstance(text_in_viewport, str)
            return text_in_viewport
        except Exception:
            return ""

    async def get_visible_text(self, page: Page) -> str:
        """
        Retrieve the text content of the browser viewport (approximately).

        Args:
            page (Page): The Playwright page object.

        Returns:
            str: The text content of the page.
        """
        assert page is not None
        try:
            await page.evaluate(self._page_script)
        except Exception:
            pass
        result = await page.evaluate("MultimodalWebSurfer.getVisibleText();")
        assert isinstance(result, str)
        return result

    async def get_page_markdown(self, page: Page) -> str:
        """
        Retrieve the markdown content of the web page.
        Currently not implemented.

        Args:
            page (Page): The Playwright page object.

        Returns:
            str: The markdown content of the page.
        """
        assert page is not None
        if self._markdown_converter is None and markitdown is not None:
            self._markdown_converter = markitdown.MarkItDown()
            assert self._markdown_converter is not None
            html = await page.evaluate("document.documentElement.outerHTML;")
            res = self._markdown_converter.convert_stream(
                io.BytesIO(html.encode("utf-8")), file_extension=".html", url=page.url
            )
            assert hasattr(res, "text_content") and isinstance(res.text_content, str)
            return res.text_content
        else:
            return await self.get_webpage_text(page, n_lines=200)


from _markdown_file_browser import MarkdownFileBrowser
from _tool_definitions import TOOL_FIND_NEXT
from _tool_definitions import TOOL_FIND_ON_PAGE_CTRL_F
from _tool_definitions import TOOL_OPEN_PATH
from _tool_definitions import TOOL_PAGE_DOWN
from _tool_definitions import TOOL_PAGE_UP

# From file_surfer/_file_surfer.py
class FileSurferConfig(BaseModel):
    """Configuration for FileSurfer agent"""

    name: str
    model_client: ComponentModel
    description: str | None = None

# From file_surfer/_file_surfer.py
class FileSurfer(BaseChatAgent, Component[FileSurferConfig]):
    """An agent, used by MagenticOne, that acts as a local file previewer. FileSurfer can open and read a variety of common file types, and can navigate the local file hierarchy.

    Installation:

    .. code-block:: bash

        pip install "autogen-ext[file-surfer]"

    Args:
        name (str): The agent's name
        model_client (ChatCompletionClient): The model to use (must be tool-use enabled)
        description (str): The agent's description used by the team. Defaults to DEFAULT_DESCRIPTION
        base_path (str): The base path to use for the file browser. Defaults to the current working directory.

    """

    component_config_schema = FileSurferConfig
    component_provider_override = "autogen_ext.agents.file_surfer.FileSurfer"

    DEFAULT_DESCRIPTION = "An agent that can handle local files."

    DEFAULT_SYSTEM_MESSAGES = [
        SystemMessage(
            content="""
        You are a helpful AI Assistant.
        When given a user query, use available functions to help the user with their request."""
        ),
    ]

    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        description: str = DEFAULT_DESCRIPTION,
        base_path: str = os.getcwd(),
    ) -> None:
        super().__init__(name, description)
        self._model_client = model_client
        self._chat_history: List[LLMMessage] = []
        self._browser = MarkdownFileBrowser(viewport_size=1024 * 5, base_path=base_path)

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:
        for chat_message in messages:
            self._chat_history.append(chat_message.to_model_message())
        try:
            _, content = await self._generate_reply(cancellation_token=cancellation_token)
            self._chat_history.append(AssistantMessage(content=content, source=self.name))
            return Response(chat_message=TextMessage(content=content, source=self.name))

        except BaseException:
            content = f"File surfing error:\n\n{traceback.format_exc()}"
            self._chat_history.append(AssistantMessage(content=content, source=self.name))
            return Response(chat_message=TextMessage(content=content, source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        self._chat_history.clear()

    def _get_browser_state(self) -> Tuple[str, str]:
        """
        Get the current state of the browser, including the header and content.
        """
        header = f"Path: {self._browser.path}\n"

        if self._browser.page_title is not None:
            header += f"Title: {self._browser.page_title}\n"

        current_page = self._browser.viewport_current_page
        total_pages = len(self._browser.viewport_pages)
        header += f"Viewport position: Showing page {current_page+1} of {total_pages}.\n"

        return (header, self._browser.viewport)

    async def _generate_reply(self, cancellation_token: CancellationToken) -> Tuple[bool, str]:
        history = self._chat_history[0:-1]
        last_message = self._chat_history[-1]
        assert isinstance(last_message, UserMessage)

        task_content = last_message.content  # the last message from the sender is the task

        assert self._browser is not None

        context_message = UserMessage(
            source="user",
            content=f"Your file viewer is currently open to the file or directory '{self._browser.page_title}' with path '{self._browser.path}'.",
        )

        task_message = UserMessage(
            source="user",
            content=task_content,
        )

        create_result = await self._model_client.create(
            messages=self._get_compatible_context(history + [context_message, task_message]),
            tools=[
                TOOL_OPEN_PATH,
                TOOL_PAGE_DOWN,
                TOOL_PAGE_UP,
                TOOL_FIND_NEXT,
                TOOL_FIND_ON_PAGE_CTRL_F,
            ],
            cancellation_token=cancellation_token,
        )

        response = create_result.content

        if isinstance(response, str):
            # Answer directly.
            return False, response

        elif isinstance(response, list) and all(isinstance(item, FunctionCall) for item in response):
            function_calls = response
            for function_call in function_calls:
                tool_name = function_call.name

                try:
                    arguments = json.loads(function_call.arguments)
                except json.JSONDecodeError as e:
                    error_str = f"File surfer encountered an error decoding JSON arguments: {e}"
                    return False, error_str

                if tool_name == "open_path":
                    path = arguments["path"]
                    self._browser.open_path(path)
                elif tool_name == "page_up":
                    self._browser.page_up()
                elif tool_name == "page_down":
                    self._browser.page_down()
                elif tool_name == "find_on_page_ctrl_f":
                    search_string = arguments["search_string"]
                    self._browser.find_on_page(search_string)
                elif tool_name == "find_next":
                    self._browser.find_next()
            header, content = self._get_browser_state()
            final_response = header.strip() + "\n=======================\n" + content
            return False, final_response

        final_response = "TERMINATE"
        return False, final_response

    def _get_compatible_context(self, messages: List[LLMMessage]) -> List[LLMMessage]:
        """Ensure that the messages are compatible with the underlying client, by removing images if needed."""
        if self._model_client.model_info["vision"]:
            return messages
        else:
            return remove_images(messages)

    def _to_config(self) -> FileSurferConfig:
        return FileSurferConfig(
            name=self.name,
            model_client=self._model_client.dump_component(),
            description=self.description,
        )

    @classmethod
    def _from_config(cls, config: FileSurferConfig) -> Self:
        return cls(
            name=config.name,
            model_client=ChatCompletionClient.load_component(config.model_client),
            description=config.description or cls.DEFAULT_DESCRIPTION,
        )

from markitdown import FileConversionException
from markitdown import MarkItDown
from markitdown import UnsupportedFormatException

# From file_surfer/_markdown_file_browser.py
class MarkdownFileBrowser:
    """
    (In preview) An extremely simple Markdown-powered file browser.
    """

    # TODO: Fix unfollowed import
    def __init__(  # type: ignore
        self,
        viewport_size: Union[int, None] = 1024 * 8,
        base_path: str | None = os.getcwd(),
        cwd: str | None = None,
    ):
        """
        Instantiate a new MarkdownFileBrowser.

        Arguments:
            viewport_size: Approximately how many *characters* fit in the viewport. Viewport dimensions are adjusted dynamically to avoid cutting off words (default: 8192).
            base_path: The base path to use for the file browser. Files outside this path cannot be accessed. Defaults to the current working directory.
            cwd: The browser's current working directory. Defaults to the system's current working directory.
        """
        self.viewport_size = viewport_size  # Applies only to the standard uri types
        self.history: List[Tuple[str, float]] = list()
        self.page_title: Optional[str] = None
        self.viewport_current_page = 0
        self.viewport_pages: List[Tuple[int, int]] = list()
        self._markdown_converter = MarkItDown()
        self._base_path = None if base_path is None else os.path.realpath(base_path)
        self._page_content: str = ""
        self._find_on_page_query: Union[str, None] = None
        self._find_on_page_last_result: Union[int, None] = None  # Location of the last result

        # Set the working directory
        if cwd is None:
            if self._validate_path(os.getcwd()):
                # Use the current working directory if it's in the base path
                cwd = os.path.realpath(os.getcwd())
            elif self._base_path is not None:
                # Otherwise, use the base path
                cwd = os.path.realpath(self._base_path)
            else:
                raise ValueError("No valid working directory (cwd) provided.")
        elif not self._validate_path(cwd):
            # A cwd was provided, but it is not valid
            raise ValueError(f"Working directory (cwd) '{cwd}' is not valid. It must be within the base path.")

        # Populate the history with the current working directory
        self.set_path(os.path.realpath(cwd))

    @property
    def path(self) -> str:
        """Return the path of the current page."""
        assert len(self.history) > 0
        return self.history[-1][0]

    def _validate_path(self, path: str) -> bool:
        """Validates the path to ensure it is within the base path.

        Arguments:
            path: The path to validate.
        Returns:
            True if the path is valid, False otherwise.
        """
        if self._base_path is None:
            return True

        # Normalize the paths
        path = os.path.realpath(path)
        base = os.path.realpath(self._base_path)

        # Check if the path is within the base path
        if os.path.commonpath([path, base]) != base:
            return False

        return True

    def set_path(self, path: str) -> None:
        """Sets the path of the current page.
        This will result in the file being opened for reading.

        Arguments:
            path: An absolute or relative path of the file or directory to open."
        """

        # Handle relative paths
        path = os.path.expanduser(path)
        if not os.path.isabs(path):
            if os.path.isfile(self.path):
                path = os.path.abspath(os.path.join(os.path.dirname(self.path), path))
            elif os.path.isdir(self.path):
                path = os.path.abspath(os.path.join(self.path, path))
            # If neither a file or a directory, take it verbatim

        # Validating the path wrt. the base path is done in _open_path
        path = os.path.realpath(path)

        self.history.append((path, time.time()))
        self._open_path(path)
        self.viewport_current_page = 0
        self.find_on_page_query = None
        self.find_on_page_viewport = None

    @property
    def viewport(self) -> str:
        """Return the content of the current viewport."""
        bounds = self.viewport_pages[self.viewport_current_page]
        return self.page_content[bounds[0] : bounds[1]]

    @property
    def page_content(self) -> str:
        """Return the full contents of the current page."""
        return self._page_content

    def _set_page_content(self, content: str, split_pages: bool = True) -> None:
        """Sets the text content of the current page."""
        self._page_content = content

        if split_pages:
            self._split_pages()
        else:
            self.viewport_pages = [(0, len(self._page_content))]

        if self.viewport_current_page >= len(self.viewport_pages):
            self.viewport_current_page = len(self.viewport_pages) - 1

    def page_down(self) -> None:
        """Move the viewport down one page, if possible."""
        self.viewport_current_page = min(self.viewport_current_page + 1, len(self.viewport_pages) - 1)

    def page_up(self) -> None:
        """Move the viewport up one page, if possible."""
        self.viewport_current_page = max(self.viewport_current_page - 1, 0)

    def find_on_page(self, query: str) -> Union[str, None]:
        """Searches for the query from the current viewport forward, looping back to the start if necessary."""

        # Did we get here via a previous find_on_page search with the same query?
        # If so, map to find_next
        if query == self._find_on_page_query and self.viewport_current_page == self._find_on_page_last_result:
            return self.find_next()

        # Ok it's a new search start from the current viewport
        self._find_on_page_query = query
        viewport_match = self._find_next_viewport(query, self.viewport_current_page)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

    def find_next(self) -> Union[str, None]:
        """Scroll to the next viewport that matches the query"""

        if self._find_on_page_query is None:
            return None

        starting_viewport = self._find_on_page_last_result
        if starting_viewport is None:
            starting_viewport = 0
        else:
            starting_viewport += 1
            if starting_viewport >= len(self.viewport_pages):
                starting_viewport = 0

        viewport_match = self._find_next_viewport(self._find_on_page_query, starting_viewport)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

    def _find_next_viewport(self, query: Optional[str], starting_viewport: int) -> Union[int, None]:
        """Search for matches between the starting viewport looping when reaching the end."""

        if query is None:
            return None

        # Normalize the query, and convert to a regular expression
        nquery = re.sub(r"\*", "__STAR__", query)
        nquery = " " + (" ".join(re.split(r"\W+", nquery))).strip() + " "
        nquery = nquery.replace(" __STAR__ ", "__STAR__ ")  # Merge isolated stars with prior word
        nquery = nquery.replace("__STAR__", ".*").lower()

        if nquery.strip() == "":
            return None

        idxs: List[int] = list()
        idxs.extend(range(starting_viewport, len(self.viewport_pages)))
        idxs.extend(range(0, starting_viewport))

        for i in idxs:
            bounds = self.viewport_pages[i]
            content = self.page_content[bounds[0] : bounds[1]]

            # TODO: Remove markdown links and images
            ncontent = " " + (" ".join(re.split(r"\W+", content))).strip().lower() + " "
            if re.search(nquery, ncontent):
                return i

        return None

    def open_path(self, path: str) -> str:
        """Open a file or directory in the file surfer."""
        self.set_path(path)
        return self.viewport

    def _split_pages(self) -> None:
        """Split the page contents into pages that are approximately the viewport size. Small deviations are permitted to ensure words are not broken."""
        # Handle empty pages
        if len(self._page_content) == 0:
            self.viewport_pages = [(0, 0)]
            return

        # Break the viewport into pages
        self.viewport_pages = []
        start_idx = 0
        while start_idx < len(self._page_content):
            end_idx = min(start_idx + self.viewport_size, len(self._page_content))  # type: ignore[operator]
            # Adjust to end on a space
            while end_idx < len(self._page_content) and self._page_content[end_idx - 1] not in [" ", "\t", "\r", "\n"]:
                end_idx += 1
            self.viewport_pages.append((start_idx, end_idx))
            start_idx = end_idx

    def _open_path(
        self,
        path: str,
    ) -> None:
        """Open a file for reading, converting it to Markdown in the process.

        Arguments:
            path: The path of the file or directory to open.
        """

        if not self._validate_path(path):
            # Not robust to TOCTOU issues.
            # Mitigate by running with limited permissions, or use a sandbox.
            self.page_title = "FileNotFoundError"
            self._set_page_content(f"# FileNotFoundError\n\nFile not found: {path}")
        else:
            try:
                if os.path.isdir(path):  # TODO: Fix markdown_converter types
                    res = self._markdown_converter.convert_stream(  # type: ignore
                        io.BytesIO(self._fetch_local_dir(path).encode("utf-8")), file_extension=".txt"
                    )
                    assert self._validate_path(path)
                    self.page_title = res.title
                    self._set_page_content(res.text_content, split_pages=False)
                else:
                    res = self._markdown_converter.convert_local(path)
                    assert self._validate_path(path)
                    self.page_title = res.title
                    self._set_page_content(res.text_content)
            except UnsupportedFormatException:
                self.page_title = "UnsupportedFormatException"
                self._set_page_content(f"# UnsupportedFormatException\n\nCannot preview '{path}' as Markdown.")
            except FileConversionException:
                self.page_title = "FileConversionException."
                self._set_page_content(f"# FileConversionException\n\nError converting '{path}' to Markdown.")
            except FileNotFoundError:
                self.page_title = "FileNotFoundError"
                self._set_page_content(f"# FileNotFoundError\n\nFile not found: {path}")

    def _fetch_local_dir(self, local_path: str) -> str:
        """Render a local directory listing in HTML to assist with local file browsing via the "file://" protocol.
        Through rendered in HTML, later parts of the pipeline will convert the listing to Markdown.

        Arguments:
            local_path: A path to the local directory whose contents are to be listed.

        Returns:
            A directory listing, rendered in HTML.
        """
        listing = f"""
# Index of {local_path}

| Name | Size | Date Modified |
| ---- | ---- | ------------- |
| .. (parent directory) | | |
"""
        for entry in os.listdir(local_path):
            size = ""
            full_path = os.path.join(local_path, entry)

            mtime = ""
            try:
                mtime = datetime.datetime.fromtimestamp(os.path.getmtime(full_path)).strftime("%Y-%m-%d %H:%M")
            except Exception as e:
                # Handles PermissionError, etc.
                mtime = f"N/A: {type(e).__name__}"

            if os.path.isdir(full_path):
                entry = entry + os.path.sep
            else:
                try:
                    size = str(os.path.getsize(full_path))
                except Exception as e:
                    # Handles PermissionError, etc.
                    size = f"N/A: {type(e).__name__}"

            listing += f"| {entry} | {size} | {mtime} |\n"
        return listing

# From file_surfer/_markdown_file_browser.py
def path(self) -> str:
        """Return the path of the current page."""
        assert len(self.history) > 0
        return self.history[-1][0]

# From file_surfer/_markdown_file_browser.py
def set_path(self, path: str) -> None:
        """Sets the path of the current page.
        This will result in the file being opened for reading.

        Arguments:
            path: An absolute or relative path of the file or directory to open."
        """

        # Handle relative paths
        path = os.path.expanduser(path)
        if not os.path.isabs(path):
            if os.path.isfile(self.path):
                path = os.path.abspath(os.path.join(os.path.dirname(self.path), path))
            elif os.path.isdir(self.path):
                path = os.path.abspath(os.path.join(self.path, path))
            # If neither a file or a directory, take it verbatim

        # Validating the path wrt. the base path is done in _open_path
        path = os.path.realpath(path)

        self.history.append((path, time.time()))
        self._open_path(path)
        self.viewport_current_page = 0
        self.find_on_page_query = None
        self.find_on_page_viewport = None

# From file_surfer/_markdown_file_browser.py
def viewport(self) -> str:
        """Return the content of the current viewport."""
        bounds = self.viewport_pages[self.viewport_current_page]
        return self.page_content[bounds[0] : bounds[1]]

# From file_surfer/_markdown_file_browser.py
def page_content(self) -> str:
        """Return the full contents of the current page."""
        return self._page_content

# From file_surfer/_markdown_file_browser.py
def page_down(self) -> None:
        """Move the viewport down one page, if possible."""
        self.viewport_current_page = min(self.viewport_current_page + 1, len(self.viewport_pages) - 1)

# From file_surfer/_markdown_file_browser.py
def page_up(self) -> None:
        """Move the viewport up one page, if possible."""
        self.viewport_current_page = max(self.viewport_current_page - 1, 0)

# From file_surfer/_markdown_file_browser.py
def find_on_page(self, query: str) -> Union[str, None]:
        """Searches for the query from the current viewport forward, looping back to the start if necessary."""

        # Did we get here via a previous find_on_page search with the same query?
        # If so, map to find_next
        if query == self._find_on_page_query and self.viewport_current_page == self._find_on_page_last_result:
            return self.find_next()

        # Ok it's a new search start from the current viewport
        self._find_on_page_query = query
        viewport_match = self._find_next_viewport(query, self.viewport_current_page)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

# From file_surfer/_markdown_file_browser.py
def find_next(self) -> Union[str, None]:
        """Scroll to the next viewport that matches the query"""

        if self._find_on_page_query is None:
            return None

        starting_viewport = self._find_on_page_last_result
        if starting_viewport is None:
            starting_viewport = 0
        else:
            starting_viewport += 1
            if starting_viewport >= len(self.viewport_pages):
                starting_viewport = 0

        viewport_match = self._find_next_viewport(self._find_on_page_query, starting_viewport)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

# From file_surfer/_markdown_file_browser.py
def open_path(self, path: str) -> str:
        """Open a file or directory in the file surfer."""
        self.set_path(path)
        return self.viewport

from autogen_core.models import CreateResult
from autogen_core.models import ModelCapabilities
from autogen_core.models import validate_model_info
from autogen_core.tools import Tool

# From replay/_replay_chat_completion_client.py
class ReplayChatCompletionClientConfig(BaseModel):
    """ReplayChatCompletionClient configuration."""

    chat_completions: Sequence[Union[str, CreateResult]]
    model_info: Optional[ModelInfo] = None

# From replay/_replay_chat_completion_client.py
class ReplayChatCompletionClient(ChatCompletionClient, Component[ReplayChatCompletionClientConfig]):
    """
    A mock chat completion client that replays predefined responses using an index-based approach.

    This class simulates a chat completion client by replaying a predefined list of responses. It supports both single completion and streaming responses. The responses can be either strings or CreateResult objects. The client now uses an index-based approach to access the responses, allowing for resetting the state.

    .. note::
        The responses can be either strings or CreateResult objects.

    Args:
        chat_completions (Sequence[Union[str, CreateResult]]): A list of predefined responses to replay.

    Raises:
        ValueError("No more mock responses available"): If the list of provided outputs are exhausted.

    Examples:

    Simple chat completion client to return pre-defined responses.

        .. code-block:: python

            from autogen_core.models import UserMessage
            from autogen_ext.models.replay import ReplayChatCompletionClient


            async def example():
                chat_completions = [
                    "Hello, how can I assist you today?",
                    "I'm happy to help with any questions you have.",
                    "Is there anything else I can assist you with?",
                ]
                client = ReplayChatCompletionClient(chat_completions)
                messages = [UserMessage(content="What can you do?", source="user")]
                response = await client.create(messages)
                print(response.content)  # Output: "Hello, how can I assist you today?"

    Simple streaming chat completion client to return pre-defined responses

        .. code-block:: python

            import asyncio
            from autogen_core.models import UserMessage
            from autogen_ext.models.replay import ReplayChatCompletionClient


            async def example():
                chat_completions = [
                    "Hello, how can I assist you today?",
                    "I'm happy to help with any questions you have.",
                    "Is there anything else I can assist you with?",
                ]
                client = ReplayChatCompletionClient(chat_completions)
                messages = [UserMessage(content="What can you do?", source="user")]

                async for token in client.create_stream(messages):
                    print(token, end="")  # Output: "Hello, how can I assist you today?"

                async for token in client.create_stream(messages):
                    print(token, end="")  # Output: "I'm happy to help with any questions you have."

                asyncio.run(example())

    Using `.reset` to reset the chat client state

        .. code-block:: python

            import asyncio
            from autogen_core.models import UserMessage
            from autogen_ext.models.replay import ReplayChatCompletionClient


            async def example():
                chat_completions = [
                    "Hello, how can I assist you today?",
                ]
                client = ReplayChatCompletionClient(chat_completions)
                messages = [UserMessage(content="What can you do?", source="user")]
                response = await client.create(messages)
                print(response.content)  # Output: "Hello, how can I assist you today?"

                response = await client.create(messages)  # Raises ValueError("No more mock responses available")

                client.reset()  # Reset the client state (current index of message and token usages)
                response = await client.create(messages)
                print(response.content)  # Output: "Hello, how can I assist you today?" again


            asyncio.run(example())

    """

    __protocol__: ChatCompletionClient
    component_type = "replay_chat_completion_client"
    component_provider_override = "autogen_ext.models.replay.ReplayChatCompletionClient"
    component_config_schema = ReplayChatCompletionClientConfig

    # TODO: Support logprobs in Responses

    def __init__(
        self,
        chat_completions: Sequence[Union[str, CreateResult]],
        model_info: Optional[ModelInfo] = None,
    ):
        self.chat_completions = list(chat_completions)
        self.provided_message_count = len(self.chat_completions)
        if model_info is not None:
            self._model_info = model_info
            validate_model_info(self._model_info)
        else:
            self._model_info = ModelInfo(
                vision=False,
                function_calling=False,
                json_output=False,
                family=ModelFamily.UNKNOWN,
                structured_output=False,
            )
        self._total_available_tokens = 10000
        self._cur_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)
        self._total_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)
        self._current_index = 0
        self._cached_bool_value = True
        self._create_calls: List[Dict[str, Any]] = []

    @property
    def create_calls(self) -> List[Dict[str, Any]]:
        """Return the arguments of the calls made to the create method."""
        return self._create_calls

    async def create(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> CreateResult:
        """Return the next completion from the list."""
        # Warn if tool_choice is specified since it's ignored in replay mode
        if tool_choice != "auto":
            logger.warning("tool_choice parameter specified but is ignored in replay mode")

        if self._current_index >= len(self.chat_completions):
            raise ValueError("No more mock responses available")

        response = self.chat_completions[self._current_index]
        _, prompt_token_count = self._tokenize(messages)
        if isinstance(response, str):
            _, output_token_count = self._tokenize(response)
            self._cur_usage = RequestUsage(prompt_tokens=prompt_token_count, completion_tokens=output_token_count)
            response = CreateResult(
                finish_reason="stop", content=response, usage=self._cur_usage, cached=self._cached_bool_value
            )
        else:
            self._cur_usage = RequestUsage(
                prompt_tokens=prompt_token_count, completion_tokens=response.usage.completion_tokens
            )

        self._update_total_usage()
        self._current_index += 1
        self._create_calls.append(
            {
                "messages": messages,
                "tools": tools,
                "json_output": json_output,
                "extra_create_args": extra_create_args,
                "cancellation_token": cancellation_token,
            }
        )
        return response

    async def create_stream(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> AsyncGenerator[Union[str, CreateResult], None]:
        """Return the next completion as a stream."""
        # Warn if tool_choice is specified since it's ignored in replay mode
        if tool_choice != "auto":
            logger.warning("tool_choice parameter specified but is ignored in replay mode")

        if self._current_index >= len(self.chat_completions):
            raise ValueError("No more mock responses available")

        response = self.chat_completions[self._current_index]
        _, prompt_token_count = self._tokenize(messages)
        if isinstance(response, str):
            output_tokens, output_token_count = self._tokenize(response)
            self._cur_usage = RequestUsage(prompt_tokens=prompt_token_count, completion_tokens=output_token_count)

            for i, token in enumerate(output_tokens):
                if i < len(output_tokens) - 1:
                    yield token + " "
                else:
                    yield token
            yield CreateResult(
                finish_reason="stop", content=response, usage=self._cur_usage, cached=self._cached_bool_value
            )
            self._update_total_usage()
        else:
            self._cur_usage = RequestUsage(
                prompt_tokens=prompt_token_count, completion_tokens=response.usage.completion_tokens
            )
            yield response
            self._update_total_usage()

        self._current_index += 1

    async def close(self) -> None:
        pass

    def actual_usage(self) -> RequestUsage:
        return self._cur_usage

    def total_usage(self) -> RequestUsage:
        return self._total_usage

    def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:
        _, token_count = self._tokenize(messages)
        return token_count

    def remaining_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:
        return max(
            0, self._total_available_tokens - self._total_usage.prompt_tokens - self._total_usage.completion_tokens
        )

    def set_cached_bool_value(self, value: bool) -> None:
        self._cached_bool_value = value

    def _tokenize(self, messages: Union[str, LLMMessage, Sequence[LLMMessage]]) -> tuple[list[str], int]:
        total_tokens = 0
        all_tokens: List[str] = []
        if isinstance(messages, str):
            tokens = messages.split()
            total_tokens += len(tokens)
            all_tokens.extend(tokens)
        elif hasattr(messages, "content"):
            if isinstance(messages.content, str):  # type: ignore [reportAttributeAccessIssue]
                tokens = messages.content.split()  # type: ignore [reportAttributeAccessIssue]
                total_tokens += len(tokens)
                all_tokens.extend(tokens)
            else:
                logger.warning("Token count has been done only on string content")
        elif isinstance(messages, Sequence):
            for message in messages:
                if isinstance(message.content, str):  # type: ignore [reportAttributeAccessIssue, union-attr]
                    tokens = message.content.split()  # type: ignore [reportAttributeAccessIssue, union-attr]
                    total_tokens += len(tokens)
                    all_tokens.extend(tokens)
                else:
                    logger.warning("Token count has been done only on string content")
        return all_tokens, total_tokens

    def _update_total_usage(self) -> None:
        self._total_usage.completion_tokens += self._cur_usage.completion_tokens
        self._total_usage.prompt_tokens += self._cur_usage.prompt_tokens

    @property
    def capabilities(self) -> ModelCapabilities:  # type: ignore
        """Return mock capabilities."""
        warnings.warn("capabilities is deprecated, use model_info instead", DeprecationWarning, stacklevel=2)
        return self._model_info

    @property
    def model_info(self) -> ModelInfo:
        return self._model_info

    def reset(self) -> None:
        """Reset the client state and usage to its initial state."""
        self._cur_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)
        self._total_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)
        self._current_index = 0

    def _to_config(self) -> ReplayChatCompletionClientConfig:
        return ReplayChatCompletionClientConfig(
            chat_completions=self.chat_completions,
            model_info=self._model_info,
        )

    @classmethod
    def _from_config(cls, config: ReplayChatCompletionClientConfig) -> Self:
        return cls(
            chat_completions=config.chat_completions,
            model_info=config.model_info,
        )

# From replay/_replay_chat_completion_client.py
def create_calls(self) -> List[Dict[str, Any]]:
        """Return the arguments of the calls made to the create method."""
        return self._create_calls

# From replay/_replay_chat_completion_client.py
def set_cached_bool_value(self, value: bool) -> None:
        self._cached_bool_value = value

# From replay/_replay_chat_completion_client.py
def reset(self) -> None:
        """Reset the client state and usage to its initial state."""
        self._cur_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)
        self._total_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)
        self._current_index = 0


# From openai/_utils.py
def assert_valid_name(name: str) -> str:
    """
    Ensure that configured names are valid, raises ValueError if not.

    For munging LLM responses use _normalize_name to ensure LLM specified names don't break the API.
    """
    if not re.match(r"^[a-zA-Z0-9_-]+$", name):
        raise ValueError(f"Invalid name: {name}. Only letters, numbers, '_' and '-' are allowed.")
    if len(name) > 64:
        raise ValueError(f"Invalid name: {name}. Name must be less than 64 characters.")
    return name

from autogen_core import TRACE_LOGGER_NAME

# From openai/_model_info.py
def resolve_model(model: str) -> str:
    if model in _MODEL_POINTERS:
        return _MODEL_POINTERS[model]
    return model

# From openai/_model_info.py
def get_info(model: str) -> ModelInfo:
    # If call it, that mean is that the config does not have cumstom model_info
    resolved_model = resolve_model(model)
    model_info: ModelInfo = _MODEL_INFO.get(
        resolved_model,
        {
            "vision": False,
            "function_calling": False,
            "json_output": False,
            "family": "FAILED",
            "structured_output": False,
        },
    )
    if model_info.get("family") == "FAILED":
        raise ValueError("model_info is required when model name is not a valid OpenAI model")
    if model_info.get("family") == ModelFamily.UNKNOWN:
        trace_logger.warning(f"Model info not found for model: {model}")

    return model_info

# From openai/_model_info.py
def get_token_limit(model: str) -> int:
    resolved_model = resolve_model(model)
    return _MODEL_TOKEN_LIMITS[resolved_model]


# From ollama/_model_info.py
def resolve_model_class(model: str) -> str:
    return model.split(":")[0]

from autogen_core.models import FinishReasons

# From _utils/normalize_stop_reason.py
def normalize_stop_reason(stop_reason: str | None) -> FinishReasons:
    if stop_reason is None:
        return "unknown"

    # Convert to lower case
    stop_reason = stop_reason.lower()

    KNOWN_STOP_MAPPINGS: Dict[str, FinishReasons] = {
        "stop": "stop",
        "length": "length",
        "content_filter": "content_filter",
        "function_calls": "function_calls",
        "end_turn": "stop",
        "tool_calls": "function_calls",
    }

    return KNOWN_STOP_MAPPINGS.get(stop_reason, "unknown")


# From _utils/parse_r1_content.py
def parse_r1_content(content: str) -> Tuple[str | None, str]:
    """Parse the content of an R1-style message that contains a `<think>...</think>` field."""
    # Find the start and end of the think field
    think_start = content.find("<think>")
    think_end = content.find("</think>")

    if think_start == -1 or think_end == -1:
        warnings.warn(
            "Could not find <think>..</think> field in model response content. " "No thought was extracted.",
            UserWarning,
            stacklevel=2,
        )
        return None, content

    if think_end < think_start:
        warnings.warn(
            "Found </think> before <think> in model response content. " "No thought was extracted.",
            UserWarning,
            stacklevel=2,
        )
        return None, content

    # Extract the think field
    thought = content[think_start + len("<think>") : think_end].strip()

    # Extract the rest of the content, skipping the think field.
    content = content[think_end + len("</think>") :].strip()

    return thought, content

from autogen_core import InMemoryStore

# From cache/_chat_completion_cache.py
class ChatCompletionCacheConfig(BaseModel):
    """ """

    client: ComponentModel
    store: Optional[ComponentModel] = None

# From cache/_chat_completion_cache.py
class ChatCompletionCache(ChatCompletionClient, Component[ChatCompletionCacheConfig]):
    """
    A wrapper around a :class:`~autogen_ext.models.cache.ChatCompletionClient` that caches
    creation results from an underlying client.
    Cache hits do not contribute to token usage of the original client.

    Typical Usage:

    Lets use caching on disk with `openai` client as an example.
    First install `autogen-ext` with the required packages:

    .. code-block:: bash

        pip install -U "autogen-ext[openai, diskcache]"

    And use it as:

    .. code-block:: python

        import asyncio
        import tempfile

        from autogen_core.models import UserMessage
        from autogen_ext.models.openai import OpenAIChatCompletionClient
        from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
        from autogen_ext.cache_store.diskcache import DiskCacheStore
        from diskcache import Cache


        async def main():
            with tempfile.TemporaryDirectory() as tmpdirname:
                # Initialize the original client
                openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

                # Then initialize the CacheStore, in this case with diskcache.Cache.
                # You can also use redis like:
                # from autogen_ext.cache_store.redis import RedisStore
                # import redis
                # redis_instance = redis.Redis()
                # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
                cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
                cache_client = ChatCompletionCache(openai_model_client, cache_store)

                response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
                print(response)  # Should print response from OpenAI
                response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
                print(response)  # Should print cached response


        asyncio.run(main())

    You can now use the `cached_client` as you would the original client, but with caching enabled.

    Args:
        client (ChatCompletionClient): The original ChatCompletionClient to wrap.
        store (CacheStore): A store object that implements get and set methods.
            The user is responsible for managing the store's lifecycle & clearing it (if needed).
            Defaults to using in-memory cache.
    """

    component_type = "chat_completion_cache"
    component_provider_override = "autogen_ext.models.cache.ChatCompletionCache"
    component_config_schema = ChatCompletionCacheConfig

    def __init__(
        self,
        client: ChatCompletionClient,
        store: Optional[CacheStore[CHAT_CACHE_VALUE_TYPE]] = None,
    ):
        self.client = client
        self.store = store or InMemoryStore[CHAT_CACHE_VALUE_TYPE]()

    def _check_cache(
        self,
        messages: Sequence[LLMMessage],
        tools: Sequence[Tool | ToolSchema],
        json_output: Optional[bool | type[BaseModel]],
        extra_create_args: Mapping[str, Any],
    ) -> tuple[Optional[Union[CreateResult, List[Union[str, CreateResult]]]], str]:
        """
        Helper function to check the cache for a result.
        Returns a tuple of (cached_result, cache_key).
        """

        json_output_data: str | bool | None = None

        if isinstance(json_output, type) and issubclass(json_output, BaseModel):
            json_output_data = json.dumps(json_output.model_json_schema())
        elif isinstance(json_output, bool):
            json_output_data = json_output

        data = {
            "messages": [message.model_dump() for message in messages],
            "tools": [(tool.schema if isinstance(tool, Tool) else tool) for tool in tools],
            "json_output": json_output_data,
            "extra_create_args": extra_create_args,
        }
        serialized_data = json.dumps(data, sort_keys=True)
        cache_key = hashlib.sha256(serialized_data.encode()).hexdigest()

        cached_result = cast(Optional[CreateResult], self.store.get(cache_key))
        if cached_result is not None:
            return cached_result, cache_key

        return None, cache_key

    async def create(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> CreateResult:
        """
        Cached version of ChatCompletionClient.create.
        If the result of a call to create has been cached, it will be returned immediately
        without invoking the underlying client.

        NOTE: cancellation_token is ignored for cached results.
        """
        cached_result, cache_key = self._check_cache(messages, tools, json_output, extra_create_args)
        if cached_result:
            assert isinstance(cached_result, CreateResult)
            cached_result.cached = True
            return cached_result

        result = await self.client.create(
            messages,
            tools=tools,
            json_output=json_output,
            tool_choice=tool_choice,
            extra_create_args=extra_create_args,
            cancellation_token=cancellation_token,
        )
        self.store.set(cache_key, result)
        return result

    def create_stream(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
    ) -> AsyncGenerator[Union[str, CreateResult], None]:
        """
        Cached version of ChatCompletionClient.create_stream.
        If the result of a call to create_stream has been cached, it will be returned
        without streaming from the underlying client.

        NOTE: cancellation_token is ignored for cached results.
        """

        async def _generator() -> AsyncGenerator[Union[str, CreateResult], None]:
            cached_result, cache_key = self._check_cache(
                messages,
                tools,
                json_output,
                extra_create_args,
            )
            if cached_result:
                assert isinstance(cached_result, list)
                for result in cached_result:
                    if isinstance(result, CreateResult):
                        result.cached = True
                    yield result
                return

            result_stream = self.client.create_stream(
                messages,
                tools=tools,
                json_output=json_output,
                tool_choice=tool_choice,
                extra_create_args=extra_create_args,
                cancellation_token=cancellation_token,
            )

            output_results: List[Union[str, CreateResult]] = []
            self.store.set(cache_key, output_results)

            async for result in result_stream:
                output_results.append(result)
                yield result

        return _generator()

    async def close(self) -> None:
        await self.client.close()

    def actual_usage(self) -> RequestUsage:
        return self.client.actual_usage()

    def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:
        return self.client.count_tokens(messages, tools=tools)

    @property
    def capabilities(self) -> ModelCapabilities:  # type: ignore
        warnings.warn("capabilities is deprecated, use model_info instead", DeprecationWarning, stacklevel=2)
        return self.client.capabilities

    @property
    def model_info(self) -> ModelInfo:
        return self.client.model_info

    def remaining_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:
        return self.client.remaining_tokens(messages, tools=tools)

    def total_usage(self) -> RequestUsage:
        return self.client.total_usage()

    def _to_config(self) -> ChatCompletionCacheConfig:
        return ChatCompletionCacheConfig(
            client=self.client.dump_component(),
            store=self.store.dump_component() if not isinstance(self.store, InMemoryStore) else None,
        )

    @classmethod
    def _from_config(cls, config: ChatCompletionCacheConfig) -> Self:
        client = ChatCompletionClient.load_component(config.client)
        store: Optional[CacheStore[CHAT_CACHE_VALUE_TYPE]] = (
            CacheStore.load_component(config.store) if config.store else InMemoryStore()
        )
        return cls(client=client, store=store)


from autogen_core.models._types import FunctionExecutionResult
from openai.types.chat import ChatCompletionMessageParam

from types import TransformerFunc
from types import TransformerMap

# From _transformation/registry.py
def build_transformer_func(
    funcs: List[Callable[[LLMMessage, Dict[str, Any]], Dict[str, Any]]], message_param_func: Callable[..., Any]
) -> TransformerFunc:
    """
    Combines multiple transformer functions into a single transformer.

    Each `func` must accept a message and a context dict, and return a partial dict
    of keyword arguments. These are merged and passed to `message_param_func`.

    This structure allows flexible transformation pipelines and future extensibility
    (e.g., prepend name, insert metadata, etc).

    message_param_func: A model-specific constructor (e.g. ChatCompletionMessageParam).
    Signature is intentionally open: Callable[..., Any].
    """

    def transformer_func(message: LLMMessage, context: Any) -> Any:
        kwargs: Dict[str, Any] = {}
        for func in funcs:
            kwargs.update(func(message, context))
        return [message_param_func(**kwargs)]

    return transformer_func

# From _transformation/registry.py
def build_conditional_transformer_func(
    funcs_map: Dict[str, List[Callable[[LLMMessage, Dict[str, Any]], Dict[str, Any]]]],
    message_param_func_map: Dict[str, Callable[..., Any]],
    condition_func: Callable[[LLMMessage, Dict[str, Any]], str],
) -> TransformerFunc:
    """
    Combines multiple transformer functions into a single transformer, with a conditional constructor.

    Each `func` must accept a message and a context dict, and return a partial dict
    of keyword arguments. These are merged and passed to the constructor selected by `condition_func`.

    This structure allows flexible transformation pipelines and future extensibility
    (e.g., prepend name, insert metadata, etc).

    message_param_func_map: A mapping of condition → constructor function.
    condition_func: A function that returns the condition for selecting the constructor.
    """

    def transformer(message: LLMMessage, context: Dict[str, Any]) -> Any:
        condition = condition_func(message, context)
        message_param_func = message_param_func_map[condition]
        kwargs: Dict[str, Any] = {}
        for func in funcs_map[condition]:
            kwargs.update(func(message, context))
        if kwargs.get("pass_message", False):
            return []
        return [message_param_func(**kwargs)]

    return transformer

# From _transformation/registry.py
def register_transformer(api: str, model_family: str, transformer_map: TransformerMap) -> None:
    """
    Registers a transformer map for a given model family.

    Example:

        .. code-block:: python

            register_transformer(
                "gpt-4o",
                {
                    UserMessage: user_message_to_oai,
                    SystemMessage: system_message_to_oai,
                },
            )
    """
    MESSAGE_TRANSFORMERS[api][model_family] = transformer_map

# From _transformation/registry.py
def get_transformer(api: str, model: str, model_family: str) -> TransformerMap:
    """
    Returns the registered transformer map for the given model family.

    This is a thin wrapper around `MESSAGE_TRANSFORMERS.get(...)`, but serves as
    an abstraction layer to allow future enhancements such as:

    - Providing fallback transformers for unknown model families
    - Injecting mock transformers during testing
    - Adding logging, metrics, or versioning later

    Keeping this as a function (instead of direct dict access) improves long-term flexibility.
    """

    if model_family not in set(get_args(ModelFamily.ANY)) or model_family == ModelFamily.UNKNOWN:
        # fallback to finding the best matching model family
        model_family = _find_model_family(api, model)

    transformer = MESSAGE_TRANSFORMERS.get(api, {}).get(model_family, {})

    if not transformer:
        # Just in case, we should never reach here
        raise ValueError(f"No transformer found for model family '{model_family}'")

    return transformer

# From _transformation/registry.py
def transformer_func(message: LLMMessage, context: Any) -> Any:
        kwargs: Dict[str, Any] = {}
        for func in funcs:
            kwargs.update(func(message, context))
        return [message_param_func(**kwargs)]

# From _transformation/registry.py
def transformer(message: LLMMessage, context: Dict[str, Any]) -> Any:
        condition = condition_func(message, context)
        message_param_func = message_param_func_map[condition]
        kwargs: Dict[str, Any] = {}
        for func in funcs_map[condition]:
            kwargs.update(func(message, context))
        if kwargs.get("pass_message", False):
            return []
        return [message_param_func(**kwargs)]

from utils._functions import UserContent
from utils.page_logger import PageLogger

# From task_centric_memory/_prompter.py
class Prompter:
    """
    Centralizes most of the Apprentice prompts sent to the model client.

    Args:
        client: The client to call the model.
        logger: An optional logger. If None, no logging will be performed.
    """

    def __init__(self, client: ChatCompletionClient, logger: PageLogger | None = None) -> None:
        if logger is None:
            logger = PageLogger()  # Nothing will be logged by this object.
        self.logger = logger

        self.client = client
        self.default_system_message_content = "You are a helpful assistant."
        self.time_spent_in_model_calls = 0.0
        self.num_model_calls = 0
        self.start_time = time.time()

        # Create the chat history
        self._chat_history: List[LLMMessage] = []

    async def call_model(
        self,
        summary: str,
        user_content: UserContent,
        system_message_content: str | None = None,
        keep_these_messages: bool = True,
    ) -> str:
        """
        Calls the model client with the given input and returns the response.
        """
        # Prepare the input message list
        if system_message_content is None:
            system_message_content = self.default_system_message_content
        system_message: LLMMessage
        if self.client.model_info["family"] == "o1":
            # No system message allowed, so pass it as the first user message.
            system_message = UserMessage(content=system_message_content, source="User")
        else:
            # System message allowed.
            system_message = SystemMessage(content=system_message_content)

        user_message = UserMessage(content=user_content, source="User")
        input_messages = [system_message] + self._chat_history + [user_message]

        # Double check the types of the input messages.
        for message in input_messages:
            for part in message.content:
                assert isinstance(part, str) or isinstance(part, Image), "Invalid message content type: {}".format(
                    type(part)
                )

        # Call the model
        start_time = time.time()
        response = await self.client.create(input_messages)
        assert isinstance(response, CreateResult)
        response_string = response.content
        assert isinstance(response_string, str)
        response_message = AssistantMessage(content=response_string, source="Assistant")
        assert isinstance(response_message, AssistantMessage)
        self.time_spent_in_model_calls += time.time() - start_time
        self.num_model_calls += 1

        # Log the model call
        self.logger.log_model_call(summary=summary, input_messages=input_messages, response=response)

        # Manage the chat history
        if keep_these_messages:
            self._chat_history.append(user_message)
            self._chat_history.append(response_message)

        # Return the response as a string for now
        return response_string

    def _clear_history(self) -> None:
        """
        Empties the message list containing the chat history.
        """
        self._chat_history = []

    async def learn_from_failure(
        self, task_description: str, memory_section: str, final_response: str, expected_answer: str, work_history: str
    ) -> str:
        """
        Tries to create an insight to help avoid the given failure in the future.
        """
        sys_message = """- You are a patient and thorough teacher.
- Your job is to review work done by students and help them learn how to do better."""

        user_message: List[Union[str, Image]] = []
        user_message.append("# A team of students made a mistake on the following task:\n")
        user_message.extend([task_description])

        if len(memory_section) > 0:
            user_message.append(memory_section)

        user_message.append("# Here's the expected answer, which would have been correct:\n")
        user_message.append(expected_answer)

        user_message.append("# Here is the students' answer, which was INCORRECT:\n")
        user_message.append(final_response)

        user_message.append("# Please review the students' work which follows:\n")
        user_message.append("**-----  START OF STUDENTS' WORK  -----**\n\n")
        user_message.append(work_history)
        user_message.append("\n**-----  END OF STUDENTS' WORK  -----**\n\n")

        user_message.append(
            "# Now carefully review the students' work above, explaining in detail what the students did right and what they did wrong.\n"
        )

        self._clear_history()
        await self.call_model(
            summary="Ask the model to learn from this failure",
            system_message_content=sys_message,
            user_content=user_message,
        )
        user_message = [
            "Now put yourself in the mind of the students. What misconception led them to their incorrect answer?"
        ]
        await self.call_model(
            summary="Ask the model to state the misconception",
            system_message_content=sys_message,
            user_content=user_message,
        )

        user_message = [
            "Please express your key insights in the form of short, general advice that will be given to the students. Just one or two sentences, or they won't bother to read it."
        ]
        insight = await self.call_model(
            summary="Ask the model to formulate a concise insight",
            system_message_content=sys_message,
            user_content=user_message,
        )
        return insight

    async def find_index_topics(self, input_string: str) -> List[str]:
        """
        Returns a list of topics related to the given string.
        """
        sys_message = """You are an expert at semantic analysis."""

        user_message: List[Union[str, Image]] = []
        user_message.append("""- My job is to create a thorough index for a book called Task Completion, and I need your help.
- Every paragraph in the book needs to be indexed by all the topics related to various kinds of tasks and strategies for completing them.
- Your job is to read the text below and extract the task-completion topics that are covered.
- The number of topics depends on the length and content of the text. But you should list at least one topic, and potentially many more.
- Each topic you list should be a meaningful phrase composed of a few words. Don't use whole sentences as topics.
- Don't include details that are unrelated to the general nature of the task, or a potential strategy for completing tasks.
- List each topic on a separate line, without any extra text like numbering, or bullets, or any other formatting, because we don't want those things in the index of the book.\n\n""")

        user_message.append("# Text to be indexed\n")
        user_message.append(input_string)

        self._clear_history()
        topics = await self.call_model(
            summary="Ask the model to extract topics", system_message_content=sys_message, user_content=user_message
        )

        # Parse the topics into a list.
        topic_list: List[str] = []
        for line in topics.split("\n"):
            if len(line) > 0:
                topic_list.append(line)

        return topic_list

    async def generalize_task(self, task_description: str, revise: bool | None = True) -> str:
        """
        Attempts to rewrite a task description in a more general form.
        """

        sys_message = """You are a helpful and thoughtful assistant."""

        user_message: List[Union[str, Image]] = [
            "We have been given a task description. Our job is not to complete the task, but merely rephrase the task in simpler, more general terms, if possible. Please reach through the following task description, then explain your understanding of the task in detail, as a single, flat list of all the important points."
        ]
        user_message.append("\n# Task description")
        user_message.append(task_description)

        self._clear_history()
        generalized_task = await self.call_model(
            summary="Ask the model to rephrase the task in a list of important points",
            system_message_content=sys_message,
            user_content=user_message,
        )

        if revise:
            user_message = [
                "Do you see any parts of this list that are irrelevant to actually solving the task? If so, explain which items are irrelevant."
            ]
            await self.call_model(
                summary="Ask the model to identify irrelevant points",
                system_message_content=sys_message,
                user_content=user_message,
            )

            user_message = [
                "Revise your original list to include only the most general terms, those that are critical to solving the task, removing any themes or descriptions that are not essential to the solution. Your final list may be shorter, but do not leave out any part of the task that is needed for solving the task. Do not add any additional commentary either before or after the list."
            ]
            generalized_task = await self.call_model(
                summary="Ask the model to make a final list of general terms",
                system_message_content=sys_message,
                user_content=user_message,
            )

        return generalized_task

    async def validate_insight(self, insight: str, task_description: str) -> bool:
        """
        Judges whether the insight could help solve the task.
        """

        sys_message = """You are a helpful and thoughtful assistant."""

        user_message: List[Union[str, Image]] = [
            """We have been given a potential insight that may or may not be useful for solving a given task.
- First review the following task.
- Then review the insight that follows, and consider whether it might help solve the given task.
- Do not attempt to actually solve the task.
- Reply with a single character, '1' if the insight may be useful, or '0' if it is not."""
        ]
        user_message.append("\n# Task description")
        user_message.append(task_description)
        user_message.append("\n# Possibly useful insight")
        user_message.append(insight)
        self._clear_history()
        response = await self.call_model(
            summary="Ask the model to validate the insight",
            system_message_content=sys_message,
            user_content=user_message,
        )
        return response == "1"

    async def extract_task(self, text: str) -> str | None:
        """
        Returns a task found in the given text, or None if not found.
        """
        sys_message = """You are a helpful and thoughtful assistant."""
        user_message: List[Union[str, Image]] = [
            """Does the following text contain a question or a some task we are being asked to perform?
- If so, please reply with the full question or task description, along with any supporting information, but without adding extra commentary or formatting.
- If the task is just to remember something, that doesn't count as a task, so don't include it.
- If there is no question or task in the text, simply write "None" with no punctuation."""
        ]
        user_message.append("\n# Text to analyze")
        user_message.append(text)
        self._clear_history()
        response = await self.call_model(
            summary="Ask the model to extract a task", system_message_content=sys_message, user_content=user_message
        )
        return response if response != "None" else None

    async def extract_advice(self, text: str) -> str | None:
        """
        Returns advice from the given text, or None if not found.
        """
        sys_message = """You are a helpful and thoughtful assistant."""
        user_message: List[Union[str, Image]] = [
            """Does the following text contain any information or advice that might be useful later?
- If so, please copy the information or advice, adding no extra commentary or formatting.
- If there is no potentially useful information or advice at all, simply write "None" with no punctuation."""
        ]
        user_message.append("\n# Text to analyze")
        user_message.append(text)
        self._clear_history()
        response = await self.call_model(
            summary="Ask the model to extract advice", system_message_content=sys_message, user_content=user_message
        )
        return response if response != "None" else None

import pickle
import chromadb
from chromadb.api.types import QueryResult
from chromadb.config import Settings

# From task_centric_memory/_string_similarity_map.py
class StringSimilarityMap:
    """
    Provides storage and similarity-based retrieval of string pairs using a vector database.
    Each DB entry is a pair of strings: an input string and an output string.
    The input string is embedded and used as the retrieval key.
    The output string can be anything, but it's typically used as a dict key.
    Vector embeddings are currently supplied by Chroma's default Sentence Transformers.

    Args:
        - reset: True to clear the DB immediately after creation.
        - path_to_db_dir: Path to the directory where the DB is stored.
        - logger: An optional logger. If None, no logging will be performed.
    """

    def __init__(self, reset: bool, path_to_db_dir: str, logger: PageLogger | None = None) -> None:
        if logger is None:
            logger = PageLogger()  # Nothing will be logged by this object.
        self.logger = logger
        self.path_to_db_dir = path_to_db_dir

        # Load or create the vector DB on disk.
        chromadb_settings = Settings(
            anonymized_telemetry=False, allow_reset=True, is_persistent=True, persist_directory=path_to_db_dir
        )
        self.db_client = chromadb.Client(chromadb_settings)
        self.vec_db = self.db_client.create_collection("string-pairs", get_or_create=True)  # The collection is the DB.

        # Load or create the associated string-pair dict on disk.
        self.path_to_dict = os.path.join(path_to_db_dir, "uid_text_dict.pkl")
        self.uid_text_dict: Dict[str, Tuple[str, str]] = {}
        self.last_string_pair_id = 0
        if (not reset) and os.path.exists(self.path_to_dict):
            self.logger.debug("\nLOADING STRING SIMILARITY MAP FROM DISK  at {}".format(self.path_to_dict))
            with open(self.path_to_dict, "rb") as f:
                self.uid_text_dict = pickle.load(f)
                self.last_string_pair_id = len(self.uid_text_dict)
                if len(self.uid_text_dict) > 0:
                    self.logger.debug("\n{} STRING PAIRS LOADED".format(len(self.uid_text_dict)))
                    self._log_string_pairs()

        # Clear the DB if requested.
        if reset:
            self.reset_db()

    def _log_string_pairs(self) -> None:
        """
        Logs all string pairs currently in the map.
        """
        self.logger.debug("LIST OF STRING PAIRS")
        for uid, text in self.uid_text_dict.items():
            input_text, output_text = text
            self.logger.debug("  ID: {}\n    INPUT TEXT: {}\n    OUTPUT TEXT: {}".format(uid, input_text, output_text))

    def save_string_pairs(self) -> None:
        """
        Saves the string-pair dict (self.uid_text_dict) to disk.
        """
        self.logger.debug("\nSAVING STRING SIMILARITY MAP TO DISK  at {}".format(self.path_to_dict))
        with open(self.path_to_dict, "wb") as file:
            pickle.dump(self.uid_text_dict, file)

    def reset_db(self) -> None:
        """
        Forces immediate deletion of the DB's contents, in memory and on disk.
        """
        self.logger.debug("\nCLEARING STRING-PAIR MAP")
        self.db_client.delete_collection("string-pairs")
        self.vec_db = self.db_client.create_collection("string-pairs")
        self.uid_text_dict = {}
        self.save_string_pairs()

    def add_input_output_pair(self, input_text: str, output_text: str) -> None:
        """
        Adds one input-output string pair to the DB.
        """
        self.last_string_pair_id += 1
        self.vec_db.add(documents=[input_text], ids=[str(self.last_string_pair_id)])
        self.uid_text_dict[str(self.last_string_pair_id)] = input_text, output_text
        self.logger.debug(
            "\nINPUT-OUTPUT PAIR ADDED TO VECTOR DATABASE:\n  ID\n    {}\n  INPUT\n    {}\n  OUTPUT\n    {}\n".format(
                self.last_string_pair_id, input_text, output_text
            )
        )
        # self._log_string_pairs()  # For deeper debugging, uncomment to log all string pairs after each addition.

    def get_related_string_pairs(
        self, query_text: str, n_results: int, threshold: Union[int, float]
    ) -> List[Tuple[str, str, float]]:
        """
        Retrieves up to n string pairs that are related to the given query text within the specified distance threshold.
        """
        string_pairs_with_distances: List[Tuple[str, str, float]] = []
        if n_results > len(self.uid_text_dict):
            n_results = len(self.uid_text_dict)
        if n_results > 0:
            results: QueryResult = self.vec_db.query(query_texts=[query_text], n_results=n_results)
            num_results = len(results["ids"][0])
            for i in range(num_results):
                uid = results["ids"][0][i]
                input_text = results["documents"][0][i] if results["documents"] else ""
                distance = results["distances"][0][i] if results["distances"] else 0.0
                if distance < threshold:
                    input_text_2, output_text = self.uid_text_dict[uid]
                    assert input_text == input_text_2
                    self.logger.debug(
                        "\nINPUT-OUTPUT PAIR RETRIEVED FROM VECTOR DATABASE:\n  INPUT1\n    {}\n  OUTPUT\n    {}\n  DISTANCE\n    {}".format(
                            input_text, output_text, distance
                        )
                    )
                    string_pairs_with_distances.append((input_text, output_text, distance))
        return string_pairs_with_distances

# From task_centric_memory/_string_similarity_map.py
def save_string_pairs(self) -> None:
        """
        Saves the string-pair dict (self.uid_text_dict) to disk.
        """
        self.logger.debug("\nSAVING STRING SIMILARITY MAP TO DISK  at {}".format(self.path_to_dict))
        with open(self.path_to_dict, "wb") as file:
            pickle.dump(self.uid_text_dict, file)

# From task_centric_memory/_string_similarity_map.py
def add_input_output_pair(self, input_text: str, output_text: str) -> None:
        """
        Adds one input-output string pair to the DB.
        """
        self.last_string_pair_id += 1
        self.vec_db.add(documents=[input_text], ids=[str(self.last_string_pair_id)])
        self.uid_text_dict[str(self.last_string_pair_id)] = input_text, output_text
        self.logger.debug(
            "\nINPUT-OUTPUT PAIR ADDED TO VECTOR DATABASE:\n  ID\n    {}\n  INPUT\n    {}\n  OUTPUT\n    {}\n".format(
                self.last_string_pair_id, input_text, output_text
            )
        )

# From task_centric_memory/_string_similarity_map.py
def get_related_string_pairs(
        self, query_text: str, n_results: int, threshold: Union[int, float]
    ) -> List[Tuple[str, str, float]]:
        """
        Retrieves up to n string pairs that are related to the given query text within the specified distance threshold.
        """
        string_pairs_with_distances: List[Tuple[str, str, float]] = []
        if n_results > len(self.uid_text_dict):
            n_results = len(self.uid_text_dict)
        if n_results > 0:
            results: QueryResult = self.vec_db.query(query_texts=[query_text], n_results=n_results)
            num_results = len(results["ids"][0])
            for i in range(num_results):
                uid = results["ids"][0][i]
                input_text = results["documents"][0][i] if results["documents"] else ""
                distance = results["distances"][0][i] if results["distances"] else 0.0
                if distance < threshold:
                    input_text_2, output_text = self.uid_text_dict[uid]
                    assert input_text == input_text_2
                    self.logger.debug(
                        "\nINPUT-OUTPUT PAIR RETRIEVED FROM VECTOR DATABASE:\n  INPUT1\n    {}\n  OUTPUT\n    {}\n  DISTANCE\n    {}".format(
                            input_text, output_text, distance
                        )
                    )
                    string_pairs_with_distances.append((input_text, output_text, distance))
        return string_pairs_with_distances


# From utils/_functions.py
def message_content_to_str(message_content: MessageContent | None) -> str:
    """
    Converts the message content to a string.
    """
    if message_content is None:
        return ""
    elif isinstance(message_content, str):
        return message_content
    elif isinstance(message_content, List):
        converted: List[str] = list()
        for item in message_content:
            if isinstance(item, str):
                converted.append(item)
            elif isinstance(item, Image):
                converted.append("<Image>")
            else:
                converted.append(str(item).rstrip())
        return "\n".join(converted)
    else:
        raise AssertionError("Unexpected response type.")

# From utils/_functions.py
def text_from_user_content(user_content: UserContent) -> str:
    """
    Extracts just the text from the user content.
    """
    if isinstance(user_content, str):
        return user_content
    elif isinstance(user_content, List):
        text_list: List[str] = list()
        for item in user_content:
            if isinstance(item, str):
                text_list.append(item.rstrip())
        return "\n\n".join(text_list)
    else:
        raise AssertionError("Unexpected response type.")

# From utils/_functions.py
def single_image_from_user_content(user_content: UserContent) -> Union[Image, None]:
    """
    Extracts a single image from the user content.
    """
    image_to_return = None
    if isinstance(user_content, str):
        return None
    elif isinstance(user_content, List):
        for item in user_content:
            if isinstance(item, Image):
                assert image_to_return is None, "Only one image is currently allowed in the user content."
                image_to_return = item
    else:
        raise AssertionError("Unexpected response type.")
    return image_to_return

# From utils/_functions.py
def hash_directory(directory: str, hash_algo: str = "sha256") -> Tuple[str, int, int]:
    """Computes a hash representing the state of a directory, including its structure and file contents."""
    hash_func = hashlib.new(hash_algo)

    # Also count the number of files and sub-directories
    num_files = 0
    num_subdirs = 0

    for root, dirs, files in sorted(os.walk(directory)):  # Ensure order for consistent hashing
        num_files += len(files)
        num_subdirs += len(dirs)
        for dir_name in sorted(dirs):
            hash_func.update(dir_name.encode())  # Hash directory names

        for file_name in sorted(files):
            file_path = os.path.join(root, file_name)
            hash_func.update(file_name.encode())  # Hash file names

            try:
                with open(file_path, "rb") as f:
                    while chunk := f.read(4096):  # Read in chunks
                        hash_func.update(chunk)
            except Exception:
                pass

    return hash_func.hexdigest(), num_files, num_subdirs

from autogen_core.models import FunctionExecutionResultMessage
from _functions import MessageContent
from _functions import hash_directory

# From utils/page_logger.py
class PageLoggerConfig(TypedDict, total=False):
    level: str
    path: str

# From utils/page_logger.py
class PageLogger:
    """
    Logs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree.

    Args:
        config: An optional dict that can be used to override the following values:

            - level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE.
            - path: The path to the directory where the log files will be written.
    """

    def __init__(self, config: PageLoggerConfig | None = None) -> None:
        self.levels = {
            "DEBUG": 10,
            "INFO": 20,
            "WARNING": 30,
            "ERROR": 40,
            "CRITICAL": 50,
            "NONE": 100,
        }

        # Apply default settings and any config overrides.
        level_str = "NONE"  # Default to no logging at all.
        self.log_dir = "./pagelogs/default"
        if config is not None:
            level_str = config.get("level", level_str)
            self.log_dir = config.get("path", self.log_dir)
        self.level = self.levels[level_str]
        self.log_dir = os.path.expanduser(self.log_dir)

        # If the logging level is set to NONE or higher, don't log anything.
        if self.level >= self.levels["NONE"]:
            return

        self.page_stack = PageStack()
        self.pages: List[Page] = []
        self.last_page_id = 0
        self.name = "0  Call Tree"
        self._create_run_dir()
        self.flush()
        self.finalized = False

    def __del__(self) -> None:
        self.finalize()

    def finalize(self) -> None:
        # Writes a hash of the log directory to a file for change detection.
        if self.level >= self.levels["NONE"]:
            return

        # Don't finalize the log if it has already been finalized.
        if self.finalized:
            return

        # Do nothing if the app is being forced to exit early.
        if self.page_stack.size() > 0:
            return

        self.flush(finished=True)

        # Write the hash and other details to a file.
        hash_str, num_files, num_subdirs = hash_directory(self.log_dir)
        hash_path = os.path.join(self.log_dir, "hash.txt")
        with open(hash_path, "w") as f:
            f.write(hash_str)
            f.write("\n")
            f.write("{} files\n".format(num_files))
            f.write("{} subdirectories\n".format(num_subdirs))

        self.finalized = True

    @staticmethod
    def _decorate_text(text: str, color: str, weight: str = "bold", demarcate: bool = False) -> str:
        """
        Returns a string of text with HTML styling for weight and color.
        """
        if demarcate:
            text = f"<<<<<  {text}  >>>>>"
        return f'<span style="color: {color}; font-weight: {weight};">{text}</span>'

    @staticmethod
    def _link_to_image(image_path: str, description: str) -> str:
        """
        Returns an HTML string defining a thumbnail link to an image.
        """
        # To avoid a bug in heml rendering aht displays underscores to the left of thumbnails,
        # define the following string on a single line.
        link = f"""<a href="{image_path}"><img src="{image_path}" alt="{description}" style="width: 300px; height: auto;"></a>"""
        return link

    def _get_next_page_id(self) -> int:
        """Returns the next page id and increments the counter."""
        self.last_page_id += 1
        return self.last_page_id

    def _create_run_dir(self) -> None:
        """Creates a fresh log directory."""
        if os.path.exists(self.log_dir):
            shutil.rmtree(self.log_dir)
        os.makedirs(self.log_dir)

    def _add_page(self, summary: str, show_in_call_tree: bool = True, finished: bool = True) -> "Page":
        """
        Adds a new page to the log.
        """
        page = Page(
            page_logger=self,
            index=self._get_next_page_id(),
            summary=summary,
            indent_level=len(self.page_stack.stack),
            show_in_call_tree=show_in_call_tree,
            finished=finished,
        )
        self.pages.append(page)
        self.flush()
        if len(self.page_stack.stack) > 0:
            # Insert a link to the new page into the calling page.
            self.info("\n" + page.full_link)
        return page

    def _log_text(self, text: str) -> None:
        """
        Adds text to the current page.
        """
        page = self.page_stack.top()
        if page is not None:
            page.add_lines(text, flush=True)

    def debug(self, line: str) -> None:
        """
        Adds DEBUG text to the current page if debugging level <= DEBUG.
        """
        if self.level <= self.levels["DEBUG"]:
            self._log_text(line)

    def info(self, line: str) -> None:
        """
        Adds INFO text to the current page if debugging level <= INFO.
        """
        if self.level <= self.levels["INFO"]:
            self._log_text(line)

    def warning(self, line: str) -> None:
        """
        Adds WARNING text to the current page if debugging level <= WARNING.
        """
        if self.level <= self.levels["WARNING"]:
            self._log_text(line)

    def error(self, line: str) -> None:
        """
        Adds ERROR text to the current page if debugging level <= ERROR.
        """
        if self.level <= self.levels["ERROR"]:
            self._log_text(line)

    def critical(self, line: str) -> None:
        """
        Adds CRITICAL text to the current page if debugging level <= CRITICAL.
        """
        if self.level <= self.levels["CRITICAL"]:
            self._log_text(line)

    def _message_source(self, message: LLMMessage) -> str:
        """
        Returns a decorated string indicating the source of a message.
        """
        source = "UNKNOWN"
        color = "black"
        if isinstance(message, SystemMessage):
            source = "SYSTEM"
            color = "purple"
        elif isinstance(message, UserMessage):
            source = "USER"
            color = "blue"
        elif isinstance(message, AssistantMessage):
            source = "ASSISTANT"
            color = "green"
        elif isinstance(message, FunctionExecutionResultMessage):
            source = "FUNCTION"
            color = "red"
        return self._decorate_text(source, color, demarcate=True)

    def _format_message_content(self, message_content: MessageContent) -> str:
        """
        Formats the message content for logging.
        """
        # Start by converting the message content to a list of strings.
        content_list: List[str] = []
        content = message_content
        if isinstance(content, str):
            content_list.append(content)
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, str):
                    content_list.append(item.rstrip())
                elif isinstance(item, Image):
                    # Save the image to disk.
                    image_filename = str(self._get_next_page_id()) + " image.jpg"
                    image_path = os.path.join(self.log_dir, image_filename)
                    item.image.save(image_path)
                    # Add a link to the image.
                    content_list.append(self._link_to_image(image_filename, "message_image"))
                elif isinstance(item, Dict):
                    # Add a dictionary to the log.
                    json_str = json.dumps(item, indent=4)
                    content_list.append(json_str)
                else:
                    content_list.append(str(item).rstrip())
        else:
            content_list.append("<UNKNOWN MESSAGE CONTENT>")

        # Convert the list of strings to a single string containing newline separators.
        output = ""
        for item in content_list:
            output += f"\n{item}\n"
        return output

    def log_message_content(self, message_content: MessageContent, summary: str) -> None:
        """
        Adds a page containing the message's content, including any images.
        """
        if self.level > self.levels["INFO"]:
            return None
        page = self._add_page(summary=summary, show_in_call_tree=False)
        self.page_stack.write_stack_to_page(page)
        page.add_lines(self._format_message_content(message_content=message_content))
        page.flush()

    def log_dict_list(self, content: List[Mapping[str, Any]], summary: str) -> None:
        """
        Adds a page containing a list of dicts.
        """
        if self.level > self.levels["INFO"]:
            return None
        page = self._add_page(summary=summary, show_in_call_tree=False)
        self.page_stack.write_stack_to_page(page)

        for item in content:
            json_str = json.dumps(item, indent=4)
            page.add_lines(json_str)

        page.flush()

    def _log_model_messages(
        self, summary: str, input_messages: List[LLMMessage], response_str: str, usage: RequestUsage | None
    ) -> Optional["Page"]:
        """
        Adds a page containing the messages to a model (including any input images) and its response.
        """
        page = self._add_page(summary=summary, show_in_call_tree=False)
        self.page_stack.write_stack_to_page(page)

        if usage is not None:
            page.add_lines("{} prompt tokens".format(usage.prompt_tokens))
            page.add_lines("{} completion tokens".format(usage.completion_tokens))
        for m in input_messages:
            page.add_lines("\n" + self._message_source(m))
            page.add_lines(self._format_message_content(message_content=m.content))
        page.add_lines("\n" + self._decorate_text("ASSISTANT RESPONSE", "green", demarcate=True))
        page.add_lines("\n" + response_str + "\n")
        page.flush()
        return page

    def log_model_call(
        self, summary: str, input_messages: List[LLMMessage], response: CreateResult
    ) -> Optional["Page"]:
        """
        Logs messages sent to a model and the TaskResult response to a new page.
        """
        if self.level > self.levels["INFO"]:
            return None

        response_str = response.content
        if not isinstance(response_str, str):
            response_str = "??"

        page = self._log_model_messages(summary, input_messages, response_str, response.usage)
        return page

    def log_model_task(
        self, summary: str, input_messages: List[LLMMessage], task_result: TaskResult
    ) -> Optional["Page"]:
        """
        Logs messages sent to a model and the TaskResult response to a new page.
        """
        if self.level > self.levels["INFO"]:
            return None

        messages: Sequence[BaseAgentEvent | BaseChatMessage] = task_result.messages
        message = messages[-1]
        response_str = message.to_text()
        if not isinstance(response_str, str):
            response_str = "??"

        if hasattr(message, "models_usage"):
            usage: RequestUsage | None = message.models_usage
        else:
            usage = RequestUsage(prompt_tokens=0, completion_tokens=0)

        page = self._log_model_messages(summary, input_messages, response_str, usage)
        return page

    def log_link_to_local_file(self, file_path: str) -> str:
        """
        Returns a link to a local file in the log.
        """
        file_name = os.path.basename(file_path)
        link = f'<a href="{file_name}">{file_name}</a>'
        return link

    def add_link_to_image(self, description: str, source_image_path: str) -> None:
        """
        Inserts a thumbnail link to an image to the page.
        """
        # Remove every character from the string 'description' that is not alphanumeric or a space.
        description = "".join(e for e in description if e.isalnum() or e.isspace())
        target_image_filename = str(self._get_next_page_id()) + " - " + description
        # Copy the image to the log directory.
        local_image_path = os.path.join(self.log_dir, target_image_filename)
        shutil.copyfile(source_image_path, local_image_path)
        self._log_text("\n" + description)
        self._log_text(self._link_to_image(target_image_filename, description))

    def flush(self, finished: bool = False) -> None:
        """
        Writes the current state of the log to disk.
        """
        if self.level > self.levels["INFO"]:
            return
        # Create a call tree of the log.
        call_tree_path = os.path.join(self.log_dir, self.name + ".html")
        with open(call_tree_path, "w") as f:
            f.write(_html_opening("0 Call Tree", finished=finished))
            f.write(f"<h3>{self.name}</h3>")
            f.write("\n")
            for page in self.pages:
                if page.show_in_call_tree:
                    f.write(page.line_text + "\n")
            f.write("\n")
            f.write(_html_closing())

    def enter_function(self) -> Optional["Page"]:
        """
        Adds a new page corresponding to the current function call.
        """
        if self.level > self.levels["INFO"]:
            return None

        page = None
        frame_type = inspect.currentframe()
        if frame_type is not None:
            frame = frame_type.f_back  # Get the calling frame
            if frame is not None:
                # Check if it's a method by looking for 'self' or 'cls' in f_locals
                if "self" in frame.f_locals:
                    class_name = type(frame.f_locals["self"]).__name__
                elif "cls" in frame.f_locals:
                    class_name = frame.f_locals["cls"].__name__
                else:
                    class_name = None  # Not part of a class

                if class_name is None:  # Not part of a class
                    caller_name = frame.f_code.co_name
                else:
                    caller_name = class_name + "." + frame.f_code.co_name

                # Create a new page for this function.
                page = self._add_page(summary=caller_name, show_in_call_tree=True, finished=False)
                self.page_stack.push(page)
                self.page_stack.write_stack_to_page(page)

                page.add_lines("\nENTER {}".format(caller_name), flush=True)
        return page

    def leave_function(self) -> None:
        """
        Finishes the page corresponding to the current function call.
        """
        if self.level > self.levels["INFO"]:
            return None
        page = self.page_stack.top()
        if page is not None:
            page.finished = True
            page.add_lines("\nLEAVE {}".format(page.summary), flush=True)
            self.page_stack.pop()

# From utils/page_logger.py
class Page:
    """
    Represents a single HTML page in the logger output.

    Args:
        page_logger: The PageLogger object that created this page.
        index: The index of the page.
        summary: A brief summary of the page's contents for display.
        indent_level: The level of indentation in the call tree.
        show_in_call_tree: Whether to display the page in the call tree.
        finished: Whether the page is complete.
    """

    def __init__(
        self,
        page_logger: PageLogger,
        index: int,
        summary: str,
        indent_level: int,
        show_in_call_tree: bool = True,
        finished: bool = True,
    ):
        """
        Initializes and writes to a new HTML page.
        """
        self.page_logger = page_logger
        self.index_str = str(index)
        self.summary = summary
        self.indent_level = indent_level
        self.show_in_call_tree = show_in_call_tree
        self.finished = finished
        self.file_title = self.index_str + "  " + self.summary
        self.indentation_text = "|&emsp;" * self.indent_level
        self.full_link = f'<a href="{self.index_str}.html">{self.file_title}</a>'
        self.line_text = self.indentation_text + self.full_link
        self.lines: List[str] = []
        self.flush()

    def add_lines(self, lines: str, flush: bool = False) -> None:
        """
        Adds one or more lines to the page.
        """
        lines_to_add: List[str] = []
        if "\n" in lines:
            lines_to_add = lines.split("\n")
        else:
            lines_to_add.append(lines)
        self.lines.extend(lines_to_add)
        if flush:
            self.flush()

    def flush(self) -> None:
        """
        Writes the HTML page to disk.
        """
        page_path = os.path.join(self.page_logger.log_dir, self.index_str + ".html")
        with open(page_path, "w") as f:
            f.write(_html_opening(self.file_title, finished=self.finished))
            f.write(f"<h3>{self.file_title}</h3>\n")
            for line in self.lines:
                try:
                    f.write(f"{line}\n")
                except UnicodeEncodeError:
                    f.write("UnicodeEncodeError in this line.\n")
            f.write(_html_closing())
            f.flush()

# From utils/page_logger.py
class PageStack:
    """
    A call stack containing a list of currently active function pages in the order they called each other.
    """

    def __init__(self) -> None:
        self.stack: List[Page] = []

    def push(self, page: Page) -> None:
        """Adds a page to the top of the stack."""
        self.stack.append(page)

    def pop(self) -> Page:
        """Removes and returns the top page from the stack"""
        return self.stack.pop()

    def size(self) -> int:
        """Returns the number of pages in the stack."""
        return len(self.stack)

    def top(self) -> Page | None:
        """Returns the top page from the stack without removing it"""
        if self.size() == 0:
            return None
        return self.stack[-1]

    def write_stack_to_page(self, page: Page) -> None:
        # Logs a properly indented string displaying the current call stack.
        page.add_lines("\nCALL STACK")
        for stack_page in self.stack:
            page.add_lines(stack_page.line_text)
        page.add_lines("")
        page.add_lines("")
        page.flush()

# From utils/page_logger.py
def finalize(self) -> None:
        # Writes a hash of the log directory to a file for change detection.
        if self.level >= self.levels["NONE"]:
            return

        # Don't finalize the log if it has already been finalized.
        if self.finalized:
            return

        # Do nothing if the app is being forced to exit early.
        if self.page_stack.size() > 0:
            return

        self.flush(finished=True)

        # Write the hash and other details to a file.
        hash_str, num_files, num_subdirs = hash_directory(self.log_dir)
        hash_path = os.path.join(self.log_dir, "hash.txt")
        with open(hash_path, "w") as f:
            f.write(hash_str)
            f.write("\n")
            f.write("{} files\n".format(num_files))
            f.write("{} subdirectories\n".format(num_subdirs))

        self.finalized = True

# From utils/page_logger.py
def debug(self, line: str) -> None:
        """
        Adds DEBUG text to the current page if debugging level <= DEBUG.
        """
        if self.level <= self.levels["DEBUG"]:
            self._log_text(line)

# From utils/page_logger.py
def info(self, line: str) -> None:
        """
        Adds INFO text to the current page if debugging level <= INFO.
        """
        if self.level <= self.levels["INFO"]:
            self._log_text(line)

# From utils/page_logger.py
def warning(self, line: str) -> None:
        """
        Adds WARNING text to the current page if debugging level <= WARNING.
        """
        if self.level <= self.levels["WARNING"]:
            self._log_text(line)

# From utils/page_logger.py
def error(self, line: str) -> None:
        """
        Adds ERROR text to the current page if debugging level <= ERROR.
        """
        if self.level <= self.levels["ERROR"]:
            self._log_text(line)

# From utils/page_logger.py
def critical(self, line: str) -> None:
        """
        Adds CRITICAL text to the current page if debugging level <= CRITICAL.
        """
        if self.level <= self.levels["CRITICAL"]:
            self._log_text(line)

# From utils/page_logger.py
def log_message_content(self, message_content: MessageContent, summary: str) -> None:
        """
        Adds a page containing the message's content, including any images.
        """
        if self.level > self.levels["INFO"]:
            return None
        page = self._add_page(summary=summary, show_in_call_tree=False)
        self.page_stack.write_stack_to_page(page)
        page.add_lines(self._format_message_content(message_content=message_content))
        page.flush()

# From utils/page_logger.py
def log_dict_list(self, content: List[Mapping[str, Any]], summary: str) -> None:
        """
        Adds a page containing a list of dicts.
        """
        if self.level > self.levels["INFO"]:
            return None
        page = self._add_page(summary=summary, show_in_call_tree=False)
        self.page_stack.write_stack_to_page(page)

        for item in content:
            json_str = json.dumps(item, indent=4)
            page.add_lines(json_str)

        page.flush()

# From utils/page_logger.py
def log_model_call(
        self, summary: str, input_messages: List[LLMMessage], response: CreateResult
    ) -> Optional["Page"]:
        """
        Logs messages sent to a model and the TaskResult response to a new page.
        """
        if self.level > self.levels["INFO"]:
            return None

        response_str = response.content
        if not isinstance(response_str, str):
            response_str = "??"

        page = self._log_model_messages(summary, input_messages, response_str, response.usage)
        return page

# From utils/page_logger.py
def log_model_task(
        self, summary: str, input_messages: List[LLMMessage], task_result: TaskResult
    ) -> Optional["Page"]:
        """
        Logs messages sent to a model and the TaskResult response to a new page.
        """
        if self.level > self.levels["INFO"]:
            return None

        messages: Sequence[BaseAgentEvent | BaseChatMessage] = task_result.messages
        message = messages[-1]
        response_str = message.to_text()
        if not isinstance(response_str, str):
            response_str = "??"

        if hasattr(message, "models_usage"):
            usage: RequestUsage | None = message.models_usage
        else:
            usage = RequestUsage(prompt_tokens=0, completion_tokens=0)

        page = self._log_model_messages(summary, input_messages, response_str, usage)
        return page

# From utils/page_logger.py
def log_link_to_local_file(self, file_path: str) -> str:
        """
        Returns a link to a local file in the log.
        """
        file_name = os.path.basename(file_path)
        link = f'<a href="{file_name}">{file_name}</a>'
        return link

# From utils/page_logger.py
def add_link_to_image(self, description: str, source_image_path: str) -> None:
        """
        Inserts a thumbnail link to an image to the page.
        """
        # Remove every character from the string 'description' that is not alphanumeric or a space.
        description = "".join(e for e in description if e.isalnum() or e.isspace())
        target_image_filename = str(self._get_next_page_id()) + " - " + description
        # Copy the image to the log directory.
        local_image_path = os.path.join(self.log_dir, target_image_filename)
        shutil.copyfile(source_image_path, local_image_path)
        self._log_text("\n" + description)
        self._log_text(self._link_to_image(target_image_filename, description))

# From utils/page_logger.py
def flush(self, finished: bool = False) -> None:
        """
        Writes the current state of the log to disk.
        """
        if self.level > self.levels["INFO"]:
            return
        # Create a call tree of the log.
        call_tree_path = os.path.join(self.log_dir, self.name + ".html")
        with open(call_tree_path, "w") as f:
            f.write(_html_opening("0 Call Tree", finished=finished))
            f.write(f"<h3>{self.name}</h3>")
            f.write("\n")
            for page in self.pages:
                if page.show_in_call_tree:
                    f.write(page.line_text + "\n")
            f.write("\n")
            f.write(_html_closing())

# From utils/page_logger.py
def enter_function(self) -> Optional["Page"]:
        """
        Adds a new page corresponding to the current function call.
        """
        if self.level > self.levels["INFO"]:
            return None

        page = None
        frame_type = inspect.currentframe()
        if frame_type is not None:
            frame = frame_type.f_back  # Get the calling frame
            if frame is not None:
                # Check if it's a method by looking for 'self' or 'cls' in f_locals
                if "self" in frame.f_locals:
                    class_name = type(frame.f_locals["self"]).__name__
                elif "cls" in frame.f_locals:
                    class_name = frame.f_locals["cls"].__name__
                else:
                    class_name = None  # Not part of a class

                if class_name is None:  # Not part of a class
                    caller_name = frame.f_code.co_name
                else:
                    caller_name = class_name + "." + frame.f_code.co_name

                # Create a new page for this function.
                page = self._add_page(summary=caller_name, show_in_call_tree=True, finished=False)
                self.page_stack.push(page)
                self.page_stack.write_stack_to_page(page)

                page.add_lines("\nENTER {}".format(caller_name), flush=True)
        return page

# From utils/page_logger.py
def leave_function(self) -> None:
        """
        Finishes the page corresponding to the current function call.
        """
        if self.level > self.levels["INFO"]:
            return None
        page = self.page_stack.top()
        if page is not None:
            page.finished = True
            page.add_lines("\nLEAVE {}".format(page.summary), flush=True)
            self.page_stack.pop()

# From utils/page_logger.py
def add_lines(self, lines: str, flush: bool = False) -> None:
        """
        Adds one or more lines to the page.
        """
        lines_to_add: List[str] = []
        if "\n" in lines:
            lines_to_add = lines.split("\n")
        else:
            lines_to_add.append(lines)
        self.lines.extend(lines_to_add)
        if flush:
            self.flush()

# From utils/page_logger.py
def push(self, page: Page) -> None:
        """Adds a page to the top of the stack."""
        self.stack.append(page)

# From utils/page_logger.py
def pop(self) -> Page:
        """Removes and returns the top page from the stack"""
        return self.stack.pop()

# From utils/page_logger.py
def size(self) -> int:
        """Returns the number of pages in the stack."""
        return len(self.stack)

# From utils/page_logger.py
def top(self) -> Page | None:
        """Returns the top page from the stack without removing it"""
        if self.size() == 0:
            return None
        return self.stack[-1]

# From utils/page_logger.py
def write_stack_to_page(self, page: Page) -> None:
        # Logs a properly indented string displaying the current call stack.
        page.add_lines("\nCALL STACK")
        for stack_page in self.stack:
            page.add_lines(stack_page.line_text)
        page.add_lines("")
        page.add_lines("")
        page.flush()

from _functions import UserContent
from page_logger import PageLogger
from apprentice import Apprentice

# From utils/grader.py
class Grader:
    """
    Runs basic tests, and determines task success without limitation to string matches.

    Args:
        client: The client to call the model.
        logger: An optional logger. If None, no logging will be performed.
    """

    def __init__(self, client: ChatCompletionClient, logger: PageLogger | None = None) -> None:
        if logger is None:
            logger = PageLogger()  # Nothing will be logged by this object.
        self.logger = logger
        self.client = client

        # Create the chat history
        self._chat_history: List[LLMMessage] = []

    async def test_apprentice(
        self,
        apprentice: Apprentice,
        task_description: str,
        expected_answer: str,
        num_trials: int,
        use_memory: bool,
        client: ChatCompletionClient,
    ) -> Tuple[int, int]:
        self.logger.enter_function()

        self.logger.info("Testing the apprentice on the given task.\n")

        num_successes = 0

        for trial in range(num_trials):
            self.logger.info("\n-----  TRIAL {}  -----\n".format(trial + 1))
            self.logger.info("Try to solve the task.\n")
            response = await apprentice.assign_task(task_description, use_memory=use_memory)
            response_is_correct, extracted_answer = await self.is_response_correct(
                task_description, response, expected_answer
            )
            self.logger.info("Extracted answer:  {}".format(extracted_answer))
            if response_is_correct:
                self.logger.info("Answer is CORRECT.\n")
                num_successes += 1
            else:
                self.logger.info("Answer is INCORRECT.\n")

        self.logger.info("\nSuccess rate:  {}%\n".format(round((num_successes / num_trials) * 100)))
        self.logger.leave_function()
        return num_successes, num_trials

    async def call_model(
        self,
        summary: str,
        user_content: UserContent,
        system_message_content: str | None = None,
        keep_these_messages: bool = True,
    ) -> str:
        """
        Calls the model client with the given input and returns the response.
        """
        # Prepare the input message list
        if system_message_content is None:
            system_message_content = "You are a helpful assistant."
        system_message: LLMMessage
        if self.client.model_info["family"] == "o1":
            # No system message allowed, so pass it as the first user message.
            system_message = UserMessage(content=system_message_content, source="User")
        else:
            # System message allowed.
            system_message = SystemMessage(content=system_message_content)
        user_message = UserMessage(content=user_content, source="User")
        input_messages = [system_message] + self._chat_history + [user_message]

        # Call the model.
        response = await self.client.create(input_messages)
        assert isinstance(response, CreateResult)
        response_string = response.content
        assert isinstance(response_string, str)
        response_message = AssistantMessage(content=response_string, source="Assistant")
        assert isinstance(response_message, AssistantMessage)

        # Log the model call
        self.logger.log_model_call(summary=summary, input_messages=input_messages, response=response)

        # Manage the chat history
        if keep_these_messages:
            self._chat_history.append(user_message)
            self._chat_history.append(response_message)

        # Return the response as a string
        return response_string

    def _clear_history(self) -> None:
        """
        Empties the message list containing the chat history.
        """
        self._chat_history = []

    async def is_response_correct(
        self, task_description: str, response_to_be_graded: str, correct_answer: str
    ) -> Tuple[bool, str]:
        """
        Determines whether the response is equivalent to the task's correct answer.
        """
        self.logger.enter_function()

        sys_message = """You are a helpful and thoughtful assistant."""

        # Ask the model to extract the answer from the response.
        user_message: List[Union[str, Image]] = []
        user_message.append("""Your job is to extract a possible answer to the following question from the given text.
- First review the following task.
- Then review the text that follows, which may an answer, plus reasoning that led to the answer.
- Do not attempt to actually solve the task yourself.
- Don't try to judge whether the reasoning steps were correct.
- Simply respond by summarizing the answer described in the text, omitting any other parts of the text.
- If no answer is present can be extracted from the text, simply reply "None".""")
        user_message.append("\n# Task description")
        user_message.append(task_description)
        user_message.append("\n# Text that may contain an answer")
        user_message.append(response_to_be_graded)
        user_message_arg: UserContent = user_message
        self._clear_history()
        extracted_answer = await self.call_model(
            summary="Ask the model to extract the answer",
            system_message_content=sys_message,
            user_content=user_message_arg,
        )
        self.logger.info("Extracted answer: " + extracted_answer)

        # Ask the model to check the answer for correctness.
        user_message = [
            """Your job is to decide whether a given answer to a task is correct or not.
- You will be given the task description and the correct, gold-standard answer, along with the answer to be graded.
- In general, an answer is correct if it is equivalent to the correct answer.
- Specifically, the given answer must contain the important information from the correct answer, and must not in any way contradict the correct answer.
- Ignore any differences of grammar, spelling mistakes, punctuation, capitalization, formatting, or extra commentary.
- An answer should be considered correct if it omits information that is clearly inferred.
  - For instance, if the correct answer is "Paris, France", the answer "Paris" should be considered correct.
- Respond with a single character: '1' if the answer to be graded is correct", '0' if not."""
        ]
        user_message.append("\n# Task description")
        user_message.append(task_description)
        user_message.append("\n# Correct answer")
        user_message.append(correct_answer)
        user_message.append("\n# Answer to be graded")
        user_message.append(extracted_answer)
        self._clear_history()
        decision = await self.call_model(
            summary="Ask the model to check the answer for correctness",
            system_message_content=sys_message,
            user_content=user_message,
        )
        self.logger.info("Decision: " + decision)

        self.logger.leave_function()
        return decision == "1", extracted_answer

from autogen_core.memory import Memory
from autogen_core.memory import MemoryContent
from autogen_core.memory import MemoryMimeType
from autogen_core.memory import MemoryQueryResult
from autogen_core.memory import UpdateContextResult
from autogen_ext.experimental.task_centric_memory import MemoryController

# From utils/teachability.py
class Teachability(Memory):
    """
    Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice.

    Steps for usage:

        1. Instantiate MemoryController.
        2. Instantiate Teachability, passing the memory controller as a parameter.
        3. Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter.
        4. Use the AssistantAgent as usual, such as for chatting with the user.
    """

    def __init__(self, memory_controller: "MemoryController", name: str | None = None) -> None:
        """Initialize Teachability."""
        self._memory_controller = memory_controller
        self._logger = memory_controller.logger
        self._name = name or "teachability"

    @property
    def name(self) -> str:
        """Get the memory instance identifier."""
        return self._name

    def _extract_text(self, content_item: str | MemoryContent) -> str:
        """Extract searchable text from content."""
        if isinstance(content_item, str):
            return content_item

        content = content_item.content
        mime_type = content_item.mime_type

        if mime_type in [MemoryMimeType.TEXT, MemoryMimeType.MARKDOWN]:
            return str(content)
        elif mime_type == MemoryMimeType.JSON:
            if isinstance(content, dict):
                # Store original JSON string representation
                return str(content).lower()
            raise ValueError("JSON content must be a dict")
        elif isinstance(content, Image):
            raise ValueError("Image content cannot be converted to text")
        else:
            raise ValueError(f"Unsupported content type: {mime_type}")

    async def update_context(
        self,
        model_context: ChatCompletionContext,
    ) -> UpdateContextResult:
        """
        Extracts any advice from the last user turn to be stored in memory,
        and adds any relevant memories to the model context.
        """
        self._logger.enter_function()

        # Extract text from the user's last message
        messages = await model_context.get_messages()
        if not messages:
            self._logger.leave_function()
            return UpdateContextResult(memories=MemoryQueryResult(results=[]))
        last_message = messages[-1]
        last_user_text = last_message.content if isinstance(last_message.content, str) else str(last_message)

        # Add any relevant memories to the chat history
        query_results = await self.query(last_user_text)
        if query_results.results:
            memory_strings = [f"{i}. {str(memory.content)}" for i, memory in enumerate(query_results.results, 1)]
            memory_context = "\nPotentially relevant memories:\n" + "\n".join(memory_strings)
            await model_context.add_message(UserMessage(content=memory_context, source="user"))

        # Add any user advice to memory
        await self._memory_controller.consider_memo_storage(last_user_text)

        self._logger.leave_function()
        return UpdateContextResult(memories=query_results)

    async def add(self, content: MemoryContent, cancellation_token: CancellationToken | None = None) -> None:
        """
        Tries to extract any advice from the passed content and add it to memory.
        """
        self._logger.enter_function()

        # Extract text from the incoming content
        text = self._extract_text(content)

        # Check for advice to add to memory for later turns.
        await self._memory_controller.consider_memo_storage(text)

        self._logger.leave_function()

    async def query(
        self,
        query: str | MemoryContent,
        cancellation_token: CancellationToken | None = None,
        **kwargs: Any,
    ) -> MemoryQueryResult:
        """
        Returns any memories that seem relevant to the query.
        """
        self._logger.enter_function()

        task = self._extract_text(query)
        memory_results: list[MemoryContent] = []
        filtered_memos = await self._memory_controller.retrieve_relevant_memos(task=task)
        filtered_insights = [memo.insight for memo in filtered_memos]
        for insight in filtered_insights:
            self._logger.info(f"Insight: {insight}")
            memory_content = MemoryContent(
                content=insight,
                mime_type="MemoryMimeType.TEXT",
                metadata={},
            )
            memory_results.append(memory_content)

        self._logger.leave_function()
        return MemoryQueryResult(results=memory_results)

    async def clear(self) -> None:
        """Clear all entries from memory."""
        self._memory_controller.reset_memory()

    async def close(self) -> None:
        """Clean up memory resources."""
        pass


# From utils/chat_completion_client_recorder.py
class RecordDict(TypedDict):
    mode: Literal["create", "create_stream"]
    messages: List[Mapping[str, Any]]
    response: Dict[str, Any]
    stream: List[Mapping[str, Any]]

# From utils/chat_completion_client_recorder.py
class ChatCompletionClientRecorder(ChatCompletionClient):
    """
    A chat completion client that supports fast, large-scale tests of code calling LLM clients.

    Two modes are supported:

      1. "record": delegates to the underlying client while also recording the input messages and responses,
         which are saved to disk when finalize() is called.
      2. "replay": loads previously recorded message and responses from disk, then on each call
         checks that its message matches the recorded message, and returns the recorded response.

    The recorded data is stored as a JSON list of records. Each record is a dictionary with a "mode"
    field (either "create" or "create_stream"), a serialized list of messages, and either a "response" (for
    create calls) or a "stream" (a list of streamed outputs for create_stream calls).

    ReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences:

        - ReplayChatCompletionClient replays pre-defined responses in a specified order without recording anything or checking the messages sent to the client.
        - ChatCompletionCache caches responses and replays them for messages that have been seen before, regardless of order, and calls the base client for any uncached messages.
    """

    def __init__(
        self,
        client: ChatCompletionClient,
        mode: Literal["record", "replay"],
        session_file_path: str,
        logger: PageLogger | None = None,
    ) -> None:
        if logger is None:
            self.logger = PageLogger()  # Disabled by default.
        else:
            self.logger = logger
        self.logger.enter_function()
        self.logger.info("Wrapping the base client in ChatCompletionClientRecorder.")

        self.base_client = client
        self.mode = mode
        self.session_file_path = os.path.expanduser(session_file_path)
        self.records: List[RecordDict] = []
        self._record_index = 0
        self._num_checked_records = 0
        if self.mode == "record":
            # Prepare to record the messages and responses.
            self.logger.info("Recording mode enabled.\nRecording session to: " + self.session_file_path)
        elif self.mode == "replay":
            # Load the previously recorded messages and responses from disk.
            self.logger.info("Replay mode enabled.\nRetrieving session from: " + self.session_file_path)
            try:
                with open(self.session_file_path, "r") as f:
                    self.records = json.load(f)
            except Exception as e:
                error_str = f"\nFailed to load recorded session: '{self.session_file_path}': {e}"
                self.logger.error(error_str)
                raise ValueError(error_str) from e

        self.logger.leave_function()

    async def create(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
    ) -> CreateResult:
        current_messages: List[Mapping[str, Any]] = [msg.model_dump() for msg in messages]
        if self.mode == "record":
            response = await self.base_client.create(
                messages,
                tools=tools,
                json_output=json_output,
                tool_choice=tool_choice,
                extra_create_args=extra_create_args,
                cancellation_token=cancellation_token,
            )

            rec: RecordDict = {
                "mode": "create",
                "messages": current_messages,
                "response": response.model_dump(),
                "stream": [],
            }
            self.records.append(rec)
            return response
        elif self.mode == "replay":
            if self._record_index >= len(self.records):
                error_str = "\nNo more recorded turns to check."
                self.logger.error(error_str)
                raise ValueError(error_str)
            rec = self.records[self._record_index]
            if rec.get("mode") != "create":
                error_str = f"\nRecorded call type mismatch at index {self._record_index}: expected 'create', got '{rec.get('mode')}'."
                self.logger.error(error_str)
                raise ValueError(error_str)
            recorded_messages = rec.get("messages")
            if recorded_messages != current_messages:
                error_str = (
                    "\nCurrent message list doesn't match the recorded message list. See the pagelogs for details."
                )
                assert recorded_messages is not None
                self.logger.log_dict_list(recorded_messages, "recorded message list")
                assert current_messages is not None
                self.logger.log_dict_list(current_messages, "current message list")
                self.logger.error(error_str)
                raise ValueError(error_str)
            self._record_index += 1
            self._num_checked_records += 1

            data = rec.get("response")
            # Populate a CreateResult from the data.
            assert data is not None
            result = CreateResult(
                content=data.get("content", ""),
                finish_reason=data.get("finish_reason", "stop"),
                usage=data.get("usage", RequestUsage(prompt_tokens=0, completion_tokens=0)),
                cached=True,
            )
            return result

        else:
            error_str = f"\nUnknown mode: {self.mode}"
            self.logger.error(error_str)
            raise ValueError(error_str)

    def create_stream(
        self,
        messages: Sequence[LLMMessage],
        *,
        tools: Sequence[Tool | ToolSchema] = [],
        json_output: Optional[bool | type[BaseModel]] = None,
        extra_create_args: Mapping[str, Any] = {},
        cancellation_token: Optional[CancellationToken] = None,
        tool_choice: Tool | Literal["auto", "required", "none"] = "auto",
    ) -> AsyncGenerator[Union[str, CreateResult], None]:
        return self.base_client.create_stream(
            messages,
            tools=tools,
            tool_choice=tool_choice,
            json_output=json_output,
            extra_create_args=extra_create_args,
            cancellation_token=cancellation_token,
        )

    async def close(self) -> None:
        await self.base_client.close()

    def actual_usage(self) -> RequestUsage:
        # Calls base_client.actual_usage() and returns the result.
        return self.base_client.actual_usage()

    def total_usage(self) -> RequestUsage:
        # Calls base_client.total_usage() and returns the result.
        return self.base_client.total_usage()

    def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:
        # Calls base_client.count_tokens() and returns the result.
        return self.base_client.count_tokens(messages, tools=tools)

    def remaining_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -> int:
        # Calls base_client.remaining_tokens() and returns the result.
        return self.base_client.remaining_tokens(messages, tools=tools)

    @property
    def capabilities(self) -> ModelCapabilities:  # type: ignore
        # Calls base_client.capabilities and returns the result.
        warnings.warn("capabilities is deprecated, use model_info instead", DeprecationWarning, stacklevel=2)
        return self.base_client.capabilities

    @property
    def model_info(self) -> ModelInfo:
        # Calls base_client.model_info and returns the result.
        return self.base_client.model_info

    def finalize(self) -> None:
        """
        In record mode, saves the accumulated records to disk.
        In replay mode, makes sure all the records were checked.
        """
        self.logger.enter_function()
        if self.mode == "record":
            try:
                # Create the directory if it doesn't exist.
                os.makedirs(os.path.dirname(self.session_file_path), exist_ok=True)
                # Write the records to disk.
                with open(self.session_file_path, "w") as f:
                    json.dump(self.records, f, indent=2)
                    self.logger.info("\nRecorded session was saved to: " + self.session_file_path)
            except Exception as e:
                error_str = f"Failed to write records to '{self.session_file_path}': {e}"
                self.logger.error(error_str)
                raise ValueError(error_str) from e
        elif self.mode == "replay":
            if self._num_checked_records < len(self.records):
                error_str = f"\nEarly termination. Only {self._num_checked_records} of the {len(self.records)} recorded turns were checked."
                self.logger.error(error_str)
                raise ValueError(error_str)
            self.logger.info("\nRecorded session was fully replayed and checked.")
        self.logger.leave_function()


# From task_centric_memory/utils.py
def create_oai_client(config: Dict[str, Any]) -> ChatCompletionClient:
    """
    Creates a chat completion client from OpenAI.
    """
    client = OpenAIChatCompletionClient(
        model=config["model"],
        max_tokens=config["max_completion_tokens"],
        max_retries=config["max_retries"],
        temperature=config["temperature"],
        presence_penalty=config["presence_penalty"],
        frequency_penalty=config["frequency_penalty"],
        top_p=config["top_p"],
    )
    return client

# From task_centric_memory/utils.py
def load_yaml_file(file_path: str) -> Any:
    """
    Opens a file and returns its contents.
    """
    with open(file_path, "r") as file:
        return yaml.safe_load(file)



import pytest_asyncio

# From http/conftest.py
class TestArgs(BaseModel):
    query: str = Field(description="The test query")
    value: int = Field(description="A test value")

# From http/conftest.py
class TestResponse(BaseModel):
    result: str = Field(description="The test result")

# From http/conftest.py
def test_config() -> ComponentModel:
    return ComponentModel(
        provider="autogen_ext.tools.http.HttpTool",
        config={
            "name": "TestHttpTool",
            "description": "A test HTTP tool",
            "scheme": "http",
            "path": "/test",
            "host": "localhost",
            "port": 8000,
            "method": "POST",
            "headers": {"Content-Type": "application/json"},
            "json_schema": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The test query"},
                    "value": {"type": "integer", "description": "A test value"},
                },
                "required": ["query", "value"],
            },
        },
    )

from unittest.mock import AsyncMock
from unittest.mock import MagicMock
from unittest.mock import patch
from azure.core.credentials import TokenCredential

# From azure/conftest.py
class AccessTokenProtocol(Protocol):
    """Protocol matching Azure AccessToken."""

    token: str
    expires_on: int

# From azure/conftest.py
class MockAccessToken:
    """Mock implementation of AccessToken."""

    def __init__(self, token: str, expires_on: int) -> None:
        self.token = token
        self.expires_on = expires_on

# From azure/conftest.py
class MockAzureKeyCredential:
    """Mock implementation of AzureKeyCredential."""

    def __init__(self, key: str) -> None:
        self.key = key

# From azure/conftest.py
class MockTokenCredential:
    """Mock implementation of TokenCredential for testing."""

    def get_token(
        self,
        *scopes: str,
        claims: str | None = None,
        tenant_id: str | None = None,
        enable_cae: bool = False,
        **kwargs: Any,
    ) -> AccessTokenProtocol:
        """Mock get_token method that implements TokenCredential protocol."""
        return MockAccessToken("mock-token", 12345)

# From azure/conftest.py
class AsyncIterator:
    """Async iterator for testing."""

    def __init__(self, items: List[Dict[str, Any]]) -> None:
        self.items = items.copy()

    def __aiter__(self) -> "AsyncIterator":
        return self

    async def __anext__(self) -> Dict[str, Any]:
        if not self.items:
            raise StopAsyncIteration
        return self.items.pop(0)

    async def get_count(self) -> int:
        """Return count of items."""
        return len(self.items)

# From azure/conftest.py
def mock_vectorized_query() -> MagicMock:
    """Create a mock VectorizedQuery for testing."""
    if azure_sdk_available:
        from azure.search.documents.models import VectorizedQuery

        return MagicMock(spec=VectorizedQuery)
    else:
        return MagicMock()

# From azure/conftest.py
def keyword_config() -> ComponentModel:
    """Return a keyword search configuration."""
    return ComponentModel(
        provider="autogen_ext.tools.azure.MockAzureAISearchTool",
        config={
            "name": "KeywordSearch",
            "description": "Keyword search tool",
            "endpoint": "https://test-search-service.search.windows.net",
            "index_name": "test-index",
            "credential": AzureKeyCredential("test-key") if azure_sdk_available else {"api_key": "test-key"},  # pyright: ignore [reportPossiblyUnboundVariable]
            "query_type": "keyword",
            "search_fields": ["content", "title"],
            "select_fields": ["id", "content", "title", "source"],
        },
    )

# From azure/conftest.py
def vector_config() -> ComponentModel:
    """Create a test configuration for vector search."""
    return ComponentModel(
        provider="autogen_ext.tools.azure.MockAzureAISearchTool",
        config={
            "name": "VectorSearch",
            "description": "Vector search tool",
            "endpoint": "https://test-search-service.search.windows.net",
            "index_name": "test-index",
            "api_version": "2023-10-01-Preview",
            "credential": AzureKeyCredential("test-key") if azure_sdk_available else {"api_key": "test-key"},  # pyright: ignore [reportPossiblyUnboundVariable]
            "query_type": "vector",
            "vector_fields": ["embedding"],
            "select_fields": ["id", "content", "title", "source"],
            "top": 5,
        },
    )

# From azure/conftest.py
def hybrid_config() -> ComponentModel:
    """Create a test configuration for hybrid search."""
    return ComponentModel(
        provider="autogen_ext.tools.azure.MockAzureAISearchTool",
        config={
            "name": "HybridSearch",
            "description": "Hybrid search tool",
            "endpoint": "https://test-search-service.search.windows.net",
            "index_name": "test-index",
            "api_version": "2023-10-01-Preview",
            "credential": AzureKeyCredential("test-key") if azure_sdk_available else {"api_key": "test-key"},  # pyright: ignore [reportPossiblyUnboundVariable]
            "query_type": "keyword",
            "search_fields": ["content", "title"],
            "vector_fields": ["embedding"],
            "select_fields": ["id", "content", "title", "source"],
            "top": 5,
        },
    )

# From azure/conftest.py
def mock_search_response() -> List[Dict[str, Any]]:
    """Create a mock search response."""
    return [
        {
            "@search.score": 0.95,
            "id": "doc1",
            "content": "This is the first document content",
            "title": "Document 1",
            "source": "test-source-1",
        },
        {
            "@search.score": 0.85,
            "id": "doc2",
            "content": "This is the second document content",
            "title": "Document 2",
            "source": "test-source-2",
        },
    ]

# From azure/conftest.py
def mock_search_client(mock_search_response: List[Dict[str, Any]]) -> Iterator[MagicMock]:
    """Create a mock search client for testing, with the patch active."""
    mock_client_instance = MagicMock()
    mock_client_instance.__aenter__ = AsyncMock(return_value=mock_client_instance)
    mock_client_instance.__aexit__ = AsyncMock(return_value=None)

    search_results_iterator = AsyncIterator(mock_search_response)
    mock_client_instance.search = MagicMock(return_value=search_results_iterator)

    patch_target = "autogen_ext.tools.azure._ai_search.SearchClient"
    patcher = patch(patch_target, return_value=mock_client_instance)

    patcher.start()
    yield mock_client_instance
    patcher.stop()


# From graphrag/conftest.py
def community_df_fixture() -> pd.DataFrame:
    data = {
        "id": ["d572b27c-a7c1-4a4e-a673-4c0efb9fdbd4", "7fa4296f-74d2-4ffb-abe4-3a16ff4b761c"],
        "human_readable_id": [0, 1],
        "community": [0, 1],
        "parent": [-1, -1],
        "level": [0, 0],
        "title": ["Community 0", "Community 1"],
        "entity_ids": [
            [
                "beba48a6-a5a6-458f-ae44-0c07f615e52f",
                "1277ec21-ab15-40e0-96e4-1eda5953a344",
                "195513fe-6e34-4e4f-ad13-e5fa88678a64",
                "3fb5e51a-819e-486e-b17c-d72832621cdd",
                "690a585a-57a9-4abc-b7f5-22051f823b11",
            ],
            [
                "da96b22a-5d6d-4810-bd1d-a6ad71a4cae6",
                "e28d29b0-20ce-4e51-803d-4e3178c59f49",
                "fb396b61-faba-4cb5-bd0c-578ac5df7aa1",
                "d0e85ed0-cb61-4b2a-b271-5dabe5e69c70",
                "b627179c-8354-4c2c-adea-df89f49d4b21",
                "2fcadfbe-61d9-47a7-a259-5c4f9a635a07",
                "d659dbe2-21ba-41fe-9d9d-3e8d800ac444",
                "edc7f87a-0dd6-4db4-9662-6e29aeb1568f",
                "7bb3b3e7-556f-44ce-bd3d-800c7a55d864",
                "fc28a9b5-cf94-4436-92fe-ef733fb3c69d",
            ],
        ],
        "relationship_ids": [
            [
                "224f8223-feff-43f5-9cb2-960d5a650731",
                "6d6dc7ac-cc5a-4a1c-bf37-2cba7b7e7340",
                "8ce25898-f9b3-464a-ac6a-49bd3f898295",
                "bbea05ce-be5e-4970-b8f0-73414284ab84",
            ],
            [
                "13a99b88-aef5-4935-b5c2-e00d24b125ba",
                "4d8a7724-430c-4477-a588-bcecff6fb9d8",
                "6d09a03f-7ecd-4b92-9a78-e8fbf1cea6fd",
                "718ad0a4-861e-496a-921f-62f3106b5e73",
                "73f01531-4563-4836-aad7-b1272c989406",
                "74e81ef6-006f-443d-89e5-e40e491ae874",
                "772ba9c4-19b2-4d13-8666-e08548e0645c",
                "7e494f4c-8692-4794-baa2-aed4f8601a5a",
                "92b4fdab-1e52-4cf5-bd8d-adb47a191875",
                "bd4b2ea4-a9c8-4fa3-aa86-87dcafe0a887",
                "cda6af84-59a6-45d3-b074-8551867bf6a4",
                "f1a16c4b-f673-49d0-be1a-bbb36ae4e5c8",
                "fe26f48b-8152-4a70-a2d5-f13023cbae4c",
            ],
        ],
        "text_unit_ids": [
            [
                "9bcc5581a92c05081bb138322f3dd38589fea781f43c1ef53d208a637a4f37a3f1ee41bd432b20e5257500126b84b62a400befd22dcc92338ebb9d764f59abca"
            ],
            [
                "043185f776c61662fdbc1e50e270edd06f1cc2cdf76158cdff34e8e825ba5bd39453c9e773fc1aca15ab77c837959dc85149e64ef4849c9be0b72512a6fdb00d",
                "60ee542fe71e676b6cc61f19046c27bfa00ff5e086af7101c8aed2bb992436cb17cd192af0389257da9edcee5f67ab8ccd5f52f91102dcb2da2ecedb08a2bb52",
                "9c426c886a92062375501320a4ddf890003d2cc62c5face8356bc4361856c4fbb0f4573865cb9026e99b67bf61837d570251aa0a76acb9e448fb6de84c515a1e",
            ],
        ],
        "period": ["2024-12-16", "2024-12-16"],
        "size": [5, 10],
    }
    return pd.DataFrame(data)

# From graphrag/conftest.py
def entity_df_fixture() -> pd.DataFrame:
    data = {
        "id": ["55536111-6a0d-464f-9b72-616ae5d86c2f", "55536111-6a0d-464f-9b72-616ae5d86c2f"],
        "human_readable_id": [0, 0],
        "title": ["PROJECT GUTENBERG", "PROJECT GUTENBERG"],
        "community": [4, 32],
        "level": [0, 1],
        "degree": [11, 11],
        "x": [0, 0],
        "y": [0, 0],
    }
    return pd.DataFrame(data)

# From graphrag/conftest.py
def report_df_fixture() -> pd.DataFrame:
    data = {
        "id": ["53670ddbd42f4518940333eeabe599ed", "2f129d4030324a7688c14eafab50c81c"],
        "human_readable_id": [105, 106],
        "community": [105, 106],
        "parent": [22, 22],
        "level": [2, 2],
        "title": ["Peterson and the Missing Billycock", "Baker Street and Sherlock Holmes Community"],
        "summary": [
            "The community centers around Peterson, a commissionaire involved in a mystery concerning a lost hat and a goose. His actions are pivotal in the investigation led by Sherlock Holmes, connecting various entities such as the row, the billycock hat, and multiple newspapers where advertisements were placed.",
            "The community centers around Baker Street, the iconic residence of Sherlock Holmes, and its connection to the London Underground. Baker Street serves as a significant landmark associated with the famous detective, while the Underground facilitates access to this notable location.",
        ],
        "full_content": [
            "# Peterson and the Missing Billycock\\n\\nThe community centers around Peterson, a commissionaire involved in a mystery concerning a lost hat and a goose. ...",
            "# Baker Street and Sherlock Holmes Community\\n\\nThe community centers around Baker Street, the iconic residence of Sherlock Holmes, and its connection to the London Underground. Baker...",
        ],
        "rank": [6.5, 6.5],
        "rank_explanation": [
            "The impact severity rating is moderate to high due to the potential implications of the investigation on public interest and media coverage.",
            "The impact severity rating is moderate due to the cultural significance of Baker Street and its association with Sherlock Holmes, which attracts considerable public interest.",
        ],
        "findings": [
            [
                {
                    "explanation": "Peterson is a key figure in the mystery involving the missing blue carbuncle, acting as a commissionaire who aids Sherlock Holmes. His involvement is crucial as he not only discovers the lost billycock hat but also plays a significant role in disseminating information related to the case. This highlights his importance in the narrative and the potential impact of his actions on the investigation's outcome. [Data: Entities (333); Relationships (521, 522)]",
                    "summary": "Peterson's central role in the investigation",
                },
                {
                    "explanation": "The row refers to the altercation that prompted Peterson's intervention, leading to the discovery of the hat. This incident is pivotal as it sets the stage for the entire investigation, illustrating how a seemingly minor event can have far-reaching consequences. The altercation not only affects Peterson but also ties into the larger mystery that Holmes is trying to solve. [Data: Entities (339); Relationships (521)]",
                    "summary": "The significance of the row incident",
                },
                {
                    "explanation": "The billycock hat is not just an accessory but a crucial piece of evidence in the investigation. Its discovery by Peterson links him directly to the case and raises questions about its owner, Henry Baker. The hat's significance is underscored by its role in the narrative, as it is the object around which the mystery revolves. [Data: Entities (340); Relationships (522)]",
                    "summary": "The billycock hat as a central object",
                },
                {
                    "explanation": "Peterson's task of placing advertisements in various evening papers, including the Globe, Star, Pall Mall, and others, indicates the media's role in the investigation. This outreach is essential for gathering information about the hat's owner and demonstrates how public engagement can influence the resolution of the case. The involvement of multiple newspapers suggests a broad interest in the mystery, which could amplify its impact on the community. [Data: Entities (355, 356, 357, 358, 359, 360, 361); Relationships (545, 546, 547, 548, 549, 550, 551)]",
                    "summary": "Media involvement through advertisements",
                },
            ],
            [
                {
                    "explanation": "Baker Street is not only the residence of Sherlock Holmes but also a symbol of his adventures and detective work. Its association with the fictional detective has made it a notable landmark in London, drawing interest from fans and tourists alike. The street's historical and cultural significance contributes to its status as a must-visit location, enhancing its impact on the community. [Data: Entities (4), Relationships (178)]",
                    "summary": "Baker Street as a cultural landmark",
                },
                {
                    "explanation": "Sherlock Holmes is intrinsically linked to Baker Street, as it serves as his residence and the hub for his investigations. This relationship is central to the narrative of his character, making Baker Street a vital part of the Sherlock Holmes lore. The detective's activities at this location have become iconic, further solidifying the street's importance in popular culture. [Data: Entities (4), Relationships (178)]",
                    "summary": "Sherlock Holmes's connection to Baker Street",
                },
                {
                    "explanation": "The London Underground plays a crucial role in facilitating access to Baker Street, making it easier for visitors to reach this iconic location. The connection between the Underground and Baker Street enhances the street's accessibility, contributing to its popularity as a tourist destination. This relationship underscores the importance of public transportation in connecting significant cultural landmarks. [Data: Entities (548), Relationships (862)]",
                    "summary": "The role of the Underground in accessing Baker Street",
                },
                {
                    "explanation": "Baker Street is synonymous with detective work, primarily due to its association with Sherlock Holmes. The street is where many of Holmes's investigations take place, making it a focal point for fans of detective fiction. This connection to crime-solving and mystery adds to the allure of Baker Street, attracting those interested in the genre and its history. [Data: Entities (4), Relationships (178)]",
                    "summary": "Baker Street's association with detective work",
                },
                {
                    "explanation": "The combination of Baker Street's historical significance and its association with Sherlock Holmes has made it a popular tourist destination. Visitors often seek to explore the street and its surroundings, contributing to the local economy and cultural heritage. The public interest in this location highlights the impact of literary figures on real-world places and their ability to draw crowds. [Data: Entities (4), Relationships (178)]",
                    "summary": "Tourism and public interest in Baker Street",
                },
            ],
        ],
        "full_content_json": [
            '{\n    "title": "Peterson and the Missing Billycock",\n    "summary": "The community centers around Peterson, a commissionaire involved in a mystery concerning a lost hat and a goose. His actions are pivotal in the investigation led by Sherlock Holmes, connecting various entities such as the row, the billycock hat, and multiple newspapers where advertisements were placed.",\n    "findings": [\n        {\n            "summary": "Peterson\'s central role in the investigation",\n            "explanation": "Peterson is a key figure in the mystery involving the missing blue carbuncle, acting as a commissionaire who aids Sherlock Holmes. His involvement is crucial as he not only discovers the lost billycock hat but also plays a significant role in disseminating information related to the case. This highlights his importance in the narrative and the potential impact of his actions on the investigation\'s outcome. [Data: Entities (333); Relationships (521, 522)]"\n        },\n        {\n            "summary": "The significance of the row incident",\n            "explanation": "The row refers to the altercation that prompted Peterson\'s intervention, leading to the discovery of the hat. This incident is pivotal as it sets the stage for the entire investigation, illustrating how a seemingly minor event can have far-reaching consequences. The altercation not only affects Peterson but also ties into the larger mystery that Holmes is trying to solve. [Data: Entities (339); Relationships (521)]"\n        },\n        {\n            "summary": "The billycock hat as a central object",\n            "explanation": "The billycock hat is not just an accessory but a crucial piece of evidence in the investigation. Its discovery by Peterson links him directly to the case and raises questions about its owner, Henry Baker. The hat\'s significance is underscored by its role in the narrative, as it is the object around which the mystery revolves. [Data: Entities (340); Relationships (522)]"\n        },\n        {\n            "summary": "Media involvement through advertisements",\n            "explanation": "Peterson\'s task of placing advertisements in various evening papers, including the Globe, Star, Pall Mall, and others, indicates the media\'s role in the investigation. This outreach is essential for gathering information about the hat\'s owner and demonstrates how public engagement can influence the resolution of the case. The involvement of multiple newspapers suggests a broad interest in the mystery, which could amplify its impact on the community. [Data: Entities (355, 356, 357, 358, 359, 360, 361); Relationships (545, 546, 547, 548, 549, 550, 551)]"\n        }\n    ],\n    "rating": 6.5,\n    "rating_explanation": "The impact severity rating is moderate to high due to the potential implications of the investigation on public interest and media coverage.",\n    "extra_attributes": {}\n}',
            '{\n    "title": "Baker Street and Sherlock Holmes Community",\n    "summary": "The community centers around Baker Street, the iconic residence of Sherlock Holmes, and its connection to the London Underground. Baker Street serves as a significant landmark associated with the famous detective, while the Underground facilitates access to this notable location.",\n    "findings": [\n        {\n            "summary": "Baker Street as a cultural landmark",\n            "explanation": "Baker Street is not only the residence of Sherlock Holmes but also a symbol of his adventures and detective work. Its association with the fictional detective has made it a notable landmark in London, drawing interest from fans and tourists alike. The street\'s historical and cultural significance contributes to its status as a must-visit location, enhancing its impact on the community. [Data: Entities (4), Relationships (178)]"\n        },\n        {\n            "summary": "Sherlock Holmes\'s connection to Baker Street",\n            "explanation": "Sherlock Holmes is intrinsically linked to Baker Street, as it serves as his residence and the hub for his investigations. This relationship is central to the narrative of his character, making Baker Street a vital part of the Sherlock Holmes lore. The detective\'s activities at this location have become iconic, further solidifying the street\'s importance in popular culture. [Data: Entities (4), Relationships (178)]"\n        },\n        {\n            "summary": "The role of the Underground in accessing Baker Street",\n            "explanation": "The London Underground plays a crucial role in facilitating access to Baker Street, making it easier for visitors to reach this iconic location. The connection between the Underground and Baker Street enhances the street\'s accessibility, contributing to its popularity as a tourist destination. This relationship underscores the importance of public transportation in connecting significant cultural landmarks. [Data: Entities (548), Relationships (862)]"\n        },\n        {\n            "summary": "Baker Street\'s association with detective work",\n            "explanation": "Baker Street is synonymous with detective work, primarily due to its association with Sherlock Holmes. The street is where many of Holmes\'s investigations take place, making it a focal point for fans of detective fiction. This connection to crime-solving and mystery adds to the allure of Baker Street, attracting those interested in the genre and its history. [Data: Entities (4), Relationships (178)]"\n        },\n        {\n            "summary": "Tourism and public interest in Baker Street",\n            "explanation": "The combination of Baker Street\'s historical significance and its association with Sherlock Holmes has made it a popular tourist destination. Visitors often seek to explore the street and its surroundings, contributing to the local economy and cultural heritage. The public interest in this location highlights the impact of literary figures on real-world places and their ability to draw crowds. [Data: Entities (4), Relationships (178)]"\n        }\n    ],\n    "rating": 6.5,\n    "rating_explanation": "The impact severity rating is moderate due to the cultural significance of Baker Street and its association with Sherlock Holmes, which attracts considerable public interest.",\n    "extra_attributes": {}\n}',
        ],
        "period": ["2024-12-16", "2024-12-16"],
        "size": [10, 2],
    }
    return pd.DataFrame(data)

# From graphrag/conftest.py
def entity_embedding_fixture() -> pd.DataFrame:
    data = {
        "id": ["55536111-6a0d-464f-9b72-616ae5d86c2f", "c60946e6-e4ef-499e-b2f2-79aae5471f50"],
        "human_readable_id": [0, 1],
        "title": ["PROJECT GUTENBERG", "ARTHUR CONAN DOYLE"],
        "type": ["ORGANIZATION", "PERSON"],
        "description": [
            "Project Gutenberg is a non-profit digital library that offers free access to a vast collection of eBooks, primarily focusing on works that are in the public domain...",
            "Arthur Conan Doyle is the author of The Adventures of Sherlock Holmes, a famous detective fiction series.",
        ],
        "text_unit_ids": [
            [
                "678a629f6366c004a2f968c2e77c3d05806c71185826352a62f1dfe5a466d4cc8c189dc82b3a43074f9a05ece829f24caf3cbb43c9240ab89936b9d53cc20239",
                "3fcdaf5df6aed13d3916fbfd9c76d9959582122362d62b89079ba1375fea6cc2c4bc7e9acb66820c02e871edbce25acf82169c06599f7643f768f6ec5a79e3fa",
                "98ef7b7dcc2d8472b448144d01d3aae840e1da98dbed56540db3a85f579b04fe15fb9ef441bca80bdd274a369e906359626b32600f56c2697e1bc324367da570",
            ],
            [
                "678a629f6366c004a2f968c2e77c3d05806c71185826352a62f1dfe5a466d4cc8c189dc82b3a43074f9a05ece829f24caf3cbb43c9240ab89936b9d53cc20239"
            ],
        ],
    }
    return pd.DataFrame(data)

# From graphrag/conftest.py
def relationship_df_fixture() -> pd.DataFrame:
    data = {
        "id": ["00fc026b-236a-4428-b836-06f337e6a89f", "8887b459-34c8-45a1-b821-64a73f518fb6"],
        "human_readable_id": [0, 1],
        "source": ["PROJECT GUTENBERG", "ARTHUR CONAN DOYLE"],
        "target": ["ARTHUR CONAN DOYLE", "SHERLOCK HOLMES"],
        "description": [
            "Project Gutenberg offers free access to the works of Arthur Conan Doyle, including The Adventures of Sherlock Holmes.",
            "Arthur Conan Doyle created the character Sherlock Holmes, who is central to his detective stories.",
        ],
        "weight": [7.0, 10.0],
        "combined_degree": [13, 111],
        "text_unit_ids": [
            [
                "678a629f6366c004a2f968c2e77c3d05806c71185826352a62f1dfe5a466d4cc8c189dc82b3a43074f9a05ece829f24caf3cbb43c9240ab89936b9d53cc20239"
            ],
            [
                "678a629f6366c004a2f968c2e77c3d05806c71185826352a62f1dfe5a466d4cc8c189dc82b3a43074f9a05ece829f24caf3cbb43c9240ab89936b9d53cc20239"
            ],
        ],
    }
    return pd.DataFrame(data)

# From graphrag/conftest.py
def text_unit_df_fixture() -> pd.DataFrame:
    data = {
        "id": [
            "678a629f6366c004a2f968c2e77c3d05806c71185826352a62f1dfe5a466d4cc8c189dc82b3a43074f9a05ece829f24caf3cbb43c9240ab89936b9d53cc20239",
            "d4a92a978533a003d4141d5e1f7462af337c1ebc469fc51f1a38961998113dc1d720407d87ae927ab886682859b47a10d485a68ad59fe0895133e8aa1947bf6d",
        ],
        "human_readable_id": [1, 2],
        "text": [
            "The Project Gutenberg eBook of The Adventures of Sherlock Holmes\n    \nThis ebook is for the use of anyone anywhere in the United States and...",
            "Some other text",
        ],
        "n_tokens": [1200, 1200],
        "document_ids": [
            [
                "c91a6627b1ed0d98ab17595f3983d0659ada68f775a9bf2e1da51aa4c8db30702bda39467ad250ba75bdd6c2c323f4bd420dec1dc7907cdc3b4f3ebe77267e08"
            ],
            [
                "c91a6627b1ed0d98ab17595f3983d0659ada68f775a9bf2e1da51aa4c8db30702bda39467ad250ba75bdd6c2c323f4bd420dec1dc7907cdc3b4f3ebe77267e08"
            ],
        ],
        "entity_ids": [
            [
                "55536111-6a0d-464f-9b72-616ae5d86c2f",
                "c60946e6-e4ef-499e-b2f2-79aae5471f50",
                "0724d9bf-5dce-44e0-b093-80d4dd2d10a5",
                "4692443a-158e-4282-a981-c6e631bef664",
                "7fde2ab8-4b80-45ec-9646-cce36134edbe",
                "0519c76d-6e18-4f64-a764-054ef3d433ef",
                "01672ffb-2298-42cd-851e-b2388e317e88",
                "9dc699c9-20bb-4ec6-9288-96882c964576",
                "51574bd9-63d9-4f78-a988-acc3a0719a32",
                "0e78bf0e-4203-4214-9fa8-8e8458442b61",
                "a85078a5-59f7-4a53-a140-e35bd19c82af",
            ],
            [
                "3999222a-aa8a-4910-9cab-596497a7f1fd",
                "86af13ba-3d64-4cef-8511-66b2aaded82e",
                "6fa5215e-5bf8-4023-ba2a-214cfb351eec",
                "937cc7ae-3240-4c4b-ba79-0039205aceb5",
                "f9eac29f-c833-4895-8080-06cf5e714df3",
                "3529ae6b-bb10-4fe5-b241-571ddb1dfa55",
                "c5418966-2204-4278-ace9-8158fff5852a",
                "dde22643-6ac6-4156-ae41-0e841dc688db",
            ],
        ],
        "relationship_ids": [
            [
                "00fc026b-236a-4428-b836-06f337e6a89f",
                "8887b459-34c8-45a1-b821-64a73f518fb6",
                "4f557c1d-dc96-4dbd-9e4c-380955d567c5",
                "2a8843a2-2921-434b-80c2-d5082282e04b",
                "f0242e99-2a49-4813-a363-0c81ae5feaef",
                "7390d425-1908-4a33-8a35-2125e4848896",
                "6111ed8f-7121-49e6-aa9d-05f746bd0b2f",
                "64f93378-0696-4ccc-9877-c1b26482394a",
                "79088678-3cfb-4fd9-8a1e-06d4085da97f",
            ],
            [
                "a91785d7-ae05-4860-b46e-8a565aad7832",
                "ccb8a028-c870-4826-8f93-687aaa5ee23c",
                "4dd46459-f146-48a3-869a-374cfbcc6ec8",
                "489ca036-cc56-4fc5-a239-ece62b98fffb",
                "d0b5c31a-3af2-4ecc-a5b8-e0cba79f94a6",
                "14dfce40-9a4d-4e75-9b7a-eeb7b3c19d78",
                "d048efe6-d7a7-4505-af11-c4b3fc4e25e7",
            ],
        ],
    }
    return pd.DataFrame(data)

from fastapi.responses import FileResponse

import platform
from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost

import chess
from autogen_core.model_context import BufferedChatCompletionContext

# From agentchat_chess_game/main.py
def create_ai_player(model_client: ChatCompletionClient) -> AssistantAgent:
    # Create an agent that can use the model client.
    player = AssistantAgent(
        name="ai_player",
        model_client=model_client,
        system_message=None,
        model_client_stream=True,  # Enable streaming for the model client.
        model_context=BufferedChatCompletionContext(buffer_size=10),  # Model context limited to the last 10 messages.
    )
    return player

# From agentchat_chess_game/main.py
def get_random_move(board: chess.Board) -> str:
    legal_moves = list(board.legal_moves)
    move = random.choice(legal_moves)
    return move.uci()

# From agentchat_chess_game/main.py
def get_ai_prompt(board: chess.Board) -> str:
    try:
        last_move = board.peek().uci()
    except IndexError:
        last_move = None
    # Current player color.
    player_color = "white" if board.turn == chess.WHITE else "black"
    user_color = "black" if player_color == "white" else "white"
    legal_moves = ", ".join([move.uci() for move in board.legal_moves])
    if last_move is None:
        prompt = f"New Game!\nBoard: {board.fen()}\nYou play {player_color}\nYour legal moves: {legal_moves}\n"
    else:
        prompt = f"Board: {board.fen()}\nYou play {player_color}\nUser ({user_color})'s last move: {last_move}\nYour legal moves: {legal_moves}\n"
    example_move = get_random_move(board)
    return (
        prompt
        + "Respond with this format: <move>{your move in UCI format}</move>. "
        + f"For example, <move>{example_move}</move>."
    )

# From agentchat_chess_game/main.py
def get_user_prompt(board: chess.Board) -> str:
    try:
        last_move = board.peek().uci()
    except IndexError:
        last_move = None
    # Current player color.
    player_color = "white" if board.turn == chess.WHITE else "black"
    legal_moves = ", ".join([move.uci() for move in board.legal_moves])
    board_display = board.unicode(borders=True)
    if last_move is None:
        prompt = f"New Game!\nBoard:\n{board_display}\nYou play {player_color}\nYour legal moves: {legal_moves}\n"
    prompt = f"Board:\n{board_display}\nYou play {player_color}\nAI's last move: {last_move}\nYour legal moves: {legal_moves}\n"
    return prompt + "Enter your move in UCI format: "

# From agentchat_chess_game/main.py
def extract_move(response: str) -> str:
    start = response.find("<move>") 
    end = response.find("</move>")
    
    if start == -1 or end == -1:
        raise ValueError("Invalid response format.")
    if end < start:
        raise ValueError("Invalid response format.")
    return response[start+ len("<move>"):end].strip()

from autogen_core import TopicId
from agent_user import UserAgent
from agent_base import AIAgent
from models import UserTask
from topics import triage_agent_topic_type
from topics import user_topic_type
from topics import sales_agent_topic_type
from topics import issues_and_repairs_agent_topic_type
from tools import execute_order_tool
from tools import execute_refund_tool
from tools import look_up_item_tool
from tools_delegate import transfer_to_issues_and_repairs_tool
from tools_delegate import transfer_to_sales_agent_tool
from tools_delegate import transfer_back_to_triage_tool
from fastapi.responses import StreamingResponse



import chainlit
from autogen_core import ClosureAgent
from autogen_core import ClosureContext
from SimpleAssistantAgent import SimpleAssistantAgent
from SimpleAssistantAgent import StreamResult
from SimpleAssistantAgent import GroupChatMessage
from SimpleAssistantAgent import RequestToSpeak

# From core_chainlit/app_team.py
class GroupChatManager(RoutedAgent):
    def __init__(
        self,
        participant_topic_types: List[str],
        model_client: ChatCompletionClient,
    ) -> None:
        super().__init__("Group chat manager")
        self._participant_topic_types = participant_topic_types
        self._model_client = model_client
        self._chat_history: List[UserMessage] = []
        self._previous_participant_idx = -1 

    @message_handler
    async def handle_message(self, message: GroupChatMessage, ctx: MessageContext) -> None:
        assert isinstance(message.body, UserMessage)
        self._chat_history.append(message.body)
        # If the message is an approval message from the user, stop the chat.
        if message.body.source == "User":
            assert isinstance(message.body.content, str)
            if message.body.content.lower().strip(string.punctuation).endswith("approve"): # type: ignore
                await self.runtime.publish_message(StreamResult(content="stop", source=self.id.type), topic_id=task_results_topic_id)
                return
        if message.body.source == "Critic":
            #if ("approve" in message.body.content.lower().strip(string.punctuation)):
            if message.body.content.lower().strip(string.punctuation).endswith("approve"): # type: ignore
                stop_msg = AssistantMessage(content="Task Finished", source=self.id.type)
                await self.runtime.publish_message(StreamResult(content=stop_msg, source=self.id.type), topic_id=task_results_topic_id)
                return

        # Simple round robin algorithm to call next client to speak
        selected_topic_type: str
        idx = self._previous_participant_idx +1
        if (idx == len(self._participant_topic_types)):
             idx = 0
        selected_topic_type = self._participant_topic_types[idx]
        self._previous_participant_idx = idx 

        # Send the RequestToSpeak message to next agent
        await self.publish_message(RequestToSpeak(), DefaultTopicId(type=selected_topic_type))

from autogen_ext.experimental.task_centric_memory.utils import Apprentice
from autogen_ext.experimental.task_centric_memory.utils import Grader
from autogen_ext.experimental.task_centric_memory.utils import PageLogger
from utils import create_oai_client
from utils import load_yaml_file





import streamlit
from agent import Agent



from typing import Iterable
from _types import AppConfig
from autogen_core import MessageSerializer
from autogen_core import try_get_known_serializers_for_type
from autogen_ext.models.openai.config import AzureOpenAIClientConfiguration

# From core_distributed-group-chat/_utils.py
def load_config(file_path: str = os.path.join(os.path.dirname(__file__), "config.yaml")) -> AppConfig:
    model_client = {}
    with open(file_path, "r") as file:
        config_data = yaml.safe_load(file)
        model_client = config_data["client_config"]
        del config_data["client_config"]
        app_config = AppConfig(**config_data)
    # This was required as it couldn't automatically instantiate AzureOpenAIClientConfiguration

    aad_params = {}
    if len(model_client.get("api_key", "")) == 0:
        aad_params["azure_ad_token_provider"] = get_bearer_token_provider(
            DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
        )

    app_config.client_config = AzureOpenAIClientConfiguration(**model_client, **aad_params)  # type: ignore[typeddict-item]
    return app_config

# From core_distributed-group-chat/_utils.py
def get_serializers(types: Iterable[Type[Any]]) -> list[MessageSerializer[Any]]:
    serializers = []
    for type in types:
        serializers.extend(try_get_known_serializers_for_type(type))  # type: ignore
    return serializers

# From core_distributed-group-chat/_utils.py
def set_all_log_levels(log_leve: int):
    # Iterate through all existing loggers and set their levels
    for _, logger in logging.root.manager.loggerDict.items():
        if isinstance(logger, logging.Logger):  # Ensure it's actually a Logger object
            logger.setLevel(log_leve)

from _agents import MessageChunk
from _agents import UIAgent
from _types import GroupChatMessage
from _types import RequestToSpeak
from _utils import get_serializers
from _utils import load_config
from _utils import set_all_log_levels
from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime
from chainlit import Message
from rich.markdown import Markdown

from _types import HostConfig

from _agents import GroupChatManager
from _agents import publish_message_to_ui
from _agents import publish_message_to_ui_and_backend
from _types import MessageChunk
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient

from autogen_ext.tools.graphrag import GlobalSearchTool
from autogen_ext.tools.graphrag import LocalSearchTool

# From agentchat_graphrag/app.py
def download_sample_data(input_dir: str) -> None:

    import requests
    from pathlib import Path
    url = "https://www.gutenberg.org/files/1661/1661-0.txt"
    file_path = Path(input_dir) / "sherlock_book.txt"
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"✅ Successfully downloaded to: {file_path}")
    except requests.exceptions.RequestException as e:
        print(f"❌ Error downloading file: {e}")
    except IOError as e:
        print(f"❌ Error saving file: {e}")


from agents import CascadingMessage
from agents import ObserverAgent

from agents import CascadingAgent
from agents import ReceiveMessageEvent

from rich.theme import Theme

# From gitty/_config.py
def get_repo_root() -> str:
    try:
        result = subprocess.run(["git", "rev-parse", "--show-toplevel"], capture_output=True, text=True, check=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        print("Error: not a git repository.")
        sys.exit(1)

# From gitty/_config.py
def get_gitty_dir() -> str:
    """Get the .gitty directory in the repository root. Create it if it doesn't exist."""
    repo_root = get_repo_root()
    gitty_dir = os.path.join(repo_root, ".gitty")
    if not os.path.exists(gitty_dir):
        os.makedirs(gitty_dir)
    return gitty_dir

from tqdm import tqdm
from chromadb import PersistentClient
from chromadb.utils import embedding_functions
from _config import get_gitty_dir
from _github import get_github_issue_content

# From gitty/_db.py
def init_db(db_path: str) -> None:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS issues (
            number INTEGER PRIMARY KEY,
            title TEXT,
            updatedAt TEXT,
            content TEXT
        )
    """)
    conn.close()

# From gitty/_db.py
def update_issue(db_path: str, number: int, title: str, updatedAt: str, content: str) -> None:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute(
        """
        INSERT OR REPLACE INTO issues (number, title, updatedAt, content)
        VALUES (?, ?, ?, ?)
        """,
        (number, title, updatedAt, content),
    )
    conn.commit()
    conn.close()

# From gitty/_db.py
def update_chroma(gitty_dir: str, db_path: str) -> None:
    persist_directory = os.path.join(gitty_dir, "chroma")
    chroma_client = PersistentClient(path=persist_directory)
    try:
        collection = chroma_client.get_collection("issues")
    except Exception:
        collection = chroma_client.create_collection("issues")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT number, title, content FROM issues")
    rows = cursor.fetchall()
    conn.close()

    sentence_transformer_ef = embedding_functions.DefaultEmbeddingFunction()

    for issue_number, title, content in rows:
        meta = {"title": title}
        embedding = sentence_transformer_ef([content])[0]
        collection.upsert(
            documents=[content],
            embeddings=[embedding],
            metadatas=[meta],
            ids=[str(issue_number)],
        )

# From gitty/_db.py
def fetch_and_update_issues(owner: str, repo: str, db_path: Optional[str] = None) -> None:
    """
    Fetch all GitHub issues for the repo and update the local database.
    Only updates issues that have a more recent updatedAt timestamp.
    The database stores full issue content as produced by get_github_issue_content.
    If db_path is not provided, it is set to "<repo_root>/.gitty.db".
    """
    if db_path is None:
        gitty_dir = get_gitty_dir()
        db_path = os.path.join(gitty_dir, "issues.db")
    print(f"Using database at: {db_path}")

    # Fetch issues using gh CLI (fetch summary without content)
    cmd = ["gh", "issue", "list", "--repo", f"{owner}/{repo}", "-L", "1000", "--json", "number,title,updatedAt"]
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print("Error fetching issues:", result.stderr)
        return
    try:
        issues = json.loads(result.stdout)
    except json.JSONDecodeError as e:
        print("Error decoding issues JSON:", e)
        return

    print(f"Fetched {len(issues)} issues. Beginning update...")

    # Connect to or create the SQLite database
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS issues (
            number INTEGER PRIMARY KEY,
            title TEXT,
            updatedAt TEXT,
            content TEXT
        )
    """)

    for issue in tqdm(issues, desc="Fetching issues"):
        number = issue.get("number")
        title = issue.get("title")
        updatedAt = issue.get("updatedAt")
        # Retrieve full issue content using the async method

        cursor.execute("SELECT updatedAt FROM issues WHERE number = ?", (number,))
        row = cursor.fetchone()
        if row:
            existing_updatedAt = row[0]
            if updatedAt > existing_updatedAt:
                content = asyncio.run(get_github_issue_content(owner, repo, number))
                cursor.execute(
                    """
                    UPDATE issues
                    SET title = ?, updatedAt = ?, content = ?
                    WHERE number = ?
                """,
                    (title, updatedAt, content, number),
                )
        else:
            content = asyncio.run(get_github_issue_content(owner, repo, number))
            cursor.execute(
                """
                INSERT INTO issues (number, title, updatedAt, content)
                VALUES (?, ?, ?, ?)
            """,
                (number, title, updatedAt, content),
            )
    conn.commit()
    conn.close()
    print("Issue database update complete.")

    # Update Chroma DB with latest issues
    gitty_dir = get_gitty_dir()
    persist_directory = os.path.join(gitty_dir, "chroma")
    # Updated Chroma client construction (removed deprecated Settings usage)
    chroma_client = PersistentClient(path=persist_directory)
    try:
        collection = chroma_client.get_collection("issues")
    except Exception:
        collection = chroma_client.create_collection("issues")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT number, title, content FROM issues")
    rows = cursor.fetchall()
    conn.close()

    # New embedding function using sentence_transformers
    sentence_transformer_ef = embedding_functions.DefaultEmbeddingFunction()

    for issue_number, title, content in rows:
        meta = {"title": title}  # metadata for each issue
        embedding = sentence_transformer_ef([content])[0]
        collection.upsert(
            documents=[content],
            embeddings=[embedding],
            metadatas=[meta],
            ids=[str(issue_number)],
        )
    print("Chroma DB update complete.")


# From gitty/_github.py
def get_mentioned_issues(issue_number: int, issue_content: str) -> List[int]:
    matches = re.findall(r"#(\d+)", issue_content)
    matches = [match for match in matches if int(match) != issue_number]
    return list(map(int, matches))

# From gitty/_github.py
def get_related_issues(issue_number: int, issue_content: str, gitty_dir: str, n_results: int = 2) -> List[int]:
    client = PersistentClient(path=os.path.join(gitty_dir, "chroma"))
    try:
        collection = client.get_collection("issues")
    except Exception:
        return []
    results = collection.query(
        query_texts=[issue_content],
        n_results=n_results,
    )
    ids = results.get("ids", [[]])[0]

    if str(issue_number) in ids:
        ids.remove(str(issue_number))

    return [int(_id) for _id in ids if _id.isdigit()]

# From gitty/_github.py
def fetch_issue_summaries(owner: str, repo: str) -> List[Dict[Any, Any]]:
    cmd = ["gh", "issue", "list", "--repo", f"{owner}/{repo}", "-L", "1000", "--json", "number,title,updatedAt"]
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print("Error fetching issues:", result.stderr)
        return []
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError as e:
        print("Error decoding issues JSON:", e)
        return []

from autogen_agentchat.messages import ToolCallSummaryMessage
from rich.prompt import Prompt
from _github import get_mentioned_issues
from _github import get_related_issues
from _github import generate_issue_tdlr
from _config import custom_theme

import tomli_w
import tomllib

from packaging import version

import agent

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from langchain_core.tools import tool
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

# From tools/fetch_web_page_raw_html.py
def fetch_web_page_raw_html(url: str) -> str:
    """Fetches the raw HTML of a web page. If a CSS selector is provided, returns only the matching elements."""
    options = Options()
    options.add_argument('--headless')
    options.add_argument("--disable-gpu")
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
        
    service = Service('/usr/bin/chromedriver')
    
    driver = webdriver.Chrome(options=options, service=service)

    driver.get(url)

    return driver.execute_script("return document.body.outerHTML;")

from langchain_community.tools import DuckDuckGoSearchResults

# From tools/duck_duck_go_news_search.py
def duck_duck_go_news_search(query: str):
    """Search for news using DuckDuckGo."""
    return DuckDuckGoSearchResults(backend="news").invoke(query)


# From tools/overwrite_file.py
def overwrite_file(file_path: str, content: str) -> str:
    """Replaces the file at the given path with the given content, returning a string message confirming success."""
    with open(file_path, 'w') as file:
        file.write(content)
    return f"File at {file_path} has been successfully overwritten."


# From tools/duck_duck_go_web_search.py
def duck_duck_go_web_search(query: str):
    """Search the web using DuckDuckGo."""
    return DuckDuckGoSearchResults().invoke(query)

from langchain_community.document_loaders.url_selenium import SeleniumURLLoader

# From tools/fetch_web_page_content.py
def fetch_web_page_content(url: str):
    """Fetch content from a web page."""
    loader = SeleniumURLLoader(
        urls=[url],
        executable_path="/usr/bin/chromedriver",
        arguments=['--headless', '--disable-gpu', '--no-sandbox', '--disable-dev-shm-usage']
    )
    pages = loader.load()
    
    return pages[0]


# From tools/request_human_input.py
def request_human_input(prompt: str) -> str:
    """Request human input via Python's input method."""
    print(prompt)
    return input("> ")


# From tools/run_shell_command.py
def run_shell_command(command: str):
    """Run a shell command and return the output."""
    print(f"Running shell command: {command}")
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    return { "stdout": result.stdout, "stderr": result.stderr, "returncode": result.returncode }


# From tools/read_file.py
def read_file(file_path: str) -> str:
    """Returns the content of the file at the given file path."""
    with open(file_path, 'r') as file:
        return file.read()


# From tools/write_to_file.py
def write_to_file(file: str, file_contents: str) -> str:
    """Write the contents to a new file, will not overwrite an existing file."""
    if os.path.exists(file):
        raise FileExistsError(f"File {file} already exists and will not be overwritten.")

    print(f"Writing to file: {file}")
    with open(file, 'w') as f:
        f.write(file_contents)

    return f"File {file} written successfully."


# From tools/delete_file.py
def delete_file(file_path: str) -> str:
    """Deletes the file at the given path and returns a string confirming success."""
    try:
        os.remove(file_path)
        return f"File at {file_path} has been deleted successfully."
    except Exception as e:
        return str(e)

from langchain_core.messages import HumanMessage
from langchain_core.messages import SystemMessage
from langgraph.graph import END
from langgraph.graph import StateGraph
from langgraph.graph import MessagesState
from langgraph.prebuilt import ToolNode
import utils
import config

# From agents/tool_maker.py
def reasoning(state: MessagesState):
    print()
    print("tool_maker is thinking...")
    messages = state['messages']
    tooled_up_model = config.default_langchain_model.bind_tools(tools)
    response = tooled_up_model.invoke(messages)
    return {"messages": [response]}

# From agents/tool_maker.py
def check_for_tool_calls(state: MessagesState) -> Literal["tools", END]:
    messages = state['messages']
    last_message = messages[-1]
    
    if last_message.tool_calls:
        if not last_message.content.strip() == "":
            print("tool_maker thought this:")
            print(last_message.content)
        print()
        print("tool_maker is acting by invoking these tools:")
        print([tool_call["name"] for tool_call in last_message.tool_calls])
        return "tools"
    
    return END

# From agents/tool_maker.py
def tool_maker(task: str) -> str:
    """Creates new tools for agents to use."""
    return graph.invoke(
        {"messages": [SystemMessage(system_prompt), HumanMessage(task)]}
    )

from tools.duck_duck_go_web_search import duck_duck_go_web_search
from tools.fetch_web_page_content import fetch_web_page_content

# From agents/web_researcher.py
def web_researcher(task: str) -> str:
    """Researches the web."""
    return graph.invoke(
        {"messages": [SystemMessage(system_prompt), HumanMessage(task)]}
    )

from tools.list_available_agents import list_available_agents
from tools.assign_agent_to_task import assign_agent_to_task

# From agents/hermes.py
def feedback_and_wait_on_human_input(state: MessagesState):
    # if messages only has one element we need to start the conversation
    if len(state['messages']) == 1:
        message_to_human = "What can I help you with?"
    else:
        message_to_human = state["messages"][-1].content
    
    print(message_to_human)

    human_input = ""
    while not human_input.strip():
        human_input = input("> ")
    
    return {"messages": [HumanMessage(human_input)]}

# From agents/hermes.py
def check_for_exit(state: MessagesState) -> Literal["reasoning", END]:
    last_message = state['messages'][-1]
    if last_message.content.lower() == "exit":
        return END
    else:
        return "reasoning"

# From agents/hermes.py
def hermes(uuid: str):
    """The orchestrator that interacts with the user to understand goals, plan out how agents can meet the goal, assign tasks, and coordinate the activities agents."""
    print(f"Starting session with AgentK (id:{uuid})")
    print("Type 'exit' to end the session.")

    return graph.invoke(
        {"messages": [SystemMessage(system_prompt)]},
        config={"configurable": {"thread_id": uuid}}
    )

from tools.write_to_file import write_to_file
from tools.overwrite_file import overwrite_file
from tools.delete_file import delete_file
from tools.read_file import read_file
from tools.run_shell_command import run_shell_command

# From agents/software_engineer.py
def software_engineer(task: str) -> str:
    """Creates, modifies, and deletes code, manages files, runs shell commands, and collaborates with other agents."""
    return graph.invoke(
        {"messages": [SystemMessage(system_prompt), HumanMessage(task)]}
    )

from json import JSONDecodeError
from langchain.experimental.autonomous_agents.autogpt.agent import AutoGPT
from FreeLLM import ChatGPTAPI
from FreeLLM import HuggingChatAPI
from FreeLLM import BingChatAPI
from FreeLLM import BardChatAPI
from langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent
from langchain.docstore.document import Document
import nest_asyncio
from langchain.agents import tool
from langchain.tools.file_management.read import ReadFileTool
from langchain.tools.file_management.write import WriteFileTool
from tempfile import TemporaryDirectory
from langchain.tools import BaseTool
from langchain.tools import DuckDuckGoSearchRun
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain
from langchain.chains.qa_with_sources.loading import BaseCombineDocumentsChain
import faiss
from langchain.vectorstores import FAISS
from langchain.docstore import InMemoryDocstore
from Embedding import HuggingFaceEmbedding
from langchain.tools.human.tool import HumanInputRun

# From Free-Auto-GPT/AUTOGPT.py
class WebpageQATool(BaseTool):
    name = "query_webpage"
    description = (
        "Browse a webpage and retrieve the information relevant to the question."
    )
    text_splitter: RecursiveCharacterTextSplitter = Field(
        default_factory=_get_text_splitter
    )
    qa_chain: BaseCombineDocumentsChain

    def _run(self, url: str, question: str) -> str:
        """Useful for browsing websites and scraping the text information."""
        result = browse_web_page.run(url)
        docs = [Document(page_content=result, metadata={"source": url})]
        web_docs = self.text_splitter.split_documents(docs)
        results = []
        # TODO: Handle this with a MapReduceChain
        for i in range(0, len(web_docs), 4):
            input_docs = web_docs[i : i + 4]
            window_result = self.qa_chain(
                {"input_documents": input_docs, "question": question},
                return_only_outputs=True,
            )
            results.append(f"Response from window {i} - {window_result}")
        results_docs = [
            Document(page_content="\n".join(results), metadata={"source": url})
        ]
        return self.qa_chain(
            {"input_documents": results_docs, "question": question},
            return_only_outputs=True,
        )

    async def _arun(self, url: str, question: str) -> str:
        raise NotImplementedError

# From Free-Auto-GPT/AUTOGPT.py
def pushd(new_dir):
    """Context manager for changing the current working directory."""
    prev_dir = os.getcwd()
    os.chdir(new_dir)
    try:
        yield
    finally:
        os.chdir(prev_dir)

# From Free-Auto-GPT/AUTOGPT.py
def process_csv(
    csv_file_path: str, instructions: str, output_path: Optional[str] = None
) -> str:
    """Process a CSV by with pandas in a limited REPL.\
 Only use this after writing data to disk as a csv file.\
 Any figures must be saved to disk to be viewed by the human.\
 Instructions should be written in natural language, not code. Assume the dataframe is already loaded."""
    with pushd(ROOT_DIR):
        try:
            df = pd.read_csv(csv_file_path)
        except Exception as e:
            return f"Error: {e}"
        agent = create_pandas_dataframe_agent(llm, df, max_iterations=30, verbose=True)
        if output_path is not None:
            instructions += f" Save output to disk at {output_path}"
        try:
            result = agent.run(instructions)
            return result
        except Exception as e:
            return f"Error: {e}"

# From Free-Auto-GPT/AUTOGPT.py
def run_async(coro):
    event_loop = asyncio.get_event_loop()
    return event_loop.run_until_complete(coro)

# From Free-Auto-GPT/AUTOGPT.py
def browse_web_page(url: str) -> str:
    """Verbose way to scrape a whole webpage. Likely to cause issues parsing."""
    return run_async(async_load_playwright(url))

from collections import deque
from langchain import HuggingFaceHub
from langchain import LLMChain
from langchain import PromptTemplate
from langchain.llms import BaseLLM
from langchain.vectorstores.base import VectorStore
from langchain.chains.base import Chain
from langchain.experimental import BabyAGI
from BabyAgi import BabyAGIMod
from langchain.agents import ZeroShotAgent
from langchain.agents import Tool
from langchain.agents import AgentExecutor
from langchain import OpenAI

from langchain.memory import ConversationBufferWindowMemory

# From Free-Auto-GPT/MetaPrompt.py
def initialize_chain(instructions, memory=None):
    if memory is None:
        memory = ConversationBufferWindowMemory()
        memory.ai_prefix = "Assistant"

    template = f"""
    Instructions: {instructions}
    {{{memory.memory_key}}}
    Human: {{human_input}}
    Assistant:"""

    prompt = PromptTemplate(
        input_variables=["history", "human_input"], template=template
    )

    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        verbose=True,
        memory=ConversationBufferWindowMemory(),
    )
    return chain

# From Free-Auto-GPT/MetaPrompt.py
def initialize_meta_chain():
    meta_template = """
    Assistant has just had the below interactions with a User. Assistant followed their "Instructions" closely. Your job is to critique the Assistant's performance and then revise the Instructions so that Assistant would quickly and correctly respond in the future.

    ####

    {chat_history}

    ####

    Please reflect on these interactions.

    You should first critique Assistant's performance. What could Assistant have done better? What should the Assistant remember about this user? Are there things this user always wants? Indicate this with "Critique: ...".

    You should next revise the Instructions so that Assistant would quickly and correctly respond in the future. Assistant's goal is to satisfy the user in as few interactions as possible. Assistant will only see the new Instructions, not the interaction history, so anything important must be summarized in the Instructions. Don't forget any important details in the current Instructions! Indicate the new Instructions by "Instructions: ...".
    """

    meta_prompt = PromptTemplate(
        input_variables=["chat_history"], template=meta_template
    )

    meta_chain = LLMChain(
        llm=llm,
        prompt=meta_prompt,
        verbose=True,
    )
    return meta_chain

# From Free-Auto-GPT/MetaPrompt.py
def get_chat_history(chain_memory):
    memory_key = chain_memory.memory_key
    chat_history = chain_memory.load_memory_variables(memory_key)[memory_key]
    return chat_history

# From Free-Auto-GPT/MetaPrompt.py
def get_new_instructions(meta_output):
    delimiter = "Instructions: "
    new_instructions = meta_output[meta_output.find(delimiter) + len(delimiter) :]
    return new_instructions

from retry import retry

# From Embedding/HuggingFaceEmbedding.py
def reshape_array(arr):
    # create an array of zeros with shape (1536)
    new_arr = np.zeros((1536,))
    # copy the original array into the new array
    new_arr[:arr.shape[0]] = arr
    # return the new array
    return new_arr

# From Embedding/HuggingFaceEmbedding.py
def newEmbeddings(texts):
    response = requests.post(api_url, headers=headers, json={"inputs": texts, "options":{"wait_for_model":True}})
    result = response.json()
    if isinstance(result, list):
      return result
    elif list(result.keys())[0] == "error":
      raise RuntimeError(
          "The model is currently loading, please re-run the query."
          )

# From Embedding/HuggingFaceEmbedding.py
def newEmbeddingFunction(texts):
    embeddings = newEmbeddings(texts)
    embeddings = np.array(embeddings, dtype=np.float32)
    shaped_embeddings = reshape_array(embeddings)
    return shaped_embeddings

from langchain.base_language import BaseLanguageModel

# From BabyAgi/task_prioritization.py
class TaskPrioritizationChain(LLMChain):
    """Chain to prioritize tasks."""

    @classmethod
    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:
        """Get the response parser."""
        task_prioritization_template = (
            "Please help me to cleaning the formatting of "
            "and reprioritizing the following tasks: {task_names}."
            "Consider the ultimate objective of your team: {objective}."
            "Do not remove any tasks. Return ONLY the result as a numbered list without anything else, like:\n"
            "1. First task\n"
            "2. Second task\n"
            "Start the task list with number {next_task_id}."
        )
        prompt = PromptTemplate(
            template=task_prioritization_template,
            input_variables=["task_names", "next_task_id", "objective"],
        )
        return cls(prompt=prompt, llm=llm, verbose=verbose)

# From BabyAgi/task_prioritization.py
def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:
        """Get the response parser."""
        task_prioritization_template = (
            "Please help me to cleaning the formatting of "
            "and reprioritizing the following tasks: {task_names}."
            "Consider the ultimate objective of your team: {objective}."
            "Do not remove any tasks. Return ONLY the result as a numbered list without anything else, like:\n"
            "1. First task\n"
            "2. Second task\n"
            "Start the task list with number {next_task_id}."
        )
        prompt = PromptTemplate(
            template=task_prioritization_template,
            input_variables=["task_names", "next_task_id", "objective"],
        )
        return cls(prompt=prompt, llm=llm, verbose=verbose)


# From BabyAgi/task_creation.py
class TaskCreationChain(LLMChain):
    """Chain to generates tasks."""

    @classmethod
    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:
        """Get the response parser."""
        task_creation_template = (
            "Can you hel me to"
            " to create new tasks with the following objective: {objective},"
            " The last completed task has the result: {result}."
            " This result was based on this task description: {task_description}."
            " These are incomplete tasks: {incomplete_tasks}."
            " Based on the result, create new tasks to be completed"
            " Return the task as an List without anything else."
        )
        prompt = PromptTemplate(
            template=task_creation_template,
            input_variables=[
                "result",
                "task_description",
                "incomplete_tasks",
                "objective",
            ],
        )
        return cls(prompt=prompt, llm=llm, verbose=verbose)

from langchain.callbacks.manager import CallbackManagerForChainRun
from task_creation import TaskCreationChain
from task_execution import TaskExecutionChain
from task_prioritization import TaskPrioritizationChain

# From BabyAgi/BabyAGIMod.py
class BabyAGI(Chain, BaseModel):
    """Controller model for the BabyAGI agent."""

    task_list: deque = Field(default_factory=deque)
    task_creation_chain: Chain = Field(...)
    task_prioritization_chain: Chain = Field(...)
    execution_chain: Chain = Field(...)
    task_id_counter: int = Field(1)
    vectorstore: VectorStore = Field(init=False)
    max_iterations: Optional[int] = None
    store: Optional[bool] = False
    write_step: Optional[int] = 0

    class Config:
        """Configuration for this pydantic object."""

        arbitrary_types_allowed = True

    def add_task(self, task: Dict) -> None:
        self.task_list.append(task)

    def print_task_list(self) -> None:
        print("\033[95m\033[1m" + "\n*****TASK LIST*****\n" + "\033[0m\033[0m")
        for t in self.task_list:
            print(str(t["task_id"]) + ": " + t["task_name"])

    def print_next_task(self, task: Dict) -> None:
        print("\033[92m\033[1m" + "\n*****NEXT TASK*****\n" + "\033[0m\033[0m")
        print(str(task["task_id"]) + ": " + task["task_name"])

    def print_task_result(self, result: str) -> None:
        print("\033[93m\033[1m" + "\n*****TASK RESULT*****\n" + "\033[0m\033[0m")
        print(result)

    @property
    def input_keys(self) -> List[str]:
        return ["objective"]

    @property
    def output_keys(self) -> List[str]:
        return []

    def get_next_task(
        self, result: str, task_description: str, objective: str
    ) -> List[Dict]:
        """Get the next task."""
        task_names = [t["task_name"] for t in self.task_list]

        incomplete_tasks = ", ".join(task_names)
        response = self.task_creation_chain.run(
            result=result,
            task_description=task_description,
            incomplete_tasks=incomplete_tasks,
            objective=objective,
        )
        new_tasks = response.split("\n")
        return [
            {"task_name": task_name} for task_name in new_tasks if task_name.strip()
        ]

    def prioritize_tasks(self, this_task_id: int, objective: str) -> List[Dict]:
        """Prioritize tasks."""
        task_names = [t["task_name"] for t in list(self.task_list)]
        next_task_id = int(this_task_id) + 1
        response = self.task_prioritization_chain.run(
            task_names=", ".join(task_names),
            next_task_id=str(next_task_id),
            objective=objective,
        )
        new_tasks = response.split("\n")
        prioritized_task_list = []
        for task_string in new_tasks:
            if not task_string.strip():
                continue
            task_parts = task_string.strip().split(".", 1)
            if len(task_parts) == 2:
                task_id = task_parts[0].strip()
                task_name = task_parts[1].strip()
                prioritized_task_list.append(
                    {"task_id": task_id, "task_name": task_name}
                )
        return prioritized_task_list

    def _get_top_tasks(self, query: str, k: int) -> List[str]:
        """Get the top k tasks based on the query."""
        results = self.vectorstore.similarity_search(query, k=k)
        if not results:
            return []
        return [str(item.metadata["task"]) for item in results]

    def execute_task(self, objective: str, task: str, k: int = 5) -> str:
        """Execute a task."""
        context = self._get_top_tasks(query=objective, k=k)
        return self.execution_chain.run(
            objective=objective, context="\n".join(context), task=task
        )

    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """Run the agent."""
        objective = inputs["objective"]
        first_task = inputs.get("first_task", "Make a todo list")
        self.add_task({"task_id": 1, "task_name": first_task})
        num_iters = 0
        
        
        dir_name=""
        if self.store:
            try:    
                # create a directory to store the results of evry task
                os.mkdir("BABYAGI_RESULTS_FOR_" + objective.replace(" ", "_"))
                dir_name = "BABYAGI_RESULTS_FOR_" + objective.replace(" ", "_")
                self.write_step = 0
            except:
                print("ATTENTION: directory already exists, Delete the directory to store the results of evry task")
                self.store = False
                
        while True:
            if self.task_list:
                self.print_task_list()

                # Step 1: Pull the first task
                task = self.task_list.popleft()
                self.print_next_task(task)

                # Step 2: Execute the task
                result = self.execute_task(objective, task["task_name"])
                this_task_id = int(task["task_id"])  # THIS LINE GIVE ERROR  WOLUD BE FIXED
                self.print_task_result(result)
                
                if self.store:
                    # save the result in a file
                    self.write_step += 1
                    with open(dir_name + "/" + str(self.write_step) + ".txt", "w") as f:
                        f.write(result)
                    print("<<BABY AGI>> : result saved in " + dir_name + "/" + str(self.write_step) +  ".txt")

                # Step 3: Store the result in Pinecone
                result_id = f"result_{task['task_id']}"
                self.vectorstore.add_texts(
                    texts=[result],
                    metadatas=[{"task": task["task_name"]}],
                    ids=[result_id],
                )

                # Step 4: Create new tasks and reprioritize task list
                new_tasks = self.get_next_task(result, task["task_name"], objective)
                for new_task in new_tasks:
                    self.task_id_counter += 1
                    new_task.update({"task_id": self.task_id_counter})
                    self.add_task(new_task)
                self.task_list = deque(self.prioritize_tasks(this_task_id, objective))
            num_iters += 1
            if self.max_iterations is not None and num_iters == self.max_iterations:
                print(
                    "\033[91m\033[1m" + "\n*****TASK ENDING*****\n" + "\033[0m\033[0m"
                )
                
                if self.store:
                    #create final file to append in order by write_step 
                    final_file = open(dir_name + "/" + "final.txt", "w")    
                    all_step = os.listdir(dir_name)
                    all_step.sort()
                    for step in all_step:
                        #append the result of each step in the final file
                        with open(dir_name + "/" + step, "r") as f:
                            final_file.write(f.read())
                    final_file.close()
                    
                    print(
                        "\033[91m\033[1m" + "\n*****RESULT STORED*****\n" + "\033[0m\033[0m"
                    )
                    
                break
        return {}

    @classmethod
    def from_llm(
        cls,
        llm: BaseLanguageModel,
        vectorstore: VectorStore,
        verbose: bool = False,
        task_execution_chain: Optional[Chain] = None,
        **kwargs: Dict[str, Any],
    ) -> "BabyAGI":
        """Initialize the BabyAGI Controller."""
        task_creation_chain = TaskCreationChain.from_llm(llm, verbose=verbose)
        task_prioritization_chain = TaskPrioritizationChain.from_llm(
            llm, verbose=verbose
        )
        if task_execution_chain is None:
            execution_chain: Chain = TaskExecutionChain.from_llm(llm, verbose=verbose)
        else:
            execution_chain = task_execution_chain
        return cls(
            task_creation_chain=task_creation_chain,
            task_prioritization_chain=task_prioritization_chain,
            execution_chain=execution_chain,
            vectorstore=vectorstore,
            **kwargs,
        )

# From BabyAgi/BabyAGIMod.py
def add_task(self, task: Dict) -> None:
        self.task_list.append(task)

# From BabyAgi/BabyAGIMod.py
def print_task_list(self) -> None:
        print("\033[95m\033[1m" + "\n*****TASK LIST*****\n" + "\033[0m\033[0m")
        for t in self.task_list:
            print(str(t["task_id"]) + ": " + t["task_name"])

# From BabyAgi/BabyAGIMod.py
def print_next_task(self, task: Dict) -> None:
        print("\033[92m\033[1m" + "\n*****NEXT TASK*****\n" + "\033[0m\033[0m")
        print(str(task["task_id"]) + ": " + task["task_name"])

# From BabyAgi/BabyAGIMod.py
def print_task_result(self, result: str) -> None:
        print("\033[93m\033[1m" + "\n*****TASK RESULT*****\n" + "\033[0m\033[0m")
        print(result)

# From BabyAgi/BabyAGIMod.py
def input_keys(self) -> List[str]:
        return ["objective"]

# From BabyAgi/BabyAGIMod.py
def output_keys(self) -> List[str]:
        return []

# From BabyAgi/BabyAGIMod.py
def get_next_task(
        self, result: str, task_description: str, objective: str
    ) -> List[Dict]:
        """Get the next task."""
        task_names = [t["task_name"] for t in self.task_list]

        incomplete_tasks = ", ".join(task_names)
        response = self.task_creation_chain.run(
            result=result,
            task_description=task_description,
            incomplete_tasks=incomplete_tasks,
            objective=objective,
        )
        new_tasks = response.split("\n")
        return [
            {"task_name": task_name} for task_name in new_tasks if task_name.strip()
        ]

# From BabyAgi/BabyAGIMod.py
def prioritize_tasks(self, this_task_id: int, objective: str) -> List[Dict]:
        """Prioritize tasks."""
        task_names = [t["task_name"] for t in list(self.task_list)]
        next_task_id = int(this_task_id) + 1
        response = self.task_prioritization_chain.run(
            task_names=", ".join(task_names),
            next_task_id=str(next_task_id),
            objective=objective,
        )
        new_tasks = response.split("\n")
        prioritized_task_list = []
        for task_string in new_tasks:
            if not task_string.strip():
                continue
            task_parts = task_string.strip().split(".", 1)
            if len(task_parts) == 2:
                task_id = task_parts[0].strip()
                task_name = task_parts[1].strip()
                prioritized_task_list.append(
                    {"task_id": task_id, "task_name": task_name}
                )
        return prioritized_task_list

# From BabyAgi/BabyAGIMod.py
def execute_task(self, objective: str, task: str, k: int = 5) -> str:
        """Execute a task."""
        context = self._get_top_tasks(query=objective, k=k)
        return self.execution_chain.run(
            objective=objective, context="\n".join(context), task=task
        )

from hugchat import hugchat
from hugchat.login import Login
from langchain.llms.base import LLM
from time import sleep

# From FreeLLM/HuggingChatAPI.py
class HuggingChat(LLM):
    
    history_data: Optional[List] = []
    chatbot : Optional[hugchat.ChatBot] = None
    conversation : Optional[str] = ""
    email : Optional[str]
    psw : Optional[str]
    #### WARNING : for each api call this library will create a new chat on chat.openai.com
    
    
    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        if stop is not None:
            pass
            #raise ValueError("stop kwargs are not permitted.")
        #token is a must check
        if self.chatbot is None:
            if self.email is None and self.psw is None:
                ValueError("Email and Password is required, pls check the documentation on github")
            else: 
                if self.conversation == "":
                    sign = Login(self.email, self.psw)
                    cookies = sign.login()

                    # Save cookies to usercookies/<email>.json
                    sign.saveCookies()

                    # Create a ChatBot
                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) 
                else:
                    raise ValueError("Something went wrong")
            
        
        sleep(2)
        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)
    
        
        #add to history
        self.history_data.append({"prompt":prompt,"response":data})    
        
        return data

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {"model": "HuggingCHAT"}

from EdgeGPT import Chatbot
from EdgeGPT import ConversationStyle
import pydantic

# From FreeLLM/BingChatAPI.py
class BingChat(LLM):
    
    history_data: Optional[List] = []
    cookiepath : Optional[str]
    chatbot : Optional[Chatbot] = None
    conversation_style : Optional[str] 
    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise
    search_result : Optional[bool] = False
    
    @property
    def _llm_type(self) -> str:
        return "custom"
    
    def select_conversation(self, conversation_style: str):
        if conversation_style == "precise":
            self.conversation_style_on = ConversationStyle.precise
        elif conversation_style == "creative":
            self.conversation_style_on = ConversationStyle.creative
        elif conversation_style == "balanced":
            self.conversation_style_on = ConversationStyle.balanced
        else:
            raise ValueError("conversation_style must be precise, creative or balaced")
        self.conversation_style = conversation_style

    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")
        #cookiepath is a must check
        if self.chatbot is None:
            if self.cookiepath is None:
                raise ValueError("Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE")
            else:
                #if self.chatbot == None:
                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)
               
        if self.conversation_style is not None:
            self.conversation_style_on = self.select_conversation(self.conversation_style)
            
        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)
        """
        this is a sample response. 
        {'type': 2, 'invocationId': '0', 
        'item': {'messages': [{'text': 'Hello, how are you?', 'author': 'user', 'from': {'id': '985157152860707', 'name': None}, 'createdAt': '2023-05-03T19:51:39.5491558+00:00', 'timestamp': '2023-05-03T19:51:39.5455787+00:00', 'locale': 'en-us', 'market': 'en-us', 'region': 'us', 'messageId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'nlu': {'scoredClassification': {'classification': 'CHAT_GPT', 'score': None}, 'classificationRanking': [{'classification': 'CHAT_GPT', 'score': None}], 'qualifyingClassifications': None, 'ood': None, 'metaData': None, 'entities': None}, 'offense': 'None', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'cib', 'privacy': None, 'inputMethod': 'Keyboard'}, {'text': "Hello! I'm doing well, thank you. How can I assist you today?", 'author': 'bot', 'createdAt': '2023-05-03T19:51:41.5176164+00:00', 'timestamp': '2023-05-03T19:51:41.5176164+00:00', 'messageId': '1d013e71-408b-4031-a131-2f5c009fe938', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'offense': 'None', 'adaptiveCards': [{'type': 'AdaptiveCard', 'version': '1.0', 'body': [{'type': 'TextBlock', 'text': "Hello! I'm doing well, thank you. How can I assist you today?\n", 'wrap': True}]}], 
        'sourceAttributions': [], 
        'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 
        'contentOrigin': 'DeepLeo', 
        'privacy': None, 
        'suggestedResponses': [{'text': 'What is the weather like today?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502696+00:00', 'timestamp': '2023-05-03T19:51:42.7502696+00:00', 'messageId': 'cd7a84d3-f9bc-47ff-9897-077b2de12e21', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'What is the latest news?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502739+00:00', 'timestamp': '2023-05-03T19:51:42.7502739+00:00', 'messageId': 'b611632a-9a8e-42de-86eb-8eb3b7b8ddbb', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'Tell me a joke.', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502743+00:00', 'timestamp': '2023-05-03T19:51:42.7502743+00:00', 'messageId': '70232e45-d7e8-4d77-83fc-752b3cd3355c', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}], 'spokenText': 'How can I assist you today?'}], 'firstNewMessageIndex': 1, 'defaultChatName': None, 'conversationId': '51D|BingProd|3E1274E188350D7BE273FFE95E02DD2984DAB52F95260300D0A2937162F98FDA', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'conversationExpiryTime': '2023-05-04T01:51:42.8260286Z', 'shouldInitiateConversation': True, 'telemetry': {'metrics': None, 'startTime': '2023-05-03T19:51:39.5456555Z'}, 'throttling': {'maxNumUserMessagesInConversation': 20, 'numUserMessagesInConversation': 1}, 'result': {'value': 'Success', 'serviceVersion': '20230501.30'}}}
        """
        response_messages = response.get("item", {}).get("messages", [])
        response_text = response_messages[1].get("text", "")
        
        if response_text == "":
            hidden_text = response_messages[1].get("hiddenText", "")
            print(">>>> [DEBBUGGER] hidden_text = " + str(hidden_text) + " [DEBBUGGER] <<<<")
            print(">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<")
            response_text = hidden_text
            """
            # reset the chatbot and remake the call
            print("[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]")
            print("[ INFO DEBUGGER ] \n<Response>\n" + str(response) + "\n</Response>\n\n")
            sleep(10)
            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)
            sleep(2)
            response = await self.chatbot.ask(prompt=prompt)
            response_messages = response.get("item", {}).get("messages", [])
            response_text = response_messages[1].get("text", "")
            """
        
        #add to history
        self.history_data.append({"prompt":prompt,"response":response_text})    
        
        return response_text
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        return asyncio.run(self.call(prompt=prompt))

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {"model": "BingCHAT", "cookiepath": self.cookiepath}

# From FreeLLM/BingChatAPI.py
def select_conversation(self, conversation_style: str):
        if conversation_style == "precise":
            self.conversation_style_on = ConversationStyle.precise
        elif conversation_style == "creative":
            self.conversation_style_on = ConversationStyle.creative
        elif conversation_style == "balanced":
            self.conversation_style_on = ConversationStyle.balanced
        else:
            raise ValueError("conversation_style must be precise, creative or balaced")
        self.conversation_style = conversation_style

from Bard import Chatbot

# From FreeLLM/BardChatAPI.py
class BardChat(LLM):
    
    history_data: Optional[List] = []
    cookie : Optional[str]
    chatbot : Optional[Chatbot] = None

    
    @property
    def _llm_type(self) -> str:
        return "custom"

    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        if stop is not None:
            pass
            #raise ValueError("stop kwargs are not permitted.")
        #cookie is a must check
        if self.chatbot is None:
            if self.cookie is None:
                raise ValueError("Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE")
            else:
                #if self.chatbot == None:
                self.chatbot = Chatbot(self.cookie)
               
        response = self.chatbot.ask(prompt)
        #print(response)
        response_text = response['content']
        #add to history
        self.history_data.append({"prompt":prompt,"response":response_text})    
        
        return response_text
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        return asyncio.run(self.call(prompt=prompt))

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {"model": "BardCHAT", "cookie": self.cookie}

from gpt4_openai import GPT4OpenAI

# From FreeLLM/ChatGPTAPI.py
class ChatGPT(LLM):
    
    history_data: Optional[List] = []
    token : Optional[str]
    chatbot : Optional[GPT4OpenAI] = None
    call : int = 0
    model : str = "gpt-3" # or gpt-4
    plugin_id : Optional[List] = []
    
    #### WARNING : for each api call this library will create a new chat on chat.openai.com
    
    
    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        if stop is not None:
            pass
            #raise ValueError("stop kwargs are not permitted.")
        #token is a must check
        if self.chatbot is None:
            if self.token is None:
                raise ValueError("Need a token , check https://chat.openai.com/api/auth/session for get your token")
            else:
                try:
                    if self.plugin_id == []:
                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)
                    else:
                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)
                except:
                    raise ValueError("Error on create chatbot, check your token, or your model")
                
        response = ""
        # OpenAI: 50 requests / hour for each account
        if (self.call >= 45 and self.model == "default") or (self.call >= 23 and self.model == "gpt4"):
            raise ValueError("You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk")
        else:
            sleep(2)
            response = self.chatbot(prompt)
            
            self.call += 1
        
        #add to history
        self.history_data.append({"prompt":prompt,"response":response})    
        
        return response

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {"model": "ChatGPT", "token": self.token, "model": self.model}










from setuptools import find_packages

# From AgentForge/setup.py
def get_long_description():
    with open("README.md") as file:
        return file.read()

import agentforge.config
from tests.utils.fakes import FakeChromaStorage
import agentforge.storage.chroma_storage
import agentforge.storage.memory
from agentforge.agent import Agent

# From testing/bootstrap.py
def bootstrap_test_env(
    *,
    use_fakes: bool = True,
    silence_output: bool = True,
    cleanup_on_exit: bool = True,
) -> None:
    """Replicate the critical test harness tweaks on demand.

    Parameters
    ----------
    use_fakes
        When *True*, replace ChromaStorage back-ends with the in-memory
        ``FakeChromaStorage`` implementation and stub out ``Agent.run`` so no
        network / LLM calls occur.  When *False*, leave production behaviour
        untouched.

    silence_output
        When *True*, disable *logging* below *CRITICAL* and monkey-patch the
        built-in ``print`` to a no-op.  Restored automatically at process
        exit.

    cleanup_on_exit
        Delete the *repo-root/.agentforge* directory that this helper creates
        (only if we created it) when the interpreter shuts down.
    """

    # Early exit if we have already bootstrapped in the desired mode ------------
    if getattr(bootstrap_test_env, "_has_run", False):  # type: ignore[attr-defined]
        # Second call may request stricter settings (e.g. silence_output=True)
        # – honour those upgrades, but never downgrade (once silenced, stay
        # silenced; once fakes are active, stay faked).
        prev_use_fakes = getattr(bootstrap_test_env, "_use_fakes", False)  # type: ignore[attr-defined]
        prev_silence_output = getattr(bootstrap_test_env, "_silence", False)  # type: ignore[attr-defined]
        if use_fakes and not prev_use_fakes:
            raise RuntimeError("bootstrap_test_env(): fakes were previously disabled; cannot enable them later")
        if silence_output and not prev_silence_output:
            _silence_stdout()
            setattr(bootstrap_test_env, "_silence", True)  # type: ignore[attr-defined]
        return

    # ---------------------------------------------------------------------
    repo_root = _discover_repo_root()
    os.chdir(repo_root)
    _ensure_src_on_path(repo_root)

    # Patch Config root resolution so tests & scripts load config relative to repo
    try:
        import agentforge.config as _afcfg  # pylint: disable=import-error

        def _fixed_find_project_root(self, _root_path: Optional[str] = None):  # noqa: D401
            return repo_root

        _afcfg.Config.find_project_root = _fixed_find_project_root  # type: ignore[assignment]
    except ImportError:
        # AgentForge not importable yet – ignore; caller is likely installing
        # dependencies.
        pass

    # Ensure a .agentforge directory exists at repo root -------------------
    default_af = repo_root / ".agentforge"
    created_dot_agentforge = False
    if not default_af.exists():
        setup_src = repo_root / "src" / "agentforge" / "setup_files"
        shutil.copytree(setup_src, default_af)
        created_dot_agentforge = True

    # Activate fakes --------------------------------------------------------
    if use_fakes:
        try:
            from tests.utils.fakes import FakeChromaStorage  # type: ignore
            import agentforge.storage.chroma_storage as cs_mod  # noqa: E402
            import agentforge.storage.memory as mem_mod  # noqa: E402

            cs_mod.ChromaStorage = FakeChromaStorage  # type: ignore[attr-defined]
            mem_mod.ChromaStorage = FakeChromaStorage  # type: ignore[attr-defined]

            # Convenience alias expected by some tests
            setattr(FakeChromaStorage, "create_collection", FakeChromaStorage.select_collection)

            # Stub Agent.run so no LLM calls hit external services ------------
            from agentforge.agent import Agent  # noqa: E402

            _orig_run = Agent.run  # type: ignore[assignment]
            _decisions = ("approve", "reject", "other")

            def _fake_run(self: "Agent", **context):  # type: ignore[override]
                # Allow opt-out via debug flag
                if getattr(self, "settings", {}).get("system", {}).get("debug", {}).get("mode", False):
                    return _orig_run(self, **context)

                if hasattr(self, "_cog") and hasattr(self._cog, "branch_call_counts"):
                    self._cog.branch_call_counts[self.agent_name] = (
                        self._cog.branch_call_counts.get(self.agent_name, 0) + 1
                    )

                name_l = self.agent_name.lower()
                if "analyze" in name_l:
                    return {"analysis": "stub-analysis"}
                if "decide" in name_l:
                    idx = getattr(self, "_call_idx", 0)
                    self._call_idx = idx + 1
                    return {"choice": _decisions[idx % len(_decisions)], "rationale": "stub"}
                if "response" in name_l or "respond" in name_l:
                    return "FINAL RESPONSE"
                if "understand" in name_l:
                    return {
                        "insights": "User is asking about programming topics",
                        "user_intent": "Seeking information or help",
                        "relevant_topics": ["programming", "learning"],
                        "persona_relevant": "User shows interest in technical topics",
                    }
                if "test" in name_l:
                    return _orig_run(self, **context)
                return f"Simulated response from {self.agent_name}"

            Agent.run = _fake_run  # type: ignore[assignment]
        except Exception:  # pragma: no cover – tests will surface issues
            if use_fakes:
                raise

    # Silence output if requested -----------------------------------------
    if silence_output:
        _silence_stdout()

    # Register cleanup -----------------------------------------------------
    if cleanup_on_exit:
        def _cleanup():  # noqa: D401
            if silence_output:
                logging.disable(logging.NOTSET)
                if hasattr(builtins, "__orig_print"):
                    builtins.print = builtins.__orig_print  # type: ignore[attr-defined]
                    del builtins.__orig_print  # type: ignore[attr-defined]
            if created_dot_agentforge and default_af.exists():
                shutil.rmtree(default_af, ignore_errors=True)
        atexit.register(_cleanup)

    # Record that we have run so subsequent calls don't redo work ----------
    setattr(bootstrap_test_env, "_has_run", True)  # type: ignore[attr-defined]
    setattr(bootstrap_test_env, "_use_fakes", use_fakes)  # type: ignore[attr-defined]
    setattr(bootstrap_test_env, "_silence", silence_output)

import anthropic
from base_api import BaseModel

# From apis/anthropic_api.py
class Claude(BaseModel):
    """
    A class for interacting with Anthropic's Claude models to generate text based on provided prompts.

    Manages API calls to Anthropic, handling errors such as rate limits, and retries failed requests with exponential
    backoff.
    """

    @staticmethod
    def _prepare_prompt(model_prompt):
        return {
            'messages': [{'role': 'user', 'content': model_prompt.get('user')}],
            'system': model_prompt.get('system')
        }

    def _do_api_call(self, prompt, **filtered_params):
        response = client.messages.create(
            model=self.model_name,
            messages=prompt.get('messages'),
            system=prompt.get('system'),
            **filtered_params
        )
        return response

    def _process_response(self, raw_response):
        return raw_response.content[0].text

import google.generativeai
from google.generativeai.types import HarmCategory
from google.generativeai.types import HarmBlockThreshold
from agentforge.utils.logger import Logger
from agentforge.apis.mixins.vision_mixin import VisionMixin

# From apis/gemini_api.py
class Gemini(BaseModel):
    """
    A class for interacting with Google's Generative AI models to generate text based on provided prompts.

    Handles API calls to Google's Generative AI, including error handling for rate limits and retries failed requests.
    """

    @staticmethod
    def _prepare_prompt(model_prompt):
        # Return the standard messages format expected by base class
        return [
            {"role": "system", "content": model_prompt.get('system')},
            {"role": "user", "content": model_prompt.get('user')}
        ]

    def _do_api_call(self, prompt, **filtered_params):
        model = genai.GenerativeModel(self.model_name)
        
        # Handle different prompt formats
        if isinstance(prompt, dict):
            if "contents" in prompt:
                # GeminiVision format - pass contents directly
                content = prompt["contents"]
            elif "messages" in prompt:
                # Standard messages format - convert to text
                messages = prompt["messages"]
                content = '\n\n'.join([msg["content"] for msg in messages if msg["content"]])
            else:
                content = prompt
        else:
            content = prompt
            
        response = model.generate_content(
            content,
            safety_settings={
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            },
            generation_config=genai.types.GenerationConfig(**filtered_params)
        )

        return response

    def _process_response(self, raw_response):
        return raw_response.text

# From apis/gemini_api.py
class GeminiVision(VisionMixin, Gemini):
    """
    Adds image support to Gemini via VisionMixin.
    Only _merge_parts needs tweaking to match Gemini's request schema.
    """
    def _merge_parts(self, parts, **params):
        # For Gemini, we need to format content properly for the Google Generative AI library
        content = []
        
        # Add text content
        if "text" in parts:
            text_parts = parts["text"]
            if isinstance(text_parts, list):
                for msg in text_parts:
                    if isinstance(msg, dict) and "content" in msg:
                        content.append(msg["content"])
                    else:
                        content.append(str(msg))
            else:
                content.append(str(text_parts))
        
        # Add image content 
        if "image" in parts:
            # Convert from VisionMixin format to Google format
            for img_part in parts["image"]:
                if isinstance(img_part, dict) and "image_url" in img_part:
                    # Extract base64 data from data URL
                    url = img_part["image_url"]["url"]
                    if url.startswith("data:image/"):
                        # Extract the base64 part after the comma
                        base64_data = url.split(",", 1)[1]
                        # Google Generative AI expects PIL Image or raw bytes
                        import base64
                        import io
                        from PIL import Image
                        
                        image_bytes = base64.b64decode(base64_data)
                        image = Image.open(io.BytesIO(image_bytes))
                        content.append(image)
        
        return {"contents": content, **params}


# From apis/openai_api.py
class GPT(BaseModel):
    """
    Concrete implementation for OpenAI GPT models.
    """

    def _do_api_call(self, prompt, **filtered_params):
        if isinstance(prompt, dict) and "messages" in prompt:
            prompt = prompt["messages"]
        response = client.chat.completions.create(
            model=self.model_name,
            messages=prompt,
            **filtered_params
        )
        return response

    def _process_response(self, raw_response):
        return raw_response.choices[0].message.content

# From apis/openai_api.py
class O1Series(GPT):
    """
    Concrete implementation for OpenAI GPT models.
    """

    def _prepare_prompt(self, model_prompt):
        # Format user messages in the appropriate style
        content = f"{model_prompt.get('system', '')}\n\n{model_prompt.get('user', '')}"
        return [{"role": "user", "content": content}]

from openai import APIError
from openai import RateLimitError
from openai import APIConnectionError

# From apis/base_api.py
class UnsupportedModalityError(Exception):
    """Raised when a model doesn't support a requested modality."""
    pass

# From apis/base_api.py
class BaseModel:
    """
    A base class encapsulating shared logic (e.g., logging, retries, prompt building).
    Subclasses must implement the _call_api method, which does the actual work of sending prompts.
    """

    # Defaults you might share across all models
    default_retries = 3
    default_backoff = 2

    supported_modalities = {"text"}

    def __init__(self, model_name, **kwargs):
        self.logger = None
        self.allowed_params = None
        self.excluded_params = None
        self.model_name = model_name
        self.num_retries = kwargs.get("num_retries", self.default_retries)
        self.base_backoff = kwargs.get("base_backoff", self.default_backoff)

    def generate(self, model_prompt=None, *, images=None, **params):
        """
        Main entry point for generating responses. This method handles retries,
        calls _call_api for the actual request, and logs everything.
        """
        self._validate_modalities(images)
        self._init_logger(model_prompt, params)

        parts        = self._build_parts(model_prompt, images)
        request_body = self._merge_parts(parts)

        return self._run_with_retries(request_body, params)

    # ─────────────────── helper trio for readability ────────────────────
    def _validate_modalities(self, images):
        if images and "image" not in self.supported_modalities:
            raise UnsupportedModalityError(f"{self.__class__.__name__} can't handle images")

    def _init_logger(self, model_prompt, params):
        self.logger = Logger(name=params.pop('agent_name', 'NamelessAgent'))
        if model_prompt:
            self.logger.log_prompt(model_prompt)

    def _build_parts(self, model_prompt, images):
        parts = {"text": self._prepare_prompt(model_prompt)}
        if images:
            parts["image"] = self._prepare_image_payload(images)
        return parts

    # ─────────────────── retry/back‑off execution ───────────────────────
    def _run_with_retries(self, request_body, params):
        reply = None
        
        for attempt in range(self.num_retries):
            backoff = self.base_backoff ** (attempt + 1)
            
            try:
                filtered = self._prepare_params(**params)
                response = self._do_api_call(request_body, **filtered)
                reply    = self._process_response(response)

                self.logger.log_response(reply)
                break
            except RateLimitError as e:
                self.logger.warning(f"Rate limit exceeded: {e}. Retrying in {backoff} seconds...")
                time.sleep(backoff)
            except APIConnectionError as e:
                self.logger.warning(f"Connection error: {e}. Retrying in {backoff} seconds...")
                time.sleep(backoff)
            except APIError as e:
                if getattr(e, "status_code", None) == 502:
                    self.logger.warning(f"502 Bad Gateway. Retrying in {backoff} seconds...")
                    time.sleep(backoff)
                else:
                    raise
            except Exception as e:
                self.logger.warning(f"Error: {e}. Retrying in {backoff} seconds...")
                time.sleep(backoff)

        if reply is None:
            self.logger.critical("Error: All retries exhausted. No response received.")
            raise ValueError("Model generation failed: All retries exhausted. No response received.")
        
        # Validate that we received a non-empty response
        if not reply or (isinstance(reply, str) and not reply.strip()):
            self.logger.critical("Error: Model returned empty response.")
            raise ValueError("Model generation failed: Received empty response.")
            
        return reply

    def _prepare_image_payload(self, images):
        """Default implementation raises UnsupportedModalityError for image modality."""
        raise UnsupportedModalityError(f"{self.__class__.__name__} can't handle images")

    def _merge_parts(self, parts):
        """
        Combine text and optional image parts into an API-specific message format.
        Default implementation follows OpenAI-style format.
        """
        messages = parts["text"]
        if "image" in parts and parts["image"]:
            # If the last message is from the user, add images to it
            if messages and messages[-1]["role"] == "user":
                content = [{"type": "text", "text": messages[-1]["content"]}]
                content.extend(parts["image"])
                messages[-1]["content"] = content
            
        return {"messages": messages}

    @staticmethod
    def _prepare_prompt(model_prompt):
        # Format system/user messages in the appropriate style
        return [
            {"role": "system", "content": model_prompt.get('system')},
            {"role": "user", "content": model_prompt.get('user')}
        ]

    def _prepare_params(self, **params):
        if self.allowed_params:
            # Keep only parameters explicitly allowed
            return {k: v for k, v in params.items() if k in self.allowed_params}
        if self.excluded_params:
            # Exclude parameters listed in excluded_params
            return {k: v for k, v in params.items() if k not in self.excluded_params}
        # If neither allowed nor excluded parameters are defined, pass all params
        return params

    def _do_api_call(self, prompt, **filtered_params):
        # The actual request to the underlying client
        raise NotImplementedError("Subclasses must implement _do_call_api method.")

    def _process_response(self, raw_response):
        # Subclasses can process the raw responses as needed
        return raw_response

# From apis/base_api.py
def generate(self, model_prompt=None, *, images=None, **params):
        """
        Main entry point for generating responses. This method handles retries,
        calls _call_api for the actual request, and logs everything.
        """
        self._validate_modalities(images)
        self._init_logger(model_prompt, params)

        parts        = self._build_parts(model_prompt, images)
        request_body = self._merge_parts(parts)

        return self._run_with_retries(request_body, params)


# From apis/lm_studio_api.py
class LMStudio(BaseModel):
    """
    Concrete implementation for OpenAI GPT models.
    """

    def _do_api_call(self, prompt, **filtered_params):
        url = filtered_params.pop('host_url', 'http://localhost:1234/v1/chat/completions')
        headers = {'Content-Type': 'application/json'}
        data = {
            "model": self.model_name,
            "messages": prompt,
            **filtered_params
        }

        response = requests.post(url, headers=headers, json=data)

        if response.status_code != 200:
            # return error content
            self.logger.error(f"Request error: {response}")
            return None

        return response.json()

    def _process_response(self, raw_response):
        return raw_response["choices"][0]["message"]["content"]


# From apis/ollama_api.py
class Ollama(BaseModel):

    @staticmethod
    def _prepare_prompt(model_prompt):
        return model_prompt

    def _do_api_call(self, prompt, **filtered_params):
        url = filtered_params.pop('host_url', 'http://localhost:11434/api/generate')
        headers = {'Content-Type': 'application/json'}
        data = {
            "model": self.model_name,
            "system": prompt.get('system'),
            "prompt": prompt.get('user'),
            **filtered_params
        }

        response = requests.post(url, headers=headers, json=data)

        if response.status_code != 200:
            # return error content
            self.logger.error(f"Request error: {response}")
            return None

        return response.json()

    def _process_response(self, raw_response):
        # Handle different Ollama endpoint responses
        if raw_response is None:
            return None
        if 'response' in raw_response:  # /api/generate
            return raw_response['response']
        elif 'message' in raw_response:  # /api/chat
            return raw_response['message']['content']
        elif 'choices' in raw_response:
            return raw_response['choices'][0]['message']['content']
        else:
            self.logger.error(f"Unexpected Ollama response format: {raw_response}")
            return None


# From apis/openrouter_api.py
class OpenRouter(BaseModel):
    """
    A class for interacting with OpenRouter's API to generate text based on provided prompts.

    Handles API calls to OpenRouter, including error handling and retries for failed requests.
    """


    def _do_api_call(self, prompt, **filtered_params):
        url = filtered_params.pop('host_url', 'https://openrouter.ai/api/v1/chat/completions')
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": filtered_params.pop("http_referer", ""),
            "X-Title": 'AgentForge'
        }
        data = {
            "model": self.model_name,
            "messages": prompt,
            **filtered_params
        }

        response = requests.post(url, headers=headers, json=data)

        if response.status_code != 200:
            # return error content
            self.logger.error(f"Request error: {response}")
            return None

        return response.json()

    def _process_response(self, raw_response):
        # First, check if the response is empty
        if not raw_response:
            self.logger.error("Empty response received from API")
            return None

        # If there's an error in the response, log it and return an error message
        if 'error' in raw_response:
            error_code = raw_response['error'].get('code', 'unknown')
            error_message = raw_response['error'].get('message', 'No error message provided.')
            self.logger.error(f"API Error {error_code}: {error_message}")
            return f"Error {error_code}: {error_message}"

        # Otherwise, try to extract the content from the choices
        try:
            return raw_response['choices'][0]['message']['content']
        except (IndexError, KeyError) as e:
            self.logger.error(f"Unexpected response format: {e}")
            return None

from groq import Groq

# From apis/groq_api.py
class GroqAPI(BaseModel):

    def _do_api_call(self, prompt, **filtered_params):
        response = client.chat.completions.create(
            model=self.model_name,
            messages=prompt,
            **filtered_params
        )
        return response

    def _process_response(self, raw_response):
        return raw_response.choices[0].message.content

import spacy

# From tools/triple_extract.py
class TripleExtract:
    """
    A class for extracting subject-predicate-object triples from sentences.
    """

    @staticmethod
    def find_subject_predicate_object(sentence):
        """
        Extract subject, predicate, and object from a given sentence.

        Args:
            sentence (str): The input sentence.

        Returns:
            tuple: A tuple containing (subject, predicate, object), each as a string or None if not found.

        Raises:
            ValueError: If the input is not a string or is empty.
            Exception: For any other unexpected errors during processing.
        """
        if not isinstance(sentence, str) or not sentence.strip():
            raise ValueError("Input sentence must be a non-empty string")

        try:
            doc = nlp(sentence)
            subject, predicate, _object = None, None, None

            # Identify named entities using SpaCy NER
            for entity in doc.ents:
                if entity.label_ in ("PERSON", "ORG"):
                    # Prioritize named entities as potential subjects
                    subject = entity
                    break

            # Use syntactic dependency labels to identify subject and verb
            for token in doc:
                if token.dep_ == "nsubj" or token.dep_ == "nsubjpass":
                    subject = token
                elif token.pos_ == "VERB":
                    # Check if it's part of a verb phrase indicating the predicate
                    if token.dep_ == "aux" and nlp(token.head.text).pos_ == "VERB":
                        continue  # Skip auxiliary verbs
                    else:
                        predicate = token.head  # Consider the head of the verb phrase as the predicate
                        break

            # If subject not found directly, explore other possibilities
            if not subject:
                if predicate:
                    # Check for subject within relative clauses
                    for child in predicate.children:
                        if child.dep_ == "relcl":
                            subject = TripleExtract.find_subject_in_clause(child)
                            if subject:
                                break
                else:
                    # Try finding a verb phrase as the subject
                    for chunk in doc.noun_chunks:
                        if any(token.pos_ == "VERB" for token in chunk):
                            subject = chunk
                            break

            # Look for candidate objects after finding subject and predicate
            if subject and predicate:
                for child in predicate.children:
                    if child.dep_ in ["dobj", "attr", "iobj"]:  # Include indirect objects
                        _object = child
                    elif child.dep_ == "prep":
                        # Iterate over children and check for "pobj" dependency
                        for grandchild in child.children:
                            if grandchild.dep_ == "pobj":
                                _object = grandchild
                                break  # Stop iterating after finding "pobj"

            # Convert identified tokens to text if they exist
            subject_text = subject.text if subject else None
            predicate_text = predicate.lemma_ if predicate else None  # Using lemma for base form of verb
            object_text = _object.text if _object else None

            print(
                f"Debug Trip:\n\nSentence: {sentence}\nSubject: {subject_text}\nPredicate: {predicate_text}\nObject: {object_text}")
            return subject_text, predicate_text, object_text  # Return the identified elements

        except Exception as e:
            raise Exception(f"An error occurred while processing the sentence: {str(e)}")

    @staticmethod
    def find_subject_in_clause(clause):
        """
        Find the subject in a given clause.

        Args:
            clause (spacy.tokens.Span): A SpaCy span representing a clause.

        Returns:
            spacy.tokens.Token or None: The subject token if found, None otherwise.
        """
        for token in clause:
            if token.dep_ in ["nsubj", "nsubjpass"]:
                return token
        return None

    @staticmethod
    def find_subject_predicate_object_with_chunk(sentence, chunk):
        """
        Extract subject, predicate, and object from a sentence, using a chunk for context.

        Args:
            sentence (str): The input sentence.
            chunk (str): A chunk of text providing context.

        Returns:
            tuple: A tuple containing (subject, predicate, object), each as a string or None if not found.

        Raises:
            ValueError: If the input sentence or chunk is not a string or is empty.
            Exception: For any other unexpected errors during processing.
        """
        if not isinstance(sentence, str) or not sentence.strip():
            raise ValueError("Input sentence must be a non-empty string")
        if not isinstance(chunk, str) or not chunk.strip():
            raise ValueError("Input chunk must be a non-empty string")

        try:
            doc = nlp(chunk)  # Process the chunk for context
            sentence_doc = nlp(sentence)  # Process the sentence separately

            # Identify named entities using SpaCy NER
            entities = [ent for ent in doc.ents if ent.label_ in ("PERSON", "ORG")]

            subject, predicate, _object = None, None, None

            # Use syntactic dependency labels to identify subject and verb
            for token in sentence_doc:
                if token.dep_ == "nsubj" or token.dep_ == "nsubjpass":
                    subject = token

                    # Filter irrelevant words based on POS tags and additional stop words
                    if subject and isinstance(subject, spacy.tokens.Doc):
                        filtered_subject_words = [
                            word.text
                            for word in subject.words
                            if word.pos_ not in ["STOP", "ADP", "DET", "AUX"]  # Add AUX for auxiliary verbs
                        ]
                    else:
                        filtered_subject_words = [subject.text] if subject else None

                    # Join the filtered words with a space
                    subject_text = " ".join(filtered_subject_words) if filtered_subject_words else None
                    print(f"\nDEBUG CHUNK: \nFiltered subject words: {filtered_subject_words}\nSubject text: {subject_text}\n")

                elif token.pos_ == "VERB":
                    # Check if it's part of a verb phrase indicating the predicate
                    if token.dep_ == "aux" and nlp(token.head.text).pos_ == "VERB":
                        continue  # Skip auxiliary verbs
                    else:
                        predicate = token.head  # Consider the head of the verb phrase as the predicate
                        break

            # If subject not found directly, explore other possibilities
            if not subject:
                if predicate:
                    # Check for subject within relative clauses or previous entities
                    for child in predicate.children:
                        if child.dep_ == "relcl":
                            subject = TripleExtract.find_subject_in_clause_with_chunk(child,
                                                                                      entities.copy())  # Pass a copy of entities
                            if subject:
                                break
                        elif child.dep_ == "pobj" and len(entities) > 0:
                            # Check if object from previous sentence is the subject
                            for entity in entities:
                                if entity.text == child.text:
                                    subject = entity
                                    break
                        else:
                            # Try finding a verb phrase as the subject
                            for chunk in doc.noun_chunks:
                                if any(token.pos_ == "VERB" for token in chunk):
                                    subject = chunk
                                    break

            # Look for candidate objects after finding subject and predicate
            if subject and predicate:
                for child in predicate.children:
                    if child.dep_ in ["dobj", "attr", "iobj"]:
                        for grandchild in child.children:  # Iterate over child.children
                            if grandchild.dep_ == "pobj":
                                _object = grandchild
                                break  # Stop iterating after finding the object
                    elif child.dep_ == "prep":
                        for grandchild in child.children:
                            if grandchild.dep_ == "pobj":
                                _object = grandchild
                                break  # Stop iterating after finding the object

            # Convert identified tokens to text if they exist
            subject_text = subject.text if subject else None
            predicate_text = predicate.lemma_ if predicate else None  # Using lemma for base form of verb
            object_text = _object.text if _object else None

            print(
                f"usingContext:\nSubject: {subject_text}\nPredicate: {predicate_text}\nObject: {object_text}")
            return subject_text, predicate_text, object_text

        except Exception as e:
            raise Exception(f"An error occurred while processing the sentence with chunk: {str(e)}")

    @staticmethod
    def find_subject_in_clause_with_chunk(clause, entities):
        """
        Find the subject in a given clause, considering entities from the context.

        Args:
            clause (spacy.tokens.Span): A SpaCy span representing a clause.
            entities (list): A list of named entities from the context.

        Returns:
            spacy.tokens.Token or spacy.tokens.Span or None: The subject if found, None otherwise.
        """
        for token in clause:
            if token.dep_ in ["nsubj", "nsubjpass"]:
                # Check if subject matches an entity from previous sentence
                for entity in entities:
                    if entity.text == token.text:
                        return token
                return token  # Found subject within clause
            elif token.pos_ == "PRON":  # Check pronouns within relative clause
                for ent in entities:
                    if ent.text == token.text:
                        return ent  # Pronoun referring to previous entity
        return None

# From tools/triple_extract.py
def download_spacy_model(model_name):
    """
    Download a SpaCy model using SpaCy's CLI.

    Args:
        model_name (str): The name of the SpaCy model to download.

    Raises:
        Exception: If the download fails.
    """
    try:
        print(f"Downloading the {model_name} model...")
        spacy.cli.download(model_name)
        print(f"Model {model_name} downloaded successfully.")
    except Exception as e:
        raise Exception(f"Failed to download model {model_name}: {str(e)}")

# From tools/triple_extract.py
def find_subject_predicate_object(sentence):
        """
        Extract subject, predicate, and object from a given sentence.

        Args:
            sentence (str): The input sentence.

        Returns:
            tuple: A tuple containing (subject, predicate, object), each as a string or None if not found.

        Raises:
            ValueError: If the input is not a string or is empty.
            Exception: For any other unexpected errors during processing.
        """
        if not isinstance(sentence, str) or not sentence.strip():
            raise ValueError("Input sentence must be a non-empty string")

        try:
            doc = nlp(sentence)
            subject, predicate, _object = None, None, None

            # Identify named entities using SpaCy NER
            for entity in doc.ents:
                if entity.label_ in ("PERSON", "ORG"):
                    # Prioritize named entities as potential subjects
                    subject = entity
                    break

            # Use syntactic dependency labels to identify subject and verb
            for token in doc:
                if token.dep_ == "nsubj" or token.dep_ == "nsubjpass":
                    subject = token
                elif token.pos_ == "VERB":
                    # Check if it's part of a verb phrase indicating the predicate
                    if token.dep_ == "aux" and nlp(token.head.text).pos_ == "VERB":
                        continue  # Skip auxiliary verbs
                    else:
                        predicate = token.head  # Consider the head of the verb phrase as the predicate
                        break

            # If subject not found directly, explore other possibilities
            if not subject:
                if predicate:
                    # Check for subject within relative clauses
                    for child in predicate.children:
                        if child.dep_ == "relcl":
                            subject = TripleExtract.find_subject_in_clause(child)
                            if subject:
                                break
                else:
                    # Try finding a verb phrase as the subject
                    for chunk in doc.noun_chunks:
                        if any(token.pos_ == "VERB" for token in chunk):
                            subject = chunk
                            break

            # Look for candidate objects after finding subject and predicate
            if subject and predicate:
                for child in predicate.children:
                    if child.dep_ in ["dobj", "attr", "iobj"]:  # Include indirect objects
                        _object = child
                    elif child.dep_ == "prep":
                        # Iterate over children and check for "pobj" dependency
                        for grandchild in child.children:
                            if grandchild.dep_ == "pobj":
                                _object = grandchild
                                break  # Stop iterating after finding "pobj"

            # Convert identified tokens to text if they exist
            subject_text = subject.text if subject else None
            predicate_text = predicate.lemma_ if predicate else None  # Using lemma for base form of verb
            object_text = _object.text if _object else None

            print(
                f"Debug Trip:\n\nSentence: {sentence}\nSubject: {subject_text}\nPredicate: {predicate_text}\nObject: {object_text}")
            return subject_text, predicate_text, object_text  # Return the identified elements

        except Exception as e:
            raise Exception(f"An error occurred while processing the sentence: {str(e)}")

# From tools/triple_extract.py
def find_subject_in_clause(clause):
        """
        Find the subject in a given clause.

        Args:
            clause (spacy.tokens.Span): A SpaCy span representing a clause.

        Returns:
            spacy.tokens.Token or None: The subject token if found, None otherwise.
        """
        for token in clause:
            if token.dep_ in ["nsubj", "nsubjpass"]:
                return token
        return None

# From tools/triple_extract.py
def find_subject_predicate_object_with_chunk(sentence, chunk):
        """
        Extract subject, predicate, and object from a sentence, using a chunk for context.

        Args:
            sentence (str): The input sentence.
            chunk (str): A chunk of text providing context.

        Returns:
            tuple: A tuple containing (subject, predicate, object), each as a string or None if not found.

        Raises:
            ValueError: If the input sentence or chunk is not a string or is empty.
            Exception: For any other unexpected errors during processing.
        """
        if not isinstance(sentence, str) or not sentence.strip():
            raise ValueError("Input sentence must be a non-empty string")
        if not isinstance(chunk, str) or not chunk.strip():
            raise ValueError("Input chunk must be a non-empty string")

        try:
            doc = nlp(chunk)  # Process the chunk for context
            sentence_doc = nlp(sentence)  # Process the sentence separately

            # Identify named entities using SpaCy NER
            entities = [ent for ent in doc.ents if ent.label_ in ("PERSON", "ORG")]

            subject, predicate, _object = None, None, None

            # Use syntactic dependency labels to identify subject and verb
            for token in sentence_doc:
                if token.dep_ == "nsubj" or token.dep_ == "nsubjpass":
                    subject = token

                    # Filter irrelevant words based on POS tags and additional stop words
                    if subject and isinstance(subject, spacy.tokens.Doc):
                        filtered_subject_words = [
                            word.text
                            for word in subject.words
                            if word.pos_ not in ["STOP", "ADP", "DET", "AUX"]  # Add AUX for auxiliary verbs
                        ]
                    else:
                        filtered_subject_words = [subject.text] if subject else None

                    # Join the filtered words with a space
                    subject_text = " ".join(filtered_subject_words) if filtered_subject_words else None
                    print(f"\nDEBUG CHUNK: \nFiltered subject words: {filtered_subject_words}\nSubject text: {subject_text}\n")

                elif token.pos_ == "VERB":
                    # Check if it's part of a verb phrase indicating the predicate
                    if token.dep_ == "aux" and nlp(token.head.text).pos_ == "VERB":
                        continue  # Skip auxiliary verbs
                    else:
                        predicate = token.head  # Consider the head of the verb phrase as the predicate
                        break

            # If subject not found directly, explore other possibilities
            if not subject:
                if predicate:
                    # Check for subject within relative clauses or previous entities
                    for child in predicate.children:
                        if child.dep_ == "relcl":
                            subject = TripleExtract.find_subject_in_clause_with_chunk(child,
                                                                                      entities.copy())  # Pass a copy of entities
                            if subject:
                                break
                        elif child.dep_ == "pobj" and len(entities) > 0:
                            # Check if object from previous sentence is the subject
                            for entity in entities:
                                if entity.text == child.text:
                                    subject = entity
                                    break
                        else:
                            # Try finding a verb phrase as the subject
                            for chunk in doc.noun_chunks:
                                if any(token.pos_ == "VERB" for token in chunk):
                                    subject = chunk
                                    break

            # Look for candidate objects after finding subject and predicate
            if subject and predicate:
                for child in predicate.children:
                    if child.dep_ in ["dobj", "attr", "iobj"]:
                        for grandchild in child.children:  # Iterate over child.children
                            if grandchild.dep_ == "pobj":
                                _object = grandchild
                                break  # Stop iterating after finding the object
                    elif child.dep_ == "prep":
                        for grandchild in child.children:
                            if grandchild.dep_ == "pobj":
                                _object = grandchild
                                break  # Stop iterating after finding the object

            # Convert identified tokens to text if they exist
            subject_text = subject.text if subject else None
            predicate_text = predicate.lemma_ if predicate else None  # Using lemma for base form of verb
            object_text = _object.text if _object else None

            print(
                f"usingContext:\nSubject: {subject_text}\nPredicate: {predicate_text}\nObject: {object_text}")
            return subject_text, predicate_text, object_text

        except Exception as e:
            raise Exception(f"An error occurred while processing the sentence with chunk: {str(e)}")

# From tools/triple_extract.py
def find_subject_in_clause_with_chunk(clause, entities):
        """
        Find the subject in a given clause, considering entities from the context.

        Args:
            clause (spacy.tokens.Span): A SpaCy span representing a clause.
            entities (list): A list of named entities from the context.

        Returns:
            spacy.tokens.Token or spacy.tokens.Span or None: The subject if found, None otherwise.
        """
        for token in clause:
            if token.dep_ in ["nsubj", "nsubjpass"]:
                # Check if subject matches an entity from previous sentence
                for entity in entities:
                    if entity.text == token.text:
                        return token
                return token  # Found subject within clause
            elif token.pos_ == "PRON":  # Check pronouns within relative clause
                for ent in entities:
                    if ent.text == token.text:
                        return ent  # Pronoun referring to previous entity
        return None

from agentforge.storage.chroma_storage import ChromaStorage

# From tools/web_scrape.py
def remove_extra_newlines(chunk):
    """
    Remove extra newlines from a chunk of text.

    Args:
        chunk (str): The input text chunk.

    Returns:
        str: The text chunk with extra newlines removed.

    Raises:
        ValueError: If the input is not a string.
    """
    if not isinstance(chunk, str):
        raise ValueError("Input chunk must be a string")
    return re.sub(r'\n+', '\n\n', chunk)

# From tools/web_scrape.py
def get_plain_text(url):
    """
    Fetch and extract plain text from a webpage.

    This function sends a GET request to the specified URL, extracts the plain text
    from the HTML content, chunks the text, and saves the chunks to memory.

    Args:
        url (str): The URL of the webpage to scrape.

    Returns:
        str: Plain text retrieved from the URL.

    Raises:
        ValueError: If the URL is not a string or is empty.
        requests.RequestException: If there's an error fetching the webpage.
        Exception: For any other unexpected errors during execution.
    """
    if not isinstance(url, str) or not url.strip():
        raise ValueError("URL must be a non-empty string")

    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Create a BeautifulSoup object with the HTML content
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract the plain text from the HTML content
        plain_text = soup.get_text()
        # chunk_text = intelligent_chunk(plain_text, chunk_size=1)
        # chunk_save(chunk_text, url)

        return plain_text

    except requests.RequestException as e:
        raise Exception(f"Error fetching the webpage: {str(e)}")
    except Exception as e:
        raise Exception(f"An error occurred while processing the webpage: {str(e)}")

# From tools/web_scrape.py
def chunk_save(chunks, url):
    """
    Save chunks of text to memory.

    This function processes each chunk of text, removes extra newlines,
    and saves it to memory using the ChromaUtils instance.

    Args:
        chunks (list): A list of text chunks to save.
        url (str): The URL associated with the chunks.

    Raises:
        ValueError: If chunks is not a list or url is not a string.
        Exception: For any unexpected errors during saving.
    """
    if not isinstance(chunks, list):
        raise ValueError("Chunks must be a list")
    if not isinstance(url, str):
        raise ValueError("URL must be a string")

    try:
        for chunk in chunks:
            chunk = remove_extra_newlines(chunk)
            storage_instance.save_memory(collection_name='Results', data=[chunk], metadata=[{"source_url": url}])
    except Exception as e:
        raise Exception(f"Error saving chunks to memory: {str(e)}")

import cv2
import pytesseract

# From tools/image_to_txt.py
def imagetotxt(image_path):
    """
    Extract text from an image using OCR (Optical Character Recognition).

    This function uses OpenCV to load and preprocess the image, and Tesseract OCR
    to extract text from the preprocessed image.

    Args:
        image_path (str): The file path to the image.

    Returns:
        str: The extracted text from the image.

    Raises:
        FileNotFoundError: If the specified image file does not exist.
        ValueError: If the image file is invalid or cannot be read.
        ImportError: If required libraries (cv2 or pytesseract) are not installed.
        Exception: For any other unexpected errors during execution.
    """
    try:
        # Load the image
        image = cv2.imread(image_path)
        
        if image is None:
            raise ValueError(f"Unable to read the image file: {image_path}")

        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Use Tesseract to extract text from the image
        extracted_text = pytesseract.image_to_string(gray)
        
        return extracted_text.strip()

    except FileNotFoundError:
        raise FileNotFoundError(f"The image file does not exist: {image_path}")
    except ImportError as e:
        raise ImportError(f"Required library not installed: {str(e)}")
    except Exception as e:
        raise Exception(f"An error occurred while processing the image: {str(e)}")


# From tools/directory.py
class DirectoryNode:
    """
    Represents a node in the directory tree.

    Attributes:
        name (str): The name of the file or directory.
        is_dir (bool): True if the node is a directory, False if it's a file.
        children (list): List of child DirectoryNode objects.
        depth (int): The depth of the node in the directory tree.
    """

    def __init__(self, name, is_dir, depth=0):
        self.name = name
        self.is_dir = is_dir
        self.children = []
        self.depth = depth

    def add_child(self, child):
        """
        Add a child node to this node.

        Args:
            child (DirectoryNode): The child node to add.
        """
        self.children.append(child)

# From tools/directory.py
class Directory:
    """
    A class for building and representing directory trees.

    This class provides methods to build a directory tree, exclude certain files or file types,
    and pretty print the directory structure.

    Attributes:
        root (DirectoryNode): The root node of the directory tree.
        excluded_files (set): Set of file names to exclude from the tree.
        excluded_file_types (set): Set of file extensions to exclude from the tree.
    """

    def __init__(self):
        """
        Initialize the Directory object.
        """
        self.root = None
        self.excluded_files = set()
        self.excluded_file_types = set()

    def build_tree(self, node=None, max_depth=None):
        """
        Recursively build the directory tree.

        Args:
            node (DirectoryNode, optional): The current node being processed. Defaults to None.
            max_depth (int, optional): The maximum depth to traverse. Defaults to None.

        Raises:
            ValueError: If the root node is not set.
        """
        if node is None:
            if self.root is None:
                raise ValueError("Root node is not set. Call read_directory() first.")
            node = self.root

        if max_depth is not None and node.depth >= max_depth:
            node.add_child(DirectoryNode('... more files ...', False, node.depth + 1))
            return

        try:
            for item in os.listdir(node.name):
                if item in self.excluded_files:
                    continue

                full_path = os.path.join(node.name, item)
                if os.path.isdir(full_path):
                    child_node = DirectoryNode(full_path, True, node.depth + 1)
                    node.add_child(child_node)
                    self.build_tree(child_node, max_depth)
                elif os.path.splitext(item)[1] not in self.excluded_file_types:
                    child_node = DirectoryNode(full_path, False, node.depth + 1)
                    node.add_child(child_node)
        except PermissionError:
            node.add_child(DirectoryNode('Permission denied', False, node.depth + 1))
        except Exception as e:
            node.add_child(DirectoryNode(f'Error: {str(e)}', False, node.depth + 1))

    def pretty_print(self, node=None, indent=""):
        """
        Generate a pretty-printed string representation of the directory tree.

        Args:
            node (DirectoryNode, optional): The current node being processed. Defaults to None.
            indent (str, optional): The indentation string. Defaults to "".

        Returns:
            str: A string representation of the directory tree.

        Raises:
            ValueError: If the root node is not set.
        """
        if node is None:
            if self.root is None:
                raise ValueError("Root node is not set. Call read_directory() first.")
            node = self.root

        directory_str = ""
        if node.is_dir:
            directory_str += f"{indent}{os.path.basename(node.name)}/\n"
        else:
            directory_str += f"{indent}{os.path.basename(node.name)}\n"
        indent += "    "
        for child in node.children:
            directory_str += self.pretty_print(child, indent)
        return directory_str

    def read_directory(self, directory_paths, max_depth=None):
        """
        Read the specified directories and build the directory tree.

        Args:
            directory_paths (str or list): A single directory path or a list of directory paths.
            max_depth (int, optional): The maximum depth to traverse. Defaults to None.

        Returns:
            str: A string representation of the directory tree(s).

        Raises:
            ValueError: If the input is not a string or a list of strings.
            OSError: If there's an error creating or accessing a directory.
        """
        if isinstance(directory_paths, str):
            directory_paths = [directory_paths]
        elif not isinstance(directory_paths, list) or not all(isinstance(path, str) for path in directory_paths):
            raise ValueError("directory_paths must be a string or a list of strings")

        output = ""
        for directory_path in directory_paths:
            try:
                if not os.path.exists(directory_path):
                    os.makedirs(directory_path, exist_ok=True)
                    output += f"Created '{directory_path}'\n"
                    continue

                if not os.listdir(directory_path):
                    output += f"The directory at '{directory_path}' is empty.\n"
                    continue

                self.root = DirectoryNode(directory_path, True)
                self.build_tree(self.root, max_depth)
                output += self.pretty_print() + "\n"

            except PermissionError:
                output += f"Permission denied: Unable to access '{directory_path}'.\n"
            except FileNotFoundError:
                output += f"File not found: The path '{directory_path}' does not exist.\n"
            except OSError as e:
                output += f"OS error occurred: {str(e)}\n"
            except Exception as e:
                output += f"An unexpected error occurred: {str(e)}\n"

        return output.strip()

# From tools/directory.py
def add_child(self, child):
        """
        Add a child node to this node.

        Args:
            child (DirectoryNode): The child node to add.
        """
        self.children.append(child)

# From tools/directory.py
def build_tree(self, node=None, max_depth=None):
        """
        Recursively build the directory tree.

        Args:
            node (DirectoryNode, optional): The current node being processed. Defaults to None.
            max_depth (int, optional): The maximum depth to traverse. Defaults to None.

        Raises:
            ValueError: If the root node is not set.
        """
        if node is None:
            if self.root is None:
                raise ValueError("Root node is not set. Call read_directory() first.")
            node = self.root

        if max_depth is not None and node.depth >= max_depth:
            node.add_child(DirectoryNode('... more files ...', False, node.depth + 1))
            return

        try:
            for item in os.listdir(node.name):
                if item in self.excluded_files:
                    continue

                full_path = os.path.join(node.name, item)
                if os.path.isdir(full_path):
                    child_node = DirectoryNode(full_path, True, node.depth + 1)
                    node.add_child(child_node)
                    self.build_tree(child_node, max_depth)
                elif os.path.splitext(item)[1] not in self.excluded_file_types:
                    child_node = DirectoryNode(full_path, False, node.depth + 1)
                    node.add_child(child_node)
        except PermissionError:
            node.add_child(DirectoryNode('Permission denied', False, node.depth + 1))
        except Exception as e:
            node.add_child(DirectoryNode(f'Error: {str(e)}', False, node.depth + 1))

# From tools/directory.py
def pretty_print(self, node=None, indent=""):
        """
        Generate a pretty-printed string representation of the directory tree.

        Args:
            node (DirectoryNode, optional): The current node being processed. Defaults to None.
            indent (str, optional): The indentation string. Defaults to "".

        Returns:
            str: A string representation of the directory tree.

        Raises:
            ValueError: If the root node is not set.
        """
        if node is None:
            if self.root is None:
                raise ValueError("Root node is not set. Call read_directory() first.")
            node = self.root

        directory_str = ""
        if node.is_dir:
            directory_str += f"{indent}{os.path.basename(node.name)}/\n"
        else:
            directory_str += f"{indent}{os.path.basename(node.name)}\n"
        indent += "    "
        for child in node.children:
            directory_str += self.pretty_print(child, indent)
        return directory_str

# From tools/directory.py
def read_directory(self, directory_paths, max_depth=None):
        """
        Read the specified directories and build the directory tree.

        Args:
            directory_paths (str or list): A single directory path or a list of directory paths.
            max_depth (int, optional): The maximum depth to traverse. Defaults to None.

        Returns:
            str: A string representation of the directory tree(s).

        Raises:
            ValueError: If the input is not a string or a list of strings.
            OSError: If there's an error creating or accessing a directory.
        """
        if isinstance(directory_paths, str):
            directory_paths = [directory_paths]
        elif not isinstance(directory_paths, list) or not all(isinstance(path, str) for path in directory_paths):
            raise ValueError("directory_paths must be a string or a list of strings")

        output = ""
        for directory_path in directory_paths:
            try:
                if not os.path.exists(directory_path):
                    os.makedirs(directory_path, exist_ok=True)
                    output += f"Created '{directory_path}'\n"
                    continue

                if not os.listdir(directory_path):
                    output += f"The directory at '{directory_path}' is empty.\n"
                    continue

                self.root = DirectoryNode(directory_path, True)
                self.build_tree(self.root, max_depth)
                output += self.pretty_print() + "\n"

            except PermissionError:
                output += f"Permission denied: Unable to access '{directory_path}'.\n"
            except FileNotFoundError:
                output += f"File not found: The path '{directory_path}' does not exist.\n"
            except OSError as e:
                output += f"OS error occurred: {str(e)}\n"
            except Exception as e:
                output += f"An unexpected error occurred: {str(e)}\n"

        return output.strip()

import pypdf

# From tools/get_text.py
class GetText:
    @staticmethod
    def resolve_path(filename: str) -> Path:
        """
        Resolves a given path to an absolute path and validates its existence.

        Parameters:
        filename (str): The file path to resolve.

        Returns:
        Path: A resolved absolute path.

        Raises:
        FileNotFoundError: If the file does not exist.
        """
        path = Path(filename).expanduser().resolve()

        if not path.is_file():
            raise FileNotFoundError(f"{path}")

        return path

    def read_file(self, file_name_or_url: str) -> str:
        """
        Reads text content from a file or URL.

        Parameters:
        file_name_or_url (str): The path to the file or the URL to read.

        Returns:
        str: The text content of the file or URL.

        Raises:
        FileNotFoundError: If the file does not exist.
        ValueError: If the file format is unsupported.
        Exception: For general errors during reading.
        """
        if file_name_or_url.startswith(('http://', 'https://')):
            return self.read_from_url(file_name_or_url)

        if file_name_or_url.endswith('.pdf'):
            return self.read_pdf(file_name_or_url)

        if file_name_or_url.endswith(('.txt', '.md')):
            return self.read_txt(file_name_or_url)

        raise ValueError("Unsupported file format - Use a URL or File with PDF, TXT, or Markdown formats.")

    def read_pdf(self, filename: str) -> str:
        """
        Reads text content from a PDF file.

        Parameters:
        filename (str): The path to the PDF file.

        Returns:
        str: The text content of the PDF.

        Raises:
        FileNotFoundError: If the file does not exist.
        Exception: For errors during PDF text extraction.
        """
        path = self.resolve_path(filename)

        try:
            with path.open('rb') as file:
                content = io.BytesIO(file.read())
                return self.extract_text_from_pdf(content)
        except Exception as e:
            raise Exception(f"Error reading PDF file: {str(e)}")

    def read_txt(self, filename: str) -> str:
        """
        Reads text content from a TXT file.

        Parameters:
        filename (str): The path to the TXT file.

        Returns:
        str: The text content of the TXT file.

        Raises:
        FileNotFoundError: If the file does not exist.
        Exception: For errors during TXT file reading.
        """
        path = self.resolve_path(filename)

        try:
            return path.read_text(encoding='utf-8')
        except Exception as e:
            raise Exception(f"Error reading TXT file: {str(e)}")

    def read_from_url(self, url: str) -> str:
        """
        Reads text content from a URL.

        Parameters:
        url (str): The URL to read from.

        Returns:
        str: The text content of the URL.

        Raises:
        ValueError: If the file format is unsupported.
        requests.RequestException: For HTTP errors during URL reading.
        """
        try:
            response = requests.get(url)
            response.raise_for_status()

            if url.endswith('.pdf'):
                return self.extract_text_from_pdf(io.BytesIO(response.content))

            if url.endswith('.txt') or url.endswith('.md'):
                return response.text

            raise ValueError("Unsupported file format for URL - Use PDF, TXT or Markdown formats.")
        except requests.RequestException as e:
            raise Exception(f"Error reading from URL: {str(e)}")

    @staticmethod
    def extract_text_from_pdf(file_stream: io.BytesIO) -> str:
        """
        Extracts text content from a PDF file stream.

        Parameters:
        file_stream (io.BytesIO): The file stream of the PDF.

        Returns:
        str: The extracted text content of the PDF.

        Raises:
        Exception: For errors during PDF text extraction.
        """
        try:
            text = ""
            reader = pypdf.PdfReader(file_stream)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
            return text.strip()
        except Exception as e:
            raise Exception(f"Error extracting text from PDF: {str(e)}")

# From tools/get_text.py
def resolve_path(filename: str) -> Path:
        """
        Resolves a given path to an absolute path and validates its existence.

        Parameters:
        filename (str): The file path to resolve.

        Returns:
        Path: A resolved absolute path.

        Raises:
        FileNotFoundError: If the file does not exist.
        """
        path = Path(filename).expanduser().resolve()

        if not path.is_file():
            raise FileNotFoundError(f"{path}")

        return path

# From tools/get_text.py
def read_pdf(self, filename: str) -> str:
        """
        Reads text content from a PDF file.

        Parameters:
        filename (str): The path to the PDF file.

        Returns:
        str: The text content of the PDF.

        Raises:
        FileNotFoundError: If the file does not exist.
        Exception: For errors during PDF text extraction.
        """
        path = self.resolve_path(filename)

        try:
            with path.open('rb') as file:
                content = io.BytesIO(file.read())
                return self.extract_text_from_pdf(content)
        except Exception as e:
            raise Exception(f"Error reading PDF file: {str(e)}")

# From tools/get_text.py
def read_txt(self, filename: str) -> str:
        """
        Reads text content from a TXT file.

        Parameters:
        filename (str): The path to the TXT file.

        Returns:
        str: The text content of the TXT file.

        Raises:
        FileNotFoundError: If the file does not exist.
        Exception: For errors during TXT file reading.
        """
        path = self.resolve_path(filename)

        try:
            return path.read_text(encoding='utf-8')
        except Exception as e:
            raise Exception(f"Error reading TXT file: {str(e)}")

# From tools/get_text.py
def read_from_url(self, url: str) -> str:
        """
        Reads text content from a URL.

        Parameters:
        url (str): The URL to read from.

        Returns:
        str: The text content of the URL.

        Raises:
        ValueError: If the file format is unsupported.
        requests.RequestException: For HTTP errors during URL reading.
        """
        try:
            response = requests.get(url)
            response.raise_for_status()

            if url.endswith('.pdf'):
                return self.extract_text_from_pdf(io.BytesIO(response.content))

            if url.endswith('.txt') or url.endswith('.md'):
                return response.text

            raise ValueError("Unsupported file format for URL - Use PDF, TXT or Markdown formats.")
        except requests.RequestException as e:
            raise Exception(f"Error reading from URL: {str(e)}")

# From tools/get_text.py
def extract_text_from_pdf(file_stream: io.BytesIO) -> str:
        """
        Extracts text content from a PDF file stream.

        Parameters:
        file_stream (io.BytesIO): The file stream of the PDF.

        Returns:
        str: The extracted text content of the PDF.

        Raises:
        Exception: For errors during PDF text extraction.
        """
        try:
            text = ""
            reader = pypdf.PdfReader(file_stream)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
            return text.strip()
        except Exception as e:
            raise Exception(f"Error extracting text from PDF: {str(e)}")


# From tools/intelligent_chunk.py
def intelligent_chunk(text, chunk_size):
    """
    Intelligently chunk a given text into smaller segments based on sentence structure.

    This function uses spaCy to tokenize the text into sentences and then groups these
    sentences into chunks of a specified size, with a 2-sentence overlap between chunks.

    Args:
        text (str): The input text to be chunked.
        chunk_size (int): An integer (0-3) representing the desired chunk size.
            0: Very small (5 sentences)
            1: Small (13 sentences)
            2: Medium (34 sentences)
            3: Large (55 sentences)

    Returns:
        list: A list of text chunks, where each chunk is a string.

    Raises:
        ValueError: If the chunk_size is not in the range 0-3, or if the input text is not a string.
        ImportError: If spaCy is not installed or the English model is not available.
        Exception: For any other unexpected errors during execution.
    """
    # Define the number of sentences per chunk based on the chunk_size
    sentences_per_chunk = {
        0: 5,
        1: 13,
        2: 34,
        3: 55
    }
    
    if not isinstance(text, str):
        raise ValueError("Input text must be a string")
    
    if chunk_size not in sentences_per_chunk:
        raise ValueError("chunk_size must be an integer between 0 and 3")

    try:
        # Load the spacy model
        nlp = spacy.blank('en')
        nlp.add_pipe('sentencizer', config={"punct_chars": None})
        nlp.max_length = 3000000  # Increase the max_length limit to accommodate large texts
        
        # Tokenize the text into sentences using spacy
        doc = nlp(str(text))
        sentences = [sent.text for sent in doc.sents]
        
        # Determine the number of sentences per chunk based on the input chunk_size
        num_sentences = sentences_per_chunk[chunk_size]
        
        # Group the sentences into chunks with a 2-sentence overlap
        chunks = []
        i = 0
        while i < len(sentences):
            chunk = '\n'.join(sentences[i:i + num_sentences])
            chunks.append(chunk)
            i += num_sentences - 2  # Move the index forward by (num_sentences - 2) to create the overlap
        
        return chunks

    except ImportError:
        raise ImportError("spaCy is not installed or the English model is not available. Please install spaCy and download the English model.")
    except Exception as e:
        raise Exception(f"An error occurred while chunking the text: {str(e)}")


# From tools/write_file.py
class WriteFile:
    """
    A class for writing content to files and managing directories.

    This class provides methods to ensure folders exist, write content to files,
    and generate messages about the file creation process.
    """

    def __init__(self):
        """
        Initialize a WriteFile object.

        This method currently doesn't set up any attributes but is included for future extensibility.
        """
        pass

    @staticmethod
    def ensure_folder_exists(folder):
        """
        Ensure the folder exists, or attempt to create it.

        Args:
            folder (str): The path to the folder.

        Returns:
            tuple: A tuple containing (success, error_message).
                   success (bool): True if the folder exists or was created successfully, False otherwise.
                   error_message (str or None): An error message if the operation failed, None otherwise.

        Raises:
            ValueError: If the folder path is not a string or is empty.
        """
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")

        if not os.path.exists(folder):
            try:
                os.makedirs(folder)
            except PermissionError:
                return False, f"Permission denied: Unable to create the folder {folder}."
            except OSError as e:
                return False, f"An error occurred while creating the folder: {str(e)}"
        return True, None

    @staticmethod
    def write_to_file(folder, file, text, mode):
        """
        Write the given text to the specified file.

        Args:
            folder (str): The path to the folder containing the file.
            file (str): The name of the file.
            text (str): The content to write to the file.
            mode (str): The mode in which to open the file ('w' for write, 'a' for append).

        Returns:
            tuple: A tuple containing (success, message).
                   success (bool): True if the write operation was successful, False otherwise.
                   message (str): A success or error message.

        Raises:
            ValueError: If any of the input parameters are invalid.
        """
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")
        if not isinstance(file, str) or not file.strip():
            raise ValueError("File name must be a non-empty string")
        if not isinstance(text, str):
            raise ValueError("Text must be a string")
        if mode not in ['w', 'a']:
            raise ValueError("Mode must be either 'w' or 'a'")

        try:
            with open(os.path.join(folder, file), mode, encoding="utf-8") as f:
                f.write(text)
            return True, "Successfully wrote to the file."
        except PermissionError:
            return False, f"Permission denied: Unable to write to the file {file}."
        except OSError as e:
            return False, f"An error occurred while writing to the file: {str(e)}"

    @staticmethod
    def generate_message(file, folder, text):
        """
        Generate a message with a preview of the file's content.

        Args:
            file (str): The name of the file.
            folder (str): The path to the folder containing the file.
            text (str): The content of the file.

        Returns:
            str: A formatted message containing file information and a content preview.

        Raises:
            ValueError: If any of the input parameters are invalid.
        """
        if not isinstance(file, str) or not file.strip():
            raise ValueError("File name must be a non-empty string")
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")
        if not isinstance(text, str):
            raise ValueError("Text must be a string")

        content_preview = "\n".join(text.splitlines()[:12])
        if len(text.splitlines()) > 10:
            content_preview += "\n... (more content below)"
        return f"The {file} has successfully been created in '{folder}':\n\n'{content_preview}'"

    def write_file(self, folder, file, text, mode='a'):
        """
        Write content to a file, ensuring the folder exists.

        This method combines the functionality of ensure_folder_exists, write_to_file,
        and generate_message to provide a complete file writing process.

        Args:
            folder (str): The path to the folder containing the file.
            file (str): The name of the file.
            text (str): The content to write to the file.
            mode (str, optional): The mode in which to open the file ('w' for write, 'a' for append).
                                  Defaults to 'a'.

        Returns:
            str: A message indicating the result of the file writing process.

        Raises:
            ValueError: If any of the input parameters are invalid.
        """
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")
        if not isinstance(file, str) or not file.strip():
            raise ValueError("File name must be a non-empty string")
        if not isinstance(text, str):
            raise ValueError("Text must be a string")
        if mode not in ['w', 'a']:
            raise ValueError("Mode must be either 'w' or 'a'")

        success, message = self.ensure_folder_exists(folder)
        if not success:
            return message

        success, message = self.write_to_file(folder, file, text, mode)
        if not success:
            return message

        return self.generate_message(file, folder, text)

# From tools/write_file.py
def ensure_folder_exists(folder):
        """
        Ensure the folder exists, or attempt to create it.

        Args:
            folder (str): The path to the folder.

        Returns:
            tuple: A tuple containing (success, error_message).
                   success (bool): True if the folder exists or was created successfully, False otherwise.
                   error_message (str or None): An error message if the operation failed, None otherwise.

        Raises:
            ValueError: If the folder path is not a string or is empty.
        """
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")

        if not os.path.exists(folder):
            try:
                os.makedirs(folder)
            except PermissionError:
                return False, f"Permission denied: Unable to create the folder {folder}."
            except OSError as e:
                return False, f"An error occurred while creating the folder: {str(e)}"
        return True, None

# From tools/write_file.py
def generate_message(file, folder, text):
        """
        Generate a message with a preview of the file's content.

        Args:
            file (str): The name of the file.
            folder (str): The path to the folder containing the file.
            text (str): The content of the file.

        Returns:
            str: A formatted message containing file information and a content preview.

        Raises:
            ValueError: If any of the input parameters are invalid.
        """
        if not isinstance(file, str) or not file.strip():
            raise ValueError("File name must be a non-empty string")
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")
        if not isinstance(text, str):
            raise ValueError("Text must be a string")

        content_preview = "\n".join(text.splitlines()[:12])
        if len(text.splitlines()) > 10:
            content_preview += "\n... (more content below)"
        return f"The {file} has successfully been created in '{folder}':\n\n'{content_preview}'"

# From tools/write_file.py
def write_file(self, folder, file, text, mode='a'):
        """
        Write content to a file, ensuring the folder exists.

        This method combines the functionality of ensure_folder_exists, write_to_file,
        and generate_message to provide a complete file writing process.

        Args:
            folder (str): The path to the folder containing the file.
            file (str): The name of the file.
            text (str): The content to write to the file.
            mode (str, optional): The mode in which to open the file ('w' for write, 'a' for append).
                                  Defaults to 'a'.

        Returns:
            str: A message indicating the result of the file writing process.

        Raises:
            ValueError: If any of the input parameters are invalid.
        """
        if not isinstance(folder, str) or not folder.strip():
            raise ValueError("Folder path must be a non-empty string")
        if not isinstance(file, str) or not file.strip():
            raise ValueError("File name must be a non-empty string")
        if not isinstance(text, str):
            raise ValueError("Text must be a string")
        if mode not in ['w', 'a']:
            raise ValueError("Mode must be either 'w' or 'a'")

        success, message = self.ensure_folder_exists(folder)
        if not success:
            return message

        success, message = self.write_to_file(folder, file, text, mode)
        if not success:
            return message

        return self.generate_message(file, folder, text)


# From tools/command_executor.py
class CommandExecutor:
    """
    A class for executing shell commands safely.

    This class provides a method to execute shell commands and return their output.
    It includes error handling and environment variable support.

    Attributes:
        None

    Raises:
        Exception: If any method in this class fails to execute properly.
    """

    def __init__(self):
        """
        Initialize the CommandExecutor.

        This method currently doesn't set up any attributes but is included for future extensibility.
        """
        pass

    def execute(self, cmd: str, env_vars: dict = None) -> str:
        """
        Execute a command and return its output.

        This method executes the given command in a shell environment and returns the output.
        It supports command chaining and custom environment variables.

        Args:
            cmd (str): Command to be executed.
            env_vars (dict, optional): Dictionary containing environment variables to be set for the command.

        Returns:
            str: Command output as a string.

        Raises:
            ValueError: If the input command is not a string or is empty.
            subprocess.CalledProcessError: If the command execution fails.
            Exception: For any other unexpected errors during execution.
        """
        if not isinstance(cmd, str) or not cmd.strip():
            raise ValueError("Command must be a non-empty string")

        if env_vars is not None and not isinstance(env_vars, dict):
            raise ValueError("env_vars must be a dictionary or None")

        # Setting the shell=True allows for command chaining (&, &&, ||, ;)
        # But also be cautious about its usage with untrusted input due to security reasons.
        environment = dict(os.environ, **(env_vars or {}))

        try:
            output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, env=environment)
            return output.decode('utf-8')
        except subprocess.CalledProcessError as e:
            # In case of an error, return the error output.
            raise Exception(f"Command execution failed: {e.output.decode('utf-8')}") from e
        except Exception as e:
            raise Exception(f"An unexpected error occurred: {str(e)}") from e

# From tools/command_executor.py
def execute(self, cmd: str, env_vars: dict = None) -> str:
        """
        Execute a command and return its output.

        This method executes the given command in a shell environment and returns the output.
        It supports command chaining and custom environment variables.

        Args:
            cmd (str): Command to be executed.
            env_vars (dict, optional): Dictionary containing environment variables to be set for the command.

        Returns:
            str: Command output as a string.

        Raises:
            ValueError: If the input command is not a string or is empty.
            subprocess.CalledProcessError: If the command execution fails.
            Exception: For any other unexpected errors during execution.
        """
        if not isinstance(cmd, str) or not cmd.strip():
            raise ValueError("Command must be a non-empty string")

        if env_vars is not None and not isinstance(env_vars, dict):
            raise ValueError("env_vars must be a dictionary or None")

        # Setting the shell=True allows for command chaining (&, &&, ||, ;)
        # But also be cautious about its usage with untrusted input due to security reasons.
        environment = dict(os.environ, **(env_vars or {}))

        try:
            output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, env=environment)
            return output.decode('utf-8')
        except subprocess.CalledProcessError as e:
            # In case of an error, return the error output.
            raise Exception(f"Command execution failed: {e.output.decode('utf-8')}") from e
        except Exception as e:
            raise Exception(f"An unexpected error occurred: {str(e)}") from e

from semantic_text_splitter import TextSplitter

# From tools/semantic_chunk.py
class Chunk:
    """
    Represents a chunk of text.

    Attributes:
        content (str): The content of the chunk.
    """

    def __init__(self, content):
        """
        Initialize a Chunk object.

        Args:
            content (str): The content of the chunk.
        """
        self.content = content

# From tools/semantic_chunk.py
def semantic_chunk(text, min_length=200, max_length=2000):
    """
    Split text into semantic chunks using the semantic_text_splitter library.

    This function splits the input text into chunks based on semantic meaning,
    with each chunk having a length between min_length and max_length.

    Args:
        text (str): The input text to be chunked.
        min_length (int, optional): The minimum length of each chunk. Defaults to 200.
        max_length (int, optional): The maximum length of each chunk. Defaults to 2000.

    Returns:
        list: A list of Chunk objects, each containing a portion of the input text.

    Raises:
        ValueError: If the input text is not a string, or if min_length or max_length are invalid.
        ImportError: If the semantic_text_splitter library is not installed.
        Exception: For any other unexpected errors during execution.
    """
    if not isinstance(text, str):
        raise ValueError("Input text must be a string")
    
    if not isinstance(min_length, int) or not isinstance(max_length, int):
        raise ValueError("min_length and max_length must be integers")
    
    if min_length <= 0 or max_length <= 0 or min_length >= max_length:
        raise ValueError("Invalid min_length or max_length values")

    try:
        splitter = TextSplitter((min_length, max_length), trim=False)

        chunks = splitter.chunks(text)
        result = []
        for chunk in chunks:
            # Preserve intentional line breaks while removing extra whitespace
            cleaned_chunk = '\n'.join(' '.join(line.split()) for line in chunk.split('\n'))
            chunk_obj = Chunk(content=cleaned_chunk)
            result.append(chunk_obj)

        return result

    except ImportError:
        raise ImportError("semantic_text_splitter library is not installed. Please install it to use this function.")
    except Exception as e:
        raise Exception(f"An error occurred while chunking the text: {str(e)}")


# From tools/brave_search.py
class BraveSearch:
    """
    A Python wrapper for the Brave Search API.

    This class provides methods to interact with the Brave Search API, allowing you to perform searches,
    retrieve search results, and access various features of the API.

    Attributes:
        base_url (str): The base URL for the Brave Search API.
        api_key (str): The API key for authentication.
        headers (dict): The headers to be used in API requests.

    Raises:
        ValueError: If the BRAVE_API_KEY environment variable is not set.
    """

    def __init__(self):
        self.base_url = 'https://api.search.brave.com'
        self.api_key = os.environ.get('BRAVE_API_KEY')
        if not self.api_key:
            raise ValueError("BRAVE_API_KEY environment variable is not set")
        self.headers = {
            'X-Subscription-Token': self.api_key,
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip'
        }

    def search(self, query, **kwargs):
        """
        Perform a search using the Brave Search API and parse the results.

        Args:
            query (str): The search query.
            **kwargs: Additional parameters to pass to the API.

        Returns:
            dict: Parsed search results containing titles, URLs, descriptions, and extra snippets.

        Raises:
            requests.RequestException: If the API request fails.
            ValueError: If the response cannot be parsed or is invalid.
        """
        try:
            params = {'q': query}
            params.update(kwargs)

            response = requests.get(self.base_url + '/res/v1/web/search', params=params, headers=self.headers)
            response.raise_for_status()
            results = response.json()

            # Parse the search results
            parsed_results = {'web_results': [], 'video_results': []}

            # Extract web search results
            if 'web' in results and 'results' in results['web']:
                for result in results['web']['results']:
                    parsed_results['web_results'].append({
                        'type': 'web_result',
                        'title': result.get('title', 'No data'),
                        'url': result.get('url', 'No data'),
                        'description': result.get('description', 'No data'),
                        'extra_snippets': result.get('extra_snippets', ["No data"])
                    })

            # Extract video search results
            if 'videos' in results and 'results' in results['videos']:
                for result in results['videos']['results']:
                    parsed_results['video_results'].append({
                        'type': 'video_result',
                        'title': result.get('title', 'No data'),
                        'url': result.get('url', 'No data'),
                        'description': result.get('description', 'No data')
                    })

            return parsed_results

        except requests.RequestException as e:
            raise Exception(f"API request failed: {str(e)}") from e
        except (KeyError, ValueError) as e:
            raise ValueError(f"Failed to parse API response: {str(e)}") from e

    def summarize(self, query, **kwargs):
        """
        Get an AI-generated summary for a query using the Summarizer API.

        Args:
            query (str): The search query.
            **kwargs: Additional parameters to pass to the API.

        Returns:
            dict: A dictionary containing the parsed summary information.

        Raises:
            requests.RequestException: If the API request fails.
            ValueError: If the response cannot be parsed or is invalid.
        """
        try:
            params = {'q': query, 'summary': 1}
            params.update(kwargs)

            response = requests.get(self.base_url + '/res/v1/web/search', params=params, headers=self.headers)
            response.raise_for_status()

            data = response.json()

            if data.get('type') == 'summarizer':
                summary_info = {
                    'status': data.get('status'),
                    'title': data.get('title'),
                    'summary': [msg.get('content') for msg in data.get('summary', [])],
                    'followups': data.get('followups', []),
                    'entities': data.get('entities_infos', {})
                }
                if 'enrichments' in data:
                    summary_info['enrichments'] = data['enrichments']
            elif data.get('type') == 'search':
                summary_info = {
                    'query': data.get('query', {}).get('original', ''),
                    'results': [],
                    'videos': []
                }

                # Handle web results
                for result in data.get('web', {}).get('results', []):
                    result_info = {
                        'type': 'web_result',
                        'title': result.get('title', ''),
                        'description': result.get('description', ''),
                        'url': result.get('url', '')
                    }
                    if 'extra_snippets' in result:
                        result_info['extra_snippets'] = result['extra_snippets']
                    summary_info['results'].append(result_info)

                # Handle video results
                for video in data.get('videos', {}).get('results', []):
                    video_info = {
                        'type': 'video_result',
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'url': video.get('url', ''),
                        'thumbnail': video.get('thumbnail', {}).get('src', '')
                    }
                    summary_info['videos'].append(video_info)
            else:
                raise ValueError(f"Unexpected response type: {data.get('type')}")

            return summary_info

        except requests.RequestException as e:
            raise Exception(f"API request failed: {str(e)}") from e
        except (KeyError, ValueError) as e:
            raise ValueError(f"Failed to parse API response: {str(e)}") from e

# From tools/brave_search.py
def search(self, query, **kwargs):
        """
        Perform a search using the Brave Search API and parse the results.

        Args:
            query (str): The search query.
            **kwargs: Additional parameters to pass to the API.

        Returns:
            dict: Parsed search results containing titles, URLs, descriptions, and extra snippets.

        Raises:
            requests.RequestException: If the API request fails.
            ValueError: If the response cannot be parsed or is invalid.
        """
        try:
            params = {'q': query}
            params.update(kwargs)

            response = requests.get(self.base_url + '/res/v1/web/search', params=params, headers=self.headers)
            response.raise_for_status()
            results = response.json()

            # Parse the search results
            parsed_results = {'web_results': [], 'video_results': []}

            # Extract web search results
            if 'web' in results and 'results' in results['web']:
                for result in results['web']['results']:
                    parsed_results['web_results'].append({
                        'type': 'web_result',
                        'title': result.get('title', 'No data'),
                        'url': result.get('url', 'No data'),
                        'description': result.get('description', 'No data'),
                        'extra_snippets': result.get('extra_snippets', ["No data"])
                    })

            # Extract video search results
            if 'videos' in results and 'results' in results['videos']:
                for result in results['videos']['results']:
                    parsed_results['video_results'].append({
                        'type': 'video_result',
                        'title': result.get('title', 'No data'),
                        'url': result.get('url', 'No data'),
                        'description': result.get('description', 'No data')
                    })

            return parsed_results

        except requests.RequestException as e:
            raise Exception(f"API request failed: {str(e)}") from e
        except (KeyError, ValueError) as e:
            raise ValueError(f"Failed to parse API response: {str(e)}") from e

# From tools/brave_search.py
def summarize(self, query, **kwargs):
        """
        Get an AI-generated summary for a query using the Summarizer API.

        Args:
            query (str): The search query.
            **kwargs: Additional parameters to pass to the API.

        Returns:
            dict: A dictionary containing the parsed summary information.

        Raises:
            requests.RequestException: If the API request fails.
            ValueError: If the response cannot be parsed or is invalid.
        """
        try:
            params = {'q': query, 'summary': 1}
            params.update(kwargs)

            response = requests.get(self.base_url + '/res/v1/web/search', params=params, headers=self.headers)
            response.raise_for_status()

            data = response.json()

            if data.get('type') == 'summarizer':
                summary_info = {
                    'status': data.get('status'),
                    'title': data.get('title'),
                    'summary': [msg.get('content') for msg in data.get('summary', [])],
                    'followups': data.get('followups', []),
                    'entities': data.get('entities_infos', {})
                }
                if 'enrichments' in data:
                    summary_info['enrichments'] = data['enrichments']
            elif data.get('type') == 'search':
                summary_info = {
                    'query': data.get('query', {}).get('original', ''),
                    'results': [],
                    'videos': []
                }

                # Handle web results
                for result in data.get('web', {}).get('results', []):
                    result_info = {
                        'type': 'web_result',
                        'title': result.get('title', ''),
                        'description': result.get('description', ''),
                        'url': result.get('url', '')
                    }
                    if 'extra_snippets' in result:
                        result_info['extra_snippets'] = result['extra_snippets']
                    summary_info['results'].append(result_info)

                # Handle video results
                for video in data.get('videos', {}).get('results', []):
                    video_info = {
                        'type': 'video_result',
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'url': video.get('url', ''),
                        'thumbnail': video.get('thumbnail', {}).get('src', '')
                    }
                    summary_info['videos'].append(video_info)
            else:
                raise ValueError(f"Unexpected response type: {data.get('type')}")

            return summary_info

        except requests.RequestException as e:
            raise Exception(f"API request failed: {str(e)}") from e
        except (KeyError, ValueError) as e:
            raise ValueError(f"Failed to parse API response: {str(e)}") from e


# From tools/user_input.py
class UserInput:
    def __init__(self, default_input=None):
        self.default_input = default_input

    def get_input(self, prompt: str) -> str:
        """
        Prompt the user for input and return the entered value.

        Parameters:
        - prompt: The message displayed to the user.

        Returns:
        - User input as a string.
        """
        response = input(prompt)
        if not response and self.default_input is not None:
            return self.default_input
        return response

    def get_yes_no(self, prompt: str, default="y") -> bool:
        """
        Prompt the user for a yes or no answer.

        Parameters:
        - prompt: The question to display to the user.

        Returns:
        - True for yes and False for no.
        """
        full_prompt = f"{prompt} [y/n, default: {default}] "
        while True:
            response = self.get_input(full_prompt).lower()
            if not response:
                response = default
            if response in ['y', 'yes']:
                return True
            elif response in ['n', 'no']:
                return False
            print("Please enter 'y' or 'n'.")

    def get_choice(self, prompt: str, choices: list) -> str:
        """
        Prompt the user to choose from a list of choices.

        Parameters:
        - prompt: The question to display to the user.
        - choices: List of possible choices.

        Returns:
        - The choice selected by the user.
        """
        while True:
            response = self.get_input(prompt + " " + "/".join(choices) + ": ").lower()
            if response in choices:
                return response
            print(f"Please select one of the following: {', '.join(choices)}")

# From tools/user_input.py
def get_input(self, prompt: str) -> str:
        """
        Prompt the user for input and return the entered value.

        Parameters:
        - prompt: The message displayed to the user.

        Returns:
        - User input as a string.
        """
        response = input(prompt)
        if not response and self.default_input is not None:
            return self.default_input
        return response

# From tools/user_input.py
def get_yes_no(self, prompt: str, default="y") -> bool:
        """
        Prompt the user for a yes or no answer.

        Parameters:
        - prompt: The question to display to the user.

        Returns:
        - True for yes and False for no.
        """
        full_prompt = f"{prompt} [y/n, default: {default}] "
        while True:
            response = self.get_input(full_prompt).lower()
            if not response:
                response = default
            if response in ['y', 'yes']:
                return True
            elif response in ['n', 'no']:
                return False
            print("Please enter 'y' or 'n'.")

# From tools/user_input.py
def get_choice(self, prompt: str, choices: list) -> str:
        """
        Prompt the user to choose from a list of choices.

        Parameters:
        - prompt: The question to display to the user.
        - choices: List of possible choices.

        Returns:
        - The choice selected by the user.
        """
        while True:
            response = self.get_input(prompt + " " + "/".join(choices) + ": ").lower()
            if response in choices:
                return response
            print(f"Please select one of the following: {', '.join(choices)}")

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# From tools/google_search.py
def google_search(query, number_result=5):
    """
    Perform a Google search using the Custom Search API.

    Args:
        query (str): The search query.
        number_result (int, optional): The number of search results to return. Defaults to 5.

    Returns:
        str: A formatted string containing the search results or an error message.

    Raises:
        ValueError: If the GOOGLE_API_KEY or SEARCH_ENGINE_ID environment variables are not set.
        HttpError: If there's an error with the API request.
        Exception: For any other unexpected errors during execution.
    """
    google_api_key = os.getenv('GOOGLE_API_KEY')
    search_engine_id = os.getenv('SEARCH_ENGINE_ID')

    if not google_api_key:
        raise ValueError("GOOGLE_API_KEY environment variable is not set")
    if not search_engine_id:
        raise ValueError("SEARCH_ENGINE_ID environment variable is not set")

    try:
        # Initialize the Custom Search API service
        service = build("customsearch", "v1", developerKey=google_api_key)

        # Send the search query and retrieve the results
        result = service.cse().list(q=query, cx=search_engine_id, num=number_result).execute()

        # Extract the search result items from the response
        search_results = result.get("items", [])

        # Create a list of only the URLs from the search results
        search_results_links = [(item["link"], item["snippet"]) for item in search_results]

        return parse_tool_results(search_results_links)

    except HttpError as e:
        # Handle errors in the API call
        error_details = json.loads(e.content.decode())
        error_code = error_details.get("error", {}).get("code")
        error_message = error_details.get("error", {}).get("message", "")

        # Check if the error is related to an invalid or missing API key
        if error_code == 403 and "invalid API key" in error_message:
            return "Error: The provided Google API key is invalid or missing."
        else:
            return f"Error: {str(e)}"

    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"

# From tools/google_search.py
def parse_tool_results(tool_result):
    """
    Parse and format the search results.

    Args:
        tool_result (list or str): The search results to parse.

    Returns:
        str: A formatted string containing the parsed search results.
    """
    if isinstance(tool_result, list):
        # Format each search result
        formatted_results = [f"URL: {url}\nDescription: {desc}\n---" for url, desc in tool_result]
        # Join all results into a single string
        final_output = "\n".join(formatted_results)
    else:
        final_output = tool_result

    return final_output

from agentforge.utils.tool_utils import ToolUtils

# From tools/python_function.py
class PythonFunction:
    """
    A class for executing Python functions dynamically.

    This class provides a method to execute Python functions specified by name,
    using the ToolUtils class for dynamic execution.

    Attributes:
        None

    Raises:
        Exception: If any method in this class fails to execute properly.
    """

    def __init__(self):
        """
        Initialize the PythonFunction object.

        This method currently doesn't set up any attributes but is included for future extensibility.
        """
        pass

    @staticmethod
    def execute_function(function_name, payload):
        """
        Execute a Python function specified by name.

        This method uses the ToolUtils class to dynamically execute a Python function
        specified by its name. The function should be available in the current Python environment.

        Args:
            function_name (str): The name of the Python function to execute.
            payload (dict): A dictionary containing the arguments to pass to the function.

        Returns:
            Any: The result of the executed function.

        Raises:
            ValueError: If the function_name is not a string or is empty.
            TypeError: If the payload is not a dictionary.
            AttributeError: If the specified function is not found.
            Exception: For any other unexpected errors during execution.
        """
        if not isinstance(function_name, str) or not function_name.strip():
            raise ValueError("function_name must be a non-empty string")

        if not isinstance(payload, dict):
            raise TypeError("payload must be a dictionary")

        try:
            tool = {'Script': function_name, 'Command': 'execute'}
            result = ToolUtils().dynamic_tool(tool, payload)
            return result
        except AttributeError as e:
            raise AttributeError(f"Function '{function_name}' not found: {str(e)}")
        except Exception as e:
            raise Exception(f"An error occurred while executing the function: {str(e)}")

# From tools/python_function.py
def execute_function(function_name, payload):
        """
        Execute a Python function specified by name.

        This method uses the ToolUtils class to dynamically execute a Python function
        specified by its name. The function should be available in the current Python environment.

        Args:
            function_name (str): The name of the Python function to execute.
            payload (dict): A dictionary containing the arguments to pass to the function.

        Returns:
            Any: The result of the executed function.

        Raises:
            ValueError: If the function_name is not a string or is empty.
            TypeError: If the payload is not a dictionary.
            AttributeError: If the specified function is not found.
            Exception: For any other unexpected errors during execution.
        """
        if not isinstance(function_name, str) or not function_name.strip():
            raise ValueError("function_name must be a non-empty string")

        if not isinstance(payload, dict):
            raise TypeError("payload must be a dictionary")

        try:
            tool = {'Script': function_name, 'Command': 'execute'}
            result = ToolUtils().dynamic_tool(tool, payload)
            return result
        except AttributeError as e:
            raise AttributeError(f"Function '{function_name}' not found: {str(e)}")
        except Exception as e:
            raise Exception(f"An error occurred while executing the function: {str(e)}")

import unicodedata

# From tools/clean_string.py
class Strip:
    """
    A utility class for cleaning and sanitizing strings.

    This class provides static methods to strip invalid characters from strings,
    making them suitable for use in specific contexts like YAML.

    Raises:
        Exception: If any of the methods fail to process the input string.
    """

    @staticmethod
    def strip_invalid_chars(text):
        """
        Strips invalid characters from a string, allowing only characters suitable for YAML.

        Args:
            text (str): The input string.

        Returns:
            str: The string with invalid characters replaced with underscores.

        Raises:
            TypeError: If the input is not a string.
            Exception: If the string cleaning process fails.
        """
        if not isinstance(text, str):
            raise TypeError("Input must be a string")

        try:
            allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!@#$%^&*()[]{};'" + chr(10) + chr(13) + " ")
            clean_text = []
            for char in text:
                if char in allowed_chars:
                    clean_text.append(char)
                else:
                    clean_text.append('_')  # Replace invalid characters with underscores
            return ''.join(clean_text)
        except Exception as e:
            raise Exception(f"Failed to strip invalid characters: {str(e)}") from e

    @staticmethod
    def normalize_unicode(text):
        """
        Normalizes Unicode characters in a string to their closest ASCII representation.

        Args:
            text (str): The input string containing Unicode characters.

        Returns:
            str: The normalized string with Unicode characters converted to ASCII.

        Raises:
            TypeError: If the input is not a string.
            Exception: If the Unicode normalization process fails.
        """
        if not isinstance(text, str):
            raise TypeError("Input must be a string")

        try:
            return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')
        except Exception as e:
            raise Exception(f"Failed to normalize Unicode characters: {str(e)}") from e

    @staticmethod
    def remove_control_characters(text):
        """
        Removes control characters from a string.

        Args:
            text (str): The input string.

        Returns:
            str: The string with control characters removed.

        Raises:
            TypeError: If the input is not a string.
            Exception: If the control character removal process fails.
        """
        if not isinstance(text, str):
            raise TypeError("Input must be a string")

        try:
            return ''.join(char for char in text if unicodedata.category(char)[0] != 'C')
        except Exception as e:
            raise Exception(f"Failed to remove control characters: {str(e)}") from e

# From tools/clean_string.py
def strip_invalid_chars(text):
        """
        Strips invalid characters from a string, allowing only characters suitable for YAML.

        Args:
            text (str): The input string.

        Returns:
            str: The string with invalid characters replaced with underscores.

        Raises:
            TypeError: If the input is not a string.
            Exception: If the string cleaning process fails.
        """
        if not isinstance(text, str):
            raise TypeError("Input must be a string")

        try:
            allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!@#$%^&*()[]{};'" + chr(10) + chr(13) + " ")
            clean_text = []
            for char in text:
                if char in allowed_chars:
                    clean_text.append(char)
                else:
                    clean_text.append('_')  # Replace invalid characters with underscores
            return ''.join(clean_text)
        except Exception as e:
            raise Exception(f"Failed to strip invalid characters: {str(e)}") from e

# From tools/clean_string.py
def normalize_unicode(text):
        """
        Normalizes Unicode characters in a string to their closest ASCII representation.

        Args:
            text (str): The input string containing Unicode characters.

        Returns:
            str: The normalized string with Unicode characters converted to ASCII.

        Raises:
            TypeError: If the input is not a string.
            Exception: If the Unicode normalization process fails.
        """
        if not isinstance(text, str):
            raise TypeError("Input must be a string")

        try:
            return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')
        except Exception as e:
            raise Exception(f"Failed to normalize Unicode characters: {str(e)}") from e


# From config_structs/trail_structs.py
class ThoughtTrailEntry:
    """A single entry in the thought flow trail."""
    agent_id: str
    output: Any
    timestamp: Optional[datetime] = None
    unix_timestamp: Optional[float] = None
    notes: Optional[str] = None
    execution_order: Optional[int] = None
    error: Optional[Union[str, Exception]] = None
    
    def __post_init__(self):
        """Auto-generate timestamps if not provided."""
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.unix_timestamp is None:
            self.unix_timestamp = self.timestamp.timestamp()

from scipy.ndimage import value_indices
from agentforge.config import Config

# From storage/chroma_storage.py
class ChromaStorage:
    """
    A utility class for managing interactions with ChromaDB,
    using a storage_id to define each instance.

    Provides a thread-safe registry of instances keyed by storage_id.

    Usage:
        storage = ChromaStorage.get_or_create(storage_id="my_storage_id")

    To reset the registry (e.g., between tests):
        ChromaStorage.clear_registry()

    To debug instance resolution:
        storage.describe_instance()
    """
    _registry = {}
    _registry_lock = threading.Lock()

    _instance = None
    client = None
    collection = None
    db_path = None
    db_embed = None
    embedding = None

    ##########################################################
    # Section 3: Class Methods
    ##########################################################

    @classmethod
    def get_or_create(cls, storage_id: str):
        """
        Retrieve a shared ChromaStorage instance for the given storage_id.
        """
        if not storage_id:
            raise ValueError("ChromaStorage.get_or_create requires a non-empty storage_id.")
        with cls._registry_lock:
            if storage_id not in cls._registry:
                cls._registry[storage_id] = cls(storage_id=storage_id)
            return cls._registry[storage_id]

    @classmethod
    def clear_registry(cls):
        """
        Clear the ChromaStorage registry. Useful for test isolation or resetting state.
        """
        with cls._registry_lock:
            cls._registry.clear()

    def describe_instance(self):
        """
        Returns a dictionary describing the storage_id and path for this instance.
        Useful for debugging and testing.
        """
        db_path, db_embed = self.chromadb_settings()
        return {
            'storage_id': self.storage_id,
            'db_path': db_path,
            'db_embed': db_embed,
        }

    ##########################################################
    # Section 4: Initialization
    ##########################################################

    def __init__(self, storage_id: str):
        """
        Initialize a ChromaStorage instance with a storage_id context.
        """
        self.config = Config()
        self.storage_id = storage_id
        self.init_embeddings()
        self.init_storage()

    def init_storage(self):
        """
        Initializes the storage client, either as a persistent client with a specified database path or as an
        ephemeral client based on the configuration.

        Raises:
            Exception: For any errors that occur during the initialization of storage.
        """
        try:
            if self.client is None:
                if self.db_path:
                    self.client = chromadb.PersistentClient(path=self.db_path, settings=Settings(allow_reset=True))
                else:
                    self.client = chromadb.EphemeralClient()

            if getattr(self.config.settings.storage, 'fresh_start', False):
                self.reset_storage()
        except Exception as e:
            logger.error(f"[init_storage] Error initializing storage: {e}")
            raise

    def init_embeddings(self):
        """
        Initializes the embedding function based on the configuration, supporting multiple embedding backends.

        Raises:
            KeyError: If a required environment variable or setting is missing.
            Exception: For any errors that occur during the initialization of embeddings.
        """
        try:
            self.db_path, self.db_embed = self.chromadb_settings()


            # Initialize embedding based on the specified backend in the configuration
            if not self.db_embed:
                self.embedding = embedding_functions.DefaultEmbeddingFunction()
                return
            
            if self.db_embed == 'text-embedding-ada-002':
                openai_api_key = os.getenv('OPENAI_API_KEY')
                self.embedding = embedding_functions.OpenAIEmbeddingFunction(
                    api_key=openai_api_key,
                    model_name=self.db_embed
                )
                return
            
            if self.db_embed:
                self.embedding = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=self.db_embed)
                return

            raise ValueError(f"Unsupported embedding backend: {self.db_embed}")
        except KeyError as e:
            logger.error(f"[init_embeddings] Missing environment variable or setting: {e}")
            raise
        except Exception as e:
            logger.error(f"[init_embeddings] Error initializing embeddings: {e}")
            raise

    ##########################################################
    # Section 5: Configuration
    ##########################################################

    def chromadb_settings(self):
        """
        Retrieves the ChromaDB settings from the configuration and resolves the database path
        for a given storage_id.

        Returns:
            tuple: (db_path, db_embed)
        """
        storage_settings = self.config.settings.storage
        db_path_setting = storage_settings['options'].get('persist_directory', None)
        selected_embed = storage_settings['embedding'].get('selected', None)
        db_embed = storage_settings['embedding_library'].get(selected_embed, None)
        if not db_path_setting:
            raise ValueError("ChromaStorage: persist_directory must be set in the settings YAML.")
        db_path = str(self.config.project_root / db_path_setting / self.storage_id)
        return db_path, db_embed

    def reset_storage(self):
        """
        Resets the entire storage, removing all collections and their data.

        This method should be used with caution as it will permanently delete all data within the storage.
        """

        self.client.reset()

    def return_embedding(self, text_to_embed: str):
        """
        Generates an embedding for the given text using the configured embedding function.

        Parameters:
            text_to_embed (str): The text to generate an embedding for.

        Returns:
            list: A list containing the generated embedding vector for the given text.
        """
        return self.embedding([text_to_embed])

    ##########################################################
    # Section 6: Inner Methods
    ##########################################################

    def _calculate_num_results(self, num_results, collection_name):
        self.select_collection(collection_name)
        max_result_count = self.collection.count()
        return max_result_count if num_results == 0 else min(num_results, max_result_count)

    def _prepare_query_params(self, query, filter_condition, include, embeddings, num_results, collection_name):
        if not query and not embeddings:
            logger.error(f"Error: No query nor embeddings were provided!  ")
            return {}

        query_params = {"n_results": self._calculate_num_results(num_results, collection_name)}
        if query_params["n_results"] <= 0:
            logger.info(f"No Results Found in '{collection_name}' collection!")
            return {}

        query_params["include"] = include if include else ["documents", "metadatas", "distances"]

        if filter_condition:
            query_params["where"] = filter_condition

        if query:
            query_params["query_texts"] = [query] if isinstance(query, str) else query

        if embeddings:
            query_params["query_embeddings"] = embeddings

        return query_params

    ##########################################################
    # Section 7: DB Methods
    ##########################################################

    def collection_list(self):
        """
        Lists all collections currently in the storage.

        Returns:
            list: A list of collection names.
        """
        return self.client.list_collections()

    def select_collection(self, collection_name: str):
        """
        Selects (or creates if not existent) a collection within the storage by name.

        Parameters:
            collection_name (str): The name of the collection to select or create.

        Raises:
            ValueError: If there's an error in getting or creating the collection.
        """
        try:
            collection_name = validate_collection_name(collection_name)
            self.collection = self.client.get_or_create_collection(name=collection_name,
                                                                   embedding_function=self.embedding,
                                                                   metadata={"hnsw:space": "cosine"})
        except Exception as e:
            raise ValueError(f"\n\nError getting or creating collection. Error: {e}")

    def delete_collection(self, collection_name: str):
        """
        Deletes a collection from the storage by its name.

        Parameters:
            collection_name (str): The name of the collection to delete.
        """
        try:
            self.client.delete_collection(collection_name)
        except Exception as e:
            print("\n\nError deleting collection: ", e)

    def count_collection(self, collection_name: str):
        """
        Counts the number of documents in a specified collection.

        Parameters:
            collection_name (str): The name of the collection to count documents in.

        Returns:
            int: The number of documents in the specified collection.
        """
        self.select_collection(collection_name)
        return self.collection.count()

    def peek(self, collection_name: str):
        """
        Peeks into a collection to retrieve a brief overview of its contents.

        Parameters:
            collection_name (str): The name of the collection to peek into.

        Returns:
            dict or None: A dictionary containing a brief overview of the collection's contents or None if an error occurs.
        """
        try:
            self.select_collection(collection_name)

            max_result_count = self.collection.count()
            num_results = min(10, max_result_count)

            if num_results > 0:
                result = self.collection.peek()
            else:
                result = {'documents': "No Results!"}

            return result
        except Exception as e:
            logger.error(f"Error peeking collection: {e}")
            return None

    def load_collection(self, collection_name: str, include: dict = None, where: dict = None, where_doc: dict = None):
        """
        Loads data from a specified collection based on provided filters.
        Parameters:
            collection_name (str): The name of the collection to load from.
            include(dict, optional): Specify which data to return. Will return all results if no filters are specified.
            where (dict, optional): Filter to apply in metadata. Will return documents and metadata by default.
            where_doc (dict, optional): Filter to apply in document. Not applied if not specified.
        Returns:
            list or None: The data loaded from the collection, or None if an error occurs.
        """
        params = {}

        # Defaulting 'include' if None
        if include is None:
            include = ["documents", "metadatas"]

        params.update(include=include)

        if where is not None:
            params.update(where=where)

        if where_doc is not None:
            params.update(where_document=where_doc)

        try:
            self.select_collection(collection_name)
            data = self.collection.get(**params)
            logger.debug(
                f"\nCollection: {collection_name}"
                f"\nData: {data}",
            )
        except Exception as e:
            print(f"\n\nError loading data: {e}")
            data = []
        return data

    ##########################################################
    # Section 7: Storage for Memory
    ##########################################################

    def get_next_sequential_id(self, collection_name: str) -> int:
        max_id_entry = self.search_metadata_min_max(collection_name, 'id', 'max')
        if max_id_entry is None or "target" not in max_id_entry:
            return 1
        else:
            return max_id_entry["target"] + 1 if max_id_entry["target"] is not None else 1

    def apply_sequential_ids(self, collection_name: str, data: list, metadata: list[dict]) -> tuple[list, list[dict]]:
        # Get the current max ID once
        current_max = self.get_next_sequential_id(collection_name) - 1
        new_ids = []
        for i, _ in enumerate(data):
            next_id = current_max + i + 1
            new_ids.append(str(next_id))
            metadata[i]['id'] = next_id
        return new_ids, metadata

    def save_to_storage(self, collection_name: str, data: list, ids: Optional[list] = None, metadata: Optional[list[dict]] = None):
        """
        Saves data to memory, creating or updating documents in a specified collection.

        Parameters:
            collection_name (str): The name of the collection to save to. Will be created if it doesn't exist.
            data (list): The documents to be saved. Can be a single document as a string or a list
             of documents. If a single string is provided, it is converted into a list with one element.
            ids (list): The IDs corresponding to the documents. If not provided, IDs will be generated automatically.
            metadata (list[dict], optional): A list of dictionaries, each representing associated metadata for
                the corresponding document in `data`. If not provided, empty dictionaries are used for each document.

        Raises:
            ValueError: If the lengths of `data`, `ids`, and `metadata` do not match, or if other errors occur
            during the save operation.
        """
        try:
            data = [data] if isinstance(data, str) else data
            metadata = [{} for _ in data] if metadata is None else metadata

            if ids is None:
                ids, metadata = self.apply_sequential_ids(collection_name,data, metadata)
            
            validate_inputs(data, ids, metadata)
            apply_uuids(metadata, self.config.settings.storage)
            apply_unix_timestamps(metadata, self.config.settings.storage)
            apply_iso_timestamps(metadata, self.config.settings.storage)

            self.select_collection(collection_name)
            self.collection.upsert(
                documents=data,
                metadatas=metadata,
                ids=ids
            )
        except Exception as e:
            raise ValueError(f"[ChromaStorage][save_to_storage] Error saving to storage. Error: {e}\n\nData:\n{data}")

    def query_storage(self, collection_name: str, query: Optional[Union[str, list]] = None,
                      filter_condition: Optional[dict] = None, include: Optional[list] = None,
                      embeddings: Optional[list] = None, num_results: int = 1):
        """
        Queries storage for documents matching a query within a specified collection.

        Parameters:
            collection_name (str): The name of the collection to query.
            query (Optional[Union[str, list]]): The query text or a list of query texts.
                If `None`, `embeddings` must be provided.
            filter_condition (Optional[dict]): A dictionary specifying any filters to apply to the query.
            include (Optional[list]): A list specifying which elements to include in the result
                (e.g., ["documents", "metadatas", "distances"]). Defaults to all elements if `None`.
            embeddings (Optional[list]): Query embeddings used if `query` is `None`.
                Must be provided if `query` is not specified.
            num_results (int): The maximum number of results to return. Default is 1.

        Returns:
            dict or None: The query results, or None if an error occurs.
        """
        try:
            params = {
                'query': query,
                'filter_condition': filter_condition,
                'include': include,
                'embeddings': embeddings,
                'num_results': num_results,
                'collection_name': collection_name
            }
            query_params = self._prepare_query_params(**params)

            result = {}
            if query_params:
                self.select_collection(collection_name)
                unformatted_result = self.collection.query(**query_params)

                if unformatted_result:
                    for key, value in unformatted_result.items():
                        if value:
                            result[key] = value[0]

            return result

        except Exception as e:
            logger.error(f"[query_memory] Error querying storage: {e}")
            return None

    def delete_from_storage(self, collection_name, ids):
        if ids and not isinstance(ids, list):
            ids = [ids]

        self.select_collection(collection_name)
        self.collection.delete(ids=ids)

    ##########################################################
    # Section 7: Advanced
    ##########################################################

    def search_storage_by_threshold(self, collection_name: str, query: str, threshold: float = 0.8,
                                    num_results: int = 1):
        """
        Searches the storage for documents that meet a specified similarity threshold to a query.

        Parameters:
            collection_name (str): The name of the collection to search within.
            query (str): The text of the query to compare against the documents in the collection.
            num_results (int): The maximum number of results to return. Default is 1.
            threshold (float): The similarity threshold that the documents must meet or exceed. Defaults to 0.8.

        Returns:
            dict: A dictionary containing the search results if successful; otherwise, returns an empty
             dictionary if no documents are found or meet the threshold.

        Raises:
            Exception: Logs an error message if an exception occurs during the search process.
        """
        try:
            query_emb = self.return_embedding(query)

            results = self.query_storage(collection_name=collection_name, embeddings=query_emb,
                                        include=["documents", "metadatas", "distances"],
                                        num_results=num_results)

            # We compare against the first result's embedding and `distance.cosine` returns
            # a similarity measure. May need to adjust the logic based on the actual behavior
            # of `distance.cosine`.
            # dist = distance.cosine(query_emb[0], results['embeddings'][0])
            if results:
                results.pop('included')
                filtered_data = {
                    key: [value for value, dist in zip(results[key], results['distances']) if float(dist) < threshold]
                    for key in results
                }
                if filtered_data['documents']:
                    return filtered_data

                logger.info('[search_storage_by_threshold] No documents found that meet the threshold.')
                return {}

            logger.info('Search by Threshold: No documents found.')
            return {}

        except Exception as e:
            logger.error(f"[search_storage_by_threshold] Error searching storage by threshold: {e}")
            return {'failed': f"Error searching storage by threshold: {e}"}

    def search_metadata_min_max(self, collection_name, metadata_tag, min_max):
        try:
            self.select_collection(collection_name)
            results = self.collection.get()

            # Gracefully handle empty or missing lists
            metadatas = results.get("metadatas", [])
            ids = results.get("ids", [])
            if not metadatas or not ids:
                # No data in collection
                return None

            metadata_values = [entry.get(metadata_tag) for entry in metadatas if metadata_tag in entry]
            if not metadata_values:
                # No metadata values to search
                return None

            # Ensure all are numeric
            if not all(isinstance(value, (int, float)) for value in metadata_values):
                logger.error(f"[search_metadata_min_max] Metadata tag '{metadata_tag}' contains non-numeric values.")
                return None

            # Find the min or max as before
            if min_max == "min":
                target_index = metadata_values.index(min(metadata_values))
            else:
                target_index = metadata_values.index(max(metadata_values))

            # Defensive: still check index range
            if target_index >= len(ids):
                return None

            target_entry = self.collection.get(ids=[ids[target_index]])
            return {
                "ids": target_entry["ids"][0],
                "target": target_entry["metadatas"][0][metadata_tag],
                "metadata": target_entry["metadatas"][0],
                "document": target_entry["documents"][0],
            }

        except Exception as e:
            # Only log errors if it's a truly unexpected error
            logger.error(f"[search_metadata_min_max] Unexpected error: {e}\nCollection: {collection_name}\nTarget Metadata: {metadata_tag}")
            return None


    # def search_metadata_min_max(self, collection_name, metadata_tag, min_max):
    #     """
    #     Retrieves the collection entry with the minimum or maximum value for the specified metadata tag.

    #     Args:
    #         collection_name: The ChromaDB collection object.
    #         metadata_tag: The name of the metadata tag to consider for finding the minimum or maximum value.
    #         min_max: The type of value to retrieve. Can be either "min" for minimum or "max" for maximum.
    #                  Default is "max".

    #     Returns:
    #         The collection entry with the minimum or maximum value for the specified metadata tag,
    #         or None if no entries are found or if the metadata tag contains non-numeric values.
    #     """
    #     try:
    #         # Retrieve only the document IDs and the specified metadata tag
    #         self.select_collection(collection_name)
    #         results = self.collection.get()

    #         # Extract the metadata values and document IDs
    #         metadata_values = [entry[metadata_tag] for entry in results["metadatas"]]
    #         document_ids = results["ids"]

    #         # Check if all metadata values are numeric (int or float)
    #         if not all(isinstance(value, (int, float)) for value in metadata_values):
    #             logger.error(f"[search_metadata_min_max] Error: The metadata tag '{metadata_tag}' contains non-numeric values.")
    #             return None

    #         if metadata_values:
    #             if min_max == "min":
    #                 target_index = metadata_values.index(min(metadata_values))
    #             else:
    #                 try:
    #                     target_index = metadata_values.index(max(metadata_values))
    #                 except:
    #                     logger.error(f"[search_metadata_min_max] Error: The metadata tag '{metadata_tag}' is empty or does not exist. Returning 0.")
    #                     target_index = 0
    #         else:
    #             target_index = 0

    #         try:
    #             # Retrieve the full entry with the highest metadata value
    #             target_entry = self.collection.get(ids=[document_ids[target_index]])

    #             max_metadata = {
    #                 "ids": target_entry["ids"][0],
    #                 "target": target_entry["metadatas"][0][metadata_tag],
    #                 "metadata": target_entry["metadatas"][0],
    #                 "document": target_entry["documents"][0],
    #             }

    #             logger.debug(
    #                 f"[search_metadata_min_max] Found the following record by max value of {metadata_tag} metadata tag:\n{max_metadata}",
    #             )
    #             return max_metadata
    #         except Exception as e:
    #             logger.error(f"[search_metadata_min_max] Error finding max metadata: {e}\nCollection: {collection_name}\nTarget Metadata: {metadata_tag}")
    #             return None

    #     except (KeyError, ValueError, IndexError) as e:
    #         logger.error(f"[search_metadata_min_max] Error finding max metadata: {e}\nCollection: {collection_name}\nTarget Metadata: {metadata_tag}")
    #         return None

    def rerank_results(self, query_results: dict, query: str, temp_collection_name: str, num_results: int = None):
        """
        Reranks the query results using a temporary collection.

        Args:
            query_results (dict): A dictionary containing the initial query results.
                Expected keys: 'documents', 'ids', 'metadatas', 'query'
            query (str): Query string to rerank against.
            temp_collection_name (str): The name of the temporary collection to use for reranking.
            num_results (int, optional): The number of results to return after reranking.
                If not provided or greater than the number of documents, all documents will be returned.

        Returns:
            dict: The reranked results, or None if an error occurs.
        """
        try:
            # Check if query_results contains the expected keys
            expected_keys = ['documents', 'ids', 'metadatas']
            if not all(key in query_results for key in expected_keys):
                raise KeyError(f"Missing expected keys in query_results. Expected: {expected_keys}")

            # Check if documents is empty
            if not query_results['documents']:
                logger.warning("[rerank_results] No documents found in query_results. Skipping reranking.")
                return query_results

            # Save the query results to a temporary collection
            self.save_to_storage(
                collection_name=temp_collection_name,
                data=query_results['documents'],
                ids=query_results['ids'],
                metadata=query_results['metadatas']
            )

            # Determine the number of results to return
            if num_results is None or num_results > len(query_results['documents']):
                num_results = len(query_results['documents'])

            # Perform reranking on the temporary collection
            reranked_results = self.query_storage(
                collection_name=temp_collection_name,
                query=query,
                num_results=num_results
            )
            count=self.count_collection(temp_collection_name)
            print(f"Count: {count}")

            # Delete the temporary collection
            self.delete_collection(temp_collection_name)

            return reranked_results
        except KeyError as e:
            logger.error(f"[rerank_results] KeyError occurred while reranking results: {e}")
            return None
        except Exception as e:
            logger.error(f"[rerank_results] Unexpected error occurred while reranking results: {e}")
            return None

    @staticmethod
    def combine_query_results(*query_results):
        """
        Combine the query results from multiple queries and remove duplicates.

        Args:
            *query_results: Variable number of query result dictionaries.

        Returns:
            dict: Combined query results with duplicates removed and new IDs assigned.
        """
        combined_results = {
            'documents': [],
            'ids': [],
            'metadatas': []
        }

        for query_result in query_results:
            combined_results['documents'].extend(query_result['documents'])
            combined_results['ids'].extend(query_result['ids'])
            combined_results['metadatas'].extend(query_result['metadatas'])

        # Remove duplicates based on the 'documents' field
        unique_results = {
            'documents': [],
            'ids': [],
            'metadatas': []
        }
        seen_documents = set()

        for i in range(len(combined_results['documents'])):
            document = combined_results['documents'][i]
            if document not in seen_documents:
                seen_documents.add(document)
                unique_results['documents'].append(document)
                unique_results['ids'].append(str(len(unique_results['ids']) + 1))  # Assign new ID
                unique_results['metadatas'].append(combined_results['metadatas'][i])

        return unique_results

    def combine_and_rerank(self, query_results: list, rerank_query, num_results=5):
        """
        Combine multiple query results, rerank them based on a new query, and return the top results.

        This function takes multiple query results, combines them, and then reranks the combined
        results based on a new query. It's useful for refining search results across multiple
        collections or queries.

        Args:
            query_results (list): A list of query result dictionaries, each containing 'ids',
                                'embeddings', 'documents', and 'metadatas'.
            rerank_query (str): The query string used for reranking the combined results.
            num_results (int, optional): The number of top results to return after reranking.
                                        Defaults to 5.

        Returns:
            dict: A dictionary containing the reranked results, including 'ids', 'embeddings',
                'documents', and 'metadatas' for the top results.

        Raises:
            ValueError: If query_results is empty or if reranking fails.

        Example:
            query_results = [results1, results2, results3]
            rerank_query = "specific query that can be the same or a new query"
            reranked = query_and_rerank(query_results, rerank_query, num_results=3)
        """

        # Combine all query results
        combined_query_results = self.combine_query_results(*query_results)

        reranked_results = self.rerank_results(
            query_results=combined_query_results,
            query=rerank_query,
            temp_collection_name="temp_reranking_collection",
            num_results=num_results
        )

        return reranked_results

    def get_last_x_entries(self, collection_name: str, x: int, include: list = None):
        """
        Retrieve the last X entries from a collection, ordered by sequential id.

        Args:
            collection_name (str): The name of the collection.
            x (int): Number of most recent entries to retrieve.
            include (list, optional): Which fields to include in the result.
                Defaults to ['documents', 'metadatas', 'ids'].

        Returns:
            dict: The collection entries, sorted by id ascending, with only the requested fields.
        """
        if not include:
            include = ['documents', 'metadatas', 'ids']

        # 1. Find the current max id
        max_id_entry = self.search_metadata_min_max(collection_name, 'id', 'max')
        if max_id_entry is None or "target" not in max_id_entry or max_id_entry["target"] is None:
            return {key: [] for key in include}

        max_id = max_id_entry["target"]
        start_id = max(1, max_id - x + 1)

        # 2. Query for entries with id >= start_id
        filters = {"id": {"$gte": start_id}}
        results = self.load_collection(collection_name, include=include, where=filters)

        # 3. Sort results by id ascending
        if results and "ids" in results and results["ids"]:
            sorted_indices = sorted(range(len(results["ids"])), key=lambda i: int(results["ids"][i]))
            # Only include requested fields
            sorted_results = {}
            for key in include:
                if key in results:
                    sorted_results[key] = [results[key][i] for i in sorted_indices]
            return sorted_results
        else:
            return {key: [] for key in include}

# From storage/chroma_storage.py
def validate_collection_name(collection_name: str):
    # We expected a collection name that:
    # (1) contains 3-63 characters
    # (2) starts and ends with an alphanumeric character
    # (3) otherwise contains only alphanumeric characters, underscores or hyphens (-)
    # (4) contains no two consecutive periods (..) and
    # (5) is not a valid IPv4 address.

    if not collection_name:
        raise ValueError("Collection name cannot be empty.")

    # Replace any invalid characters with an underscore
    collection_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', collection_name)

    # Remove consecutive periods
    while ".." in collection_name:
        collection_name = collection_name.replace("..", ".")

    # Check if the first character is not alphanumeric and remove it
    while len(collection_name) > 0 and not collection_name[0].isalnum():
        collection_name = collection_name[1:]

    # Check if the last character is not alphanumeric and remove it
    while len(collection_name) > 0 and not collection_name[-1].isalnum():
        collection_name = collection_name[:-1]

    # Ensure we don't proceed with an empty name
    if len(collection_name) <= 0:
        collection_name = "0"

    # Ensure the name starts with an alphanumeric character
    if not collection_name[0].isalnum():
        collection_name = "0" + collection_name

    # Ensure the name ends with an alphanumeric character
    if not collection_name[-1].isalnum():
        collection_name = collection_name + "0"

    # Ensure the name is at least 3 characters long
    while len(collection_name) < 3:
        collection_name = collection_name + "0"

    # Check if the name exceeds 63 characters
    if len(collection_name) > 63:
        raise ValueError(f"Collection name exceeds 63 characters. Ensure it starts/ends with alphanumeric, "
                         f"contains only alphanumeric, underscores, hyphens, and no consecutive periods.\n"
                         f"Got: '{collection_name}'")

    return collection_name

# From storage/chroma_storage.py
def validate_inputs(data: Union[list, str], ids: list, metadata: list[dict]):
    """
    Validates the inputs for the save_memory method.

    Parameters:
        data (Union[list, str]): The documents to be saved.
        ids (list): The IDs for the documents.
        metadata (list[dict]): The metadata for the documents.

    Raises:
        ValueError: If any of the inputs are invalid.
    """
    if not data:
        raise ValueError("Data cannot be empty.")

    if not (len(data) == len(ids) == len(metadata)):
        raise ValueError("The length of data, ids, and metadata lists must match.")

# From storage/chroma_storage.py
def generate_defaults(data: Union[list, str], ids: list = None, metadata: list[dict] = None):
    """
    Generates default values for ids and metadata if they are not provided.

    Parameters:
        data (Union[list, str]): The documents to be saved.
        ids (list, optional): The IDs for the documents.
        metadata (list[dict], optional): The metadata for the documents.

    Returns:
        tuple: A tuple containing the generated ids and metadata.
    """
    if isinstance(data, str):
        data = [data]

    ids = [str(uuid.uuid4()) for _ in data] if ids is None else ids
    metadata = [{} for _ in data] if metadata is None else metadata

    return ids, metadata

# From storage/chroma_storage.py
def apply_uuids(metadata: list[dict], config: dict):
    do_uuid = config['options'].get('add_uuid', False)
    if do_uuid:
        for m in metadata:
            m['uuid'] = str(uuid.uuid4())

# From storage/chroma_storage.py
def apply_iso_timestamps(metadata: list[dict], config):
    do_time_stamp = config['options'].get('iso_timestamp', False)
    if do_time_stamp:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        for m in metadata:
            m['iso_timestamp'] = timestamp

# From storage/chroma_storage.py
def apply_unix_timestamps(metadata: list[dict], config):
    do_time_stamp = config['options'].get('unix_timestamp', False)
    if do_time_stamp:
        timestamp = datetime.now().timestamp()
        for m in metadata:
            m['unix_timestamp'] = timestamp

# From storage/chroma_storage.py
def save_to_collection(collection, data: list, ids: list, metadata: list[dict]):
    """
    Saves the data to the collection.

    Parameters:
        collection: The collection object.
        data (list): The documents to be saved.
        ids (list): The IDs for the documents.
        metadata (list[dict]): The metadata for the documents.
    """
    collection.upsert(
        documents=data,
        metadatas=metadata,
        ids=ids
    )

# From storage/chroma_storage.py
def get_or_create(cls, storage_id: str):
        """
        Retrieve a shared ChromaStorage instance for the given storage_id.
        """
        if not storage_id:
            raise ValueError("ChromaStorage.get_or_create requires a non-empty storage_id.")
        with cls._registry_lock:
            if storage_id not in cls._registry:
                cls._registry[storage_id] = cls(storage_id=storage_id)
            return cls._registry[storage_id]

# From storage/chroma_storage.py
def clear_registry(cls):
        """
        Clear the ChromaStorage registry. Useful for test isolation or resetting state.
        """
        with cls._registry_lock:
            cls._registry.clear()

# From storage/chroma_storage.py
def describe_instance(self):
        """
        Returns a dictionary describing the storage_id and path for this instance.
        Useful for debugging and testing.
        """
        db_path, db_embed = self.chromadb_settings()
        return {
            'storage_id': self.storage_id,
            'db_path': db_path,
            'db_embed': db_embed,
        }

# From storage/chroma_storage.py
def init_storage(self):
        """
        Initializes the storage client, either as a persistent client with a specified database path or as an
        ephemeral client based on the configuration.

        Raises:
            Exception: For any errors that occur during the initialization of storage.
        """
        try:
            if self.client is None:
                if self.db_path:
                    self.client = chromadb.PersistentClient(path=self.db_path, settings=Settings(allow_reset=True))
                else:
                    self.client = chromadb.EphemeralClient()

            if getattr(self.config.settings.storage, 'fresh_start', False):
                self.reset_storage()
        except Exception as e:
            logger.error(f"[init_storage] Error initializing storage: {e}")
            raise

# From storage/chroma_storage.py
def init_embeddings(self):
        """
        Initializes the embedding function based on the configuration, supporting multiple embedding backends.

        Raises:
            KeyError: If a required environment variable or setting is missing.
            Exception: For any errors that occur during the initialization of embeddings.
        """
        try:
            self.db_path, self.db_embed = self.chromadb_settings()


            # Initialize embedding based on the specified backend in the configuration
            if not self.db_embed:
                self.embedding = embedding_functions.DefaultEmbeddingFunction()
                return
            
            if self.db_embed == 'text-embedding-ada-002':
                openai_api_key = os.getenv('OPENAI_API_KEY')
                self.embedding = embedding_functions.OpenAIEmbeddingFunction(
                    api_key=openai_api_key,
                    model_name=self.db_embed
                )
                return
            
            if self.db_embed:
                self.embedding = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=self.db_embed)
                return

            raise ValueError(f"Unsupported embedding backend: {self.db_embed}")
        except KeyError as e:
            logger.error(f"[init_embeddings] Missing environment variable or setting: {e}")
            raise
        except Exception as e:
            logger.error(f"[init_embeddings] Error initializing embeddings: {e}")
            raise

# From storage/chroma_storage.py
def chromadb_settings(self):
        """
        Retrieves the ChromaDB settings from the configuration and resolves the database path
        for a given storage_id.

        Returns:
            tuple: (db_path, db_embed)
        """
        storage_settings = self.config.settings.storage
        db_path_setting = storage_settings['options'].get('persist_directory', None)
        selected_embed = storage_settings['embedding'].get('selected', None)
        db_embed = storage_settings['embedding_library'].get(selected_embed, None)
        if not db_path_setting:
            raise ValueError("ChromaStorage: persist_directory must be set in the settings YAML.")
        db_path = str(self.config.project_root / db_path_setting / self.storage_id)
        return db_path, db_embed

# From storage/chroma_storage.py
def reset_storage(self):
        """
        Resets the entire storage, removing all collections and their data.

        This method should be used with caution as it will permanently delete all data within the storage.
        """

        self.client.reset()

# From storage/chroma_storage.py
def return_embedding(self, text_to_embed: str):
        """
        Generates an embedding for the given text using the configured embedding function.

        Parameters:
            text_to_embed (str): The text to generate an embedding for.

        Returns:
            list: A list containing the generated embedding vector for the given text.
        """
        return self.embedding([text_to_embed])

# From storage/chroma_storage.py
def collection_list(self):
        """
        Lists all collections currently in the storage.

        Returns:
            list: A list of collection names.
        """
        return self.client.list_collections()

# From storage/chroma_storage.py
def select_collection(self, collection_name: str):
        """
        Selects (or creates if not existent) a collection within the storage by name.

        Parameters:
            collection_name (str): The name of the collection to select or create.

        Raises:
            ValueError: If there's an error in getting or creating the collection.
        """
        try:
            collection_name = validate_collection_name(collection_name)
            self.collection = self.client.get_or_create_collection(name=collection_name,
                                                                   embedding_function=self.embedding,
                                                                   metadata={"hnsw:space": "cosine"})
        except Exception as e:
            raise ValueError(f"\n\nError getting or creating collection. Error: {e}")

# From storage/chroma_storage.py
def delete_collection(self, collection_name: str):
        """
        Deletes a collection from the storage by its name.

        Parameters:
            collection_name (str): The name of the collection to delete.
        """
        try:
            self.client.delete_collection(collection_name)
        except Exception as e:
            print("\n\nError deleting collection: ", e)

# From storage/chroma_storage.py
def count_collection(self, collection_name: str):
        """
        Counts the number of documents in a specified collection.

        Parameters:
            collection_name (str): The name of the collection to count documents in.

        Returns:
            int: The number of documents in the specified collection.
        """
        self.select_collection(collection_name)
        return self.collection.count()

# From storage/chroma_storage.py
def peek(self, collection_name: str):
        """
        Peeks into a collection to retrieve a brief overview of its contents.

        Parameters:
            collection_name (str): The name of the collection to peek into.

        Returns:
            dict or None: A dictionary containing a brief overview of the collection's contents or None if an error occurs.
        """
        try:
            self.select_collection(collection_name)

            max_result_count = self.collection.count()
            num_results = min(10, max_result_count)

            if num_results > 0:
                result = self.collection.peek()
            else:
                result = {'documents': "No Results!"}

            return result
        except Exception as e:
            logger.error(f"Error peeking collection: {e}")
            return None

# From storage/chroma_storage.py
def load_collection(self, collection_name: str, include: dict = None, where: dict = None, where_doc: dict = None):
        """
        Loads data from a specified collection based on provided filters.
        Parameters:
            collection_name (str): The name of the collection to load from.
            include(dict, optional): Specify which data to return. Will return all results if no filters are specified.
            where (dict, optional): Filter to apply in metadata. Will return documents and metadata by default.
            where_doc (dict, optional): Filter to apply in document. Not applied if not specified.
        Returns:
            list or None: The data loaded from the collection, or None if an error occurs.
        """
        params = {}

        # Defaulting 'include' if None
        if include is None:
            include = ["documents", "metadatas"]

        params.update(include=include)

        if where is not None:
            params.update(where=where)

        if where_doc is not None:
            params.update(where_document=where_doc)

        try:
            self.select_collection(collection_name)
            data = self.collection.get(**params)
            logger.debug(
                f"\nCollection: {collection_name}"
                f"\nData: {data}",
            )
        except Exception as e:
            print(f"\n\nError loading data: {e}")
            data = []
        return data

# From storage/chroma_storage.py
def get_next_sequential_id(self, collection_name: str) -> int:
        max_id_entry = self.search_metadata_min_max(collection_name, 'id', 'max')
        if max_id_entry is None or "target" not in max_id_entry:
            return 1
        else:
            return max_id_entry["target"] + 1 if max_id_entry["target"] is not None else 1

# From storage/chroma_storage.py
def apply_sequential_ids(self, collection_name: str, data: list, metadata: list[dict]) -> tuple[list, list[dict]]:
        # Get the current max ID once
        current_max = self.get_next_sequential_id(collection_name) - 1
        new_ids = []
        for i, _ in enumerate(data):
            next_id = current_max + i + 1
            new_ids.append(str(next_id))
            metadata[i]['id'] = next_id
        return new_ids, metadata

# From storage/chroma_storage.py
def save_to_storage(self, collection_name: str, data: list, ids: Optional[list] = None, metadata: Optional[list[dict]] = None):
        """
        Saves data to memory, creating or updating documents in a specified collection.

        Parameters:
            collection_name (str): The name of the collection to save to. Will be created if it doesn't exist.
            data (list): The documents to be saved. Can be a single document as a string or a list
             of documents. If a single string is provided, it is converted into a list with one element.
            ids (list): The IDs corresponding to the documents. If not provided, IDs will be generated automatically.
            metadata (list[dict], optional): A list of dictionaries, each representing associated metadata for
                the corresponding document in `data`. If not provided, empty dictionaries are used for each document.

        Raises:
            ValueError: If the lengths of `data`, `ids`, and `metadata` do not match, or if other errors occur
            during the save operation.
        """
        try:
            data = [data] if isinstance(data, str) else data
            metadata = [{} for _ in data] if metadata is None else metadata

            if ids is None:
                ids, metadata = self.apply_sequential_ids(collection_name,data, metadata)
            
            validate_inputs(data, ids, metadata)
            apply_uuids(metadata, self.config.settings.storage)
            apply_unix_timestamps(metadata, self.config.settings.storage)
            apply_iso_timestamps(metadata, self.config.settings.storage)

            self.select_collection(collection_name)
            self.collection.upsert(
                documents=data,
                metadatas=metadata,
                ids=ids
            )
        except Exception as e:
            raise ValueError(f"[ChromaStorage][save_to_storage] Error saving to storage. Error: {e}\n\nData:\n{data}")

# From storage/chroma_storage.py
def query_storage(self, collection_name: str, query: Optional[Union[str, list]] = None,
                      filter_condition: Optional[dict] = None, include: Optional[list] = None,
                      embeddings: Optional[list] = None, num_results: int = 1):
        """
        Queries storage for documents matching a query within a specified collection.

        Parameters:
            collection_name (str): The name of the collection to query.
            query (Optional[Union[str, list]]): The query text or a list of query texts.
                If `None`, `embeddings` must be provided.
            filter_condition (Optional[dict]): A dictionary specifying any filters to apply to the query.
            include (Optional[list]): A list specifying which elements to include in the result
                (e.g., ["documents", "metadatas", "distances"]). Defaults to all elements if `None`.
            embeddings (Optional[list]): Query embeddings used if `query` is `None`.
                Must be provided if `query` is not specified.
            num_results (int): The maximum number of results to return. Default is 1.

        Returns:
            dict or None: The query results, or None if an error occurs.
        """
        try:
            params = {
                'query': query,
                'filter_condition': filter_condition,
                'include': include,
                'embeddings': embeddings,
                'num_results': num_results,
                'collection_name': collection_name
            }
            query_params = self._prepare_query_params(**params)

            result = {}
            if query_params:
                self.select_collection(collection_name)
                unformatted_result = self.collection.query(**query_params)

                if unformatted_result:
                    for key, value in unformatted_result.items():
                        if value:
                            result[key] = value[0]

            return result

        except Exception as e:
            logger.error(f"[query_memory] Error querying storage: {e}")
            return None

# From storage/chroma_storage.py
def delete_from_storage(self, collection_name, ids):
        if ids and not isinstance(ids, list):
            ids = [ids]

        self.select_collection(collection_name)
        self.collection.delete(ids=ids)

# From storage/chroma_storage.py
def search_storage_by_threshold(self, collection_name: str, query: str, threshold: float = 0.8,
                                    num_results: int = 1):
        """
        Searches the storage for documents that meet a specified similarity threshold to a query.

        Parameters:
            collection_name (str): The name of the collection to search within.
            query (str): The text of the query to compare against the documents in the collection.
            num_results (int): The maximum number of results to return. Default is 1.
            threshold (float): The similarity threshold that the documents must meet or exceed. Defaults to 0.8.

        Returns:
            dict: A dictionary containing the search results if successful; otherwise, returns an empty
             dictionary if no documents are found or meet the threshold.

        Raises:
            Exception: Logs an error message if an exception occurs during the search process.
        """
        try:
            query_emb = self.return_embedding(query)

            results = self.query_storage(collection_name=collection_name, embeddings=query_emb,
                                        include=["documents", "metadatas", "distances"],
                                        num_results=num_results)

            # We compare against the first result's embedding and `distance.cosine` returns
            # a similarity measure. May need to adjust the logic based on the actual behavior
            # of `distance.cosine`.
            # dist = distance.cosine(query_emb[0], results['embeddings'][0])
            if results:
                results.pop('included')
                filtered_data = {
                    key: [value for value, dist in zip(results[key], results['distances']) if float(dist) < threshold]
                    for key in results
                }
                if filtered_data['documents']:
                    return filtered_data

                logger.info('[search_storage_by_threshold] No documents found that meet the threshold.')
                return {}

            logger.info('Search by Threshold: No documents found.')
            return {}

        except Exception as e:
            logger.error(f"[search_storage_by_threshold] Error searching storage by threshold: {e}")
            return {'failed': f"Error searching storage by threshold: {e}"}

# From storage/chroma_storage.py
def search_metadata_min_max(self, collection_name, metadata_tag, min_max):
        try:
            self.select_collection(collection_name)
            results = self.collection.get()

            # Gracefully handle empty or missing lists
            metadatas = results.get("metadatas", [])
            ids = results.get("ids", [])
            if not metadatas or not ids:
                # No data in collection
                return None

            metadata_values = [entry.get(metadata_tag) for entry in metadatas if metadata_tag in entry]
            if not metadata_values:
                # No metadata values to search
                return None

            # Ensure all are numeric
            if not all(isinstance(value, (int, float)) for value in metadata_values):
                logger.error(f"[search_metadata_min_max] Metadata tag '{metadata_tag}' contains non-numeric values.")
                return None

            # Find the min or max as before
            if min_max == "min":
                target_index = metadata_values.index(min(metadata_values))
            else:
                target_index = metadata_values.index(max(metadata_values))

            # Defensive: still check index range
            if target_index >= len(ids):
                return None

            target_entry = self.collection.get(ids=[ids[target_index]])
            return {
                "ids": target_entry["ids"][0],
                "target": target_entry["metadatas"][0][metadata_tag],
                "metadata": target_entry["metadatas"][0],
                "document": target_entry["documents"][0],
            }

        except Exception as e:
            # Only log errors if it's a truly unexpected error
            logger.error(f"[search_metadata_min_max] Unexpected error: {e}\nCollection: {collection_name}\nTarget Metadata: {metadata_tag}")
            return None

# From storage/chroma_storage.py
def rerank_results(self, query_results: dict, query: str, temp_collection_name: str, num_results: int = None):
        """
        Reranks the query results using a temporary collection.

        Args:
            query_results (dict): A dictionary containing the initial query results.
                Expected keys: 'documents', 'ids', 'metadatas', 'query'
            query (str): Query string to rerank against.
            temp_collection_name (str): The name of the temporary collection to use for reranking.
            num_results (int, optional): The number of results to return after reranking.
                If not provided or greater than the number of documents, all documents will be returned.

        Returns:
            dict: The reranked results, or None if an error occurs.
        """
        try:
            # Check if query_results contains the expected keys
            expected_keys = ['documents', 'ids', 'metadatas']
            if not all(key in query_results for key in expected_keys):
                raise KeyError(f"Missing expected keys in query_results. Expected: {expected_keys}")

            # Check if documents is empty
            if not query_results['documents']:
                logger.warning("[rerank_results] No documents found in query_results. Skipping reranking.")
                return query_results

            # Save the query results to a temporary collection
            self.save_to_storage(
                collection_name=temp_collection_name,
                data=query_results['documents'],
                ids=query_results['ids'],
                metadata=query_results['metadatas']
            )

            # Determine the number of results to return
            if num_results is None or num_results > len(query_results['documents']):
                num_results = len(query_results['documents'])

            # Perform reranking on the temporary collection
            reranked_results = self.query_storage(
                collection_name=temp_collection_name,
                query=query,
                num_results=num_results
            )
            count=self.count_collection(temp_collection_name)
            print(f"Count: {count}")

            # Delete the temporary collection
            self.delete_collection(temp_collection_name)

            return reranked_results
        except KeyError as e:
            logger.error(f"[rerank_results] KeyError occurred while reranking results: {e}")
            return None
        except Exception as e:
            logger.error(f"[rerank_results] Unexpected error occurred while reranking results: {e}")
            return None

# From storage/chroma_storage.py
def combine_query_results(*query_results):
        """
        Combine the query results from multiple queries and remove duplicates.

        Args:
            *query_results: Variable number of query result dictionaries.

        Returns:
            dict: Combined query results with duplicates removed and new IDs assigned.
        """
        combined_results = {
            'documents': [],
            'ids': [],
            'metadatas': []
        }

        for query_result in query_results:
            combined_results['documents'].extend(query_result['documents'])
            combined_results['ids'].extend(query_result['ids'])
            combined_results['metadatas'].extend(query_result['metadatas'])

        # Remove duplicates based on the 'documents' field
        unique_results = {
            'documents': [],
            'ids': [],
            'metadatas': []
        }
        seen_documents = set()

        for i in range(len(combined_results['documents'])):
            document = combined_results['documents'][i]
            if document not in seen_documents:
                seen_documents.add(document)
                unique_results['documents'].append(document)
                unique_results['ids'].append(str(len(unique_results['ids']) + 1))  # Assign new ID
                unique_results['metadatas'].append(combined_results['metadatas'][i])

        return unique_results

# From storage/chroma_storage.py
def combine_and_rerank(self, query_results: list, rerank_query, num_results=5):
        """
        Combine multiple query results, rerank them based on a new query, and return the top results.

        This function takes multiple query results, combines them, and then reranks the combined
        results based on a new query. It's useful for refining search results across multiple
        collections or queries.

        Args:
            query_results (list): A list of query result dictionaries, each containing 'ids',
                                'embeddings', 'documents', and 'metadatas'.
            rerank_query (str): The query string used for reranking the combined results.
            num_results (int, optional): The number of top results to return after reranking.
                                        Defaults to 5.

        Returns:
            dict: A dictionary containing the reranked results, including 'ids', 'embeddings',
                'documents', and 'metadatas' for the top results.

        Raises:
            ValueError: If query_results is empty or if reranking fails.

        Example:
            query_results = [results1, results2, results3]
            rerank_query = "specific query that can be the same or a new query"
            reranked = query_and_rerank(query_results, rerank_query, num_results=3)
        """

        # Combine all query results
        combined_query_results = self.combine_query_results(*query_results)

        reranked_results = self.rerank_results(
            query_results=combined_query_results,
            query=rerank_query,
            temp_collection_name="temp_reranking_collection",
            num_results=num_results
        )

        return reranked_results

# From storage/chroma_storage.py
def get_last_x_entries(self, collection_name: str, x: int, include: list = None):
        """
        Retrieve the last X entries from a collection, ordered by sequential id.

        Args:
            collection_name (str): The name of the collection.
            x (int): Number of most recent entries to retrieve.
            include (list, optional): Which fields to include in the result.
                Defaults to ['documents', 'metadatas', 'ids'].

        Returns:
            dict: The collection entries, sorted by id ascending, with only the requested fields.
        """
        if not include:
            include = ['documents', 'metadatas', 'ids']

        # 1. Find the current max id
        max_id_entry = self.search_metadata_min_max(collection_name, 'id', 'max')
        if max_id_entry is None or "target" not in max_id_entry or max_id_entry["target"] is None:
            return {key: [] for key in include}

        max_id = max_id_entry["target"]
        start_id = max(1, max_id - x + 1)

        # 2. Query for entries with id >= start_id
        filters = {"id": {"$gte": start_id}}
        results = self.load_collection(collection_name, include=include, where=filters)

        # 3. Sort results by id ascending
        if results and "ids" in results and results["ids"]:
            sorted_indices = sorted(range(len(results["ids"])), key=lambda i: int(results["ids"][i]))
            # Only include requested fields
            sorted_results = {}
            for key in include:
                if key in results:
                    sorted_results[key] = [results[key][i] for i in sorted_indices]
            return sorted_results
        else:
            return {key: [] for key in include}

import xmltodict
import configparser
import csv
from io import StringIO

# From utils/parsing_processor.py
class ParsingError(Exception):
    """Custom exception for parsing errors."""
    pass

# From utils/parsing_processor.py
class ParsingProcessor:
    """
    A robust parsing processor that supports multiple serialization formats with two-stage parsing.
    
    The ParsingProcessor implements a two-stage parsing approach for all supported formats:
    1. Code-fenced parsing: First attempts to extract and parse content from code blocks
    2. Bare parsing fallback: If code-fenced parsing fails, attempts to parse the entire content
    
    Supported formats: JSON, YAML, XML, INI, CSV, Markdown
    
    Features:
    - Robust error handling with detailed logging
    - Support for both fenced and unfenced agent outputs
    - Format-specific preprocessing (e.g., YAML sanitization)
    - Comprehensive debug logging for troubleshooting
    - Default code fence extraction using triple backticks (['```'])
    """
    
    # Default code fences to use for extraction - can be overridden by passing code_fences parameter
    DEFAULT_CODE_FENCES = ['```']
    
    def __init__(self):
        self.logger = Logger(name=self.__class__.__name__, default_logger=self.__class__.__name__.lower())
        self._define_parsers()

    def _define_parsers(self):
        # Central parser dispatcher mapping format to a lambda that takes the content string and code fences.
        self.parsers: Dict[str, Callable[[str, Optional[List[str]]], Any]] = {
            'json': lambda s, fences: self.parse_content(s, json.loads, 'json', code_fences=fences),
            'yaml': lambda s, fences: self.parse_content(s, yaml.safe_load, 'yaml', code_fences=fences),
            'xml': lambda s, fences: self.parse_content(s, xmltodict.parse, 'xml', code_fences=fences),
            'ini': lambda s, fences: self.parse_content(s, self._parse_ini, 'ini', code_fences=fences),
            'csv': lambda s, fences: self.parse_content(s, self._parse_csv, 'csv', code_fences=fences),
            'markdown': lambda s, fences: self.parse_content(
                s, lambda t: self.parse_markdown_to_dict(t, 2, 6), 'markdown', code_fences=fences)
        }

    def extract_code_block(self, text: str, code_fences: Optional[List[str]] = None) -> Tuple[Optional[str], str]:
        """
        Extract the first code block from text, returning the language and content.
        
        This method is used in the first stage of the two-stage parsing approach.
        If no code fences are found, returns the entire stripped text for fallback parsing.
        
        Args:
            text: The text containing potential code blocks
            code_fences: List of fence markers to use. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction and return full text.
            
        Returns:
            Tuple of (language, content) where:
            - language: The detected language specifier (or None if not found/specified)
            - content: The extracted code block content or stripped full text if no fence found
        """
        # Use default code fences if None provided, otherwise use the provided list (including empty list)
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
        # Early exit if no code fences provided
        if not code_fences:
            return None, text.strip()
        
        # First try standard fence pattern matching
        for fence in code_fences:
            escaped_fence = re.escape(fence)
            # Match fenced code blocks with language specifier
            pattern = fr"{escaped_fence}([a-zA-Z]*)[ \t]*\r?\n?([\s\S]*?){escaped_fence}"
            match = re.search(pattern, text, re.DOTALL)
            if match:
                language = match.group(1).strip() or None
                content = match.group(2).strip()
                return language, content

            # Also try handling no-language blocks with just the fence
            pattern = fr"{escaped_fence}\s*\r?\n([\s\S]*?){escaped_fence}"
            match = re.search(pattern, text, re.DOTALL)
            if match:
                return None, match.group(1).strip()
        
        # No code blocks found with standard patterns
        return None, text.strip()

    def sanitize_yaml_content(self, content: str, 
                             primary_fence: str = None, 
                             alternate_fence: str = None) -> str:
        """
        Sanitize YAML content by handling nested code blocks.
        
        Args:
            content: The YAML content to sanitize
            primary_fence: The main fence to replace
            alternate_fence: The replacement fence
        """
        if not primary_fence or not alternate_fence:
            return content.strip()
        
        def replace_inner_fences(match):
            inner_block = match.group(0)
            return inner_block.replace(primary_fence, alternate_fence)
        
        # Pattern based on primary fence
        pattern = fr"{re.escape(primary_fence)}[a-zA-Z]*\s*\r?\n[\s\S]*?{re.escape(primary_fence)}"
        content = re.sub(pattern, replace_inner_fences, content)
        
        # Then handle any other sanitization
        if content.strip().startswith('```'):
            content = re.sub(r'^```.*?\n', '', content, flags=re.DOTALL)
            content = re.sub(r'```\s*$', '', content)
            self.logger.warning("Removed outer code blocks in YAML content")
        
        return content.strip()

    @staticmethod
    def preprocess_json_string(s: str) -> str:
        """
        Preprocess a string to increase the chance of successful JSON parsing from LLM outputs.
        - Trims whitespace
        - Extracts the first JSON object if stray text is present
        - Removes trailing commas in objects/arrays
        - Double-escapes backslashes not already escaped (for LaTeX/math)
        """
        import re
        s = s.strip()
        # Extract the first JSON object (handles stray text)
        match = re.search(r'(\{.*\})', s, re.DOTALL)
        if match:
            s = match.group(1)
        # Remove trailing commas in objects/arrays
        s = re.sub(r',([ \t\r\n]*[}\]])', r'\1', s)
        # Double-escape backslashes that are not already double-escaped
        s = re.sub(r'(?<!\\)\\(?![\\/\"bfnrtu])', r'\\\\', s)
        return s

    def parse_content(self, content_string: str, parser_func: Callable[[str], Any],
                      expected_language: str, code_fences: Optional[List[str]] = None) -> Any:
        """
        Parse content using a two-stage approach: code-fenced first, then bare parsing fallback.
        
        Args:
            content_string: The input string to parse
            parser_func: The function to use for parsing (e.g., json.loads, yaml.safe_load)
            expected_language: The expected language/format type
            code_fences: List of fence markers to look for. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction and parse full content.
            
        Returns:
            Parsed content or None if parsing fails
        """
        # Use default code fences if None provided
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
        
        # Stage 1: Try code-fenced parsing
        language, extracted_content = self.extract_code_block(content_string, code_fences)
        
        # Log the code-fenced extraction attempt
        if language is not None:
            self.logger.debug(f"Code-fenced block detected with language '{language}' for {expected_language.upper()} parsing")
        elif code_fences:
            self.logger.debug(f"No code-fenced block found using fences {code_fences}, extracted content length: {len(extracted_content)}")
        else:
            self.logger.debug(f"No code fences specified, proceeding with full content for {expected_language.upper()} parsing")
        
        # Check language match and warn if different
        if language and language.lower() != expected_language.lower():
            self.logger.warning(f"Expected {expected_language.upper()} code block, but found '{language}'")
        
        # Attempt to parse the extracted (code-fenced) content
        if extracted_content:
            try:
                processed_content = extracted_content
                if expected_language.lower() == 'yaml':
                    processed_content = self.sanitize_yaml_content(extracted_content)
                if expected_language.lower() == 'json':
                    processed_content = self.preprocess_json_string(processed_content)
                    self.logger.debug(f"Cleaned JSON string before parsing (first 500 chars): {processed_content[:500]}")
                self.logger.debug(f"Attempting code-fenced {expected_language.upper()} parsing on content of length {len(processed_content)}")
                result = parser_func(processed_content)
                self.logger.debug(f"Code-fenced {expected_language.upper()} parsing succeeded")
                return result
                
            except Exception as e:
                # Log the code-fenced parsing failure
                preview = processed_content[:100] + ('...' if len(processed_content) > 100 else '')
                self.logger.warning(f"Code-fenced {expected_language.upper()} parsing failed: {str(e)}")
                self.logger.debug(f"Failed content preview: {preview}")
                if expected_language.lower() == 'json':
                    self.logger.error(f"Cleaned JSON string on failure (first 500 chars): {processed_content[:500]}")
                
                # Stage 2: Fallback to bare parsing if code-fenced parsing failed
                # Only attempt if we had extracted a code block and it failed
                if language is not None or code_fences:
                    return self._attempt_bare_parsing(content_string, parser_func, expected_language)
                else:
                    # If no code fences were specified, don't try again
                    raise ParsingError(f"Failed to parse {expected_language}: {e}") from e
        
        # If no content was extracted, attempt bare parsing
        return self._attempt_bare_parsing(content_string, parser_func, expected_language)
    
    def _attempt_bare_parsing(self, content_string: str, parser_func: Callable[[str], Any], 
                            expected_language: str) -> Any:
        """
        Attempt to parse the entire content string as the expected format (bare parsing).
        
        Args:
            content_string: The full input string
            parser_func: The parsing function to use
            expected_language: The expected format type
            
        Returns:
            Parsed content or raises ParsingError if parsing fails
        """
        
        stripped_content = content_string.strip()
        self.logger.info(f"Attempting bare {expected_language.upper()} parsing fallback on full content (length: {len(stripped_content)})")
        
        try:
            # Apply format-specific preprocessing for bare content too
            processed_content = stripped_content
            if expected_language.lower() == 'yaml':
                processed_content = self.sanitize_yaml_content(stripped_content)
            
            result = parser_func(processed_content)
            self.logger.info(f"Bare {expected_language.upper()} parsing fallback succeeded")
            return result
            
        except Exception as e:
            # Log the bare parsing failure
            preview = processed_content[:100] + ('...' if len(processed_content) > 100 else '')
            self.logger.error(f"Bare {expected_language.upper()} parsing fallback failed: {str(e)}")
            self.logger.debug(f"Failed content preview: {preview}")
            self.logger.error(f"Content type: {expected_language}, Content length: {len(processed_content)}")
            
            # Special handling for YAML auto-cleanup (keeping existing behavior as requested)
            if expected_language.lower() == 'yaml':
                try:
                    # More aggressive YAML cleanup as a last resort
                    cleaned_content = re.sub(r'[`~]', '', processed_content)  # Remove any remaining backticks
                    self.logger.info("Attempting alternative YAML parsing after aggressive cleanup")
                    result = parser_func(cleaned_content)
                    self.logger.info("Alternative YAML parsing succeeded")
                    return result
                except Exception as e2:
                    self.logger.error(f"Alternative YAML parsing also failed: {e2}")
            
            raise ParsingError(f"Failed to parse {expected_language}: {e}") from e

    def parse_by_format(self, content_string: str, parser_type: Optional[str], code_fences: Optional[List[str]] = None) -> Any:
        """
        Parse content using the specified format with two-stage parsing approach.
        
        This method uses the two-stage parsing approach:
        1. First attempts to extract and parse content from code blocks (using DEFAULT_CODE_FENCES by default)
        2. Falls back to parsing the entire content as the specified format if stage 1 fails
        
        Args:
            content_string: The input string to parse
            parser_type: The format type to parse (json, yaml, xml, ini, csv, markdown), or None to return content unchanged
            code_fences: List of fence markers to look for. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction and parse full content.
            
        Returns:
            Parsed content in the appropriate Python data structure, or the original content_string if parser_type is None
            
        Raises:
            ParsingError: If the format is unsupported or parsing fails completely
        """
        # If parser_type is None or empty, return the content unchanged
        if not parser_type:
            return content_string
            
        # Explicitly set default code fences when None is provided
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
            
        parser = self.parsers.get(parser_type.lower())
        if parser:
            return parser(content_string, code_fences)
        self.logger.error(f"No parser method found for type '{parser_type}'")
        raise ParsingError(f"No parser method found for type '{parser_type}'")

    def auto_parse_content(self, text: str, code_fences: Optional[List[str]] = None) -> Any:
        """
        Automatically parse content by detecting the language from code fences.
        
        Args:
            text: The input text to parse
            code_fences: List of fence markers to look for. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction.
                        
        Returns:
            Parsed content if a supported language is detected, otherwise returns the raw text.
        """
        # Explicitly set default code fences when None is provided
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
            
        language, content = self.extract_code_block(text, code_fences)
        if language and language.lower() in self.list_supported_formats():
            return self.parse_by_format(content, language, code_fences=code_fences)
        self.logger.debug("No valid language detected for automatic parsing, returning raw text instead.")
        return text

    @staticmethod
    def _parse_ini(s: str) -> Dict[str, Any]:
        parser = configparser.ConfigParser()
        parser.read_string(s)
        return {section: dict(parser.items(section)) for section in parser.sections()}

    @staticmethod
    def _parse_csv(s: str) -> List[Dict[str, Any]]:
        reader = csv.DictReader(StringIO(s))
        return list(reader)

    @staticmethod
    def list_supported_formats():
        return ['xml', 'json', 'yaml', 'ini', 'csv', 'markdown']

    @staticmethod
    def get_dot_notated(source: dict, key: str):
        """
        Helper to get a value from a dict using dot notation (e.g., 'foo.bar.baz').
        Returns None if any part of the path is missing.
        
        Args:
            source: Source dictionary
            key: Dot-notated key string
            
        Returns:
            Value at the specified path, or None if not found
        """
        if not isinstance(source, dict):
            return None
        parts = key.split('.')
        current = source
        for part in parts:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return None
        return current

    @staticmethod
    def flatten_dict(d: dict, parent_key: str = '', sep: str = '.') -> dict:
        """
        Flattens a nested dictionary.

        Args:
            d (dict): The dictionary to flatten.
            parent_key (str): The parent key (used for recursion).
            sep (str): The separator to use between keys.

        Returns:
            dict: The flattened dictionary.
        """
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(ParsingProcessor.flatten_dict(v, new_key, sep=sep).items())
            else:
                items.append((new_key, v))
        return dict(items)

    @staticmethod
    def parse_markdown_to_dict(markdown_text: str, min_heading_level=2, max_heading_level=6) -> Optional[
        Dict[str, Any]]:
        parsed_dict = {}
        current_heading = None
        content_lines = []
        heading_pattern = re.compile(r'^(#{%d,%d})\s+(.*)' % (min_heading_level, max_heading_level))
        for line in markdown_text.split('\n'):
            match = heading_pattern.match(line)
            if match:
                if current_heading is not None:
                    parsed_dict[current_heading] = '\n'.join(content_lines).strip()
                    content_lines = []
                current_heading = match.group(2).strip()
            else:
                if current_heading is not None:
                    content_lines.append(line)
        if current_heading is not None:
            parsed_dict[current_heading] = '\n'.join(content_lines).strip()
        return parsed_dict if parsed_dict else None

    def format_string(self, input_str):
        """
        Formats a string to meet requirements of chroma collection name. Performs the following steps in order:

        Remove leading and trailing whitespace
        Replace non-alphanumeric
        Replace consecutive periods
        Ensure not a valid IPv4
        Ensure it starts with an alphanumeric character
        Ensure it ends with an alphanumeric character
        Ensure length is at least 3 characters
        Ensure length is not more than 64 characters
        Lower casing string

        Parameters:
        - input_str (str): The string to format.

        Returns:
        - str: The formatted string.
        """

        self.logger.debug(f"Formatting string:\n{input_str}")
        # Remove leading and trailing whitespace
        input_str = input_str.strip()
        self.logger.debug(f"Remove leading and trailing whitespace:\n{input_str}")

        # Replace non-alphanumeric, non-underscore, non-hyphen characters with underscores
        input_str = re.sub("[^a-zA-Z0-9_-]", "_", input_str)
        self.logger.debug(f"Replacing non-alphanumeric:\n{input_str}")

        # Replace consecutive periods with a single period
        while ".." in input_str:
            input_str = input_str.replace("..", ".")
            self.logger.debug(f"Replacing consecutive periods:\n{input_str}")

        # Ensure it's not a valid IPv4 address
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', input_str):
            input_str = "a" + input_str
            self.logger.debug(f"Ensuring not a valid IPv4:\n{input_str}")

        # Ensure it starts and ends with an alphanumeric character
        if not input_str[0].isalnum():
            input_str = "a" + input_str[1:]
            self.logger.debug(f"Ensure it starts with an alphanumeric character:\n{input_str}")
        if not input_str[-1].isalnum():
            input_str = input_str[:-1] + "a"
            self.logger.debug(f"Ensure it ends with an alphanumeric character:\n{input_str}")

        # Ensure length is between 3 and 64 characters
        while len(input_str) < 3:
            input_str += input_str
            self.logger.debug(f"Ensure length is at least 3 characters:\n{input_str}")
        if len(input_str) > 63:
            input_str = input_str[:63]
            self.logger.debug(f"Ensure length is not more than 64 characters:\n{input_str}")

        input_str = input_str.lower()
        self.logger.debug(f"Lower casing string:\n{input_str}")

        return input_str

    @staticmethod
    def flatten_to_string_list(data) -> list[str]:
        """
        Recursively flattens any dict or list into a list of 'key: value' strings, where keys are dot/bracket notated paths.
        This is used to serialize memory updates for ChromaDB, ensuring all data is stored as flat strings.
        Example: {'a': {'b': [1, 2]}} -> ['a.b[0]: 1', 'a.b[1]: 2']
        """
        def _flatten(obj, parent_key=''):
            items = []
            if isinstance(obj, dict):
                for k, v in obj.items():
                    new_key = f"{parent_key}.{k}" if parent_key else k
                    items.extend(_flatten(v, new_key))
            elif isinstance(obj, list):
                for i, v in enumerate(obj):
                    new_key = f"{parent_key}[{i}]"
                    items.extend(_flatten(v, new_key))
            else:
                items.append((parent_key, obj))
            return items

        flat = _flatten(data)
        return [f"{k}: {v}" for k, v in flat]

# From utils/parsing_processor.py
def extract_code_block(self, text: str, code_fences: Optional[List[str]] = None) -> Tuple[Optional[str], str]:
        """
        Extract the first code block from text, returning the language and content.
        
        This method is used in the first stage of the two-stage parsing approach.
        If no code fences are found, returns the entire stripped text for fallback parsing.
        
        Args:
            text: The text containing potential code blocks
            code_fences: List of fence markers to use. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction and return full text.
            
        Returns:
            Tuple of (language, content) where:
            - language: The detected language specifier (or None if not found/specified)
            - content: The extracted code block content or stripped full text if no fence found
        """
        # Use default code fences if None provided, otherwise use the provided list (including empty list)
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
        # Early exit if no code fences provided
        if not code_fences:
            return None, text.strip()
        
        # First try standard fence pattern matching
        for fence in code_fences:
            escaped_fence = re.escape(fence)
            # Match fenced code blocks with language specifier
            pattern = fr"{escaped_fence}([a-zA-Z]*)[ \t]*\r?\n?([\s\S]*?){escaped_fence}"
            match = re.search(pattern, text, re.DOTALL)
            if match:
                language = match.group(1).strip() or None
                content = match.group(2).strip()
                return language, content

            # Also try handling no-language blocks with just the fence
            pattern = fr"{escaped_fence}\s*\r?\n([\s\S]*?){escaped_fence}"
            match = re.search(pattern, text, re.DOTALL)
            if match:
                return None, match.group(1).strip()
        
        # No code blocks found with standard patterns
        return None, text.strip()

# From utils/parsing_processor.py
def sanitize_yaml_content(self, content: str, 
                             primary_fence: str = None, 
                             alternate_fence: str = None) -> str:
        """
        Sanitize YAML content by handling nested code blocks.
        
        Args:
            content: The YAML content to sanitize
            primary_fence: The main fence to replace
            alternate_fence: The replacement fence
        """
        if not primary_fence or not alternate_fence:
            return content.strip()
        
        def replace_inner_fences(match):
            inner_block = match.group(0)
            return inner_block.replace(primary_fence, alternate_fence)
        
        # Pattern based on primary fence
        pattern = fr"{re.escape(primary_fence)}[a-zA-Z]*\s*\r?\n[\s\S]*?{re.escape(primary_fence)}"
        content = re.sub(pattern, replace_inner_fences, content)
        
        # Then handle any other sanitization
        if content.strip().startswith('```'):
            content = re.sub(r'^```.*?\n', '', content, flags=re.DOTALL)
            content = re.sub(r'```\s*$', '', content)
            self.logger.warning("Removed outer code blocks in YAML content")
        
        return content.strip()

# From utils/parsing_processor.py
def preprocess_json_string(s: str) -> str:
        """
        Preprocess a string to increase the chance of successful JSON parsing from LLM outputs.
        - Trims whitespace
        - Extracts the first JSON object if stray text is present
        - Removes trailing commas in objects/arrays
        - Double-escapes backslashes not already escaped (for LaTeX/math)
        """
        import re
        s = s.strip()
        # Extract the first JSON object (handles stray text)
        match = re.search(r'(\{.*\})', s, re.DOTALL)
        if match:
            s = match.group(1)
        # Remove trailing commas in objects/arrays
        s = re.sub(r',([ \t\r\n]*[}\]])', r'\1', s)
        # Double-escape backslashes that are not already double-escaped
        s = re.sub(r'(?<!\\)\\(?![\\/\"bfnrtu])', r'\\\\', s)
        return s

# From utils/parsing_processor.py
def parse_content(self, content_string: str, parser_func: Callable[[str], Any],
                      expected_language: str, code_fences: Optional[List[str]] = None) -> Any:
        """
        Parse content using a two-stage approach: code-fenced first, then bare parsing fallback.
        
        Args:
            content_string: The input string to parse
            parser_func: The function to use for parsing (e.g., json.loads, yaml.safe_load)
            expected_language: The expected language/format type
            code_fences: List of fence markers to look for. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction and parse full content.
            
        Returns:
            Parsed content or None if parsing fails
        """
        # Use default code fences if None provided
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
        
        # Stage 1: Try code-fenced parsing
        language, extracted_content = self.extract_code_block(content_string, code_fences)
        
        # Log the code-fenced extraction attempt
        if language is not None:
            self.logger.debug(f"Code-fenced block detected with language '{language}' for {expected_language.upper()} parsing")
        elif code_fences:
            self.logger.debug(f"No code-fenced block found using fences {code_fences}, extracted content length: {len(extracted_content)}")
        else:
            self.logger.debug(f"No code fences specified, proceeding with full content for {expected_language.upper()} parsing")
        
        # Check language match and warn if different
        if language and language.lower() != expected_language.lower():
            self.logger.warning(f"Expected {expected_language.upper()} code block, but found '{language}'")
        
        # Attempt to parse the extracted (code-fenced) content
        if extracted_content:
            try:
                processed_content = extracted_content
                if expected_language.lower() == 'yaml':
                    processed_content = self.sanitize_yaml_content(extracted_content)
                if expected_language.lower() == 'json':
                    processed_content = self.preprocess_json_string(processed_content)
                    self.logger.debug(f"Cleaned JSON string before parsing (first 500 chars): {processed_content[:500]}")
                self.logger.debug(f"Attempting code-fenced {expected_language.upper()} parsing on content of length {len(processed_content)}")
                result = parser_func(processed_content)
                self.logger.debug(f"Code-fenced {expected_language.upper()} parsing succeeded")
                return result
                
            except Exception as e:
                # Log the code-fenced parsing failure
                preview = processed_content[:100] + ('...' if len(processed_content) > 100 else '')
                self.logger.warning(f"Code-fenced {expected_language.upper()} parsing failed: {str(e)}")
                self.logger.debug(f"Failed content preview: {preview}")
                if expected_language.lower() == 'json':
                    self.logger.error(f"Cleaned JSON string on failure (first 500 chars): {processed_content[:500]}")
                
                # Stage 2: Fallback to bare parsing if code-fenced parsing failed
                # Only attempt if we had extracted a code block and it failed
                if language is not None or code_fences:
                    return self._attempt_bare_parsing(content_string, parser_func, expected_language)
                else:
                    # If no code fences were specified, don't try again
                    raise ParsingError(f"Failed to parse {expected_language}: {e}") from e
        
        # If no content was extracted, attempt bare parsing
        return self._attempt_bare_parsing(content_string, parser_func, expected_language)

# From utils/parsing_processor.py
def parse_by_format(self, content_string: str, parser_type: Optional[str], code_fences: Optional[List[str]] = None) -> Any:
        """
        Parse content using the specified format with two-stage parsing approach.
        
        This method uses the two-stage parsing approach:
        1. First attempts to extract and parse content from code blocks (using DEFAULT_CODE_FENCES by default)
        2. Falls back to parsing the entire content as the specified format if stage 1 fails
        
        Args:
            content_string: The input string to parse
            parser_type: The format type to parse (json, yaml, xml, ini, csv, markdown), or None to return content unchanged
            code_fences: List of fence markers to look for. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction and parse full content.
            
        Returns:
            Parsed content in the appropriate Python data structure, or the original content_string if parser_type is None
            
        Raises:
            ParsingError: If the format is unsupported or parsing fails completely
        """
        # If parser_type is None or empty, return the content unchanged
        if not parser_type:
            return content_string
            
        # Explicitly set default code fences when None is provided
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
            
        parser = self.parsers.get(parser_type.lower())
        if parser:
            return parser(content_string, code_fences)
        self.logger.error(f"No parser method found for type '{parser_type}'")
        raise ParsingError(f"No parser method found for type '{parser_type}'")

# From utils/parsing_processor.py
def auto_parse_content(self, text: str, code_fences: Optional[List[str]] = None) -> Any:
        """
        Automatically parse content by detecting the language from code fences.
        
        Args:
            text: The input text to parse
            code_fences: List of fence markers to look for. If None, uses DEFAULT_CODE_FENCES (['```']).
                        Pass an empty list [] to disable code fence extraction.
                        
        Returns:
            Parsed content if a supported language is detected, otherwise returns the raw text.
        """
        # Explicitly set default code fences when None is provided
        if code_fences is None:
            code_fences = self.DEFAULT_CODE_FENCES
            
        language, content = self.extract_code_block(text, code_fences)
        if language and language.lower() in self.list_supported_formats():
            return self.parse_by_format(content, language, code_fences=code_fences)
        self.logger.debug("No valid language detected for automatic parsing, returning raw text instead.")
        return text

# From utils/parsing_processor.py
def list_supported_formats():
        return ['xml', 'json', 'yaml', 'ini', 'csv', 'markdown']

# From utils/parsing_processor.py
def get_dot_notated(source: dict, key: str):
        """
        Helper to get a value from a dict using dot notation (e.g., 'foo.bar.baz').
        Returns None if any part of the path is missing.
        
        Args:
            source: Source dictionary
            key: Dot-notated key string
            
        Returns:
            Value at the specified path, or None if not found
        """
        if not isinstance(source, dict):
            return None
        parts = key.split('.')
        current = source
        for part in parts:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return None
        return current

# From utils/parsing_processor.py
def flatten_dict(d: dict, parent_key: str = '', sep: str = '.') -> dict:
        """
        Flattens a nested dictionary.

        Args:
            d (dict): The dictionary to flatten.
            parent_key (str): The parent key (used for recursion).
            sep (str): The separator to use between keys.

        Returns:
            dict: The flattened dictionary.
        """
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(ParsingProcessor.flatten_dict(v, new_key, sep=sep).items())
            else:
                items.append((new_key, v))
        return dict(items)

# From utils/parsing_processor.py
def parse_markdown_to_dict(markdown_text: str, min_heading_level=2, max_heading_level=6) -> Optional[
        Dict[str, Any]]:
        parsed_dict = {}
        current_heading = None
        content_lines = []
        heading_pattern = re.compile(r'^(#{%d,%d})\s+(.*)' % (min_heading_level, max_heading_level))
        for line in markdown_text.split('\n'):
            match = heading_pattern.match(line)
            if match:
                if current_heading is not None:
                    parsed_dict[current_heading] = '\n'.join(content_lines).strip()
                    content_lines = []
                current_heading = match.group(2).strip()
            else:
                if current_heading is not None:
                    content_lines.append(line)
        if current_heading is not None:
            parsed_dict[current_heading] = '\n'.join(content_lines).strip()
        return parsed_dict if parsed_dict else None

# From utils/parsing_processor.py
def format_string(self, input_str):
        """
        Formats a string to meet requirements of chroma collection name. Performs the following steps in order:

        Remove leading and trailing whitespace
        Replace non-alphanumeric
        Replace consecutive periods
        Ensure not a valid IPv4
        Ensure it starts with an alphanumeric character
        Ensure it ends with an alphanumeric character
        Ensure length is at least 3 characters
        Ensure length is not more than 64 characters
        Lower casing string

        Parameters:
        - input_str (str): The string to format.

        Returns:
        - str: The formatted string.
        """

        self.logger.debug(f"Formatting string:\n{input_str}")
        # Remove leading and trailing whitespace
        input_str = input_str.strip()
        self.logger.debug(f"Remove leading and trailing whitespace:\n{input_str}")

        # Replace non-alphanumeric, non-underscore, non-hyphen characters with underscores
        input_str = re.sub("[^a-zA-Z0-9_-]", "_", input_str)
        self.logger.debug(f"Replacing non-alphanumeric:\n{input_str}")

        # Replace consecutive periods with a single period
        while ".." in input_str:
            input_str = input_str.replace("..", ".")
            self.logger.debug(f"Replacing consecutive periods:\n{input_str}")

        # Ensure it's not a valid IPv4 address
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', input_str):
            input_str = "a" + input_str
            self.logger.debug(f"Ensuring not a valid IPv4:\n{input_str}")

        # Ensure it starts and ends with an alphanumeric character
        if not input_str[0].isalnum():
            input_str = "a" + input_str[1:]
            self.logger.debug(f"Ensure it starts with an alphanumeric character:\n{input_str}")
        if not input_str[-1].isalnum():
            input_str = input_str[:-1] + "a"
            self.logger.debug(f"Ensure it ends with an alphanumeric character:\n{input_str}")

        # Ensure length is between 3 and 64 characters
        while len(input_str) < 3:
            input_str += input_str
            self.logger.debug(f"Ensure length is at least 3 characters:\n{input_str}")
        if len(input_str) > 63:
            input_str = input_str[:63]
            self.logger.debug(f"Ensure length is not more than 64 characters:\n{input_str}")

        input_str = input_str.lower()
        self.logger.debug(f"Lower casing string:\n{input_str}")

        return input_str

# From utils/parsing_processor.py
def flatten_to_string_list(data) -> list[str]:
        """
        Recursively flattens any dict or list into a list of 'key: value' strings, where keys are dot/bracket notated paths.
        This is used to serialize memory updates for ChromaDB, ensuring all data is stored as flat strings.
        Example: {'a': {'b': [1, 2]}} -> ['a.b[0]: 1', 'a.b[1]: 2']
        """
        def _flatten(obj, parent_key=''):
            items = []
            if isinstance(obj, dict):
                for k, v in obj.items():
                    new_key = f"{parent_key}.{k}" if parent_key else k
                    items.extend(_flatten(v, new_key))
            elif isinstance(obj, list):
                for i, v in enumerate(obj):
                    new_key = f"{parent_key}[{i}]"
                    items.extend(_flatten(v, new_key))
            else:
                items.append((parent_key, obj))
            return items

        flat = _flatten(data)
        return [f"{k}: {v}" for k, v in flat]

# From utils/parsing_processor.py
def replace_inner_fences(match):
            inner_block = match.group(0)
            return inner_block.replace(primary_fence, alternate_fence)


# From utils/tool_utils.py
class ToolUtils:
    """
    A utility class for dynamically interacting with tools. It supports dynamically importing tool modules,
    executing specified commands within those modules, and handling tool priming for display purposes.

    Attributes:
        logger (Logger): Logger instance for logging messages.
    """

    BUILTIN_FUNCTIONS = {
        'print': print,
        'len': len,
        'sum': sum,
        'max': max,
        'min': min
        # Add more built-in functions if needed
    }

    def __init__(self):
        """
        Initializes the ToolUtils class with a Logger instance.
        """
        self.logger = Logger(name=self.__class__.__name__)
        self.storage = ChromaStorage.get_or_create(storage_id="tool_library")

    # --------------------------------------------------------------------------------------------------------
    # ----------------------------------------- Dynamic Tool Methods -----------------------------------------
    # --------------------------------------------------------------------------------------------------------

    def dynamic_tool(self, tool: Dict[str, str], payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Dynamically loads a tool module and executes a specified command within it, using arguments provided in the
        payload.

        Parameters:
            tool (dict): The tool to be dynamically imported.
            payload (dict): A dictionary containing the 'command' to be executed and 'args' for the command.

        Returns:
            dict: The result of executing the command within the tool, or an error dictionary if an error occurs.
        """
        tool_module = tool.get('Script')
        tool_class = tool.get('Class')
        command = tool.get('Command')
        args = payload['args']
        self.logger.info(f"\nRunning {tool_class} ...")

        try:
            result = self._execute_tool(tool_module, tool_class, command, args)
            self.logger.log(f'\n{tool_class} Result:\n{result}', 'info', 'Actions')
            return {'status': 'success', 'data': result}
        except (AttributeError, TypeError, Exception) as e:
            return self._handle_error(e, tool_module, tool_class, command)

    def _execute_tool(self, tool_module: str, tool_class: str, command: str, args: Dict[str, Any]) -> Any:
        """
        Executes the specified command within the tool module.

        Parameters:
            tool_module (str): The tool module to be imported.
            tool_class (str): The class within the tool module.
            command (str): The command to be executed.
            args (dict): The arguments for the command.

        Returns:
            Any: The result of executing the command.
        """
        if tool_module in self.BUILTIN_FUNCTIONS:
            command_func = self.BUILTIN_FUNCTIONS[tool_module]  # type: ignore
            result = command_func(**args)
        else:
            if tool_module.startswith('.agentforge'):
                # Remove '.agentforge' from the beginning of the path
                relative_path = tool_module.replace('.agentforge', '', 1)
                tool = importlib.import_module(relative_path, package='agentforge')
            else:
                tool = importlib.import_module(tool_module)
            if hasattr(tool, tool_class):
                tool_instance = getattr(tool, tool_class)()
                command_func = getattr(tool_instance, command)
            else:
                command_func = getattr(tool, command)

            result = command_func(**args)

        return result

    def _handle_error(self, e: Exception, tool_module: str, tool_class: str, command: str) -> Dict[str, Any]:
        """
        Handles errors encountered during command execution.

        Parameters:
            e (Exception): The exception raised.
            tool_module (str): The tool module where the error occurred.
            tool_class (str): The class within the tool module where the error occurred.
            command (str): The command that caused the error.

        Returns:
            dict: An error dictionary with the error message and traceback.
        """
        if isinstance(e, AttributeError):
            error_message = f"Tool '{tool_module}' does not have a class named '{tool_class}' or command named '{command}'.\nError: {e}"
        elif isinstance(e, TypeError):
            error_message = f"Error passing arguments: {e}"
        else:
            error_message = f"Error executing command: {e}"

        self.logger.error(error_message)
        return {'status': 'failure', 'message': error_message, 'traceback': traceback.format_exc()}

    # --------------------------------------------------------------------------------------------------------
    # ------------------------------------ Parsing and Formatting Methods ------------------------------------
    # --------------------------------------------------------------------------------------------------------

    @staticmethod
    def format_item(item: Dict[str, Union[str, List[str]]],
                    order: Optional[List[str]] = None) -> str:
        """
        Formats an item (action or tool) into a human-readable string.

        Parameters:
            item (Dict[str, Union[str, List[str]]]): The item to format.
            order (Optional[List[str]]): The order in which to format the item's keys.

        Returns:
            str: The formatted item string.
        """
        if order is None:
            order = list(item.keys())

        formatted_string = ""
        for key in order:
            if key in item:
                value = item[key]
                if isinstance(value, list):
                    formatted_list = "\n- ".join([str(items).strip() for items in value])
                    formatted_string += f"{key}:\n- {formatted_list}\n\n"
                elif isinstance(value, str):
                    if len(value.splitlines()) > 1:
                        formatted_string += f"{key}:\n{value.strip()}\n\n"
                    else:
                        formatted_string += f"{key}: {value.strip()}\n"
        return formatted_string.strip()

    def format_item_list(self, items: Dict, order: Optional[List[str]] = None) -> Optional[str]:
        """
        Formats the actions into a human-readable string based on a given order and stores it in the agent's data for
        later use.

        Parameters:
            items (Dict): The list of actions or tools to format.
            order (Optional[List[str]]): The order in which to format the action's keys.

        Returns:
            Optional[str]: The formatted string of actions, or None if an error occurs.
        """
        try:
            formatted_actions = []
            for item_name, metadata in items.items():
                formatted_action = self.format_item(metadata, order)
                formatted_actions.append(formatted_action)
            return "---\n" + "\n---\n".join(formatted_actions) + "\n---"
        except Exception as e:
            self.logger.error(f"Error Formatting Item List:\n{items}\n\nError: {e}")
            return None

# From utils/tool_utils.py
def dynamic_tool(self, tool: Dict[str, str], payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Dynamically loads a tool module and executes a specified command within it, using arguments provided in the
        payload.

        Parameters:
            tool (dict): The tool to be dynamically imported.
            payload (dict): A dictionary containing the 'command' to be executed and 'args' for the command.

        Returns:
            dict: The result of executing the command within the tool, or an error dictionary if an error occurs.
        """
        tool_module = tool.get('Script')
        tool_class = tool.get('Class')
        command = tool.get('Command')
        args = payload['args']
        self.logger.info(f"\nRunning {tool_class} ...")

        try:
            result = self._execute_tool(tool_module, tool_class, command, args)
            self.logger.log(f'\n{tool_class} Result:\n{result}', 'info', 'Actions')
            return {'status': 'success', 'data': result}
        except (AttributeError, TypeError, Exception) as e:
            return self._handle_error(e, tool_module, tool_class, command)

# From utils/tool_utils.py
def format_item(item: Dict[str, Union[str, List[str]]],
                    order: Optional[List[str]] = None) -> str:
        """
        Formats an item (action or tool) into a human-readable string.

        Parameters:
            item (Dict[str, Union[str, List[str]]]): The item to format.
            order (Optional[List[str]]): The order in which to format the item's keys.

        Returns:
            str: The formatted item string.
        """
        if order is None:
            order = list(item.keys())

        formatted_string = ""
        for key in order:
            if key in item:
                value = item[key]
                if isinstance(value, list):
                    formatted_list = "\n- ".join([str(items).strip() for items in value])
                    formatted_string += f"{key}:\n- {formatted_list}\n\n"
                elif isinstance(value, str):
                    if len(value.splitlines()) > 1:
                        formatted_string += f"{key}:\n{value.strip()}\n\n"
                    else:
                        formatted_string += f"{key}: {value.strip()}\n"
        return formatted_string.strip()

# From utils/tool_utils.py
def format_item_list(self, items: Dict, order: Optional[List[str]] = None) -> Optional[str]:
        """
        Formats the actions into a human-readable string based on a given order and stores it in the agent's data for
        later use.

        Parameters:
            items (Dict): The list of actions or tools to format.
            order (Optional[List[str]]): The order in which to format the action's keys.

        Returns:
            Optional[str]: The formatted string of actions, or None if an error occurs.
        """
        try:
            formatted_actions = []
            for item_name, metadata in items.items():
                formatted_action = self.format_item(metadata, order)
                formatted_actions.append(formatted_action)
            return "---\n" + "\n---\n".join(formatted_actions) + "\n---"
        except Exception as e:
            self.logger.error(f"Error Formatting Item List:\n{items}\n\nError: {e}")
            return None


# From utils/prompt_processor.py
class PromptProcessor:
    """
    A utility class for handling dynamic prompt templates. It supports extracting variables from templates,
    checking for the presence of required variables in data, and rendering templates with values from provided data,
    including nested placeholders such as {agent_id.nested_key.sub_key}.
    """

    # Pattern to find all occurrences of {some_key} or {some_key.nested_key}
    # Explanation:
    #   - \{ and \} match literal curly braces
    #   - ([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)
    #       - [a-zA-Z_][a-zA-Z0-9_]* matches an initial variable name, e.g. "A1"
    #       - (?:\.[a-zA-Z_][a-zA-Z0-9_]*)* optionally allows dot notation for nested keys, e.g. ".answer.more"
    #
    pattern = r"\{([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\}"

    def __init__(self):
        """
        Initializes the PromptHandling class with a Logger instance.
        """
        self.logger = Logger(name=self.__class__.__name__, default_logger=self.__class__.__name__.lower())

    ##################################################
    # Variable Extraction & Nested Lookups
    ##################################################

    def extract_prompt_variables(self, template: str) -> list:
        """
        Extracts variable names from a prompt template.

        Parameters:
            template (str): The prompt template containing variables within curly braces.

        Returns:
            list: A list of variable names extracted from the template.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during the extraction process.
        """
        try:
            return re.findall(self.pattern, template)
        except Exception as e:
            error_message = f"Error extracting prompt variables: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

    @staticmethod
    def _nested_lookup(data: dict, path: str):
        """
        Performs a nested lookup in 'data' by splitting 'path' on dots.
        Returns None if any key isn't found along the way.

        Example:
            data = {"A2": {"answer": "42"}}, path = "A2.answer"
            -> returns "42"
        """
        keys = path.split('.')
        val = data
        for key in keys:
            if isinstance(val, dict) and key in val:
                val = val[key]
            else:
                return None
        return val

    ##################################################
    # Template Checking
    ##################################################

    def handle_prompt_template(self, prompt_template: str, data: dict) -> str | None:
        """
        Checks if all required variables in a prompt template are present and not empty in the provided data.
        Returns the template if conditions are met, or None otherwise.

        Parameters:
            prompt_template (str): The prompt template to check.
            data (dict): The data dictionary to check for required variables.

        Returns:
            str or None: The original prompt template if all variables are present and not empty, None otherwise.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during the process.
        """
        try:
            required_vars = self.extract_prompt_variables(prompt_template)
            if not required_vars:
                return prompt_template  # no placeholders, so just return as is

            # For each required variable, ensure there's a non-empty value in data
            # if we do nested lookups, we check them individually
            for var in required_vars:
                val = self._nested_lookup(data, var)
                if not val:
                    return None
            return prompt_template
        except Exception as e:
            error_message = f"Error handling prompt template: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

    ##################################################
    # Template Rendering
    ##################################################

    def render_prompt_template(self, template: str, data: dict) -> str:
        """
        Renders a prompt template by replacing each variable with its corresponding value from provided data.
        Supports nested placeholders like {A2.answer}.

        Parameters:
            template (str): The prompt template containing variables.
            data (dict): The data dictionary containing values for the variables in the template.

        Returns:
            str: The rendered template with variables replaced by their corresponding data values.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during the rendering process.
        """
        try:
            def replacement_function(match):
                variable_name = match.group(1)  # e.g. "A2.answer"
                val = self._nested_lookup(data, variable_name)
                # If val is None, we preserve the original placeholder.
                if val is not None:
                    formatted_val = self.value_to_markdown(val)
                    # Detect indentation before the placeholder
                    start = match.start()
                    # Find the start of the line
                    line_start = template.rfind('\n', 0, start) + 1
                    indent = template[line_start:start]
                    # Apply indentation to all lines except the first
                    lines = formatted_val.split('\n')
                    if len(lines) > 1:
                        lines = [lines[0]] + [indent + line if line.strip() else line for line in lines[1:]]
                    return '\n'.join(lines).strip()
                else:
                    return match.group(0)


            variable_pattern = re.compile(self.pattern)
            # Perform variable substitution
            prompt = variable_pattern.sub(replacement_function, template)

            # Then, unescape any escaped braces
            prompt = self.unescape_braces(prompt)

            return prompt
        except Exception as e:
            error_message = f"Error rendering prompt template: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

    def render_prompts(self, prompts, data):
        """
        Renders the 'system' and 'user' prompts separately and validates that they are not empty.

        Parameters:
            prompts (dict): The dictionary containing 'system' and 'user' prompts.
            data (dict): The data dictionary containing values for the variables.

        Returns:
            dict: A dictionary containing the rendered 'system' and 'user' prompts.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during prompt rendering.
            ValueError: If any of the rendered prompts are empty strings.
        """
        try:
            rendered_prompts = {}
            for prompt_type in ['system', 'user']:
                rendered_sections = []
                prompt_content = prompts.get(prompt_type, {})
                if isinstance(prompt_content, str):
                    prompt_sections = {'main': prompt_content}
                else:
                    prompt_sections = prompt_content

                for prompt_name, prompt_template in prompt_sections.items():
                    template = self.handle_prompt_template(prompt_template, data)
                    if template:
                        rendered_prompt = self.render_prompt_template(template, data)
                        rendered_sections.append(rendered_prompt)
                    else:
                        self.logger.info(
                            f"Skipping '{prompt_name}' in '{prompt_type}' prompt due to missing variables."
                        )
                # Join the rendered sections into a single string for each prompt type
                final_prompt = '\n'.join(rendered_sections)
                rendered_prompts[prompt_type] = final_prompt
            
            # Validate rendered prompts before returning
            self._validate_rendered_prompts(rendered_prompts)
            
            return rendered_prompts
        except Exception as e:
            error_message = f"Error rendering prompts: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

    def _validate_rendered_prompts(self, rendered_prompts):
        """
        Internal method to validate the rendered prompts to ensure none are empty.

        Parameters:
            rendered_prompts (dict): A dictionary containing the rendered prompts.

        Raises:
            ValueError: If any of the prompts are empty strings after rendering.
        """
        for prompt_type, prompt_content in rendered_prompts.items():
            if not prompt_content.strip():
                error_message = (
                    f"Error: The '{prompt_type}' prompt is empty after rendering. "
                    f"Please check your prompt templates and data."
                )
                self.logger.error(error_message)
                raise ValueError(error_message)

    def build_persona_markdown(self, static_content, persona_settings):
        """
        Build markdown representation of static persona content for system prompt injection.
        Truncate if exceeds character cap from settings.
        
        Args:
            static_content (dict): Dictionary containing static content from persona
            persona_settings (dict): Dictionary containing persona settings
            
        Returns:
            str: Markdown formatted representation of persona static content
        """
        if not static_content:
            return None
            
        # Use the centralized markdown formatting helper
        persona_md = self.value_to_markdown(static_content)
        
        # Get character cap from settings - treat 0 as no cap
        if hasattr(persona_settings, 'static_char_cap'):
            static_char_cap = getattr(persona_settings, 'static_char_cap', 8000)
        elif isinstance(persona_settings, dict):
            static_char_cap = persona_settings.get('static_char_cap', 8000)
        else:
            static_char_cap = 8000
        
        # Only truncate if cap is greater than 0 and persona_md exceeds the cap
        if static_char_cap > 0 and len(persona_md) > static_char_cap:
            self.logger.warning(
                f"Persona markdown exceeds character cap ({len(persona_md)} > {static_char_cap}). "
                f"Truncating to {static_char_cap} characters."
            )
            persona_md = persona_md[:static_char_cap] + "..."
        
        return persona_md

    @staticmethod
    def unescape_braces(template: str) -> str:
        """
        Replaces all instances of /{.../} with {...} in the template.

        Parameters:
            template (str): The prompt template containing escaped braces.

        Returns:
            str: The template with escaped braces unescaped.
        """
        return re.sub(r'/\{(.*?)/}', r'{\1}', template)

    ##################################################
    # Value Formatting
    ##################################################

    def value_to_markdown(self, val: Any, indent: int = 0) -> str:
        """Render a dict, list, or scalar into minimalist Markdown."""
        pad = "  " * indent     # two-space indent per level

        if isinstance(val, Mapping):
            segments = []
            for k, v in val.items():
                if isinstance(v, (Mapping, Sequence)) and not isinstance(v, str):
                    segments.append(f"{pad}{k}:")
                    segments.append(self.value_to_markdown(v, indent + 1))
                else:
                    segments.append(f"{pad}{k}: {v}")
            return "\n".join(segments)

        if isinstance(val, Sequence) and not isinstance(val, str):
            segments = []
            for item in val:
                if isinstance(item, (Mapping, Sequence)) and not isinstance(item, str):
                    segments.append(f"{pad}- {self.value_to_markdown(item, indent + 1)}")
                else:
                    segments.append(f"{pad}- {item}")
            return "\n".join(segments)

        return f"{pad}{val}"

# From utils/prompt_processor.py
def extract_prompt_variables(self, template: str) -> list:
        """
        Extracts variable names from a prompt template.

        Parameters:
            template (str): The prompt template containing variables within curly braces.

        Returns:
            list: A list of variable names extracted from the template.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during the extraction process.
        """
        try:
            return re.findall(self.pattern, template)
        except Exception as e:
            error_message = f"Error extracting prompt variables: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

# From utils/prompt_processor.py
def handle_prompt_template(self, prompt_template: str, data: dict) -> str | None:
        """
        Checks if all required variables in a prompt template are present and not empty in the provided data.
        Returns the template if conditions are met, or None otherwise.

        Parameters:
            prompt_template (str): The prompt template to check.
            data (dict): The data dictionary to check for required variables.

        Returns:
            str or None: The original prompt template if all variables are present and not empty, None otherwise.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during the process.
        """
        try:
            required_vars = self.extract_prompt_variables(prompt_template)
            if not required_vars:
                return prompt_template  # no placeholders, so just return as is

            # For each required variable, ensure there's a non-empty value in data
            # if we do nested lookups, we check them individually
            for var in required_vars:
                val = self._nested_lookup(data, var)
                if not val:
                    return None
            return prompt_template
        except Exception as e:
            error_message = f"Error handling prompt template: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

# From utils/prompt_processor.py
def render_prompt_template(self, template: str, data: dict) -> str:
        """
        Renders a prompt template by replacing each variable with its corresponding value from provided data.
        Supports nested placeholders like {A2.answer}.

        Parameters:
            template (str): The prompt template containing variables.
            data (dict): The data dictionary containing values for the variables in the template.

        Returns:
            str: The rendered template with variables replaced by their corresponding data values.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during the rendering process.
        """
        try:
            def replacement_function(match):
                variable_name = match.group(1)  # e.g. "A2.answer"
                val = self._nested_lookup(data, variable_name)
                # If val is None, we preserve the original placeholder.
                if val is not None:
                    formatted_val = self.value_to_markdown(val)
                    # Detect indentation before the placeholder
                    start = match.start()
                    # Find the start of the line
                    line_start = template.rfind('\n', 0, start) + 1
                    indent = template[line_start:start]
                    # Apply indentation to all lines except the first
                    lines = formatted_val.split('\n')
                    if len(lines) > 1:
                        lines = [lines[0]] + [indent + line if line.strip() else line for line in lines[1:]]
                    return '\n'.join(lines).strip()
                else:
                    return match.group(0)


            variable_pattern = re.compile(self.pattern)
            # Perform variable substitution
            prompt = variable_pattern.sub(replacement_function, template)

            # Then, unescape any escaped braces
            prompt = self.unescape_braces(prompt)

            return prompt
        except Exception as e:
            error_message = f"Error rendering prompt template: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

# From utils/prompt_processor.py
def render_prompts(self, prompts, data):
        """
        Renders the 'system' and 'user' prompts separately and validates that they are not empty.

        Parameters:
            prompts (dict): The dictionary containing 'system' and 'user' prompts.
            data (dict): The data dictionary containing values for the variables.

        Returns:
            dict: A dictionary containing the rendered 'system' and 'user' prompts.

        Raises:
            Exception: Logs an error message and raises an exception if an error occurs during prompt rendering.
            ValueError: If any of the rendered prompts are empty strings.
        """
        try:
            rendered_prompts = {}
            for prompt_type in ['system', 'user']:
                rendered_sections = []
                prompt_content = prompts.get(prompt_type, {})
                if isinstance(prompt_content, str):
                    prompt_sections = {'main': prompt_content}
                else:
                    prompt_sections = prompt_content

                for prompt_name, prompt_template in prompt_sections.items():
                    template = self.handle_prompt_template(prompt_template, data)
                    if template:
                        rendered_prompt = self.render_prompt_template(template, data)
                        rendered_sections.append(rendered_prompt)
                    else:
                        self.logger.info(
                            f"Skipping '{prompt_name}' in '{prompt_type}' prompt due to missing variables."
                        )
                # Join the rendered sections into a single string for each prompt type
                final_prompt = '\n'.join(rendered_sections)
                rendered_prompts[prompt_type] = final_prompt
            
            # Validate rendered prompts before returning
            self._validate_rendered_prompts(rendered_prompts)
            
            return rendered_prompts
        except Exception as e:
            error_message = f"Error rendering prompts: {e}"
            self.logger.error(error_message)
            raise Exception(error_message)

# From utils/prompt_processor.py
def build_persona_markdown(self, static_content, persona_settings):
        """
        Build markdown representation of static persona content for system prompt injection.
        Truncate if exceeds character cap from settings.
        
        Args:
            static_content (dict): Dictionary containing static content from persona
            persona_settings (dict): Dictionary containing persona settings
            
        Returns:
            str: Markdown formatted representation of persona static content
        """
        if not static_content:
            return None
            
        # Use the centralized markdown formatting helper
        persona_md = self.value_to_markdown(static_content)
        
        # Get character cap from settings - treat 0 as no cap
        if hasattr(persona_settings, 'static_char_cap'):
            static_char_cap = getattr(persona_settings, 'static_char_cap', 8000)
        elif isinstance(persona_settings, dict):
            static_char_cap = persona_settings.get('static_char_cap', 8000)
        else:
            static_char_cap = 8000
        
        # Only truncate if cap is greater than 0 and persona_md exceeds the cap
        if static_char_cap > 0 and len(persona_md) > static_char_cap:
            self.logger.warning(
                f"Persona markdown exceeds character cap ({len(persona_md)} > {static_char_cap}). "
                f"Truncating to {static_char_cap} characters."
            )
            persona_md = persona_md[:static_char_cap] + "..."
        
        return persona_md

# From utils/prompt_processor.py
def unescape_braces(template: str) -> str:
        """
        Replaces all instances of /{.../} with {...} in the template.

        Parameters:
            template (str): The prompt template containing escaped braces.

        Returns:
            str: The template with escaped braces unescaped.
        """
        return re.sub(r'/\{(.*?)/}', r'{\1}', template)

# From utils/prompt_processor.py
def value_to_markdown(self, val: Any, indent: int = 0) -> str:
        """Render a dict, list, or scalar into minimalist Markdown."""
        pad = "  " * indent     # two-space indent per level

        if isinstance(val, Mapping):
            segments = []
            for k, v in val.items():
                if isinstance(v, (Mapping, Sequence)) and not isinstance(v, str):
                    segments.append(f"{pad}{k}:")
                    segments.append(self.value_to_markdown(v, indent + 1))
                else:
                    segments.append(f"{pad}{k}: {v}")
            return "\n".join(segments)

        if isinstance(val, Sequence) and not isinstance(val, str):
            segments = []
            for item in val:
                if isinstance(item, (Mapping, Sequence)) and not isinstance(item, str):
                    segments.append(f"{pad}- {self.value_to_markdown(item, indent + 1)}")
                else:
                    segments.append(f"{pad}- {item}")
            return "\n".join(segments)

        return f"{pad}{val}"

# From utils/prompt_processor.py
def replacement_function(match):
                variable_name = match.group(1)  # e.g. "A2.answer"
                val = self._nested_lookup(data, variable_name)
                # If val is None, we preserve the original placeholder.
                if val is not None:
                    formatted_val = self.value_to_markdown(val)
                    # Detect indentation before the placeholder
                    start = match.start()
                    # Find the start of the line
                    line_start = template.rfind('\n', 0, start) + 1
                    indent = template[line_start:start]
                    # Apply indentation to all lines except the first
                    lines = formatted_val.split('\n')
                    if len(lines) > 1:
                        lines = [lines[0]] + [indent + line if line.strip() else line for line in lines[1:]]
                    return '\n'.join(lines).strip()
                else:
                    return match.group(0)


# From utils/logger.py
class ColoredFormatter(logging.Formatter):
    """
    A custom logging formatter to add colors to console logs based on the log level.
    Uses ANSI escape codes for coloring without external dependencies.
    """

    COLOR_CODES = {
        logging.DEBUG: '\033[36m',     # Cyan
        logging.INFO: '\033[32m',      # Green
        logging.WARNING: '\033[33m',   # Yellow
        logging.ERROR: '\033[31m',     # Red
        logging.CRITICAL: '\033[41m',  # Red background
    }
    RESET_CODE = '\033[0m'

    def format(self, record: logging.LogRecord) -> str:
        color_code = self.COLOR_CODES.get(record.levelno, self.RESET_CODE)
        message = super().format(record)
        return f"{color_code}{message}{self.RESET_CODE}"

# From utils/logger.py
class BaseLogger:
    """
    A base logger class for setting up file and console logging with support for multiple handlers and log levels.

    This class provides mechanisms for initializing file and console log handlers, logging messages at various
    levels, and dynamically adjusting log levels.

    Attributes:
        file_handlers (dict): A class-level dictionary tracking file handlers by log file name.
        console_handlers (dict): A class-level dictionary tracking console handlers by logger name.
    """

    # Class-level dictionaries to track existing handlers
    file_handlers = {}
    console_handlers = {}

    def __init__(self, name: str = 'BaseLogger', log_file: str = 'default.log', log_level: str = 'error') -> None:
        """
        Initializes the BaseLogger with optional name, log file, and log level.

        Parameters:
            name (str): The name of the logger.
            log_file (str): The name of the file to log messages to.
            log_level (str): The initial log level for the file handler.
        """
        self.config = Config()
        self.logger = logging.getLogger(name)
        self.log_folder = self.config.settings.system.logging.folder
        self.log_file = log_file

        if not self.config.settings.system.logging.enabled:
            self.logger.setLevel(logging.CRITICAL + 1)  # Disable logging
            return

        file_level = self._get_level_code(log_level)
        console_level = self._get_level_code(
            self.config.settings.system.logging.console_level
        )
        self.logger.setLevel(min(file_level, console_level))

        self._setup_file_handler(file_level)
        self._setup_console_handler(console_level)

    @staticmethod
    def _get_level_code(level: str) -> int:
        """
        Converts a log level as a string to the corresponding logging module level code.

        Parameters:
            level (str): The log level as a string (e.g., 'debug', 'info', 'warning', 'error', 'critical').

        Returns:
            int: The logging module level code corresponding to the provided string.
        """
        level_dict = {
            'debug': logging.DEBUG,
            'info': logging.INFO,
            'warning': logging.WARNING,
            'error': logging.ERROR,
            'critical': logging.CRITICAL,
        }
        return level_dict.get(level.lower(), logging.INFO)

    def _setup_console_handler(self, level: int) -> None:
        """
        Sets up a console handler for logging messages to the console. Configures logging format and level.

        Parameters:
            level (int): The logging level to set for the console handler.
        """
        formatter = ColoredFormatter('%(levelname)s: %(message)s')

        if self.logger.name in BaseLogger.console_handlers:
            ch = BaseLogger.console_handlers[self.logger.name]
            if ch not in self.logger.handlers:
                ch.setLevel(level)
                ch.setFormatter(formatter)
                self.logger.addHandler(ch)
            return

        ch = logging.StreamHandler()
        ch.setLevel(level)
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)
        BaseLogger.console_handlers[self.logger.name] = ch

    def _setup_file_handler(self, level: int) -> None:
        """
        Sets up a file handler for logging messages to a file. Initializes the log folder and file if they do not exist,
        and configures logging format and level.

        Parameters:
            level (int): The logging level to set for the file handler.
        """
        self._initialize_logging()

        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s\n-------------------------------------------------------------',
            datefmt='%Y-%m-%d %H:%M:%S'
        )

        if self.log_file in BaseLogger.file_handlers:
            fh = BaseLogger.file_handlers[self.log_file]
            if fh not in self.logger.handlers:
                fh.setLevel(level)
                fh.setFormatter(formatter)
                self.logger.addHandler(fh)
            return

        log_file_path = os.path.join(self.log_folder, self.log_file)
        fh = logging.FileHandler(log_file_path, encoding='utf-8')
        fh.setLevel(level)
        fh.setFormatter(formatter)
        self.logger.addHandler(fh)
        BaseLogger.file_handlers[self.log_file] = fh

    def _initialize_logging(self) -> None:
        """
        Initializes logging by ensuring the log folder exists.
        """
        if not os.path.exists(self.log_folder):
            os.makedirs(self.log_folder)

    def log_msg(self, msg: str, level: str = 'info') -> None:
        """
        Logs a message at the specified log level.

        Parameters:
            msg (str): The message to log.
            level (str): The level at which to log the message (e.g., 'info', 'debug', 'error').
        """
        level_code = self._get_level_code(level)
        self.logger.log(level_code, msg)

    def set_level(self, level: str) -> None:
        """
        Sets the log level for the logger and its handlers.

        Parameters:
            level (str): The new log level to set (e.g., 'info', 'debug', 'error').
        """
        level_code = self._get_level_code(level)
        self.logger.setLevel(level_code)
        for handler in self.logger.handlers:
            handler.setLevel(level_code)

# From utils/logger.py
class Logger:
    """
    A wrapper class for managing multiple BaseLogger instances, supporting different log files and levels
    as configured in the system settings.

    This class facilitates logging across different modules and components of the application, allowing
    for specific logs for agent activities, model interactions, and results.

    Attributes:
        _instances (dict): A dictionary of Logger instances keyed by name.
        loggers (dict): A dictionary of BaseLogger instances keyed by log type.
    """

    _instances = {}
    _lock = threading.Lock()  # Class-level lock for thread safety
    VALID_LOGGER_NAME_PATTERN = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')

    def __new__(cls, name: str, default_logger: str = 'agentforge'):
        """
        Create a new instance of Logger if one doesn't exist, or return the existing instance.

        Parameters:
            name (str): The name of the module or component using the logger.
            default_logger (str): The default logger file to use.
        """
        with cls._lock:
            if name not in cls._instances:
                instance = super(Logger, cls).__new__(cls)
                cls._instances[name] = instance
                instance._initialized = False
        return cls._instances[name]

    def __init__(self, name: str, default_logger: str = 'agentforge') -> None:
        """
        Initializes the Logger class with names for different types of logs.
        Initialization will only happen once.

        Parameters:
            name (str): The name of the module or component using the logger.
            default_logger (str): The default logger file to use.
        """

        if self._initialized:
            return

        with Logger._lock:
            self.config = Config()
            self.caller_name = name  # Stores the __name__ of the script that instantiated the Logger
            self.default_logger = default_logger
            self.logging_config = None
            self.loggers = {}

            self.load_logging_config()

        self.update_logger_config(default_logger)
        self.init_loggers()

        self._initialized = True

    def load_logging_config(self):
        self.logging_config = self.config.settings.system.logging.files

    def update_logger_config(self, logger_file: str):
        """
        Adds a new logger to the configuration if it doesn't exist.

        Parameters:
            logger_file (str): The name of the logger file to add.
        """
        if logger_file and not self.VALID_LOGGER_NAME_PATTERN.match(logger_file):
            raise ValueError(
                f"Invalid logger_file name: '{logger_file}'. Must match pattern: {self.VALID_LOGGER_NAME_PATTERN.pattern}")

        with Logger._lock:
            if logger_file not in self.logging_config:
                self.logging_config[logger_file] = 'warning'
                self.config.save()

    def init_loggers(self):
        # Initialize loggers dynamically based on configuration settings
        self.loggers = {}
        for logger_file, log_level in self.logging_config.items():
            self.create_logger(logger_file, log_level)

    def create_logger(self, logger_file: str, log_level: str = 'warning'):
        with Logger._lock:
            logger_name = f'{self.caller_name}.{logger_file}'
            log_file_name = f'{logger_file}.log'
            new_logger = BaseLogger(name=logger_name, log_file=log_file_name, log_level=log_level)
            self.loggers[logger_file] = new_logger

    def log(self, msg: str, level: str = 'info', logger_file: str = None) -> None:
        """
        Logs a message to a specified logger.

        Parameters:
            msg (str): The message to log.
            level (str): The log level (e.g., 'info', 'debug', 'error').
            logger_file (str): The specific logger to use. If None, uses the default logger.
        """
        # Prepend the caller's module name to the log message
        msg_with_caller = f'[{self.caller_name}] {msg}'

        if logger_file is None:
            logger_file = self.default_logger

        if logger_file not in self.loggers:
            self.update_logger_config(logger_file)
            self.create_logger(logger_file)

        logger = self.loggers.get(logger_file)
        if logger:
            logger.log_msg(msg_with_caller, level)
            return

        raise ValueError(f"Logger '{logger_file}' could not be created.")

    def debug(self, msg: str, logger_file: str = None) -> None:
        """Logs a debug level message."""
        self.log(msg, level='debug', logger_file=logger_file)

    def info(self, msg: str, logger_file: str = None) -> None:
        """Logs an info level message."""
        self.log(msg, level='info', logger_file=logger_file)

    def warning(self, msg: str, logger_file: str = None) -> None:
        """Logs a warning level message."""
        self.log(msg, level='warning', logger_file=logger_file)

    def error(self, msg: str, logger_file: str = None) -> None:
        """Logs an error level message."""
        self.log(msg, level='error', logger_file=logger_file)

    def critical(self, msg: str, logger_file: str = None) -> None:
        """Logs a critical level message."""
        self.log(msg, level='critical', logger_file=logger_file)

    def log_prompt(self, model_prompt: dict) -> None:
        """
        Logs a prompt to the model interaction logger.

        Parameters:
            model_prompt (dict): A dictionary containing the model prompts.
        """
        system_prompt = model_prompt.get('system', '')
        user_prompt = model_prompt.get('user', '')
        msg = (
            f'******\nSystem Prompt\n******\n{system_prompt}\n'
            f'******\nUser Prompt\n******\n{user_prompt}\n'
            f'******'
        )
        self.debug(msg, logger_file='model_io')

    def log_response(self, response: str) -> None:
        """
        Logs a model response to the model interaction logger.

        Parameters:
            response (str): The model response to log.
        """
        msg = f'******\nModel Response\n******\n{response}\n******'
        self.debug(msg, logger_file='model_io')

    def parsing_error(self, model_response: str, error: Exception) -> None:
        """
        Logs parsing errors along with the model response.

        Parameters:
            model_response (str): The model response associated with the parsing error.
            error (Exception): The exception object representing the parsing error.
        """
        msg = (
            f"Parsing Error - The model may not have responded in the required format.\n\n"
            f"Model Response:\n******\n{model_response}\n******\n\nError: {error}"
        )
        self.error(msg)

# From utils/logger.py
def encode_msg(msg: str) -> str:
    """Encodes a message to UTF-8, replacing any invalid characters."""
    return msg.encode('utf-8', 'replace').decode('utf-8')

# From utils/logger.py
def format(self, record: logging.LogRecord) -> str:
        color_code = self.COLOR_CODES.get(record.levelno, self.RESET_CODE)
        message = super().format(record)
        return f"{color_code}{message}{self.RESET_CODE}"

# From utils/logger.py
def log_msg(self, msg: str, level: str = 'info') -> None:
        """
        Logs a message at the specified log level.

        Parameters:
            msg (str): The message to log.
            level (str): The level at which to log the message (e.g., 'info', 'debug', 'error').
        """
        level_code = self._get_level_code(level)
        self.logger.log(level_code, msg)

# From utils/logger.py
def set_level(self, level: str) -> None:
        """
        Sets the log level for the logger and its handlers.

        Parameters:
            level (str): The new log level to set (e.g., 'info', 'debug', 'error').
        """
        level_code = self._get_level_code(level)
        self.logger.setLevel(level_code)
        for handler in self.logger.handlers:
            handler.setLevel(level_code)

# From utils/logger.py
def load_logging_config(self):
        self.logging_config = self.config.settings.system.logging.files

# From utils/logger.py
def update_logger_config(self, logger_file: str):
        """
        Adds a new logger to the configuration if it doesn't exist.

        Parameters:
            logger_file (str): The name of the logger file to add.
        """
        if logger_file and not self.VALID_LOGGER_NAME_PATTERN.match(logger_file):
            raise ValueError(
                f"Invalid logger_file name: '{logger_file}'. Must match pattern: {self.VALID_LOGGER_NAME_PATTERN.pattern}")

        with Logger._lock:
            if logger_file not in self.logging_config:
                self.logging_config[logger_file] = 'warning'
                self.config.save()

# From utils/logger.py
def init_loggers(self):
        # Initialize loggers dynamically based on configuration settings
        self.loggers = {}
        for logger_file, log_level in self.logging_config.items():
            self.create_logger(logger_file, log_level)

# From utils/logger.py
def create_logger(self, logger_file: str, log_level: str = 'warning'):
        with Logger._lock:
            logger_name = f'{self.caller_name}.{logger_file}'
            log_file_name = f'{logger_file}.log'
            new_logger = BaseLogger(name=logger_name, log_file=log_file_name, log_level=log_level)
            self.loggers[logger_file] = new_logger

# From utils/logger.py
def log(self, msg: str, level: str = 'info', logger_file: str = None) -> None:
        """
        Logs a message to a specified logger.

        Parameters:
            msg (str): The message to log.
            level (str): The log level (e.g., 'info', 'debug', 'error').
            logger_file (str): The specific logger to use. If None, uses the default logger.
        """
        # Prepend the caller's module name to the log message
        msg_with_caller = f'[{self.caller_name}] {msg}'

        if logger_file is None:
            logger_file = self.default_logger

        if logger_file not in self.loggers:
            self.update_logger_config(logger_file)
            self.create_logger(logger_file)

        logger = self.loggers.get(logger_file)
        if logger:
            logger.log_msg(msg_with_caller, level)
            return

        raise ValueError(f"Logger '{logger_file}' could not be created.")

# From utils/logger.py
def log_prompt(self, model_prompt: dict) -> None:
        """
        Logs a prompt to the model interaction logger.

        Parameters:
            model_prompt (dict): A dictionary containing the model prompts.
        """
        system_prompt = model_prompt.get('system', '')
        user_prompt = model_prompt.get('user', '')
        msg = (
            f'******\nSystem Prompt\n******\n{system_prompt}\n'
            f'******\nUser Prompt\n******\n{user_prompt}\n'
            f'******'
        )
        self.debug(msg, logger_file='model_io')

# From utils/logger.py
def log_response(self, response: str) -> None:
        """
        Logs a model response to the model interaction logger.

        Parameters:
            response (str): The model response to log.
        """
        msg = f'******\nModel Response\n******\n{response}\n******'
        self.debug(msg, logger_file='model_io')

# From utils/logger.py
def parsing_error(self, model_response: str, error: Exception) -> None:
        """
        Logs parsing errors along with the model response.

        Parameters:
            model_response (str): The model response associated with the parsing error.
            error (Exception): The exception object representing the parsing error.
        """
        msg = (
            f"Parsing Error - The model may not have responded in the required format.\n\n"
            f"Model Response:\n******\n{model_response}\n******\n\nError: {error}"
        )
        self.error(msg)


# From mixins/vision_mixin.py
class VisionMixin:
    """Shared helpers + capability flag for image modalities."""
    supported_modalities = {"text", "image"}

    def _prepare_image_payload(self, images):
        if Image is None:
            raise ImportError("Vision support requires Pillow.  pip install pillow")

        def to_png_b64(obj):
            if isinstance(obj, (str, Path)):
                data = Path(obj).read_bytes()
            elif isinstance(obj, bytes):
                data = obj
            elif isinstance(obj, Image.Image):
                buf = io.BytesIO()
                obj.save(buf, format="PNG")
                data = buf.getvalue()
            else:
                raise TypeError(f"Unsupported image input: {type(obj)}")
            return base64.b64encode(data).decode()

        parts = []
        for img in images:
            b64 = to_png_b64(img)
            parts.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{b64}"}
            })
        return parts

# From mixins/vision_mixin.py
def to_png_b64(obj):
            if isinstance(obj, (str, Path)):
                data = Path(obj).read_bytes()
            elif isinstance(obj, bytes):
                data = obj
            elif isinstance(obj, Image.Image):
                buf = io.BytesIO()
                obj.save(buf, format="PNG")
                data = buf.getvalue()
            else:
                raise TypeError(f"Unsupported image input: {type(obj)}")
            return base64.b64encode(data).decode()

import discord
from agentforge.tools.semantic_chunk import semantic_chunk

# From discord/discord_utils.py
class DiscordUtils:
    def __init__(self, client, logger):
        """
        Initialize Discord utilities with client and logger instances.

        Args:
            client (discord.Client): The Discord client instance
            logger (Logger): Logger instance for error handling
        """
        self.client = client
        self.logger = logger

    def send_message(self, channel_id, content):
        """
        Send a message to a specified Discord channel.
        
        Args:
            channel_id (int): The ID of the channel to send the message to
            content (str): The content of the message to send
        """
        async def send():
            try:
                messages = semantic_chunk(content, min_length=200, max_length=1900)
                channel = self.client.get_channel(channel_id)
                
                if not channel:
                    self.logger.error(f"[DiscordUtils.send_message] Channel {channel_id} not found")
                    return

                for msg in messages:
                    if len(msg.content) > 2000:
                        # Re-chunk the over-sized message
                        sub_messages = semantic_chunk(msg.content, min_length=200, max_length=1900)
                        for sub_msg in sub_messages:
                            await channel.send(sub_msg.content)
                    else:
                        await channel.send(msg.content)

            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.send_message] Bot doesn't have permission to send messages in channel {channel_id}")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.send_message] Error sending message to channel {channel_id}: {str(e)}")

        try:
            return asyncio.run_coroutine_threadsafe(send(), self.client.loop)
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.send_message] Failed to schedule message sending: {str(e)}")

    def send_dm(self, user_id, content):
        """
        Send a direct message to a specified Discord user.

        Args:
            user_id (int): The ID of the user to send the direct message to
            content (str): The content of the direct message to send
        """
        async def send_dm_async():
            try:
                user = await self.client.fetch_user(user_id)
                if user:
                    messages = semantic_chunk(content, min_length=200, max_length=1900)
                    for msg in messages:
                        if len(msg.content) > 2000:
                            # Re-chunk the over-sized message
                            sub_messages = semantic_chunk(msg.content, min_length=200, max_length=1900)
                            for sub_msg in sub_messages:
                                await user.send(sub_msg.content)
                        else:
                            await user.send(msg.content)
                else:
                    self.logger.error(f"[DiscordUtils.send_dm] User {user_id} not found")
            except discord.errors.NotFound:
                self.logger.error(f"[DiscordUtils.send_dm] User {user_id} not found")
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.send_dm] Cannot send DM to user {user_id}. Forbidden.")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.send_dm] Error sending DM to user {user_id}: {str(e)}")

        try:
            asyncio.run_coroutine_threadsafe(send_dm_async(), self.client.loop)
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.send_dm] Failed to schedule DM sending: {str(e)}")

    def send_embed(self, channel_id, title, fields, color='blue', image_url=None):
        """
        Send an embed message to a specified Discord channel.

        Args:
            channel_id (int): The ID of the channel to send the embed message to
            title (str): The title of the embed message
            fields (list): A list of tuples representing the fields of the embed message
            color (str, optional): The color of the embed message. Defaults to 'blue'
            image_url (str, optional): The URL of the image to include in the embed message
        """
        async def send_embed_async():
            try:
                channel = self.client.get_channel(channel_id)
                if channel:
                    # Convert color string to discord.Color
                    embed_color = getattr(discord.Color, color.lower(), discord.Color.blue)()

                    embed = discord.Embed(
                        title=title,
                        color=embed_color
                    )
                    if image_url:
                        embed.set_image(url=image_url)
                    for name, value in fields:
                        embed.add_field(name=name, value=value, inline=False)

                    await channel.send(embed=embed)
                else:
                    self.logger.error(f"[DiscordUtils.send_embed] Channel with ID {channel_id} not found")
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.send_embed] Cannot send embed to channel {channel_id}. Forbidden")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.send_embed] Error sending embed to channel {channel_id}: {str(e)}")

        try:
            asyncio.run_coroutine_threadsafe(send_embed_async(), self.client.loop)
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.send_embed] Failed to schedule embed sending: {str(e)}")

    def create_thread(self, channel_id, message_id, name, auto_archive_duration=1440, remove_author=True):
        """
        Create a new thread in a specified channel, attached to a specific message.

        Args:
            channel_id (int): The ID of the channel to create the thread in
            message_id (int): The ID of the message to attach the thread to
            name (str): The name of the new thread
            auto_archive_duration (int, optional): Duration in minutes after which the thread
                                               will automatically archive. Default is 1440 (24 hours)
            remove_author (bool, optional): Whether to remove the message author from the thread. Default is True

        Returns:
            int: The ID of the created thread, or None if creation failed
        """
        async def create_thread_async():
            try:
                channel = self.client.get_channel(channel_id)
                if not channel:
                    self.logger.error(f"[DiscordUtils.create_thread] Channel with ID {channel_id} not found")
                    return None

                message = await channel.fetch_message(message_id)
                if not message:
                    self.logger.error(f"[DiscordUtils.create_thread] Message with ID {message_id} not found in channel {channel_id}")
                    return None

                # Safely check if thread exists using hasattr
                if hasattr(message, 'thread') and message.thread:
                    self.logger.info(f"[DiscordUtils.create_thread] Thread already exists for message {message_id}")
                    return message.thread.id

                thread = await message.create_thread(name=name, auto_archive_duration=auto_archive_duration)
                self.logger.info(f"[DiscordUtils.create_thread] Thread '{name}' created successfully")

                if remove_author:
                    await thread.remove_user(message.author)
                    self.logger.info(f"[DiscordUtils.create_thread] Removed author {message.author} from thread '{name}'")

                return thread.id
            except discord.errors.HTTPException as e:
                if e.code == 160004:  # Thread already exists error code
                    if message.thread:
                        return message.thread.id
                    self.logger.error(f"[DiscordUtils.create_thread] Thread exists but cannot be accessed")
                else:
                    self.logger.error(f"[DiscordUtils.create_thread] Error creating thread: {str(e)}")
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.create_thread] Bot doesn't have permission to create threads in channel {channel_id}")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.create_thread] Error creating thread: {str(e)}")
            return None

        try:
            return asyncio.run_coroutine_threadsafe(create_thread_async(), self.client.loop).result()
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.create_thread] Failed to schedule thread creation: {str(e)}")
            return None

    def reply_to_thread(self, thread_id, content):
        """
        Reply to a specific thread.

        Args:
            thread_id (int): The ID of the thread to reply to
            content (str): The content of the reply message

        Returns:
            bool: True if the reply was sent successfully, False otherwise
        """
        async def reply_async():
            try:
                thread = self.client.get_channel(thread_id)
                if not thread:
                    self.logger.error(f"[DiscordUtils.reply_to_thread] Thread {thread_id} not found")
                    return False

                # Split the content into semantic chunks
                chunks = semantic_chunk(content, min_length=200, max_length=1900)
                for i, chunk in enumerate(chunks, 1):
                    message = f"```{chunk.content}```"
                    await thread.send(message)

                self.logger.info(f"[DiscordUtils.reply_to_thread] Reply sent to thread {thread_id}")
                return True
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.reply_to_thread] Bot doesn't have permission to reply to thread {thread_id}")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.reply_to_thread] Error replying to thread: {str(e)}")
            return False

        try:
            return asyncio.run_coroutine_threadsafe(reply_async(), self.client.loop).result()
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.reply_to_thread] Failed to schedule reply: {str(e)}")
            return False

# From discord/discord_utils.py
def send_message(self, channel_id, content):
        """
        Send a message to a specified Discord channel.
        
        Args:
            channel_id (int): The ID of the channel to send the message to
            content (str): The content of the message to send
        """
        async def send():
            try:
                messages = semantic_chunk(content, min_length=200, max_length=1900)
                channel = self.client.get_channel(channel_id)
                
                if not channel:
                    self.logger.error(f"[DiscordUtils.send_message] Channel {channel_id} not found")
                    return

                for msg in messages:
                    if len(msg.content) > 2000:
                        # Re-chunk the over-sized message
                        sub_messages = semantic_chunk(msg.content, min_length=200, max_length=1900)
                        for sub_msg in sub_messages:
                            await channel.send(sub_msg.content)
                    else:
                        await channel.send(msg.content)

            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.send_message] Bot doesn't have permission to send messages in channel {channel_id}")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.send_message] Error sending message to channel {channel_id}: {str(e)}")

        try:
            return asyncio.run_coroutine_threadsafe(send(), self.client.loop)
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.send_message] Failed to schedule message sending: {str(e)}")

# From discord/discord_utils.py
def send_dm(self, user_id, content):
        """
        Send a direct message to a specified Discord user.

        Args:
            user_id (int): The ID of the user to send the direct message to
            content (str): The content of the direct message to send
        """
        async def send_dm_async():
            try:
                user = await self.client.fetch_user(user_id)
                if user:
                    messages = semantic_chunk(content, min_length=200, max_length=1900)
                    for msg in messages:
                        if len(msg.content) > 2000:
                            # Re-chunk the over-sized message
                            sub_messages = semantic_chunk(msg.content, min_length=200, max_length=1900)
                            for sub_msg in sub_messages:
                                await user.send(sub_msg.content)
                        else:
                            await user.send(msg.content)
                else:
                    self.logger.error(f"[DiscordUtils.send_dm] User {user_id} not found")
            except discord.errors.NotFound:
                self.logger.error(f"[DiscordUtils.send_dm] User {user_id} not found")
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.send_dm] Cannot send DM to user {user_id}. Forbidden.")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.send_dm] Error sending DM to user {user_id}: {str(e)}")

        try:
            asyncio.run_coroutine_threadsafe(send_dm_async(), self.client.loop)
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.send_dm] Failed to schedule DM sending: {str(e)}")

# From discord/discord_utils.py
def send_embed(self, channel_id, title, fields, color='blue', image_url=None):
        """
        Send an embed message to a specified Discord channel.

        Args:
            channel_id (int): The ID of the channel to send the embed message to
            title (str): The title of the embed message
            fields (list): A list of tuples representing the fields of the embed message
            color (str, optional): The color of the embed message. Defaults to 'blue'
            image_url (str, optional): The URL of the image to include in the embed message
        """
        async def send_embed_async():
            try:
                channel = self.client.get_channel(channel_id)
                if channel:
                    # Convert color string to discord.Color
                    embed_color = getattr(discord.Color, color.lower(), discord.Color.blue)()

                    embed = discord.Embed(
                        title=title,
                        color=embed_color
                    )
                    if image_url:
                        embed.set_image(url=image_url)
                    for name, value in fields:
                        embed.add_field(name=name, value=value, inline=False)

                    await channel.send(embed=embed)
                else:
                    self.logger.error(f"[DiscordUtils.send_embed] Channel with ID {channel_id} not found")
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.send_embed] Cannot send embed to channel {channel_id}. Forbidden")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.send_embed] Error sending embed to channel {channel_id}: {str(e)}")

        try:
            asyncio.run_coroutine_threadsafe(send_embed_async(), self.client.loop)
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.send_embed] Failed to schedule embed sending: {str(e)}")

# From discord/discord_utils.py
def create_thread(self, channel_id, message_id, name, auto_archive_duration=1440, remove_author=True):
        """
        Create a new thread in a specified channel, attached to a specific message.

        Args:
            channel_id (int): The ID of the channel to create the thread in
            message_id (int): The ID of the message to attach the thread to
            name (str): The name of the new thread
            auto_archive_duration (int, optional): Duration in minutes after which the thread
                                               will automatically archive. Default is 1440 (24 hours)
            remove_author (bool, optional): Whether to remove the message author from the thread. Default is True

        Returns:
            int: The ID of the created thread, or None if creation failed
        """
        async def create_thread_async():
            try:
                channel = self.client.get_channel(channel_id)
                if not channel:
                    self.logger.error(f"[DiscordUtils.create_thread] Channel with ID {channel_id} not found")
                    return None

                message = await channel.fetch_message(message_id)
                if not message:
                    self.logger.error(f"[DiscordUtils.create_thread] Message with ID {message_id} not found in channel {channel_id}")
                    return None

                # Safely check if thread exists using hasattr
                if hasattr(message, 'thread') and message.thread:
                    self.logger.info(f"[DiscordUtils.create_thread] Thread already exists for message {message_id}")
                    return message.thread.id

                thread = await message.create_thread(name=name, auto_archive_duration=auto_archive_duration)
                self.logger.info(f"[DiscordUtils.create_thread] Thread '{name}' created successfully")

                if remove_author:
                    await thread.remove_user(message.author)
                    self.logger.info(f"[DiscordUtils.create_thread] Removed author {message.author} from thread '{name}'")

                return thread.id
            except discord.errors.HTTPException as e:
                if e.code == 160004:  # Thread already exists error code
                    if message.thread:
                        return message.thread.id
                    self.logger.error(f"[DiscordUtils.create_thread] Thread exists but cannot be accessed")
                else:
                    self.logger.error(f"[DiscordUtils.create_thread] Error creating thread: {str(e)}")
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.create_thread] Bot doesn't have permission to create threads in channel {channel_id}")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.create_thread] Error creating thread: {str(e)}")
            return None

        try:
            return asyncio.run_coroutine_threadsafe(create_thread_async(), self.client.loop).result()
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.create_thread] Failed to schedule thread creation: {str(e)}")
            return None

# From discord/discord_utils.py
def reply_to_thread(self, thread_id, content):
        """
        Reply to a specific thread.

        Args:
            thread_id (int): The ID of the thread to reply to
            content (str): The content of the reply message

        Returns:
            bool: True if the reply was sent successfully, False otherwise
        """
        async def reply_async():
            try:
                thread = self.client.get_channel(thread_id)
                if not thread:
                    self.logger.error(f"[DiscordUtils.reply_to_thread] Thread {thread_id} not found")
                    return False

                # Split the content into semantic chunks
                chunks = semantic_chunk(content, min_length=200, max_length=1900)
                for i, chunk in enumerate(chunks, 1):
                    message = f"```{chunk.content}```"
                    await thread.send(message)

                self.logger.info(f"[DiscordUtils.reply_to_thread] Reply sent to thread {thread_id}")
                return True
            except discord.errors.Forbidden:
                self.logger.error(f"[DiscordUtils.reply_to_thread] Bot doesn't have permission to reply to thread {thread_id}")
            except Exception as e:
                self.logger.error(f"[DiscordUtils.reply_to_thread] Error replying to thread: {str(e)}")
            return False

        try:
            return asyncio.run_coroutine_threadsafe(reply_async(), self.client.loop).result()
        except RuntimeError as e:
            self.logger.error(f"[DiscordUtils.reply_to_thread] Failed to schedule reply: {str(e)}")
            return False

from agentforge.utils.discord.discord_utils import DiscordUtils

# From discord/discord_client.py
class DiscordClient:
    """
    A Discord client that handles bot functionality, message processing, and role management.

    This class uses a combination of asyncio and threading to manage Discord operations:
    - The Discord client runs in a separate thread to avoid blocking the main application.
    - Asynchronous methods are used for Discord API calls, which are then run in the client's event loop.
    - Thread-safe methods are provided for external code to interact with the Discord client.

    Attributes:
        token (str): The Discord bot token.
        intents (discord.Intents): The intents for the Discord client.
        client (discord.Client): The main Discord client instance.
        logger (Logger): A custom logger for the Discord client.
        tree (discord.app_commands.CommandTree): The command tree for slash commands.
        message_queue (dict): A queue to store incoming messages, keyed by channel ID.
        running (bool): A flag indicating whether the client is running.
        discord_thread (threading.Thread): The thread running the Discord client.
    """

    def __init__(self):
        """
        Initialize the DiscordClient with necessary attributes and event handlers.
        """
        self.discord_thread = None
        self.token = str(os.getenv('DISCORD_TOKEN'))
        self.intents = discord.Intents.default()
        self.intents.message_content = True
        self.client = discord.Client(intents=self.intents)
        self.logger = Logger('DiscordClient', 'DiscordClient')
        self.tree = discord.app_commands.CommandTree(self.client)
        self.message_queue = {}
        self.running = False
        self.load_commands()
        self.utils = DiscordUtils(self.client, self.logger)


        @self.client.event
        async def on_ready():
            await self.tree.sync()
            self.logger.info(f'[DiscordClient.on_ready] {self.client.user} has connected to Discord!')

        @self.client.event
        async def on_message(message: discord.Message):
            self.logger.debug(f"[DiscordClient.on_message] Received message:\n{message}")

            content = message.content
            for mention in message.mentions:
                # If a mention is copy/pasted, this does not work. The mention value will come through as Null.
                content = content.replace(f'<@{mention.id}>', f'@{mention.display_name}')

            message_data = {
                "channel": str(message.channel),
                "channel_id": message.channel.id,
                "message": content,
                "message_id": message.id,
                "author": message.author.display_name,
                "author_id": message.author,
                "timestamp": message.created_at.strftime('%Y-%m-%d %H:%M:%S'),
                "mentions": message.mentions,
                "attachments": message.attachments
            }

            # Add thread information to message_data if the message is in a thread
            if isinstance(message.channel, discord.Thread):
                message_data["thread_id"] = message.channel.id
                message_data["thread_name"] = message.channel.name

            self.logger.debug(
                f"[DiscordClient.on_message] Channel: {str(message.channel)}({message.channel.id}) - {message.author.display_name} said:\n{content}")

            if message.author != self.client.user:
                if message.channel.id not in self.message_queue:
                    self.message_queue[message.channel.id] = []
                self.message_queue[message.channel.id].append(message_data)
                self.logger.debug("[DiscordClient.on_message] Message added to queue")
            else:
                self.logger.debug(f"[DiscordClient.on_message] Message not added to queue:\n{message_data}")

    def run(self):
        """
        Start the Discord client in a separate thread.

        This method creates a new thread that runs the Discord client's event loop.
        The thread allows the Discord client to operate independently of the main
        application thread, preventing it from blocking other operations.
        """

        def run_discord():
            self.logger.info("[DiscordClient.run] Client Starting")
            asyncio.run(self.client.start(self.token))

        self.discord_thread = threading.Thread(target=run_discord)
        self.discord_thread.start()
        self.running = True

    def stop(self):
        """
        Stop the Discord client and join the client thread.

        This method closes the Discord client's connection and waits for the
        client thread to finish, ensuring a clean shutdown.
        """
        self.running = False
        asyncio.run(self.client.close())
        self.discord_thread.join()
        self.logger.info("[DiscordClient.stop] Client Stopped")

    def process_channel_messages(self):
        """
        Process and yield messages from the message queue.

        This function retrieves all messages sent to a discord channel from the
        message_queue and yields them.
        Each message is represented as a tuple with the following structure:

        (channel_id, [message_data])

        where:
        - channel_id (int): The ID of the Discord channel where the message was sent.
        - message_data (list): A list containing a single dictionary with message details:
            {
                'channel': str,       # The name of the channel (e.g., 'system')
                'channel_id': int,    # The ID of the channel (same as the tuple's first element)
                'message': str,       # The content of the message
                'author': str,        # The display name of the message author
                'author_id': Member,  # The Discord Member object of the author
                'timestamp': str      # The timestamp of the message in 'YYYY-MM-DD HH:MM:SS' format
                'mentions': list      # A list of Discord Member objects mentioned in the message
            }

        Yields:
        tuple: A message tuple as described above.

        Note:
        - This function is designed to work with Discord message objects.
        - If the message queue is empty, the function will print "No Message Found" and pass.
        - Any exceptions during message processing will be caught and printed.
        """
        if self.message_queue:
            try:
                next_message = self.message_queue.popitem()
                yield next_message
            except Exception as e:
                print(f"Exception: {e}")
        else:
            pass

    def send_message(self, channel_id, content):
        """
        Send a message to a specified Discord channel.
        
        Args:
            channel_id (int): The ID of the channel to send the message to.
            content (str): The content of the message to send.
        """
        self.utils.send_message(channel_id, content)

    def send_dm(self, user_id, content):
        """
        Send a direct message to a specified Discord user.
        
        Args:
            user_id (int): The ID of the user to send the direct message to.
            content (str): The content of the direct message to send.
        """
        self.utils.send_dm(user_id, content)
        
    def send_embed(self, channel_id, title, fields, color='blue', image_url=None):
        """
        Send an embed message to a specified Discord channel.

        Args:
            channel_id (int): The ID of the channel to send the embed message to.
            title (str): The title of the embed message.
            fields (list): A list of tuples representing the fields of the embed message.
            color (str, optional): The color of the embed message. Defaults to 'blue'.
            image_url (str, optional): The URL of the image to include in the embed message.
        """
        self.utils.send_embed(channel_id, title, fields, color, image_url)

    def load_commands(self):
        """
        Load slash commands for the Discord client.

        This method registers a single slash command ("bot") for the Discord client.
        The command is added to the command tree and will be available for users to
        interact with in Discord.
        """
        name = 'bot'
        description = 'send a command to the bot'
        function_name = 'bot'

        @discord.app_commands.command(name=name, description=description)
        async def command_callback(interaction: discord.Interaction, command: str):
            kwargs = {"arg": command}
            await self.handle_command(interaction, name, function_name, kwargs)

        param_name = "command"
        param_description = "send a command to the bot"
        command_callback = discord.app_commands.describe(**{param_name: param_description})(command_callback)

        self.logger.info(f"[DiscordClient.load_commands] Register Command: {name} - Function: {function_name}")
        self.tree.add_command(command_callback)

    async def handle_command(self, interaction: discord.Interaction, command_name: str, function_name: str,
                             kwargs: dict):
        """
        Handle a slash command interaction.

        This method is called asynchronously by the Discord client when a slash
        command is invoked. It adds the command to the message queue for processing.

        Args:
            interaction (discord.Interaction): The interaction object for the command.
            command_name (str): The name of the command.
            function_name (str): The name of the function to handle the command.
            kwargs (dict): Additional arguments for the command.
        """
        message_data = {
            "channel": str(interaction.channel),
            "channel_id": interaction.channel_id,
            "message": f"/{command_name}",
            "author": interaction.user.display_name,
            "author_id": interaction.user,
            "timestamp": interaction.created_at.strftime('%Y-%m-%d %H:%M:%S'),
            "mentions": interaction.data.get("resolved", {}).get("members", []),
            "function_name": function_name,
            "arg": kwargs.get('arg', None)
        }

        if interaction.channel_id not in self.message_queue:
            self.message_queue[interaction.channel_id] = []
        self.message_queue[interaction.channel_id].append(message_data)
        self.logger.info(f"[DiscordClient.handle_command] Command '{command_name}' received and added to the queue")

        await interaction.response.send_message(f"Command '{command_name}' received and added to the queue.")

    async def set_typing_indicator(self, channel_id, is_typing):
        """
        Set the typing indicator for a specified Discord channel.

        This method uses asyncio.run_coroutine_threadsafe to safely schedule the
        asynchronous typing indicator operation in the Discord client's event loop,
        allowing it to be called from any thread.

        Args:
            channel_id (int): The ID of the channel to set the typing indicator for.
            is_typing (bool): Whether to start or stop the typing indicator.
        """
        channel = self.client.get_channel(channel_id)

        if channel:
            if is_typing:
                async with channel.typing():
                    # Keep the typing indicator on for a specific duration
                    await asyncio.sleep(5)  # Adjust the duration as needed
            else:
                # Stop the typing indicator immediately
                await asyncio.sleep(0)
        else:
            self.logger.error(f"Channel with ID {channel_id} not found.")

    def create_thread(self, channel_id, message_id, name, auto_archive_duration=1440, remove_author=True):
        """
        Create a new thread in a specified channel, attached to a specific message.

        Args:
            channel_id (int): The ID of the channel to create the thread in.
            message_id (int): The ID of the message to attach the thread to.
            name (str): The name of the new thread.
            auto_archive_duration (int, optional): Duration in minutes after which the thread
                                               will automatically archive. Default is 1440 (24 hours).
            remove_author (bool, optional): Whether to remove the message author from the thread. Default is True.

        Returns:
            int: The ID of the created thread, or None if creation failed.
        """
        return self.utils.create_thread(channel_id, message_id, name, auto_archive_duration, remove_author)

    def reply_to_thread(self, thread_id, content):
        """
        Reply to a specific thread.

        Args:
            thread_id (int): The ID of the thread to reply to.
            content (str): The content of the reply message.

        Returns:
            bool: True if the reply was sent successfully, False otherwise.
        """
        return self.utils.reply_to_thread(thread_id, content)

# From discord/discord_client.py
def process_channel_messages(self):
        """
        Process and yield messages from the message queue.

        This function retrieves all messages sent to a discord channel from the
        message_queue and yields them.
        Each message is represented as a tuple with the following structure:

        (channel_id, [message_data])

        where:
        - channel_id (int): The ID of the Discord channel where the message was sent.
        - message_data (list): A list containing a single dictionary with message details:
            {
                'channel': str,       # The name of the channel (e.g., 'system')
                'channel_id': int,    # The ID of the channel (same as the tuple's first element)
                'message': str,       # The content of the message
                'author': str,        # The display name of the message author
                'author_id': Member,  # The Discord Member object of the author
                'timestamp': str      # The timestamp of the message in 'YYYY-MM-DD HH:MM:SS' format
                'mentions': list      # A list of Discord Member objects mentioned in the message
            }

        Yields:
        tuple: A message tuple as described above.

        Note:
        - This function is designed to work with Discord message objects.
        - If the message queue is empty, the function will print "No Message Found" and pass.
        - Any exceptions during message processing will be caught and printed.
        """
        if self.message_queue:
            try:
                next_message = self.message_queue.popitem()
                yield next_message
            except Exception as e:
                print(f"Exception: {e}")
        else:
            pass

# From discord/discord_client.py
def load_commands(self):
        """
        Load slash commands for the Discord client.

        This method registers a single slash command ("bot") for the Discord client.
        The command is added to the command tree and will be available for users to
        interact with in Discord.
        """
        name = 'bot'
        description = 'send a command to the bot'
        function_name = 'bot'

        @discord.app_commands.command(name=name, description=description)
        async def command_callback(interaction: discord.Interaction, command: str):
            kwargs = {"arg": command}
            await self.handle_command(interaction, name, function_name, kwargs)

        param_name = "command"
        param_description = "send a command to the bot"
        command_callback = discord.app_commands.describe(**{param_name: param_description})(command_callback)

        self.logger.info(f"[DiscordClient.load_commands] Register Command: {name} - Function: {function_name}")
        self.tree.add_command(command_callback)

# From discord/discord_client.py
def run_discord():
            self.logger.info("[DiscordClient.run] Client Starting")
            asyncio.run(self.client.start(self.token))

from agentforge.testing.bootstrap import bootstrap_test_env
from agentforge.cog import Cog


# From utils/fakes.py
class _FakeCollection:
    """Extremely small subset of Chroma collection API sufficient for tests."""

    def __init__(self) -> None:
        self._docs: Dict[str, str] = {}
        self._metas: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()

    # Core operations -----------------------------------------------------
    def upsert(self, documents: List[str], metadatas: List[dict], ids: List[str]) -> None:  # noqa: D401
        with self._lock:
            for _id, doc, meta in zip(ids, documents, metadatas):
                self._docs[_id] = doc
                self._metas[_id] = meta or {}

    def get(self, ids: Optional[List[str]] = None):  # noqa: D401
        # Lock-free read: copy current state once to avoid partial reads.
        snapshot_docs = self._docs.copy()
        snapshot_meta = self._metas.copy()
        if ids is None:
            ids = list(snapshot_docs.keys())
        return {
            "ids": ids,
            "documents": [snapshot_docs[i] for i in ids if i in snapshot_docs],
            "metadatas": [snapshot_meta.get(i, {}) for i in ids],
        }

    def delete(self, ids: Optional[List[str]] = None) -> None:  # noqa: D401
        with self._lock:
            ids = ids or list(self._docs.keys())
            for _id in ids:
                self._docs.pop(_id, None)
                self._metas.pop(_id, None)

    def count(self) -> int:
        with self._lock:
            return len(self._docs)

    def query(self, num_results: int = 1):
        with self._lock:
            ids = list(self._docs.keys())[:num_results]
            return self.get(ids=ids)

# From utils/fakes.py
class FakeChromaStorage:
    """Drop-in replacement for `agentforge.storage.chroma_storage.ChromaStorage`."""

    _registry: Dict[str, "FakeChromaStorage"] = {}
    _registry_lock = threading.Lock()

    def __init__(self, storage_id: str):
        self.storage_id = storage_id
        self._collections: Dict[str, _FakeCollection] = {}
        self._current: Optional[_FakeCollection] = None
        self._lock = threading.RLock()

    # Registry helpers ----------------------------------------------------
    @classmethod
    def get_or_create(cls, storage_id: str):
        with cls._registry_lock:
            if storage_id not in cls._registry:
                cls._registry[storage_id] = cls(storage_id)
            return cls._registry[storage_id]

    @classmethod
    def clear_registry(cls):
        with cls._registry_lock:
            cls._registry.clear()

    # Collection management ----------------------------------------------
    def select_collection(self, collection_name: str):
        if not collection_name or not isinstance(collection_name, str) or collection_name.strip() == "":
            raise Exception("Invalid collection name")
        self._current = self._collections.setdefault(collection_name, _FakeCollection())
        return self._current

    def delete_collection(self, collection_name: str):
        self._collections.pop(collection_name, None)
        if self._current is not None and self._current is self._collections.get(collection_name):
            self._current = None

    def count_collection(self, collection_name: str):
        return self.select_collection(collection_name).count()

    def peek(self, collection_name: str):
        col = self.select_collection(collection_name)
        ids = list(col._docs.keys())[:10]
        if not ids:
            return {"documents": "No Results!"}
        return {
            "documents": [col._docs[i] for i in ids],
            "ids": ids,
            "metadatas": [col._metas.get(i, {}) for i in ids],
        }

    # High-level API (subset) --------------------------------------------
    def save_to_storage(self, collection_name: str, data: list | str, ids: Optional[list] = None, metadata: Optional[list[dict]] = None):
        data = [data] if isinstance(data, str) else list(data)
        if ids is None:
            # Generate incremental ids based on current collection size
            existing = self.select_collection(collection_name).count()
            ids = [str(existing + i + 1) for i in range(len(data))]
        metadata = metadata or [{} for _ in data]
        if not (len(data) == len(ids) == len(metadata)):
            raise ValueError("data, ids, and metadata must have the same length")
        # Ensure each metadata dict has an integer 'id' mirroring the sequential id
        processed_metas = []
        for idx, meta in enumerate(metadata):
            meta_copy = dict(meta)  # avoid mutating caller's dict
            # ids[idx] is a string; store numeric version
            try:
                meta_copy.setdefault("id", int(ids[idx]))
            except ValueError:
                # fall back to string if non-numeric but still provide something
                meta_copy.setdefault("id", ids[idx])
            processed_metas.append(meta_copy)

        self.select_collection(collection_name).upsert(data, processed_metas, ids)

    def query_storage(self, *, collection_name: str, query: Optional[str | List[str]] = None, num_results: int = 1, **_):
        col = self.select_collection(collection_name)
        res = col.query(num_results=num_results)
        # If collection empty, return {} to mirror production behaviour
        if not res["documents"]:
            return {}
        return res

    def delete_from_storage(self, collection_name: str, ids: List[str] | str):
        if not isinstance(ids, list):
            ids = [ids]
        self.select_collection(collection_name).delete(ids)

    def reset_storage(self):
        self._collections.clear()

    def get_last_x_entries(self, collection_name: str, x: int, include: list = None):
        """
        Retrieve the last X entries from a collection, ordered by insertion (id ascending).
        Args:
            collection_name (str): The name of the collection.
            x (int): Number of most recent entries to retrieve.
            include (list, optional): Which fields to include in the result.
                Defaults to ['documents', 'metadatas', 'ids'].
        Returns:
            dict: The collection entries, sorted by id ascending, with only the requested fields.
        """
        if include is None:
            include = ['documents', 'metadatas', 'ids']
        col = self.select_collection(collection_name)
        # Sort ids as integers if possible, else as strings
        try:
            sorted_ids = sorted(col._docs.keys(), key=lambda i: int(i))
        except Exception:
            sorted_ids = sorted(col._docs.keys())
        last_ids = sorted_ids[-x:] if x > 0 else []
        # Always return in ascending order
        last_ids = sorted(last_ids, key=lambda i: int(i) if i.isdigit() else i)
        result = {
            'documents': [col._docs[i] for i in last_ids],
            'metadatas': [col._metas.get(i, {}) for i in last_ids],
            'ids': last_ids
        }
        # Only include requested fields
        return {k: result[k] for k in include if k in result}

# From utils/fakes.py
def count(self) -> int:
        with self._lock:
            return len(self._docs)

# From utils/fakes.py
def query(self, num_results: int = 1):
        with self._lock:
            ids = list(self._docs.keys())[:num_results]
            return self.get(ids=ids)

from tempfile import gettempdir
from pydantic import BaseSettings
from yarl import URL
from reworkd_platform.constants import ENV_PREFIX

# From reworkd_platform/settings.py
def kafka_consumer_group(self) -> str:
        """
        Kafka consumer group will be the name of the host in development
        mode, making it easier to share a dev cluster.
        """

        if self.environment == "development":
            return platform.node()

        return "platform"

# From reworkd_platform/settings.py
def db_url(self) -> URL:
        return URL.build(
            scheme="mysql+aiomysql",
            host=self.db_host,
            port=self.db_port,
            user=self.db_user,
            password=self.db_pass,
            path=f"/{self.db_base}",
        )

# From reworkd_platform/settings.py
def pusher_enabled(self) -> bool:
        return all(
            [
                self.pusher_app_id,
                self.pusher_key,
                self.pusher_secret,
                self.pusher_cluster,
            ]
        )

# From reworkd_platform/settings.py
def kafka_enabled(self) -> bool:
        return all(
            [
                self.kafka_bootstrap_servers,
                self.kafka_username,
                self.kafka_password,
            ]
        )

# From reworkd_platform/settings.py
def helicone_enabled(self) -> bool:
        return all(
            [
                self.helicone_api_base,
                self.helicone_api_key,
            ]
        )

# From reworkd_platform/settings.py
def sid_enabled(self) -> bool:
        return all(
            [
                self.sid_client_id,
                self.sid_client_secret,
                self.sid_redirect_uri,
            ]
        )


from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncEngine
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.ext.asyncio import async_sessionmaker
from sqlalchemy.ext.asyncio import create_async_engine
from reworkd_platform.db.dependencies import get_db_session
from reworkd_platform.db.utils import create_database
from reworkd_platform.db.utils import drop_database
from reworkd_platform.settings import settings
from reworkd_platform.web.application import get_app
from reworkd_platform.db.meta import meta
from reworkd_platform.db.models import load_all_models

# From reworkd_platform/conftest.py
def anyio_backend() -> str:
    """
    Backend for anyio pytest plugin.

    :return: backend name.
    """
    return "asyncio"

# From reworkd_platform/conftest.py
def fastapi_app(dbsession: AsyncSession) -> FastAPI:
    """
    Fixture for creating FastAPI app.

    :return: fastapi app with mocked dependencies.
    """
    application = get_app()
    application.dependency_overrides[get_db_session] = lambda: dbsession
    return application


# From reworkd_platform/logging.py
class InterceptHandler(logging.Handler):
    """
    Default handler from examples in loguru documentation.

    This handler intercepts all log requests and
    passes them to loguru.

    For more info see:
    https://loguru.readthedocs.io/en/stable/overview.html#entirely-compatible-with-standard-logging
    """

    def emit(self, record: logging.LogRecord) -> None:  # pragma: no cover
        """
        Propagates logs to loguru.

        :param record: record to log.
        """
        try:
            level: Union[str, int] = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Find caller from where originated the logged message
        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back  # type: ignore
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(
            level,
            record.getMessage(),
        )

# From reworkd_platform/logging.py
def configure_logging() -> None:  # pragma: no cover
    """Configures logging."""
    intercept_handler = InterceptHandler()

    logging.basicConfig(handlers=[intercept_handler], level=logging.NOTSET)

    for logger_name in logging.root.manager.loggerDict:
        if logger_name.startswith("uvicorn."):
            logging.getLogger(logger_name).handlers = []

    # change handler for default uvicorn logger
    logging.getLogger("uvicorn").handlers = [intercept_handler]
    logging.getLogger("uvicorn.access").handlers = [intercept_handler]

    # set logs output, level and format
    logger.remove()
    logger.add(
        sys.stdout,
        level=settings.log_level,
    )

from functools import wraps
from time import time

# From reworkd_platform/timer.py
def timed_function(level: Log_Level = "INFO") -> Callable[..., Any]:
    def decorator(func: Any) -> Callable[..., Any]:
        @wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            start_time = time()
            result = func(*args, **kwargs)
            execution_time = time() - start_time
            logger.log(
                level,
                f"Function '{func.__qualname__}' executed in {execution_time:.4f} seconds",
            )

            return result

        return wrapper

    return decorator

# From reworkd_platform/timer.py
def decorator(func: Any) -> Callable[..., Any]:
        @wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            start_time = time()
            result = func(*args, **kwargs)
            execution_time = time() - start_time
            logger.log(
                level,
                f"Function '{func.__qualname__}' executed in {execution_time:.4f} seconds",
            )

            return result

        return wrapper

from ssl import SSLContext
from ssl import create_default_context
from reworkd_platform.settings import Settings

# From services/ssl.py
def get_ssl_context(
    settings: Settings, paths: Optional[List[str]] = None
) -> SSLContext:
    if settings.db_ca_path:
        return create_default_context(cafile=settings.db_ca_path)

    for path in paths or [MACOS_CERT_PATH, DOCKER_CERT_PATH]:
        try:
            return create_default_context(cafile=path)
        except FileNotFoundError:
            continue

    raise ValueError(
        "No CA certificates found for your OS. To fix this, please run change "
        "db_ca_path in your settings.py to point to a valid CA certificate file."
    )

from anthropic import AsyncAnthropic

# From services/anthropic.py
class AbstractPrompt(BaseModel):
    def to_string(self) -> str:
        raise NotImplementedError

# From services/anthropic.py
class HumanAssistantPrompt(AbstractPrompt):
    assistant_prompt: str
    human_prompt: str

    def to_string(self) -> str:
        return (
            f"""\n\nHuman: {self.human_prompt}\n\nAssistant: {self.assistant_prompt}"""
        )

# From services/anthropic.py
class ClaudeService:
    def __init__(self, api_key: Optional[str], model: str = "claude-2"):
        self.claude = AsyncAnthropic(api_key=api_key)
        self.model = model

    async def completion(
        self,
        prompt: AbstractPrompt,
        max_tokens_to_sample: int,
        temperature: int = 0,
        **kwargs: Any,
    ) -> str:
        return (
            await self.claude.completions.create(
                model=self.model,
                prompt=prompt.to_string(),
                max_tokens_to_sample=max_tokens_to_sample,
                temperature=temperature,
                **kwargs,
            )
        ).completion.strip()

# From services/anthropic.py
def to_string(self) -> str:
        raise NotImplementedError

from fastapi import Path
from reworkd_platform.db.crud.oauth import OAuthCrud
from reworkd_platform.db.models.auth import OauthCredentials
from reworkd_platform.schemas import UserBase
from reworkd_platform.services.security import encryption_service
from reworkd_platform.web.api.http_responses import forbidden

# From services/oauth_installers.py
class OAuthInstaller(ABC):
    def __init__(self, crud: OAuthCrud, settings: Settings):
        self.crud = crud
        self.settings = settings

    @abstractmethod
    async def install(self, user: UserBase, redirect_uri: str) -> str:
        raise NotImplementedError()

    @abstractmethod
    async def install_callback(self, code: str, state: str) -> OauthCredentials:
        raise NotImplementedError()

    @abstractmethod
    async def uninstall(self, user: UserBase) -> bool:
        raise NotImplementedError()

    @staticmethod
    def store_access_token(creds: OauthCredentials, access_token: str) -> None:
        creds.access_token_enc = encryption_service.encrypt(access_token)

    @staticmethod
    def store_refresh_token(creds: OauthCredentials, refresh_token: str) -> None:
        creds.refresh_token_enc = encryption_service.encrypt(refresh_token)

# From services/oauth_installers.py
class SIDInstaller(OAuthInstaller):
    PROVIDER = "sid"

    async def install(self, user: UserBase, redirect_uri: str) -> str:
        # gracefully handle the case where the installation already exists
        # this can happen if the user starts the process from multiple tabs
        installation = await self.crud.get_installation_by_user_id(
            user.id, self.PROVIDER
        )
        if not installation:
            installation = await self.crud.create_installation(
                user,
                self.PROVIDER,
                redirect_uri,
            )
        scopes = ["data:query", "offline_access"]
        params = {
            "client_id": self.settings.sid_client_id,
            "redirect_uri": self.settings.sid_redirect_uri,
            "response_type": "code",
            "scope": " ".join(scopes),
            "state": installation.state,
            "audience": "https://api.sid.ai/api/v1/",
        }
        auth_url = "https://me.sid.ai/api/oauth/authorize"
        auth_url += "?" + urlencode(params)
        return auth_url

    async def install_callback(self, code: str, state: str) -> OauthCredentials:
        creds = await self.crud.get_installation_by_state(state)
        if not creds:
            raise forbidden()
        req = {
            "grant_type": "authorization_code",
            "client_id": self.settings.sid_client_id,
            "client_secret": self.settings.sid_client_secret,
            "redirect_uri": self.settings.sid_redirect_uri,
            "code": code,
        }
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://auth.sid.ai/oauth/token",
                headers={
                    "Content-Type": "application/json",
                    "Accept": "application/json",
                },
                data=json.dumps(req),
            ) as response:
                res_data = await response.json()

        OAuthInstaller.store_access_token(creds, res_data["access_token"])
        OAuthInstaller.store_refresh_token(creds, res_data["refresh_token"])
        creds.access_token_expiration = datetime.now() + timedelta(
            seconds=res_data["expires_in"]
        )
        return await creds.save(self.crud.session)

    async def uninstall(self, user: UserBase) -> bool:
        creds = await self.crud.get_installation_by_user_id(user.id, self.PROVIDER)
        # check if credentials exist and contain a refresh token
        if not creds:
            return False

        # use refresh token to revoke access
        delete_token = encryption_service.decrypt(creds.refresh_token_enc)
        # delete credentials from database
        await self.crud.session.delete(creds)

        # revoke refresh token
        async with aiohttp.ClientSession() as session:
            await session.post(
                "https://auth.sid.ai/oauth/revoke",
                headers={
                    "Content-Type": "application/json",
                },
                data=json.dumps(
                    {
                        "client_id": self.settings.sid_client_id,
                        "client_secret": self.settings.sid_client_secret,
                        "token": delete_token,
                    }
                ),
            )
        return True

# From services/oauth_installers.py
def installer_factory(
    provider: str = Path(description="OAuth Provider"),
    crud: OAuthCrud = Depends(OAuthCrud.inject),
) -> OAuthInstaller:
    """Factory for OAuth installers
    Args:
        provider (str): OAuth Provider (can be slack, github, etc.) (injected)
        crud (OAuthCrud): OAuth Crud (injected)
    """

    if provider in integrations:
        return integrations[provider](crud, platform_settings)
    raise NotImplementedError()

# From services/oauth_installers.py
def store_access_token(creds: OauthCredentials, access_token: str) -> None:
        creds.access_token_enc = encryption_service.encrypt(access_token)

# From services/oauth_installers.py
def store_refresh_token(creds: OauthCredentials, refresh_token: str) -> None:
        creds.refresh_token_enc = encryption_service.encrypt(refresh_token)

from cryptography.fernet import Fernet
from cryptography.fernet import InvalidToken

# From services/security.py
class EncryptionService:
    def __init__(self, secret: bytes):
        self.fernet = Fernet(secret)

    def encrypt(self, text: str) -> bytes:
        return self.fernet.encrypt(text.encode("utf-8"))

    def decrypt(self, encoded_bytes: Union[bytes, str]) -> str:
        try:
            return self.fernet.decrypt(encoded_bytes).decode("utf-8")
        except InvalidToken:
            raise forbidden()

# From services/security.py
def encrypt(self, text: str) -> bytes:
        return self.fernet.encrypt(text.encode("utf-8"))

# From services/security.py
def decrypt(self, encoded_bytes: Union[bytes, str]) -> str:
        try:
            return self.fernet.decrypt(encoded_bytes).decode("utf-8")
        except InvalidToken:
            raise forbidden()


# From schemas/user.py
class OrganizationRole(BaseModel):
    id: str
    role: str
    organization_id: str

# From schemas/user.py
class UserBase(BaseModel):
    id: str
    name: Optional[str]
    email: Optional[str]
    image: Optional[str] = Field(default=None)
    organization: Optional[OrganizationRole] = Field(default=None)

    @property
    def organization_id(self) -> Optional[str]:
        return self.organization.organization_id if self.organization else None

# From schemas/user.py
def organization_id(self) -> Optional[str]:
        return self.organization.organization_id if self.organization else None

import sqlalchemy

from ssl import CERT_REQUIRED
from reworkd_platform.services.ssl import get_ssl_context

# From db/utils.py
def create_engine() -> AsyncEngine:
    """
    Creates SQLAlchemy engine instance.

    :return: SQLAlchemy engine instance.
    """
    if settings.environment == "development":
        return create_async_engine(
            str(settings.db_url),
            echo=settings.db_echo,
        )

    ssl_context = get_ssl_context(settings)
    ssl_context.verify_mode = CERT_REQUIRED
    connect_args = {"ssl": ssl_context}

    return create_async_engine(
        str(settings.db_url),
        echo=settings.db_echo,
        connect_args=connect_args,
    )

from sqlalchemy import DateTime
from sqlalchemy import String
from sqlalchemy import func
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy.orm import Mapped
from sqlalchemy.orm import mapped_column
from reworkd_platform.web.api.http_responses import not_found

# From db/base.py
class Base(DeclarativeBase):
    """Base for all models."""

    metadata = meta
    id: Mapped[str] = mapped_column(
        String,
        primary_key=True,
        default=lambda _: str(uuid.uuid4()),
        unique=True,
        nullable=False,
    )

    @classmethod
    async def get(cls: Type[T], session: AsyncSession, id_: str) -> Optional[T]:
        return await session.get(cls, id_)

    @classmethod
    async def get_or_404(cls: Type[T], session: AsyncSession, id_: str) -> T:
        if model := await cls.get(session, id_):
            return model

        raise not_found(detail=f"{cls.__name__}[{id_}] not found")

    async def save(self: T, session: AsyncSession) -> T:
        session.add(self)
        await session.flush()
        return self

    async def delete(self: T, session: AsyncSession) -> None:
        await session.delete(self)

# From db/base.py
class TrackedModel(Base):
    """Base for all tracked models."""

    __abstract__ = True

    create_date = mapped_column(
        DateTime, name="create_date", server_default=func.now(), nullable=False
    )
    update_date = mapped_column(
        DateTime, name="update_date", onupdate=func.now(), nullable=True
    )
    delete_date = mapped_column(DateTime, name="delete_date", nullable=True)

    async def delete(self, session: AsyncSession) -> None:
        """Marks the model as deleted."""
        self.delete_date = datetime.now()
        await self.save(session)

# From db/base.py
class UserMixin:
    user_id = mapped_column(String, name="user_id", nullable=False)
    organization_id = mapped_column(String, name="organization_id", nullable=True)

from starlette.requests import Request

from importlib import metadata
from fastapi.responses import UJSONResponse
from reworkd_platform.logging import configure_logging
from reworkd_platform.web.api.error_handling import platformatic_exception_handler
from reworkd_platform.web.api.errors import PlatformaticError
from reworkd_platform.web.api.router import api_router
from reworkd_platform.web.lifetime import register_shutdown_event
from reworkd_platform.web.lifetime import register_startup_event

# From web/application.py
def get_app() -> FastAPI:
    """
    Get FastAPI application.

    This is the main constructor of an application.

    :return: application.
    """
    configure_logging()

    app = FastAPI(
        title="Reworkd Platform API",
        version=metadata.version("reworkd_platform"),
        docs_url="/api/docs",
        redoc_url="/api/redoc",
        openapi_url="/api/openapi.json",
        default_response_class=UJSONResponse,
    )

    app.add_middleware(
        CORSMiddleware,
        allow_origins=[settings.frontend_url],
        allow_origin_regex=settings.allowed_origins_regex,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Adds startup and shutdown events.
    register_startup_event(app)
    register_shutdown_event(app)

    # Main router for the API.
    app.include_router(router=api_router, prefix="/api")

    app.exception_handler(PlatformaticError)(platformatic_exception_handler)

    return app

from reworkd_platform.db.utils import create_engine
from reworkd_platform.services.tokenizer.lifetime import init_tokenizer

# From web/lifetime.py
def register_startup_event(
    app: FastAPI,
) -> Callable[[], Awaitable[None]]:  # pragma: no cover
    """
    Actions to run on application startup.

    This function uses fastAPI app to store data
    in the state, such as db_engine.

    :param app: the fastAPI application.
    :return: function that actually performs actions.
    """

    @app.on_event("startup")
    async def _startup() -> None:  # noqa: WPS430
        _setup_db(app)
        init_tokenizer(app)
        # await _create_tables()

    return _startup

# From web/lifetime.py
def register_shutdown_event(
    app: FastAPI,
) -> Callable[[], Awaitable[None]]:  # pragma: no cover
    """
    Actions to run on application's shutdown.

    :param app: fastAPI application.
    :return: function that actually performs actions.
    """

    @app.on_event("shutdown")
    async def _shutdown() -> None:  # noqa: WPS430
        await app.state.db_engine.dispose()

    return _shutdown

from tiktoken import Encoding
from tiktoken import get_encoding
from reworkd_platform.schemas.agent import LLM_MODEL_MAX_TOKENS
from reworkd_platform.schemas.agent import LLM_Model
from reworkd_platform.web.api.agent.model_factory import WrappedChatOpenAI

# From tokenizer/token_service.py
class TokenService:
    def __init__(self, encoding: Encoding):
        self.encoding = encoding

    @classmethod
    def create(cls, encoding: str = "cl100k_base") -> "TokenService":
        return cls(get_encoding(encoding))

    def tokenize(self, text: str) -> list[int]:
        return self.encoding.encode(text)

    def detokenize(self, tokens: list[int]) -> str:
        return self.encoding.decode(tokens)

    def count(self, text: str) -> int:
        return len(self.tokenize(text))

    def get_completion_space(self, model: LLM_Model, *prompts: str) -> int:
        max_allowed_tokens = LLM_MODEL_MAX_TOKENS.get(model, 4000)
        prompt_tokens = sum([self.count(p) for p in prompts])
        return max_allowed_tokens - prompt_tokens

    def calculate_max_tokens(self, model: WrappedChatOpenAI, *prompts: str) -> None:
        requested_tokens = self.get_completion_space(model.model_name, *prompts)

        model.max_tokens = min(model.max_tokens, requested_tokens)
        model.max_tokens = max(model.max_tokens, 1)

# From tokenizer/token_service.py
def create(cls, encoding: str = "cl100k_base") -> "TokenService":
        return cls(get_encoding(encoding))

# From tokenizer/token_service.py
def tokenize(self, text: str) -> list[int]:
        return self.encoding.encode(text)

# From tokenizer/token_service.py
def detokenize(self, tokens: list[int]) -> str:
        return self.encoding.decode(tokens)

# From tokenizer/token_service.py
def get_completion_space(self, model: LLM_Model, *prompts: str) -> int:
        max_allowed_tokens = LLM_MODEL_MAX_TOKENS.get(model, 4000)
        prompt_tokens = sum([self.count(p) for p in prompts])
        return max_allowed_tokens - prompt_tokens

# From tokenizer/token_service.py
def calculate_max_tokens(self, model: WrappedChatOpenAI, *prompts: str) -> None:
        requested_tokens = self.get_completion_space(model.model_name, *prompts)

        model.max_tokens = min(model.max_tokens, requested_tokens)
        model.max_tokens = max(model.max_tokens, 1)


# From tokenizer/lifetime.py
def init_tokenizer(app: FastAPI) -> None:  # pragma: no cover
    """
    Initialize tokenizer.

    TikToken downloads the encoding on start. It is then
    stored in the state of the application.

    :param app: current application.
    """
    app.state.token_encoding = tiktoken.get_encoding(ENCODING_NAME)

from reworkd_platform.services.tokenizer.token_service import TokenService

# From tokenizer/dependencies.py
def get_token_service(request: Request) -> TokenService:
    return TokenService(request.app.state.token_encoding)

from aiohttp import ClientError
from boto3 import client

# From aws/s3.py
class PresignedPost(BaseModel):
    url: str
    fields: Dict[str, str]

# From aws/s3.py
class SimpleStorageService:
    # TODO: would be great if with could make this async

    def __init__(self, bucket: Optional[str]) -> None:
        if not bucket:
            raise ValueError("Bucket name must be provided")

        self._client = boto3_client("s3", region_name=REGION)
        self._bucket = bucket

    def create_presigned_upload_url(
        self,
        object_name: str,
    ) -> PresignedPost:
        return PresignedPost(
            **self._client.generate_presigned_post(
                Bucket=self._bucket,
                Key=object_name,
            )
        )

    def create_presigned_download_url(self, object_name: str) -> str:
        return self._client.generate_presigned_url(
            "get_object",
            Params={"Bucket": self._bucket, "Key": object_name},
        )

    def upload_to_bucket(
        self,
        object_name: str,
        file: io.BytesIO,
    ) -> None:
        try:
            self._client.put_object(
                Bucket=self._bucket, Key=object_name, Body=file.getvalue()
            )
        except ClientError as e:
            logger.error(e)
            raise e

    def download_file(self, object_name: str, local_filename: str) -> None:
        self._client.download_file(
            Bucket=self._bucket, Key=object_name, Filename=local_filename
        )

    def list_keys(self, prefix: str) -> List[str]:
        files = self._client.list_objects_v2(Bucket=self._bucket, Prefix=prefix)
        if "Contents" not in files:
            return []

        return [file["Key"] for file in files["Contents"]]

    def download_folder(self, prefix: str, path: str) -> List[str]:
        local_files = []
        for key in self.list_keys(prefix):
            local_filename = os.path.join(path, key.split("/")[-1])
            self.download_file(key, local_filename)
            local_files.append(local_filename)

        return local_files

    def delete_folder(self, prefix: str) -> None:
        keys = self.list_keys(prefix)
        self._client.delete_objects(
            Bucket=self._bucket,
            Delete={"Objects": [{"Key": key} for key in keys]},
        )

# From aws/s3.py
def create_presigned_upload_url(
        self,
        object_name: str,
    ) -> PresignedPost:
        return PresignedPost(
            **self._client.generate_presigned_post(
                Bucket=self._bucket,
                Key=object_name,
            )
        )

# From aws/s3.py
def create_presigned_download_url(self, object_name: str) -> str:
        return self._client.generate_presigned_url(
            "get_object",
            Params={"Bucket": self._bucket, "Key": object_name},
        )

# From aws/s3.py
def upload_to_bucket(
        self,
        object_name: str,
        file: io.BytesIO,
    ) -> None:
        try:
            self._client.put_object(
                Bucket=self._bucket, Key=object_name, Body=file.getvalue()
            )
        except ClientError as e:
            logger.error(e)
            raise e

# From aws/s3.py
def download_file(self, object_name: str, local_filename: str) -> None:
        self._client.download_file(
            Bucket=self._bucket, Key=object_name, Filename=local_filename
        )

# From aws/s3.py
def list_keys(self, prefix: str) -> List[str]:
        files = self._client.list_objects_v2(Bucket=self._bucket, Prefix=prefix)
        if "Contents" not in files:
            return []

        return [file["Key"] for file in files["Contents"]]

# From aws/s3.py
def download_folder(self, prefix: str, path: str) -> List[str]:
        local_files = []
        for key in self.list_keys(prefix):
            local_filename = os.path.join(path, key.split("/")[-1])
            self.download_file(key, local_filename)
            local_files.append(local_filename)

        return local_files

# From aws/s3.py
def delete_folder(self, prefix: str) -> None:
        keys = self.list_keys(prefix)
        self._client.delete_objects(
            Bucket=self._bucket,
            Delete={"Objects": [{"Key": key} for key in keys]},
        )

import pinecone

# From pinecone/lifetime.py
def init_pinecone() -> None:
    if settings.pinecone_api_key and settings.pinecone_environment:
        pinecone.init(
            api_key=settings.pinecone_api_key,
            environment=settings.pinecone_environment,
        )

from sqlalchemy import and_
from sqlalchemy import select
from sqlalchemy.orm import selectinload
from reworkd_platform.db.crud.base import BaseCrud
from reworkd_platform.db.models.auth import OrganizationUser
from reworkd_platform.db.models.user import UserSession

# From crud/user.py
class UserCrud(BaseCrud):
    async def get_user_session(self, token: str) -> UserSession:
        query = (
            select(UserSession)
            .filter(UserSession.session_token == token)
            .options(selectinload(UserSession.user))
        )
        return (await self.session.execute(query)).scalar_one()

    async def get_user_organization(
        self, user_id: str, organization_id: str
    ) -> Optional[OrganizationUser]:
        query = select(OrganizationUser).filter(
            and_(
                OrganizationUser.user_id == user_id,
                OrganizationUser.organization_id == organization_id,
            )
        )

        # TODO: Only returns the first organization
        return (await self.session.execute(query)).scalar()


# From crud/base.py
class BaseCrud:
    def __init__(self, session: AsyncSession):
        self.session = session

from sqlalchemy.orm import aliased
from reworkd_platform.db.models.auth import Organization
from reworkd_platform.db.models.user import User
from reworkd_platform.web.api.dependencies import get_current_user

# From crud/organization.py
class OrgUser(BaseModel):
    id: str
    role: str
    user: UserBase

# From crud/organization.py
class OrganizationUsers(BaseModel):
    id: str
    name: str
    users: List[OrgUser]

# From crud/organization.py
class OrganizationCrud(BaseCrud):
    def __init__(self, session: AsyncSession, user: UserBase):
        super().__init__(session)
        self.user = user

    @classmethod
    def inject(
        cls,
        session: AsyncSession = Depends(get_db_session),
        user: UserBase = Depends(get_current_user),
    ) -> "OrganizationCrud":
        return cls(session, user)

    async def create_organization(self, name: str) -> Organization:
        return await Organization(
            created_by=self.user.id,
            name=name,
        ).save(self.session)

    async def get_by_name(self, name: str) -> Optional[OrganizationUsers]:
        owner = aliased(OrganizationUser, name="owner")

        query = (
            select(
                Organization,
                User,
                OrganizationUser,
            )
            .join(
                OrganizationUser,
                and_(
                    Organization.id == OrganizationUser.organization_id,
                ),
            )
            .join(
                User,
                User.id == OrganizationUser.user_id,
            )
            .join(  # Owner
                owner,
                and_(
                    OrganizationUser.organization_id == Organization.id,
                    OrganizationUser.user_id == self.user.id,
                ),
            )
            .filter(Organization.name == name)
        )

        rows = (await self.session.execute(query)).all()
        if not rows:
            return None

        org: Organization = rows[0][0]
        return OrganizationUsers(
            id=org.id,
            name=org.name,
            users=[
                OrgUser(
                    id=org_user.user_id,
                    role=org_user.role,
                    user=UserBase(
                        id=user.id,
                        email=user.email,
                        name=user.name,
                    ),
                )
                for [_, user, org_user] in rows
            ],
        )

# From crud/organization.py
def inject(
        cls,
        session: AsyncSession = Depends(get_db_session),
        user: UserBase = Depends(get_current_user),
    ) -> "OrganizationCrud":
        return cls(session, user)


# From crud/oauth.py
class OAuthCrud(BaseCrud):
    @classmethod
    async def inject(
        cls,
        session: AsyncSession = Depends(get_db_session),
    ) -> "OAuthCrud":
        return cls(session)

    async def create_installation(
        self, user: UserBase, provider: str, redirect_uri: Optional[str]
    ) -> OauthCredentials:
        return await OauthCredentials(
            user_id=user.id,
            organization_id=user.organization_id,
            provider=provider,
            state=secrets.token_hex(16),
            redirect_uri=redirect_uri,
        ).save(self.session)

    async def get_installation_by_state(self, state: str) -> Optional[OauthCredentials]:
        query = select(OauthCredentials).filter(OauthCredentials.state == state)

        return (await self.session.execute(query)).scalar_one_or_none()

    async def get_installation_by_user_id(
        self, user_id: str, provider: str
    ) -> Optional[OauthCredentials]:
        query = select(OauthCredentials).filter(
            OauthCredentials.user_id == user_id,
            OauthCredentials.provider == provider,
            OauthCredentials.access_token_enc.isnot(None),
        )

        return (await self.session.execute(query)).scalars().first()

    async def get_installation_by_organization_id(
        self, organization_id: str, provider: str
    ) -> Optional[OauthCredentials]:
        query = select(OauthCredentials).filter(
            OauthCredentials.organization_id == organization_id,
            OauthCredentials.provider == provider,
            OauthCredentials.access_token_enc.isnot(None),
            OauthCredentials.organization_id.isnot(None),
        )

        return (await self.session.execute(query)).scalars().first()

    async def get_all(self, user: UserBase) -> Dict[str, str]:
        query = (
            select(
                OauthCredentials.provider,
                func.any_value(OauthCredentials.access_token_enc),
            )
            .filter(
                OauthCredentials.access_token_enc.isnot(None),
                OauthCredentials.organization_id == user.organization_id,
            )
            .group_by(OauthCredentials.provider)
        )

        return {
            provider: token
            for provider, token in (await self.session.execute(query)).all()
        }

from sqlalchemy import Index
from sqlalchemy.orm import relationship
from reworkd_platform.db.base import Base

# From models/user.py
class UserSession(Base):
    __tablename__ = "Session"

    session_token = mapped_column(String, unique=True, name="sessionToken")
    user_id = mapped_column(
        String, ForeignKey("User.id", ondelete="CASCADE"), name="userId"
    )
    expires = mapped_column(DateTime)

    user = relationship("User")

    __table_args__ = (Index("user_id"),)

from reworkd_platform.db.base import TrackedModel

# From models/auth.py
class Organization(TrackedModel):
    __tablename__ = "organization"

    name = mapped_column(String, nullable=False)
    created_by = mapped_column(String, nullable=False)

# From models/auth.py
class OrganizationUser(TrackedModel):
    __tablename__ = "organization_user"

    user_id = mapped_column(String, nullable=False)
    organization_id = mapped_column(String, nullable=False)
    role = mapped_column(String, nullable=False, default="member")

# From models/auth.py
class OauthCredentials(TrackedModel):
    __tablename__ = "oauth_credentials"

    user_id = mapped_column(String, nullable=False)
    organization_id = mapped_column(String, nullable=True)
    provider = mapped_column(String, nullable=False)
    state = mapped_column(String, nullable=False)
    redirect_uri = mapped_column(String, nullable=False)

    # Post-installation
    token_type = mapped_column(String, nullable=True)
    access_token_enc = mapped_column(String, nullable=True)
    access_token_expiration = mapped_column(DateTime, nullable=True)
    refresh_token_enc = mapped_column(String, nullable=True)
    scope = mapped_column(String, nullable=True)

from urllib.parse import urlparse
from httpx import HTTPStatusError
from httpx import RequestError

# From api/metadata.py
class Metadata(BaseModel):
    title: Optional[str] = Field(default=None, description="Title of the page")
    hostname: Optional[str] = Field(default=None, description="Hostname of the page")
    favicon: Optional[str] = Field(default=None, description="Favicon of the page")


# From api/http_responses.py
def forbidden(detail: str = "Forbidden") -> HTTPException:
    return HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=detail)

# From api/http_responses.py
def not_found(detail: str = "Not Found") -> HTTPException:
    return HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=detail)


from fastapi import Header
from fastapi.security import HTTPAuthorizationCredentials
from fastapi.security import HTTPBearer
from sqlalchemy.orm.exc import NoResultFound
from reworkd_platform.db.crud.user import UserCrud
from reworkd_platform.schemas.user import UserBase

# From api/dependencies.py
def user_crud(
    session: AsyncSession = Depends(get_db_session),
) -> UserCrud:
    return UserCrud(session)


# From api/errors.py
class PlatformaticError(Exception):
    """
    Parent exception class for all expected backend exceptions
    Will be caught and handled by the platform_exception_handler
    Shoutout to https://platformatic.dev/
    """

    detail: str
    code: int
    should_log: bool = True

    def __init__(
        self,
        base_exception: Exception,
        detail: str = "",
        code: int = 409,
        should_log: bool = True,
    ):
        super().__init__(base_exception)
        self.detail = detail
        self.code = code
        self.should_log = should_log

# From api/errors.py
class OpenAIError(PlatformaticError):
    pass

# From api/errors.py
class ReplicateError(PlatformaticError):
    pass

# From api/errors.py
class MaxLoopsError(PlatformaticError):
    pass

# From api/errors.py
class MultipleSummaryError(PlatformaticError):
    pass

from fastapi.routing import APIRouter
from reworkd_platform.web.api import agent
from reworkd_platform.web.api import auth
from reworkd_platform.web.api import metadata
from reworkd_platform.web.api import models
from reworkd_platform.web.api import monitoring

from fastapi.responses import RedirectResponse
from reworkd_platform.db.crud.organization import OrganizationCrud
from reworkd_platform.db.crud.organization import OrganizationUsers
from reworkd_platform.services.oauth_installers import OAuthInstaller
from reworkd_platform.services.oauth_installers import installer_factory


# From agent/stream_mock.py
def stream_string(data: str, delayed: bool = False) -> FastAPIStreamingResponse:
    return FastAPIStreamingResponse(
        stream_generator(data, delayed),
    )


import ast
from langchain.schema import BaseOutputParser
from langchain.schema import OutputParserException

# From agent/task_output_parser.py
class TaskOutputParser(BaseOutputParser[List[str]]):
    """
    Extension of LangChain's BaseOutputParser
    Responsible for parsing task creation output into a list of task strings
    """

    completed_tasks: List[str] = []

    def __init__(self, *, completed_tasks: List[str]):
        super().__init__()
        self.completed_tasks = completed_tasks

    def parse(self, text: str) -> List[str]:
        try:
            array_str = extract_array(text)
            all_tasks = [
                remove_prefix(task) for task in array_str if real_tasks_filter(task)
            ]
            return [task for task in all_tasks if task not in self.completed_tasks]
        except Exception as e:
            msg = f"Failed to parse tasks from completion '{text}'. Exception: {e}"
            raise OutputParserException(msg)

    def get_format_instructions(self) -> str:
        return """
        The response should be a JSON array of strings. Example:

        ["Search the web for NBA news", "Write some code to build a web scraper"]

        This should be parsable by json.loads()
        """

# From agent/task_output_parser.py
def extract_array(input_str: str) -> List[str]:
    regex = (
        r"\[\s*\]|"  # Empty array check
        r"(\[(?:\s*(?:\"(?:[^\"\\]|\\.)*\"|\'(?:[^\'\\]|\\.)*\')\s*,?)*\s*\])"
    )
    match = re.search(regex, input_str)
    if match is not None:
        return ast.literal_eval(match[0])
    else:
        return handle_multiline_string(input_str)

# From agent/task_output_parser.py
def handle_multiline_string(input_str: str) -> List[str]:
    # Handle multiline string as a list
    processed_lines = [
        re.sub(r".*?(\d+\..+)", r"\1", line).strip()
        for line in input_str.split("\n")
        if line.strip() != ""
    ]

    # Check if there is at least one line that starts with a digit and a period
    if any(re.match(r"\d+\..+", line) for line in processed_lines):
        return processed_lines
    else:
        raise RuntimeError(f"Failed to extract array from {input_str}")

# From agent/task_output_parser.py
def remove_prefix(input_str: str) -> str:
    prefix_pattern = (
        r"^(Task\s*\d*\.\s*|Task\s*\d*[-:]?\s*|Step\s*\d*["
        r"-:]?\s*|Step\s*[-:]?\s*|\d+\.\s*|\d+\s*[-:]?\s*|^\.\s*|^\.*)"
    )
    return re.sub(prefix_pattern, "", input_str, flags=re.IGNORECASE)

# From agent/task_output_parser.py
def real_tasks_filter(input_str: str) -> bool:
    no_task_regex = (
        r"^No( (new|further|additional|extra|other))? tasks? (is )?("
        r"required|needed|added|created|inputted).*"
    )
    task_complete_regex = r"^Task (complete|completed|finished|done|over|success).*"
    do_nothing_regex = r"^(\s*|Do nothing(\s.*)?)$"

    return (
        not re.search(no_task_regex, input_str, re.IGNORECASE)
        and not re.search(task_complete_regex, input_str, re.IGNORECASE)
        and not re.search(do_nothing_regex, input_str, re.IGNORECASE)
    )

# From agent/task_output_parser.py
def parse(self, text: str) -> List[str]:
        try:
            array_str = extract_array(text)
            all_tasks = [
                remove_prefix(task) for task in array_str if real_tasks_filter(task)
            ]
            return [task for task in all_tasks if task not in self.completed_tasks]
        except Exception as e:
            msg = f"Failed to parse tasks from completion '{text}'. Exception: {e}"
            raise OutputParserException(msg)

# From agent/task_output_parser.py
def get_format_instructions(self) -> str:
        return """
        The response should be a JSON array of strings. Example:

        ["Search the web for NBA news", "Write some code to build a web scraper"]

        This should be parsable by json.loads()
        """

from pydantic import validator
from reworkd_platform.web.api.agent.tools.tools import get_available_tools_names
from reworkd_platform.web.api.agent.tools.search import Search
from reworkd_platform.web.api.agent.tools.tools import get_tool_name
from reworkd_platform.web.api.agent.tools.tools import get_default_tool_name

# From agent/analysis.py
class AnalysisArguments(BaseModel):
    """
    Arguments for the analysis function of a tool. OpenAI functions will resolve these values but leave out the action.
    """

    reasoning: str
    arg: str

# From agent/analysis.py
class Analysis(AnalysisArguments):
    action: str

    @validator("action")
    def action_must_be_valid_tool(cls, v: str) -> str:
        # TODO: Remove circular import
        from reworkd_platform.web.api.agent.tools.tools import get_available_tools_names

        if v not in get_available_tools_names():
            raise ValueError(f"Analysis action '{v}' is not a valid tool")
        return v

    @validator("action")
    def search_action_must_have_arg(cls, v: str, values: Dict[str, str]) -> str:
        from reworkd_platform.web.api.agent.tools.search import Search
        from reworkd_platform.web.api.agent.tools.tools import get_tool_name

        if v == get_tool_name(Search) and not values["arg"]:
            raise ValueError("Analysis arg cannot be empty if action is 'search'")
        return v

    @classmethod
    def get_default_analysis(cls, task: str) -> "Analysis":
        # TODO: Remove circular import
        from reworkd_platform.web.api.agent.tools.tools import get_default_tool_name

        return cls(
            reasoning="Hmm... I'll try searching it up",
            action=get_default_tool_name(),
            arg=task,
        )

# From agent/analysis.py
def action_must_be_valid_tool(cls, v: str) -> str:
        # TODO: Remove circular import
        from reworkd_platform.web.api.agent.tools.tools import get_available_tools_names

        if v not in get_available_tools_names():
            raise ValueError(f"Analysis action '{v}' is not a valid tool")
        return v

# From agent/analysis.py
def search_action_must_have_arg(cls, v: str, values: Dict[str, str]) -> str:
        from reworkd_platform.web.api.agent.tools.search import Search
        from reworkd_platform.web.api.agent.tools.tools import get_tool_name

        if v == get_tool_name(Search) and not values["arg"]:
            raise ValueError("Analysis arg cannot be empty if action is 'search'")
        return v

# From agent/analysis.py
def get_default_analysis(cls, task: str) -> "Analysis":
        # TODO: Remove circular import
        from reworkd_platform.web.api.agent.tools.tools import get_default_tool_name

        return cls(
            reasoning="Hmm... I'll try searching it up",
            action=get_default_tool_name(),
            arg=task,
        )

from langchain.chat_models import AzureChatOpenAI
from langchain.chat_models import ChatOpenAI
from reworkd_platform.schemas.agent import ModelSettings

# From agent/model_factory.py
class WrappedChatOpenAI(ChatOpenAI):
    client: Any = Field(
        default=None,
        description="Meta private value but mypy will complain its missing",
    )
    max_tokens: int
    model_name: LLM_Model = Field(alias="model")

# From agent/model_factory.py
class WrappedAzureChatOpenAI(AzureChatOpenAI, WrappedChatOpenAI):
    openai_api_base: str
    openai_api_version: str
    deployment_name: str

# From agent/model_factory.py
def create_model(
    settings: Settings,
    model_settings: ModelSettings,
    user: UserBase,
    streaming: bool = False,
    force_model: Optional[LLM_Model] = None,
) -> WrappedChat:
    use_azure = (
        not model_settings.custom_api_key and "azure" in settings.openai_api_base
    )

    llm_model = force_model or model_settings.model
    model: Type[WrappedChat] = WrappedChatOpenAI
    base, headers, use_helicone = get_base_and_headers(settings, model_settings, user)
    kwargs = {
        "openai_api_base": base,
        "openai_api_key": model_settings.custom_api_key or settings.openai_api_key,
        "temperature": model_settings.temperature,
        "model": llm_model,
        "max_tokens": model_settings.max_tokens,
        "streaming": streaming,
        "max_retries": 5,
        "model_kwargs": {"user": user.email, "headers": headers},
    }

    if use_azure:
        model = WrappedAzureChatOpenAI
        deployment_name = llm_model.replace(".", "")
        kwargs.update(
            {
                "openai_api_version": settings.openai_api_version,
                "deployment_name": deployment_name,
                "openai_api_type": "azure",
                "openai_api_base": base.rstrip("v1"),
            }
        )

        if use_helicone:
            kwargs["model"] = deployment_name

    return model(**kwargs)

# From agent/model_factory.py
def get_base_and_headers(
    settings_: Settings, model_settings: ModelSettings, user: UserBase
) -> Tuple[str, Optional[Dict[str, str]], bool]:
    use_helicone = settings_.helicone_enabled and not model_settings.custom_api_key
    base = (
        settings_.helicone_api_base
        if use_helicone
        else (
            "https://api.openai.com/v1"
            if model_settings.custom_api_key
            else settings_.openai_api_base
        )
    )

    headers = (
        {
            "Helicone-Auth": f"Bearer {settings_.helicone_api_key}",
            "Helicone-Cache-Enabled": "true",
            "Helicone-User-Id": user.id,
            "Helicone-OpenAI-Api-Base": settings_.openai_api_base,
        }
        if use_helicone
        else None
    )

    return base, headers, use_helicone

from langchain import BasePromptTemplate
from langchain.chat_models.base import BaseChatModel
from openai.error import AuthenticationError
from openai.error import InvalidRequestError
from openai.error import RateLimitError
from openai.error import ServiceUnavailableError
from reworkd_platform.web.api.errors import OpenAIError

# From agent/helpers.py
def parse_with_handling(parser: BaseOutputParser[T], completion: str) -> T:
    try:
        return parser.parse(completion)
    except OutputParserException as e:
        raise OpenAIError(
            e, "There was an issue parsing the response from the AI model."
        )


# From models/views.py
class ModelWithAccess(BaseModel):
    name: str
    max_tokens: int
    has_access: bool = Field(
        default=False, description="Whether the user has access to this model"
    )

    @staticmethod
    def from_model(name: str, max_tokens: int, user: UserBase) -> "ModelWithAccess":
        has_access = user is not None
        return ModelWithAccess(name=name, max_tokens=max_tokens, has_access=has_access)

# From models/views.py
def from_model(name: str, max_tokens: int, user: UserBase) -> "ModelWithAccess":
        has_access = user is not None
        return ModelWithAccess(name=name, max_tokens=max_tokens, has_access=has_access)


# From monitoring/views.py
def health_check() -> None:
    """
    Checks the health of a project.

    It returns 200 if the project is healthy.
    """

# From monitoring/views.py
def error_check() -> None:
    """
    Checks that errors are being correctly logged.
    """
    raise Exception("This is an expected error from the error check endpoint!")

from lanarky.responses import StreamingResponse
from reworkd_platform.web.api.agent.tools.tool import Tool
from reworkd_platform.web.api.agent.prompts import execute_task_prompt

# From tools/reason.py
class Reason(Tool):
    description = (
        "Reason about task via existing information or understanding. "
        "Make decisions / selections from options."
    )

    async def call(
        self, goal: str, task: str, input_str: str, *args: Any, **kwargs: Any
    ) -> FastAPIStreamingResponse:
        from reworkd_platform.web.api.agent.prompts import execute_task_prompt

        chain = LLMChain(llm=self.model, prompt=execute_task_prompt)

        return StreamingResponse.from_chain(
            chain,
            {"goal": goal, "language": self.language, "task": task},
            media_type="text/event-stream",
        )


# From tools/tool.py
class Tool(ABC):
    description: str = ""
    public_description: str = ""
    arg_description: str = "The argument to the function."
    image_url: str = "/tools/openai-white.png"

    model: BaseChatModel
    language: str

    def __init__(self, model: BaseChatModel, language: str):
        self.model = model
        self.language = language

    @staticmethod
    def available() -> bool:
        return True

    @staticmethod
    async def dynamic_available(user: UserBase, oauth_crud: OAuthCrud) -> bool:
        return True

    @abstractmethod
    async def call(
        self,
        goal: str,
        task: str,
        input_str: str,
        user: UserBase,
        oauth_crud: OAuthCrud,
    ) -> StreamingResponse:
        pass

# From tools/tool.py
def available() -> bool:
        return True

from langchain import WikipediaAPIWrapper
from reworkd_platform.web.api.agent.stream_mock import stream_string

# From tools/wikipedia_search.py
class Wikipedia(Tool):
    description = (
        "Search Wikipedia for information about historical people, companies, events, "
        "places or research. This should be used over search for broad overviews of "
        "specific nouns."
    )
    public_description = "Search Wikipedia for historical information."
    arg_description = "A simple query string of just the noun in question."
    image_url = "/tools/wikipedia.png"

    async def call(
        self, goal: str, task: str, input_str: str, *args: Any, **kwargs: Any
    ) -> StreamingResponse:
        wikipedia_client = WikipediaAPIWrapper(
            wiki_client=None,  # Meta private value but mypy will complain its missing
        )

        # TODO: Make the below async
        wikipedia_search = wikipedia_client.run(input_str)
        # return summarize_with_sources(self.model, self.language, goal, task, [wikipedia_search])
        return stream_string("Wikipedia is currently not working")

import openai
import replicate
from replicate.exceptions import ModelError
from replicate.exceptions import ReplicateError
from reworkd_platform.web.api.errors import ReplicateError

from reworkd_platform.web.api.agent.prompts import code_prompt

from reworkd_platform.web.api.agent.prompts import summarize_prompt
from reworkd_platform.web.api.agent.prompts import summarize_with_sources_prompt
from reworkd_platform.web.api.agent.prompts import summarize_sid_prompt

# From tools/utils.py
class CitedSnippet:
    index: int
    text: str
    url: str = ""

    def __repr__(self) -> str:
        """
        The string representation the AI model will see
        """
        return f"{{i: {self.index}, text: {self.text}, url: {self.url}}}"

# From tools/utils.py
class Snippet:
    text: str

    def __repr__(self) -> str:
        """
        The string representation the AI model will see
        """
        return f"{{text: {self.text}}}"

# From tools/utils.py
def summarize_with_sources(
    model: BaseChatModel,
    language: str,
    goal: str,
    query: str,
    snippets: List[CitedSnippet],
) -> FastAPIStreamingResponse:
    from reworkd_platform.web.api.agent.prompts import summarize_with_sources_prompt

    chain = LLMChain(llm=model, prompt=summarize_with_sources_prompt)

    return StreamingResponse.from_chain(
        chain,
        {
            "goal": goal,
            "query": query,
            "language": language,
            "snippets": snippets,
        },
        media_type="text/event-stream",
    )

# From tools/utils.py
def summarize_sid(
    model: BaseChatModel,
    language: str,
    goal: str,
    query: str,
    snippets: List[Snippet],
) -> FastAPIStreamingResponse:
    from reworkd_platform.web.api.agent.prompts import summarize_sid_prompt

    chain = LLMChain(llm=model, prompt=summarize_sid_prompt)

    return StreamingResponse.from_chain(
        chain,
        {
            "goal": goal,
            "query": query,
            "language": language,
            "snippets": snippets,
        },
        media_type="text/event-stream",
    )


# From tools/open_ai_function.py
class FunctionDescription(TypedDict):
    """Representation of a callable function to the OpenAI API."""

    name: str
    """The name of the function."""
    description: str
    """A description of the function."""
    parameters: dict[str, object]
    """The parameters of the function."""

# From tools/open_ai_function.py
def get_tool_function(tool: Type[Tool]) -> FunctionDescription:
    """A function that will return the tool's function specification"""
    name = get_tool_name(tool)

    return {
        "name": name,
        "description": tool.description,
        "parameters": {
            "type": "object",
            "properties": {
                "reasoning": {
                    "type": "string",
                    "description": (
                        f"Reasoning is how the task will be accomplished with the current function. "
                        "Detail your overall plan along with any concerns you have."
                        "Ensure this reasoning value is in the user defined langauge "
                    ),
                },
                "arg": {
                    "type": "string",
                    "description": tool.arg_description,
                },
            },
            "required": ["reasoning", "arg"],
        },
    }

from reworkd_platform.web.api.agent.tools.utils import Snippet
from reworkd_platform.web.api.agent.tools.utils import summarize_sid

# From tools/sidsearch.py
class SID(Tool):
    public_description = "Grant access to your Notion, Google Drive, etc."
    description = """
        Find private information by searching through notion, email and google drive.
        Should be used when questions refer to personal information.
    """
    arg_description = (
        "The query to search for. It should be a question in natural language."
    )
    image_url = "/tools/sid.png"

    @staticmethod
    def available() -> bool:
        return settings.sid_enabled

    @staticmethod
    async def dynamic_available(user: UserBase, oauth_crud: OAuthCrud) -> bool:
        installation = await oauth_crud.get_installation_by_user_id(
            user_id=user.id, provider="sid"
        )

        return bool(installation and installation.access_token_enc)

    async def _run_sid(
        self,
        goal: str,
        task: str,
        input_str: str,
        user: UserBase,
        oauth_crud: OAuthCrud,
    ) -> Optional[FastAPIStreamingResponse]:
        installation = await oauth_crud.get_installation_by_user_id(
            user_id=user.id, provider="sid"
        )
        if not installation:
            logger.warning("No sid installation found for user {user.id}")
            return None

        token = await get_access_token(oauth_crud, installation)
        if not token:
            logger.warning("Unable to fetch sid access token for {user.id}")
            return None

        try:
            res = await _sid_search_results(input_str, limit=10, token=token)
            snippets: List[Snippet] = [
                Snippet(text=result["text"]) for result in (res.get("results", []))
            ]
        except Exception as e:
            logger.exception(e)
            return None

        if not snippets:
            return None

        return summarize_sid(self.model, self.language, goal, task, snippets)


    async def call(
        self,
        goal: str,
        task: str,
        input_str: str,
        user: UserBase,
        oauth_crud: OAuthCrud,
        *args: Any,
        **kwargs: Any,
    ) -> FastAPIStreamingResponse:
         # fall back to search if no results are found
        return await self._run_sid(goal, task, input_str, user, oauth_crud) or await Search(self.model, self.language).call(
        goal, task, input_str, user, oauth_crud
    )

from urllib.parse import quote
from aiohttp import ClientResponseError
from reworkd_platform.web.api.agent.tools.reason import Reason
from reworkd_platform.web.api.agent.tools.utils import CitedSnippet
from reworkd_platform.web.api.agent.tools.utils import summarize_with_sources

# From tools/search.py
class Search(Tool):
    description = (
        "Search Google for short up to date searches for simple questions about public information "
        "news and people.\n"
    )
    public_description = "Search google for information about current events."
    arg_description = "The query argument to search for. This value is always populated and cannot be an empty string."
    image_url = "/tools/google.png"

    @staticmethod
    def available() -> bool:
        return settings.serp_api_key is not None and settings.serp_api_key != ""

    async def call(
        self, goal: str, task: str, input_str: str, *args: Any, **kwargs: Any
    ) -> FastAPIStreamingResponse:
        try:
            return await self._call(goal, task, input_str, *args, **kwargs)
        except ClientResponseError:
            logger.exception("Error calling Serper API, falling back to reasoning")
            return await Reason(self.model, self.language).call(
                goal, task, input_str, *args, **kwargs
            )

    async def _call(
        self, goal: str, task: str, input_str: str, *args: Any, **kwargs: Any
    ) -> FastAPIStreamingResponse:
        results = await _google_serper_search_results(
            input_str,
        )

        k = 5  # Number of results to return
        snippets: List[CitedSnippet] = []

        if results.get("answerBox"):
            answer_values = []
            answer_box = results.get("answerBox", {})
            if answer_box.get("answer"):
                answer_values.append(answer_box.get("answer"))
            elif answer_box.get("snippet"):
                answer_values.append(answer_box.get("snippet").replace("\n", " "))
            elif answer_box.get("snippetHighlighted"):
                answer_values.append(", ".join(answer_box.get("snippetHighlighted")))

            if len(answer_values) > 0:
                snippets.append(
                    CitedSnippet(
                        len(snippets) + 1,
                        "\n".join(answer_values),
                        f"https://www.google.com/search?q={quote(input_str)}",
                    )
                )

        for i, result in enumerate(results["organic"][:k]):
            texts = []
            link = ""
            if "snippet" in result:
                texts.append(result["snippet"])
            if "link" in result:
                link = result["link"]
            for attribute, value in result.get("attributes", {}).items():
                texts.append(f"{attribute}: {value}.")
            snippets.append(CitedSnippet(len(snippets) + 1, "\n".join(texts), link))

        if len(snippets) == 0:
            return stream_string("No good Google Search Result was found", True)

        return summarize_with_sources(self.model, self.language, goal, task, snippets)


# From tools/conclude.py
class Conclude(Tool):
    description = "Use when there is nothing else to do. The task has been concluded."

    async def call(
        self, goal: str, task: str, input_str: str, *args: Any, **kwargs: Any
    ) -> FastAPIStreamingResponse:
        return stream_string("Task execution concluded.", delayed=True)


# From babyagi/setup.py
def parse_requirements(filename):
    with open(filename, "r") as f:
        lines = f.readlines()
    # Remove comments and empty lines
    return [line.strip() for line in lines if line.strip() and not line.startswith("#")]

import babyagi

# From babyagi/main.py
def home():
    return f"Welcome to the main app. Visit <a href=\"/dashboard\">/dashboard</a> for BabyAGI dashboard."


# From examples/simple_example.py
def world():
    return "world"

# From examples/simple_example.py
def hello_world():
    x = world()
    return f"Hello {x}!"

from babyagi import create_app
from babyagi import register_function

# From examples/custom_route_example.py
def another_custom_function():
    return "Hello from another custom function!"


# From examples/trigger_example.py
def function_a():
    print("Result from function A")
    return "Result from function A"

# From examples/trigger_example.py
def function_b(input_data):
    print(f"Function B triggered with input: {input_data}")
    return f"Function B triggered with input: {input_data}"

from flask import Flask
from babyagi import load_functions

# From examples/custom_flask_example.py
def integrated_function():
    return "Hello from integrated function!"



from execution import FunctionExecutor

# From core/registration.py
class FunctionRegistrar:
    def __init__(self, python_func):
        self.python_func = python_func

    def register_function(self, metadata: Optional[Dict[str, Any]] = None,
                          imports: Optional[List[str]] = None,
                          dependencies: Optional[List[str]] = None,
                          triggers: Optional[List[str]] = None,
                          key_dependencies: Optional[List[str]] = None):
        """Decorator to register a function."""
        def decorator(func):
            function_name = func.__name__
            source_lines = inspect.getsourcelines(func)[0]
            func_start = next(i for i, line in enumerate(source_lines) if line.strip().startswith('def '))
            function_code = ''.join(source_lines[func_start:]).strip()

            # Store metadata on the function object
            func.__pythonfunc_metadata__ = {
                'metadata': metadata or {},
                'imports': imports or [],
                'dependencies': dependencies or [],
                'triggers': triggers or [], 
                'key_dependencies': key_dependencies or []
            }

            self.add_function(function_name, metadata=metadata, code=function_code,
                              imports=imports, dependencies=dependencies, 
                              key_dependencies=key_dependencies, triggers=triggers)
            def wrapper(*args, **kwargs):
                return self.python_func.executor.execute(function_name, *args, **kwargs)
            return wrapper
        return decorator

    def parse_function_parameters(self, code: str):
        """
        Parse the input and output parameters of a given function code.
        """
        try:
            # Parse the source code into an AST
            tree = ast.parse(code)

            # Find the function definition node
            function_def = next(node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef))

            # Parse input parameters
            input_params = []
            for arg in function_def.args.args:
                param_type = 'Any'
                if arg.annotation:
                    param_type = ast.unparse(arg.annotation)
                input_params.append({'name': arg.arg, 'type': param_type})

            # Parse return statement to identify output parameters
            output_params = []
            returns = [node for node in ast.walk(function_def) if isinstance(node, ast.Return)]
            if returns:
                return_node = returns[0].value
                if isinstance(return_node, ast.Dict):
                    # If returning a dictionary, treat each key as an output parameter
                    for key in return_node.keys:
                        if isinstance(key, ast.Str):
                            output_params.append({'name': key.s, 'type': 'Any'})
                elif isinstance(return_node, (ast.Name, ast.Attribute)):
                    # If returning a single variable
                    output_params.append({'name': 'output', 'type': 'Any'})
                elif isinstance(return_node, ast.Str):
                    # If returning a string literal
                    output_params.append({'name': 'output', 'type': 'str'})
                else:
                    # For other types of returns, use a generic 'output' parameter
                    output_params.append({'name': 'output', 'type': 'Any'})

            return input_params, output_params
        except Exception as e:
            # print(f"Error parsing function parameters: {str(e)}")
            return [], []

    def parse_import(self, imp):
        try:
            #print("Attempt to parse the string as JSON")
            #print(imp.type())
            parsed = json.loads(imp)
            #print("Parsed string as JSON")
            return parsed
        except json.JSONDecodeError:
            print("Failed to parse the string as JSON")
            # If it fails, return the original string (it's a simple import)
            return imp

    # Register imports helper function
    def register_imports(self, imports):
        print(f"Registering imports: {imports}")
        if isinstance(imports, list):
            for imp in imports:
                self.process_single_import(imp)
        elif isinstance(imports, dict):
            self.process_single_import(imports)

    def process_single_import(self, imp):
        if isinstance(imp, str):
            self.python_func.db.add_import(imp, 'external', lib=None)
        elif isinstance(imp, dict):
            name = imp.get('name')
            lib = imp.get('lib')
            if name:
                self.python_func.db.add_import(name, 'external', lib=lib)
        else:
            print(f"Unsupported import format: {imp}")

    def process_imports(self, imports):
        import_names = []
        if imports:
            if isinstance(imports, list):
                for imp in imports:
                    if isinstance(imp, str):
                        import_names.append(imp)
                    elif isinstance(imp, dict):
                        import_name = imp.get('name')
                        if import_name:
                            import_names.append(import_name)
            elif isinstance(imports, dict):
                import_name = imports.get('name')
                if import_name:
                    import_names.append(import_name)
        return import_names

    def function_has_no_changes(self, name, code, metadata, import_names, dependencies, triggers):
        existing_function = self.python_func.db.get_function(name)
        if not existing_function:
            return False  # Function does not exist, so changes are needed

        existing_code = existing_function.get('code')
        existing_metadata = existing_function.get('metadata', {})
        existing_description = existing_metadata.get('description')
        existing_imports = existing_function.get('imports') or []
        existing_dependencies = existing_function.get('dependencies') or []
        existing_triggers = existing_function.get('triggers') or []

        new_description = metadata.get('description') if metadata else None

        if (existing_code == code and
            existing_description == new_description and
            set(existing_imports) == set(import_names) and
            set(existing_dependencies) == set(dependencies) and
            set(existing_triggers) == set(triggers)):
            return True  # No changes
        else:
            return False  # Changes detected

    def add_function(self, name: str, metadata: Optional[Dict[str, Any]] = None,
                    code: Optional[str] = None, imports: Optional[List[Union[str, Dict[str, str]]]] = None,
                    dependencies: Optional[List[str]] = None, triggers: Optional[List[str]] = None,
                    key_dependencies: Optional[List[str]] = None,
                    input_parameters: Optional[List[Dict[str, str]]] = None,
                    output_parameters: Optional[List[Dict[str, str]]] = None) -> None:

        if code:
            # Parse input and output parameters if not provided
            if input_parameters is None or output_parameters is None:
                parsed_input, parsed_output = self.parse_function_parameters(code)
                input_parameters = input_parameters or parsed_input
                output_parameters = output_parameters or parsed_output

        # Process imports
        import_names = self.process_imports(imports)

        # Ensure lists are not None
        dependencies = dependencies or []
        triggers = triggers or []

        # Check for changes
        if self.function_has_no_changes(name, code, metadata, import_names, dependencies, triggers):
            #print(f"Function {name} has no changes.")
            return

        # Register imports
        if imports:
            self.register_imports(imports)

        # Add or update the function in the database
        existing_function = self.python_func.db.get_function(name)
        if existing_function:
            # Function exists, update it
            self.python_func.db.update_function(
                name, code=code, metadata=metadata, dependencies=dependencies,
                input_parameters=input_parameters, output_parameters=output_parameters,
                imports=import_names, triggers=triggers
            )
        else:
            # Function does not exist, add it
            self.python_func.db.add_function(
                name, code=code, metadata=metadata, dependencies=dependencies,
                input_parameters=input_parameters, output_parameters=output_parameters,
                imports=import_names, triggers=triggers
            )

        if key_dependencies:
            print(f"Function {name} requires keys: {key_dependencies}")
            metadata = metadata or {}
            metadata['key_dependencies'] = key_dependencies

        if self.python_func.db.get_function('function_added_or_updated'):
            try:
                action = 'updated' if existing_function else 'added'
                self.python_func.executor.execute(function_name='function_added_or_updated', action=action, triggered_function_name=name)
            except Exception as e:
                logger.error(f"Error executing trigger function 'function_added_or_updated': {str(e)}")

    def update_function(self, name: str, code: Optional[str] = None, imports: Optional[List[Union[str, Dict[str, str]]]] = None,
                        metadata: Optional[Dict[str, Any]] = None,
                        dependencies: Optional[List[str]] = None,
                        triggers: Optional[List[str]] = None,
                        key_dependencies: Optional[List[str]] = None,
                        input_parameters: Optional[List[Dict[str, str]]] = None,
                        output_parameters: Optional[List[Dict[str, str]]] = None) -> None:

        if code:
            # Parse input and output parameters if not provided
            if input_parameters is None or output_parameters is None:
                parsed_input, parsed_output = self.parse_function_parameters(code)
                input_parameters = input_parameters or parsed_input
                output_parameters = output_parameters or parsed_output

        # Process imports
        import_names = self.process_imports(imports)

        # Ensure lists are not None
        dependencies = dependencies or []
        triggers = triggers or []

        # Check for changes
        if self.function_has_no_changes(name, code, metadata, import_names, dependencies, triggers):
            # print(f"Function {name} has no changes.")
            return

        # Update the function in the database
        self.python_func.db.update_function(
            name, code=code, metadata=metadata, dependencies=dependencies,
            input_parameters=input_parameters, output_parameters=output_parameters,
            imports=import_names, triggers=triggers
        )

        if key_dependencies:
            metadata = metadata or {}
            metadata['key_dependencies'] = key_dependencies

        # Register imports
        if imports:
            self.register_imports(imports)

        if self.python_func.db.get_function('function_added_or_updated'):
            try:
                self.python_func.executor.execute(function_name='function_added_or_updated', action='updated', triggered_function_name=name)
            except Exception as e:
                logger.error(f"Error executing trigger function 'function_added_or_updated': {str(e)}")

# From core/registration.py
def register_function(self, metadata: Optional[Dict[str, Any]] = None,
                          imports: Optional[List[str]] = None,
                          dependencies: Optional[List[str]] = None,
                          triggers: Optional[List[str]] = None,
                          key_dependencies: Optional[List[str]] = None):
        """Decorator to register a function."""
        def decorator(func):
            function_name = func.__name__
            source_lines = inspect.getsourcelines(func)[0]
            func_start = next(i for i, line in enumerate(source_lines) if line.strip().startswith('def '))
            function_code = ''.join(source_lines[func_start:]).strip()

            # Store metadata on the function object
            func.__pythonfunc_metadata__ = {
                'metadata': metadata or {},
                'imports': imports or [],
                'dependencies': dependencies or [],
                'triggers': triggers or [], 
                'key_dependencies': key_dependencies or []
            }

            self.add_function(function_name, metadata=metadata, code=function_code,
                              imports=imports, dependencies=dependencies, 
                              key_dependencies=key_dependencies, triggers=triggers)
            def wrapper(*args, **kwargs):
                return self.python_func.executor.execute(function_name, *args, **kwargs)
            return wrapper
        return decorator

# From core/registration.py
def parse_function_parameters(self, code: str):
        """
        Parse the input and output parameters of a given function code.
        """
        try:
            # Parse the source code into an AST
            tree = ast.parse(code)

            # Find the function definition node
            function_def = next(node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef))

            # Parse input parameters
            input_params = []
            for arg in function_def.args.args:
                param_type = 'Any'
                if arg.annotation:
                    param_type = ast.unparse(arg.annotation)
                input_params.append({'name': arg.arg, 'type': param_type})

            # Parse return statement to identify output parameters
            output_params = []
            returns = [node for node in ast.walk(function_def) if isinstance(node, ast.Return)]
            if returns:
                return_node = returns[0].value
                if isinstance(return_node, ast.Dict):
                    # If returning a dictionary, treat each key as an output parameter
                    for key in return_node.keys:
                        if isinstance(key, ast.Str):
                            output_params.append({'name': key.s, 'type': 'Any'})
                elif isinstance(return_node, (ast.Name, ast.Attribute)):
                    # If returning a single variable
                    output_params.append({'name': 'output', 'type': 'Any'})
                elif isinstance(return_node, ast.Str):
                    # If returning a string literal
                    output_params.append({'name': 'output', 'type': 'str'})
                else:
                    # For other types of returns, use a generic 'output' parameter
                    output_params.append({'name': 'output', 'type': 'Any'})

            return input_params, output_params
        except Exception as e:
            # print(f"Error parsing function parameters: {str(e)}")
            return [], []

# From core/registration.py
def parse_import(self, imp):
        try:
            #print("Attempt to parse the string as JSON")
            #print(imp.type())
            parsed = json.loads(imp)
            #print("Parsed string as JSON")
            return parsed
        except json.JSONDecodeError:
            print("Failed to parse the string as JSON")
            # If it fails, return the original string (it's a simple import)
            return imp

# From core/registration.py
def register_imports(self, imports):
        print(f"Registering imports: {imports}")
        if isinstance(imports, list):
            for imp in imports:
                self.process_single_import(imp)
        elif isinstance(imports, dict):
            self.process_single_import(imports)

# From core/registration.py
def process_single_import(self, imp):
        if isinstance(imp, str):
            self.python_func.db.add_import(imp, 'external', lib=None)
        elif isinstance(imp, dict):
            name = imp.get('name')
            lib = imp.get('lib')
            if name:
                self.python_func.db.add_import(name, 'external', lib=lib)
        else:
            print(f"Unsupported import format: {imp}")

# From core/registration.py
def process_imports(self, imports):
        import_names = []
        if imports:
            if isinstance(imports, list):
                for imp in imports:
                    if isinstance(imp, str):
                        import_names.append(imp)
                    elif isinstance(imp, dict):
                        import_name = imp.get('name')
                        if import_name:
                            import_names.append(import_name)
            elif isinstance(imports, dict):
                import_name = imports.get('name')
                if import_name:
                    import_names.append(import_name)
        return import_names

# From core/registration.py
def function_has_no_changes(self, name, code, metadata, import_names, dependencies, triggers):
        existing_function = self.python_func.db.get_function(name)
        if not existing_function:
            return False  # Function does not exist, so changes are needed

        existing_code = existing_function.get('code')
        existing_metadata = existing_function.get('metadata', {})
        existing_description = existing_metadata.get('description')
        existing_imports = existing_function.get('imports') or []
        existing_dependencies = existing_function.get('dependencies') or []
        existing_triggers = existing_function.get('triggers') or []

        new_description = metadata.get('description') if metadata else None

        if (existing_code == code and
            existing_description == new_description and
            set(existing_imports) == set(import_names) and
            set(existing_dependencies) == set(dependencies) and
            set(existing_triggers) == set(triggers)):
            return True  # No changes
        else:
            return False

# From core/registration.py
def add_function(self, name: str, metadata: Optional[Dict[str, Any]] = None,
                    code: Optional[str] = None, imports: Optional[List[Union[str, Dict[str, str]]]] = None,
                    dependencies: Optional[List[str]] = None, triggers: Optional[List[str]] = None,
                    key_dependencies: Optional[List[str]] = None,
                    input_parameters: Optional[List[Dict[str, str]]] = None,
                    output_parameters: Optional[List[Dict[str, str]]] = None) -> None:

        if code:
            # Parse input and output parameters if not provided
            if input_parameters is None or output_parameters is None:
                parsed_input, parsed_output = self.parse_function_parameters(code)
                input_parameters = input_parameters or parsed_input
                output_parameters = output_parameters or parsed_output

        # Process imports
        import_names = self.process_imports(imports)

        # Ensure lists are not None
        dependencies = dependencies or []
        triggers = triggers or []

        # Check for changes
        if self.function_has_no_changes(name, code, metadata, import_names, dependencies, triggers):
            #print(f"Function {name} has no changes.")
            return

        # Register imports
        if imports:
            self.register_imports(imports)

        # Add or update the function in the database
        existing_function = self.python_func.db.get_function(name)
        if existing_function:
            # Function exists, update it
            self.python_func.db.update_function(
                name, code=code, metadata=metadata, dependencies=dependencies,
                input_parameters=input_parameters, output_parameters=output_parameters,
                imports=import_names, triggers=triggers
            )
        else:
            # Function does not exist, add it
            self.python_func.db.add_function(
                name, code=code, metadata=metadata, dependencies=dependencies,
                input_parameters=input_parameters, output_parameters=output_parameters,
                imports=import_names, triggers=triggers
            )

        if key_dependencies:
            print(f"Function {name} requires keys: {key_dependencies}")
            metadata = metadata or {}
            metadata['key_dependencies'] = key_dependencies

        if self.python_func.db.get_function('function_added_or_updated'):
            try:
                action = 'updated' if existing_function else 'added'
                self.python_func.executor.execute(function_name='function_added_or_updated', action=action, triggered_function_name=name)
            except Exception as e:
                logger.error(f"Error executing trigger function 'function_added_or_updated': {str(e)}")

# From core/registration.py
def update_function(self, name: str, code: Optional[str] = None, imports: Optional[List[Union[str, Dict[str, str]]]] = None,
                        metadata: Optional[Dict[str, Any]] = None,
                        dependencies: Optional[List[str]] = None,
                        triggers: Optional[List[str]] = None,
                        key_dependencies: Optional[List[str]] = None,
                        input_parameters: Optional[List[Dict[str, str]]] = None,
                        output_parameters: Optional[List[Dict[str, str]]] = None) -> None:

        if code:
            # Parse input and output parameters if not provided
            if input_parameters is None or output_parameters is None:
                parsed_input, parsed_output = self.parse_function_parameters(code)
                input_parameters = input_parameters or parsed_input
                output_parameters = output_parameters or parsed_output

        # Process imports
        import_names = self.process_imports(imports)

        # Ensure lists are not None
        dependencies = dependencies or []
        triggers = triggers or []

        # Check for changes
        if self.function_has_no_changes(name, code, metadata, import_names, dependencies, triggers):
            # print(f"Function {name} has no changes.")
            return

        # Update the function in the database
        self.python_func.db.update_function(
            name, code=code, metadata=metadata, dependencies=dependencies,
            input_parameters=input_parameters, output_parameters=output_parameters,
            imports=import_names, triggers=triggers
        )

        if key_dependencies:
            metadata = metadata or {}
            metadata['key_dependencies'] = key_dependencies

        # Register imports
        if imports:
            self.register_imports(imports)

        if self.python_func.db.get_function('function_added_or_updated'):
            try:
                self.python_func.executor.execute(function_name='function_added_or_updated', action='updated', triggered_function_name=name)
            except Exception as e:
                logger.error(f"Error executing trigger function 'function_added_or_updated': {str(e)}")

from db.db_router import DBRouter
from registration import FunctionRegistrar

# From core/framework.py
class Functionz:
    def __init__(self, db_type='local', **db_kwargs):
        self.db = DBRouter(db_type, **db_kwargs)
        self.executor = FunctionExecutor(self)
        self.registrar = FunctionRegistrar(self)

    # Function execution
    def execute_function(self, function_name: str, *args, **kwargs):
        return self.executor.execute(function_name, *args, **kwargs)

    def __getattr__(self, name):
        if self.db.get_function(name):
            return lambda *args, **kwargs: self.executor.execute(name, *args, **kwargs)
        raise AttributeError(f"'PythonFunc' object has no attribute '{name}'")

    # Function management
    def get_function(self, name: str):
        return self.db.get_function(name)

    def get_function_versions(self, name: str):
        return self.db.get_function_versions(name)

    def get_all_functions(self) -> List[Dict[str, Any]]:
        return self.db.get_all_functions()

    def activate_function_version(self, name: str, version: int) -> None:
        self.db.activate_function_version(name, version)

    def get_function_imports(self, name: str):
        return self.db.get_function_imports(name)

    # Function registration (exposing registrar methods)
    def register_function(self, *args, **kwargs):
        return self.registrar.register_function(*args, **kwargs)

    def update_function(self, *args, **kwargs):
        return self.registrar.update_function(*args, **kwargs)

    def add_function(self, *args, **kwargs):
        return self.registrar.add_function(*args, **kwargs)

    # Key management
    def add_key(self, key_name: str, key_value: str) -> None:
        self.db.add_secret_key(key_name, key_value)

    def get_all_secret_keys(self, *args, **kwargs):
        return self.db.get_all_secret_keys(*args, **kwargs)

    # Import management
    def get_all_imports(self, *args, **kwargs):
        return self.db.get_all_imports(*args, **kwargs)

    # Function pack and file loading
    def load_function_pack(self, pack_name: str):
        packs_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'packs')
        pack_path = os.path.join(packs_dir, pack_name + '.py')

        if not os.path.exists(pack_path):
            logger.error(f"Function pack '{pack_name}' not found.")
            return

        self._load_module_from_path(pack_path, pack_name)

    def load_functions_from_file(self, file_path: str):
        if not os.path.exists(file_path):
            logger.error(f"File '{file_path}' not found.")
            return

        module_name = os.path.splitext(os.path.basename(file_path))[0]
        self._load_module_from_path(file_path, module_name)

    def _load_module_from_path(self, path: str, module_name: str):
        spec = importlib.util.spec_from_file_location(module_name, path)
        module = importlib.util.module_from_spec(spec)
        module.func = self

        original_sys_path = sys.path[:]
        try:
            babyagi_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            if babyagi_root not in sys.path:
                sys.path.insert(0, babyagi_root)

            spec.loader.exec_module(module)
            logger.info(f"Loaded module '{module_name}' from '{path}'")
        except Exception as e:
            logger.error(f"Error loading module '{module_name}' from '{path}': {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        finally:
            sys.path = original_sys_path

    # Trigger management
    def add_trigger(self, triggered_function_name, triggering_function_name=None):
        self.db.add_trigger(triggered_function_name, triggering_function_name)

    def get_triggers_for_function(self, function_name: str) -> List[str]:
        function_data = self.get_function(function_name)
        return function_data.get('triggers', []) if function_data else []

    # Logging and display
    def get_logs(self, function_name: Optional[str] = None,
                 start_date: Optional[datetime] = None,
                 end_date: Optional[datetime] = None) -> List[Dict[str, Any]]:
        return self.db.get_logs(function_name, start_date, end_date)

    def display(self):
        functions = self.db.get_all_functions()
        result = []

        for function in functions:
            function_info = [
                f"Function: {function['name']}",
                f"  Version: {function['version']}",
                f"  Created Date: {function['created_date']}",
                f"  Metadata: {function['metadata']}",
                f"  Dependencies: {function['dependencies']}",
                f"  Triggers: {function.get('triggers', [])}",
                "  Input Parameters:",
            ]
            for param in function['input_parameters']:
                function_info.append(f"    - {param['name']} ({param['type']})")

            function_info.append("  Output Parameters:")
            for param in function['output_parameters']:
                function_info.append(f"    - {param['name']} ({param['type']})")

            function_info.append(f"  Code:\n{function['code']}")
            function_info.append("---")

            result.append("\n".join(function_info))

        return "\n\n".join(result)

# From core/framework.py
def get_function(self, name: str):
        return self.db.get_function(name)

# From core/framework.py
def get_function_versions(self, name: str):
        return self.db.get_function_versions(name)

# From core/framework.py
def get_all_functions(self) -> List[Dict[str, Any]]:
        return self.db.get_all_functions()

# From core/framework.py
def activate_function_version(self, name: str, version: int) -> None:
        self.db.activate_function_version(name, version)

# From core/framework.py
def get_function_imports(self, name: str):
        return self.db.get_function_imports(name)

# From core/framework.py
def add_key(self, key_name: str, key_value: str) -> None:
        self.db.add_secret_key(key_name, key_value)

# From core/framework.py
def get_all_secret_keys(self, *args, **kwargs):
        return self.db.get_all_secret_keys(*args, **kwargs)

# From core/framework.py
def get_all_imports(self, *args, **kwargs):
        return self.db.get_all_imports(*args, **kwargs)

# From core/framework.py
def load_function_pack(self, pack_name: str):
        packs_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'packs')
        pack_path = os.path.join(packs_dir, pack_name + '.py')

        if not os.path.exists(pack_path):
            logger.error(f"Function pack '{pack_name}' not found.")
            return

        self._load_module_from_path(pack_path, pack_name)

# From core/framework.py
def load_functions_from_file(self, file_path: str):
        if not os.path.exists(file_path):
            logger.error(f"File '{file_path}' not found.")
            return

        module_name = os.path.splitext(os.path.basename(file_path))[0]
        self._load_module_from_path(file_path, module_name)

# From core/framework.py
def add_trigger(self, triggered_function_name, triggering_function_name=None):
        self.db.add_trigger(triggered_function_name, triggering_function_name)

# From core/framework.py
def get_triggers_for_function(self, function_name: str) -> List[str]:
        function_data = self.get_function(function_name)
        return function_data.get('triggers', []) if function_data else []

# From core/framework.py
def get_logs(self, function_name: Optional[str] = None,
                 start_date: Optional[datetime] = None,
                 end_date: Optional[datetime] = None) -> List[Dict[str, Any]]:
        return self.db.get_logs(function_name, start_date, end_date)

# From core/framework.py
def display(self):
        functions = self.db.get_all_functions()
        result = []

        for function in functions:
            function_info = [
                f"Function: {function['name']}",
                f"  Version: {function['version']}",
                f"  Created Date: {function['created_date']}",
                f"  Metadata: {function['metadata']}",
                f"  Dependencies: {function['dependencies']}",
                f"  Triggers: {function.get('triggers', [])}",
                "  Input Parameters:",
            ]
            for param in function['input_parameters']:
                function_info.append(f"    - {param['name']} ({param['type']})")

            function_info.append("  Output Parameters:")
            for param in function['output_parameters']:
                function_info.append(f"    - {param['name']} ({param['type']})")

            function_info.append(f"  Code:\n{function['code']}")
            function_info.append("---")

            result.append("\n".join(function_info))

        return "\n\n".join(result)


# From db/base_db.py
class BaseDB(ABC):
    # Function management
    @abstractmethod
    def add_function(self, name: str, code: str, metadata: Optional[Dict[str, Any]] = None,
                     dependencies: Optional[List[str]] = None,
                     input_parameters: Optional[List[Dict[str, Any]]] = None,
                     output_parameters: Optional[List[Dict[str, Any]]] = None) -> None:
        pass

    @abstractmethod
    def get_function(self, name: str) -> Optional[Dict[str, Any]]:
        pass

    @abstractmethod
    def get_all_functions(self) -> List[Dict[str, Any]]:
        pass

    @abstractmethod
    def update_function(self, name: str, code: Optional[str] = None,
                        metadata: Optional[Dict[str, Any]] = None,
                        dependencies: Optional[List[str]] = None,
                        input_parameters: Optional[List[Dict[str, Any]]] = None,
                        output_parameters: Optional[List[Dict[str, Any]]] = None) -> None:
        pass

    @abstractmethod
    def remove_function(self, name: str) -> None:
        pass

    @abstractmethod
    def get_function_versions(self, name: str) -> List[Dict[str, Any]]:
        pass

    @abstractmethod
    def activate_function_version(self, name: str, version: int) -> None:
        pass

    # Import management
    @abstractmethod
    def add_import(self, name: str, source: str, lib: Optional[str] = None) -> None:
        pass

    @abstractmethod
    def get_all_imports(self) -> List[Dict[str, Any]]:
        pass

    # Logging
    @abstractmethod
    def add_log(self, function_name: str, message: str, timestamp: datetime,
                params: Optional[Dict[str, Any]] = None,
                output: Optional[Any] = None,
                time_spent: Optional[float] = None) -> None:
        pass

    @abstractmethod
    def get_logs(self, function_name: Optional[str] = None,
                 start_date: Optional[datetime] = None,
                 end_date: Optional[datetime] = None) -> List[Dict[str, Any]]:
        pass

# From db/base_db.py
def remove_function(self, name: str) -> None:
        pass

# From db/base_db.py
def add_import(self, name: str, source: str, lib: Optional[str] = None) -> None:
        pass

# From db/base_db.py
def add_log(self, function_name: str, message: str, timestamp: datetime,
                params: Optional[Dict[str, Any]] = None,
                output: Optional[Any] = None,
                time_spent: Optional[float] = None) -> None:
        pass

from local_db import LocalDB
from base_db import BaseDB
from models import Import
from models import Function
from models import FunctionVersion
from models import Log

# From db/db_router.py
class ImportResult:
    def __init__(self, name: str, source: str):
        self.name = name
        self.source = source

# From db/db_router.py
class DBRouter(BaseDB):
    def __init__(self, db_type: str = 'local', **kwargs):
        if db_type == 'local':
            self.db = LocalDB(**kwargs)
        else:
            raise ValueError(f"Unsupported database type: {db_type}")

    @contextmanager
    def session_scope(self):
        with self.db.session_scope() as session:
            yield session

    # Function management
    def add_function(self, name: str, code: str, metadata: Optional[Dict[str, Any]] = None, 
                     dependencies: Optional[List[str]] = None, 
                     triggers: Optional[List[str]] = None,
                     input_parameters: Optional[List[Dict[str, str]]] = None,
                     output_parameters: Optional[List[Dict[str, str]]] = None,
                     imports: Optional[List[str]] = None) -> None:
        with self.session_scope() as session:
            self.db.add_or_update_function(session, name, code, metadata, dependencies, triggers, input_parameters, output_parameters, imports)

    def update_function(self, name: str, code: Optional[str] = None, 
                        metadata: Optional[Dict[str, Any]] = None, 
                        dependencies: Optional[List[str]] = None,
                        triggers: Optional[List[str]] = None,
                        input_parameters: Optional[List[Dict[str, str]]] = None,
                        output_parameters: Optional[List[Dict[str, str]]] = None,
                        imports: Optional[List[str]] = None) -> None:
        with self.session_scope() as session:
            function = self.db.get_function(session, name)
            if function:
                active_version = self.db.get_active_version(session, function)
                self.db.add_or_update_function(
                    session, name, 
                    code if code is not None else active_version.code,
                    metadata or active_version.function_metadata,
                    dependencies, 
                    triggers if triggers is not None else active_version.triggers,
                    input_parameters or active_version.input_parameters,
                    output_parameters or active_version.output_parameters,
                    imports
                )

    def get_function(self, name: str) -> Optional[Dict[str, Any]]:
        with self.session_scope() as session:
            function = self.db.get_function(session, name)
            if function:
                active_version = self.db.get_active_version(session, function)
                if active_version:
                    return {
                        'name': function.name,
                        'version': active_version.version,
                        'code': active_version.code,
                        'metadata': active_version.function_metadata,
                        'dependencies': [dep.name for dep in active_version.dependencies],
                        'imports': [imp.name for imp in active_version.imports],
                        'created_date': active_version.created_date.isoformat(),
                        'input_parameters': active_version.input_parameters,
                        'output_parameters': active_version.output_parameters,
                        'triggers': active_version.triggers
                    }
            return None

    def get_all_functions(self) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            functions = self.db.get_all_functions(session)
            return [
                {
                    'name': function.name,
                    'version': active_version.version,
                    'code': active_version.code,
                    'metadata': active_version.function_metadata,
                    'dependencies': [dep.name for dep in active_version.dependencies],
                    'imports': [imp.name for imp in active_version.imports],
                    'created_date': active_version.created_date.isoformat(),
                    'input_parameters': active_version.input_parameters,
                    'output_parameters': active_version.output_parameters,
                    'triggers': active_version.triggers
                }
                for function in functions
                if (active_version := next((v for v in function.versions if v.is_active), None))
            ]

    def remove_function(self, name: str) -> None:
        with self.session_scope() as session:
            function = self.db.get_function(session, name)
            if function:
                session.delete(function)

    def get_function_versions(self, name: str) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            function = self.db.get_function(session, name)
            if function:
                return [
                    {
                        'version': v.version,
                        'code': v.code,
                        'metadata': v.function_metadata,
                        'is_active': v.is_active,
                        'dependencies': [dep.name for dep in v.dependencies],
                        'created_date': v.created_date.isoformat(),
                        'input_parameters': v.input_parameters,
                        'output_parameters': v.output_parameters,
                        'triggers': v.triggers
                    }
                    for v in function.versions
                ]
            return []

    def activate_function_version(self, name: str, version: int) -> None:
        with self.session_scope() as session:
            function = self.db.get_function(session, name)
            if function:
                for v in function.versions:
                    v.is_active = (v.version == version)

    # Import management
    def add_import(self, name: str, source: str, lib: Optional[str] = None) -> None:
        with self.session_scope() as session:
            self.db.add_import(session, name, source, lib)

    def get_all_imports(self) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            imports = session.query(Import).all()
            return [{"name": imp.name, "source": imp.source, "lib": imp.lib} for imp in imports]

    def get_function_imports(self, function_name: str) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            function = session.query(Function).filter_by(name=function_name).first()
            if function:
                imports = (session.query(Import)
                           .join(FunctionVersion.imports)
                           .filter(FunctionVersion.function_id == function.id)
                           .all())
                return [{"name": imp.name, "source": imp.source, "lib": imp.lib} for imp in imports]
            return []

    # Logging
    def add_log(self, function_name: str, message: str, timestamp: datetime, 
                params: Optional[Dict[str, Any]] = None, 
                output: Optional[Any] = None, 
                time_spent: Optional[float] = None, 
                parent_log_id: Optional[int] = None, 
                triggered_by_log_id: Optional[int] = None, 
                log_type: str = 'info') -> int:
        with self.session_scope() as session:
            return self.db.add_log(
                session=session,
                function_name=function_name,
                message=message,
                timestamp=timestamp,
                params=params,
                output=output,
                time_spent=time_spent,
                parent_log_id=parent_log_id,
                triggered_by_log_id=triggered_by_log_id,
                log_type=log_type
            )

    def update_log(self, log_id: int, **kwargs) -> None:
        with self.session_scope() as session:
            self.db.update_log(
                session=session,
                log_id=log_id,
                **kwargs
            )


    def update_log_params(self, log_id: int, params: Dict[str, Any]) -> None:
        with self.session_scope() as session:
            self.db.update_log_params(
                session=session,
                log_id=log_id,
                params=params
            )



    def get_logs(self, function_name: Optional[str] = None, 
                 start_date: Optional[datetime] = None, 
                 end_date: Optional[datetime] = None, 
                 triggered_by_log_id: Optional[int] = None) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            logs = self.db.get_logs(session, function_name, start_date, end_date, triggered_by_log_id)
            return [
                {
                    'id': log.id,
                    'function_name': log.function_name,
                    'message': log.message,
                    'timestamp': log.timestamp.isoformat(),
                    'params': log.params,
                    'output': log.output,
                    'time_spent': log.time_spent,
                    'parent_log_id': log.parent_log_id,
                    'triggered_by_log_id': log.triggered_by_log_id,
                    'log_type': log.log_type
                }
                for log in logs
            ]

    def get_log_bundle(self, log_id: int) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            logs_collected = {}

            def fetch_related_logs(current_log_id):
                if current_log_id in logs_collected:
                    return
                log = self.db.get_log(session, current_log_id)
                if log:
                    logs_collected[current_log_id] = log
                else:
                    logger.warning(f"Log ID {current_log_id} not found.")
                    return

                # Fetch parent log
                if log.parent_log_id:
                    fetch_related_logs(log.parent_log_id)

                    # Fetch sibling logs
                    sibling_logs = session.query(Log).filter(
                        Log.parent_log_id == log.parent_log_id,
                        Log.id != current_log_id
                    ).all()
                    for sibling in sibling_logs:
                        fetch_related_logs(sibling.id)

                # Fetch child logs
                child_logs = self.db.get_child_logs(session, current_log_id)
                for child in child_logs:
                    fetch_related_logs(child.id)

            fetch_related_logs(log_id)

            # Convert logs to dictionaries
            all_logs = [
                {
                    'id': log.id,
                    'function_name': log.function_name,
                    'message': log.message,
                    'timestamp': log.timestamp.isoformat(),
                    'params': log.params,
                    'output': log.output,
                    'time_spent': log.time_spent,
                    'parent_log_id': log.parent_log_id,
                    'triggered_by_log_id': log.triggered_by_log_id,
                    'log_type': log.log_type
                }
                for log in logs_collected.values()
            ]

            return all_logs


    
    # Secret key management
    def add_secret_key(self, key_name: str, key_value: str) -> None:
        with self.session_scope() as session:
            existing_key = self.db.get_secret_key(session, key_name)
            if existing_key:
                existing_key.value = key_value
            else:
                self.db.add_secret_key(session, key_name, key_value)

    def get_secret_key(self, key_name: str) -> Optional[str]:
        with self.session_scope() as session:
            secret_key = self.db.get_secret_key(session, key_name)
            return secret_key.value if secret_key else None

    def get_all_secret_keys(self) -> Dict[str, str]:
        with self.session_scope() as session:
            secret_keys = self.db.get_all_secret_keys(session)
            return {key.name: key.value for key in secret_keys if key.value is not None}

    # Trigger management
    def add_trigger(self, triggered_function_name: str, triggering_function_name: Optional[str] = None) -> None:
        with self.session_scope() as session:
            self.db.add_trigger(session, triggered_function_name, triggering_function_name)

    def get_triggers_for_function(self, function_name: str) -> List[str]:
        with self.session_scope() as session:
            all_functions = self.db.get_all_functions(session)
            triggered_functions = []
            for func in all_functions:
                active_version = self.db.get_active_version(session, func)
                if active_version and active_version.triggers:
                    if function_name in active_version.triggers:
                        triggered_functions.append(func.name)
            return triggered_functions

# From db/db_router.py
def session_scope(self):
        with self.db.session_scope() as session:
            yield session

# From db/db_router.py
def update_log(self, log_id: int, **kwargs) -> None:
        with self.session_scope() as session:
            self.db.update_log(
                session=session,
                log_id=log_id,
                **kwargs
            )

# From db/db_router.py
def update_log_params(self, log_id: int, params: Dict[str, Any]) -> None:
        with self.session_scope() as session:
            self.db.update_log_params(
                session=session,
                log_id=log_id,
                params=params
            )

# From db/db_router.py
def get_log_bundle(self, log_id: int) -> List[Dict[str, Any]]:
        with self.session_scope() as session:
            logs_collected = {}

            def fetch_related_logs(current_log_id):
                if current_log_id in logs_collected:
                    return
                log = self.db.get_log(session, current_log_id)
                if log:
                    logs_collected[current_log_id] = log
                else:
                    logger.warning(f"Log ID {current_log_id} not found.")
                    return

                # Fetch parent log
                if log.parent_log_id:
                    fetch_related_logs(log.parent_log_id)

                    # Fetch sibling logs
                    sibling_logs = session.query(Log).filter(
                        Log.parent_log_id == log.parent_log_id,
                        Log.id != current_log_id
                    ).all()
                    for sibling in sibling_logs:
                        fetch_related_logs(sibling.id)

                # Fetch child logs
                child_logs = self.db.get_child_logs(session, current_log_id)
                for child in child_logs:
                    fetch_related_logs(child.id)

            fetch_related_logs(log_id)

            # Convert logs to dictionaries
            all_logs = [
                {
                    'id': log.id,
                    'function_name': log.function_name,
                    'message': log.message,
                    'timestamp': log.timestamp.isoformat(),
                    'params': log.params,
                    'output': log.output,
                    'time_spent': log.time_spent,
                    'parent_log_id': log.parent_log_id,
                    'triggered_by_log_id': log.triggered_by_log_id,
                    'log_type': log.log_type
                }
                for log in logs_collected.values()
            ]

            return all_logs

# From db/db_router.py
def add_secret_key(self, key_name: str, key_value: str) -> None:
        with self.session_scope() as session:
            existing_key = self.db.get_secret_key(session, key_name)
            if existing_key:
                existing_key.value = key_value
            else:
                self.db.add_secret_key(session, key_name, key_value)

# From db/db_router.py
def get_secret_key(self, key_name: str) -> Optional[str]:
        with self.session_scope() as session:
            secret_key = self.db.get_secret_key(session, key_name)
            return secret_key.value if secret_key else None

# From db/db_router.py
def fetch_related_logs(current_log_id):
                if current_log_id in logs_collected:
                    return
                log = self.db.get_log(session, current_log_id)
                if log:
                    logs_collected[current_log_id] = log
                else:
                    logger.warning(f"Log ID {current_log_id} not found.")
                    return

                # Fetch parent log
                if log.parent_log_id:
                    fetch_related_logs(log.parent_log_id)

                    # Fetch sibling logs
                    sibling_logs = session.query(Log).filter(
                        Log.parent_log_id == log.parent_log_id,
                        Log.id != current_log_id
                    ).all()
                    for sibling in sibling_logs:
                        fetch_related_logs(sibling.id)

                # Fetch child logs
                child_logs = self.db.get_child_logs(session, current_log_id)
                for child in child_logs:
                    fetch_related_logs(child.id)

from sqlalchemy import Column
from sqlalchemy import JSON
from sqlalchemy import Boolean
from sqlalchemy import Table
from sqlalchemy import Float
from sqlalchemy import LargeBinary
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.ext.hybrid import hybrid_property

# From db/models.py
class FunctionVersion(Base):
    __tablename__ = 'function_versions'
    id = Column(Integer, primary_key=True)
    function_id = Column(Integer, ForeignKey('functions.id'))
    version = Column(Integer)
    code = Column(String)
    function_metadata = Column(JSON)
    is_active = Column(Boolean, default=False)
    created_date = Column(DateTime, default=datetime.utcnow)
    input_parameters = Column(JSON)
    output_parameters = Column(JSON)
    function = relationship("Function", back_populates="versions")
    dependencies = relationship('Function', secondary=function_dependency,
                                primaryjoin=(function_dependency.c.function_version_id == id),
                                secondaryjoin=(function_dependency.c.dependency_id == Function.id))
    imports = relationship('Import', secondary=function_version_imports, back_populates='function_versions')
    triggers = Column(JSON, nullable=True)

# From db/models.py
class Import(Base):
    __tablename__ = 'imports'
    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True)
    lib = Column(String, nullable=True)
    source = Column(String)
    function_versions = relationship('FunctionVersion', secondary=function_version_imports, back_populates='imports')

# From db/models.py
class Log(Base):
    __tablename__ = 'logs'

    id = Column(Integer, primary_key=True)
    function_name = Column(String, nullable=False)
    message = Column(String, nullable=False)
    timestamp = Column(DateTime, nullable=False)
    params = Column(JSON, nullable=True)
    output = Column(JSON, nullable=True)
    time_spent = Column(Float, nullable=True)
    log_type = Column(String, nullable=False)

    # Parent Log Relationship
    parent_log_id = Column(Integer, ForeignKey('logs.id'), nullable=True)
    parent_log = relationship(
        'Log',
        remote_side=[id],
        backref='child_logs',
        foreign_keys=[parent_log_id]
    )

    # Triggered By Log Relationship
    triggered_by_log_id = Column(Integer, ForeignKey('logs.id'), nullable=True)
    triggered_by_log = relationship(
        'Log',
        remote_side=[id],
        backref='triggered_logs',
        foreign_keys=[triggered_by_log_id]
    )

# From db/models.py
class SecretKey(Base):
    __tablename__ = 'secret_keys'
    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False, unique=True)  # Make name unique
    _encrypted_value = Column(LargeBinary, nullable=False)

    @hybrid_property
    def value(self):
        if self._encrypted_value:
            try:
                return fernet.decrypt(self._encrypted_value).decode()
            except InvalidToken:
                print(f"Error decrypting value for key: {self.name}. The encryption key may have changed.")
                return None
        return None

    @value.setter
    def value(self, plaintext_value):
        if plaintext_value:
            self._encrypted_value = fernet.encrypt(plaintext_value.encode())
        else:
            self._encrypted_value = None

# From db/models.py
def get_or_create_key():
    if os.path.exists(KEY_FILE):
        with open(KEY_FILE, 'r') as f:
            return json.load(f)['key']
    else:
        key = Fernet.generate_key().decode()
        with open(KEY_FILE, 'w') as f:
            json.dump({'key': key}, f)
        return key

# From db/models.py
def value(self):
        if self._encrypted_value:
            try:
                return fernet.decrypt(self._encrypted_value).decode()
            except InvalidToken:
                print(f"Error decrypting value for key: {self.name}. The encryption key may have changed.")
                return None
        return None

from sqlalchemy import create_engine
from sqlalchemy import or_
from sqlalchemy.orm import sessionmaker
from sqlalchemy.orm import scoped_session
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import SQLAlchemyError
from models import Base
from models import SecretKey
from models import fernet

# From db/local_db.py
class LocalDB:
    def __init__(self, db_path='sqlite:///funztionz.db'):
        self.engine = create_engine(db_path)
        Base.metadata.create_all(self.engine)
        self.Session = scoped_session(sessionmaker(bind=self.engine))

    @contextmanager
    def session_scope(self):
        session = self.Session()
        try:
            yield session
            session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            raise e
        finally:
            self.Session.remove()

    def serialize_for_json(self, obj):
        """
        Recursively convert datetime objects to ISO format strings within the given object.
        Handles dictionaries, lists, and individual datetime objects.
        """
        if isinstance(obj, dict):
            return {k: self.serialize_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.serialize_for_json(element) for element in obj]
        elif isinstance(obj, datetime.datetime):
            return obj.isoformat()
        else:
            return obj


    def get_function(self, session, name):
        return session.query(Function).filter_by(name=name).first()

    def get_active_version(self, session, function):
        return session.query(FunctionVersion).filter_by(function_id=function.id, is_active=True).options(
            joinedload(FunctionVersion.dependencies)
        ).first()

    def get_all_functions(self, session):
        return session.query(Function).options(
            joinedload(Function.versions).joinedload(FunctionVersion.dependencies)
        ).all()

    def add_or_update_function(self, session, name, code, metadata, dependencies, triggers, input_parameters, output_parameters, imports=None):
        function = self.get_function(session, name)
        if not function:
            function = Function(name=name)
            session.add(function)
            session.flush()

        # Handle imports before creating the FunctionVersion
        import_objects = []
        if imports:
            for import_name in imports:
                imp = session.query(Import).filter_by(name=import_name).first()
                if not imp:
                    imp = Import(name=import_name, source='external')
                    session.add(imp)
                    session.flush()
                import_objects.append(imp)

        # Create the FunctionVersion instance, now including triggers as JSON
        version = FunctionVersion(
            function=function,
            version=len(function.versions) + 1,
            code=code,
            function_metadata=metadata or {},
            is_active=True,
            input_parameters=input_parameters or [],
            output_parameters=output_parameters or [],
            imports=import_objects,  # Pass the list of Import objects here
            triggers=triggers or []  # Store the triggers as JSON
        )
        session.add(version)

        # Handle dependencies
        if dependencies:
            for dep in dependencies:
                dep_func = self.get_function(session, dep)
                if dep_func:
                    version.dependencies.append(dep_func)

        # Deactivate previous versions
        for v in function.versions:
            if v != version:
                v.is_active = False


    def add_import(self, session, name, source, lib=None):
        existing_import = session.query(Import).filter_by(name=name).first()
        if not existing_import:
            new_import = Import(name=name, source=source, lib=lib)
            session.add(new_import)



    def add_log(self, session, function_name, message, timestamp, params, output, time_spent, parent_log_id=None, triggered_by_log_id=None, log_type='info'):
        if isinstance(timestamp, str):
            # Convert the string timestamp back to a datetime object
            timestamp = datetime.datetime.fromisoformat(timestamp)

        # Serialize params and output to ensure JSON serializability
        serialized_params = self.serialize_for_json(params) if params else None
        serialized_output = self.serialize_for_json(output) if output else None

        new_log = Log(
            function_name=function_name,
            message=message,
            timestamp=timestamp,
            params=serialized_params,
            output=serialized_output,
            time_spent=time_spent,
            parent_log_id=parent_log_id,
            triggered_by_log_id=triggered_by_log_id,
            log_type=log_type
        )
        session.add(new_log)
        session.flush()  # This ensures new_log.id is populated
        return new_log.id

    def update_log(self, session, log_id: int, **kwargs) -> None:
        # Fetch the log entry by id
        log_entry = session.query(Log).filter(Log.id == log_id).first()
        if log_entry:
            # Update only the fields provided in kwargs
            for key, value in kwargs.items():
                if hasattr(log_entry, key):
                    setattr(log_entry, key, value)
                else:
                    raise ValueError(f"Log has no attribute '{key}'")
            # No need to call session.commit(); it will be committed in the session scope
        else:
            raise ValueError(f"Log with id {log_id} not found.")



    def update_log_params(self, session, log_id: int, params) -> None:
        log_entry = session.query(Log).filter_by(id=log_id).one_or_none()
        if log_entry is None:
            raise ValueError(f"Log entry with id {log_id} not found.")

        log_entry.params = params
        session.commit()



    def get_log(self, session, log_id: int):
        """
        Fetches a single log entry by its ID.

        :param session: SQLAlchemy session object.
        :param log_id: The ID of the log to retrieve.
        :return: Log object if found, else None.
        """
        return session.query(Log).filter_by(id=log_id).first()

    def get_child_logs(self, session, parent_id: int):
        """
        Retrieves all child logs that have the given parent_log_id.

        :param session: SQLAlchemy session object.
        :param parent_id: The ID of the parent log.
        :return: List of Log objects.
        """
        return session.query(Log).filter_by(parent_log_id=parent_id).all()
    
    def get_logs(self, session, function_name=None, start_date=None, end_date=None, triggered_by_log_id=None):
        query = session.query(Log)

        if function_name:
            query = query.filter(Log.function_name == function_name)

        if start_date:
            query = query.filter(Log.timestamp >= start_date)

        if end_date:
            query = query.filter(Log.timestamp <= end_date)

        if triggered_by_log_id:
            query = query.filter(Log.triggered_by_log_id == triggered_by_log_id)

        return query.all()


    def get_log_bundle(self, session, log_id):
        logs_collected = {}

        def fetch_related_logs(current_log_id):
            if current_log_id in logs_collected:
                return
            log = session.query(Log).filter_by(id=current_log_id).one_or_none()
            if not log:
                return
            logs_collected[current_log_id] = log

            # Fetch parent log
            if log.parent_log_id:
                fetch_related_logs(log.parent_log_id)

                # Fetch sibling logs
                sibling_logs = session.query(Log).filter(
                    Log.parent_log_id == log.parent_log_id,
                    Log.id != current_log_id
                ).all()
                for sibling in sibling_logs:
                    if sibling.id not in logs_collected:
                        fetch_related_logs(sibling.id)

            # Fetch child logs
            child_logs = session.query(Log).filter_by(parent_log_id=current_log_id).all()
            for child in child_logs:
                if child.id not in logs_collected:
                    fetch_related_logs(child.id)

        fetch_related_logs(log_id)
        return list(logs_collected.values())




    def add_secret_key(self, session, function_id, key_name, key_value):
        print(f"Encrypting value for key '{key_name}'")
        try:
            encrypted_value = fernet.encrypt(key_value.encode())
            print(f"Value encrypted successfully for key '{key_name}'")
            secret_key = SecretKey(function_id=function_id, name=key_name, _encrypted_value=encrypted_value)
            session.add(secret_key)
            print(f"Secret key '{key_name}' added to session")
        except Exception as e:
            print(f"Error in add_secret_key: {str(e)}")
            raise

    
    def add_secret_key(self, session, key_name, key_value):
        encrypted_value = fernet.encrypt(key_value.encode())
        secret_key = SecretKey(name=key_name, _encrypted_value=encrypted_value)
        session.add(secret_key)

    def get_secret_key(self, session, key_name):
        return session.query(SecretKey).filter_by(name=key_name).first()

    
    def get_all_secret_keys(self, session):
        return session.query(SecretKey).all()

# From db/local_db.py
def get_active_version(self, session, function):
        return session.query(FunctionVersion).filter_by(function_id=function.id, is_active=True).options(
            joinedload(FunctionVersion.dependencies)
        ).first()

# From db/local_db.py
def add_or_update_function(self, session, name, code, metadata, dependencies, triggers, input_parameters, output_parameters, imports=None):
        function = self.get_function(session, name)
        if not function:
            function = Function(name=name)
            session.add(function)
            session.flush()

        # Handle imports before creating the FunctionVersion
        import_objects = []
        if imports:
            for import_name in imports:
                imp = session.query(Import).filter_by(name=import_name).first()
                if not imp:
                    imp = Import(name=import_name, source='external')
                    session.add(imp)
                    session.flush()
                import_objects.append(imp)

        # Create the FunctionVersion instance, now including triggers as JSON
        version = FunctionVersion(
            function=function,
            version=len(function.versions) + 1,
            code=code,
            function_metadata=metadata or {},
            is_active=True,
            input_parameters=input_parameters or [],
            output_parameters=output_parameters or [],
            imports=import_objects,  # Pass the list of Import objects here
            triggers=triggers or []  # Store the triggers as JSON
        )
        session.add(version)

        # Handle dependencies
        if dependencies:
            for dep in dependencies:
                dep_func = self.get_function(session, dep)
                if dep_func:
                    version.dependencies.append(dep_func)

        # Deactivate previous versions
        for v in function.versions:
            if v != version:
                v.is_active = False

# From db/local_db.py
def get_log(self, session, log_id: int):
        """
        Fetches a single log entry by its ID.

        :param session: SQLAlchemy session object.
        :param log_id: The ID of the log to retrieve.
        :return: Log object if found, else None.
        """
        return session.query(Log).filter_by(id=log_id).first()

# From db/local_db.py
def get_child_logs(self, session, parent_id: int):
        """
        Retrieves all child logs that have the given parent_log_id.

        :param session: SQLAlchemy session object.
        :param parent_id: The ID of the parent log.
        :return: List of Log objects.
        """
        return session.query(Log).filter_by(parent_log_id=parent_id).all()

from functionz.core.framework import func

# From drafts/self_build.py
def generate_queries(user_description, X=3, max_retries=3):
    """
    Generates X distinct queries that require action based on the user description using gpt_call. 

    Args:
        user_description (str): Description of the user or their needs.
        X (int, optional): Number of queries to generate. Defaults to 3.
        max_retries (int, optional): Maximum number of retries for generating valid queries. Defaults to 3.

    Returns:
        list: A list of generated queries.

    Raises:
        ValueError: If unable to generate valid queries within the retry limit.
    """
    prompt = f"""
You are an AI assistant. Based on the following user description, generate {X} distinct queries that such a user might ask:

User Description:
"{user_description}"

Provide the queries in JSON format as a list:

[
  "Query 1",
  "Query 2",
  ...
]

Ensure the queries are diverse, relevant to the user description, and represent realistic questions that this user might ask. Make requests that require action, such as using tools and APIs, which you should specify in the request. Based on the user description, guess what types of tools they use and specify them in the query.
"""

    errors = []  # To collect error messages from each attempt

    for attempt in range(1, max_retries + 1):
        response = gpt_call(prompt)
        try:
            queries = json.loads(response)
            if isinstance(queries, list) and len(queries) == X and all(isinstance(q, str) for q in queries):
                return queries
            else:
                error_message = (
                    f"Attempt {attempt}: Invalid JSON structure or incorrect number of queries. "
                    f"Expected {X} string queries, but received: {response}"
                )
                errors.append(error_message)
        except json.JSONDecodeError as e:
            error_message = (
                f"Attempt {attempt}: JSON decoding failed with error: {str(e)}. "
                f"Response received: {response}"
            )
            errors.append(error_message)
        except Exception as e:
            error_message = (
                f"Attempt {attempt}: An unexpected error occurred: {str(e)}. "
                f"Response received: {response}"
            )
            errors.append(error_message)

    # After all attempts, raise an error with all collected messages
    full_error_message = " | ".join(errors)
    raise ValueError(f"Failed to generate {X} valid queries after {max_retries} attempts. Errors: {full_error_message}")

# From drafts/self_build.py
def self_build(user_description, X=3):
    """
    Generates queries based on the user description and processes each query.

    Args:
        user_description (str): Description of the user or their needs.
        X (int, optional): Number of queries to generate and process. Defaults to 3.

    Returns:
        list: A list of dictionaries containing each query and its corresponding output.

    Raises:
        ValueError: If query generation fails.
    """
    try:

        print("\033[1;33mUser Description: ", user_description, "\033[0m")
        # Generate queries
        queries = generate_queries(user_description, X)
    except ValueError as e:
        # Log the error message for debugging
        print(f"Error in generate_queries: {str(e)}")
        return []

    print("\033[1;34mQueries generated by self_build: ", queries, "\033[0m")
    results = []
    for idx, query in enumerate(queries, start=1):
        try:
            output = process_user_input(query)
            results.append({'query': query, 'output': output})
        except Exception as e:
            # Log the error message for debugging
            print(f"Error processing query {idx} ('{query}'): {str(e)}")
            results.append({'query': query, 'output': None, 'error': str(e)})

    return results

from litellm import completion

# From drafts/generate_function.py
class FunctionSuggestion(BaseModel):
        reusable_functions: List[str] = Field(default_factory=list)
        reference_functions: List[str] = Field(default_factory=list)

# From drafts/generate_function.py
class Endpoint(BaseModel):
        method: Optional[str]
        url: str
        description: Optional[str] = None

# From drafts/generate_function.py
class APIDetails(BaseModel):
        api_name: str = Field(alias="name")  # Use alias to map 'name' to 'api_name'
        purpose: str
        endpoints: Optional[List[Union[Endpoint, str]]] = Field(default_factory=list)

        @validator("endpoints", pre=True, each_item=True)
        def convert_to_endpoint(cls, v):
            """Convert string URLs into Endpoint objects if necessary."""
            if isinstance(v, str):
                return Endpoint(url=v)  # Create an Endpoint object from a URL string
            return v

# From drafts/generate_function.py
class APIResponse(BaseModel):
        name: str
        purpose: str
        endpoints: List[Endpoint]

# From drafts/generate_function.py
class URLSelection(BaseModel):
        selected_urls: List[str] = Field(default_factory=list)

# From drafts/generate_function.py
class ExtractionInfo(BaseModel):
        relevant_info: str
        additional_urls: List[str] = Field(default_factory=list)
        requires_more_info: bool

# From drafts/generate_function.py
class GeneratedFunction(BaseModel):
        name: str
        code: str
        metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
        imports: Optional[List[Dict[str, str]]] = Field(default_factory=list)
        dependencies: List[str] = Field(default_factory=list)
        key_dependencies: List[str] = Field(default_factory=list)
        triggers: List[str] = Field(default_factory=list)

        class Config:
            extra = "forbid"

# From drafts/generate_function.py
def fetch_existing_functions(description: str) -> dict:
    """
    Fetches existing functions and returns them along with the initial intermediate_steps.

    Args:
        description (str): User description of the function to generate.

    Returns:
        dict: A dictionary containing existing functions and intermediate steps.
    """
    intermediate_steps = []
    try:
        existing_functions = display_functions_wrapper()
        print(f"[DEBUG] Existing Functions: {existing_functions}")
        intermediate_steps.append({"step": "Fetch Existing Functions", "content": existing_functions})
        return {"existing_functions": existing_functions, "intermediate_steps": intermediate_steps}
    except Exception as e:
        print(f"[ERROR] Failed to fetch existing functions: {e}")
        intermediate_steps.append({"step": "Error Fetching Existing Functions", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error fetching existing functions."}

# From drafts/generate_function.py
def analyze_internal_functions(description: str, existing_functions: str, intermediate_steps: list) -> dict:
    """
    Analyzes existing functions to identify reusable and reference functions.

    Args:
        description (str): User description of the function to generate.
        existing_functions (str): Existing functions obtained from the previous step.
        intermediate_steps (list): List of intermediate steps.

    Returns:
        dict: A dictionary containing updated intermediate steps, reusable_functions, and reference_functions.
    """
    from litellm import completion
    from pydantic import BaseModel, Field, ValidationError
    from typing import List
    import json

    # Define Pydantic model for parsing internal function responses
    class FunctionSuggestion(BaseModel):
        reusable_functions: List[str] = Field(default_factory=list)
        reference_functions: List[str] = Field(default_factory=list)

    # System prompt for code generation adhering to the functionz framework guidelines.
    system_prompt = """
    You are an AI designed to help developers write Python functions using the functionz framework. Every function you generate must adhere to the following rules:

    Function Registration: All functions must be registered with the functionz framework using the @babyagi.register_function() decorator. Each function can include metadata, dependencies, imports, and key dependencies.

    Basic Function Registration Example:

    def function_name(param1, param2):
        # function logic here
        return result

    Metadata and Dependencies: When writing functions, you may include optional metadata (such as descriptions) and dependencies. Dependencies can be other functions or secrets (API keys, etc.).

    Import Handling: Manage imports by specifying them in the decorator as dictionaries with 'name' and 'lib' keys. Include these imports within the function body.

    Secret Management: When using API keys or authentication secrets, reference the stored key with globals()['key_name'].

    Error Handling: Functions should handle errors gracefully, catching exceptions if necessary.

    General Guidelines: Use simple, clean, and readable code. Follow the structure and syntax of the functionz framework. Ensure proper function documentation via metadata.
    """

    display_prompt = f"""You are an assistant helping a developer build a function using the functionz framework.

    The user has provided the following function description: {description}

    The current available functions are listed below. Please specify if any of these functions can be used directly (for reuse), or if any should be referenced while building the new function. Return your response as structured JSON.

    Available Functions:
    {existing_functions}
    """

    # Step 2.1: Make the LLM call using JSON mode with Pydantic model
    try:
        display_response = completion(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": display_prompt}
            ],
            response_format=FunctionSuggestion
        )
        print(f"[DEBUG] Display Response: {display_response}")
    except Exception as e:
        print(f"[ERROR] LLM call for FunctionSuggestion failed: {e}")
        intermediate_steps.append({"step": "Error in FunctionSuggestion LLM Call", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error during FunctionSuggestion analysis."}

    # Step 2.2: Access and parse the response
    try:
        content = display_response.choices[0].message.content
        print(f"[DEBUG] Raw Display Content: {content}")
        display_response_parsed = FunctionSuggestion.parse_raw(content)
        print(f"[DEBUG] Parsed FunctionSuggestion: {display_response_parsed}")
        intermediate_steps.append({"step": "Analyze Internal Functions", "content": display_response_parsed.dict()})
    except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
        print(f"[ERROR] Parsing FunctionSuggestion response failed: {e}")
        intermediate_steps.append({"step": "Error Parsing FunctionSuggestion Response", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error parsing FunctionSuggestion response."}

    reusable_functions = display_response_parsed.reusable_functions
    reference_functions = display_response_parsed.reference_functions
    print(f"[DEBUG] Reusable Functions: {reusable_functions}")
    print(f"[DEBUG] Reference Functions: {reference_functions}")

    return {
        "intermediate_steps": intermediate_steps,
        "reusable_functions": reusable_functions,
        "reference_functions": reference_functions
    }

# From drafts/generate_function.py
def fetch_function_codes(function_names, intermediate_steps):
    """
    Fetches function codes for given function names.

    Args:
        function_names (List[str]): List of function names to fetch.
        intermediate_steps (list): List of intermediate steps.

    Returns:
        dict: A dictionary containing updated intermediate steps and function_codes.
    """
    from typing import List
    try:
        function_codes = {
            func_name: get_function_wrapper(func_name).get("code", "")
            for func_name in function_names
        }
        print(f"[DEBUG] Function Codes: {function_codes}")
        intermediate_steps.append({"step": "Fetch Function Codes", "content": function_codes})
        return {
            "intermediate_steps": intermediate_steps,
            "function_codes": function_codes
        }
    except Exception as e:
        print(f"[ERROR] Fetching function codes failed: {e}")
        intermediate_steps.append({"step": "Error Fetching Function Codes", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error fetching function codes."}

# From drafts/generate_function.py
def determine_required_external_apis(description: str, intermediate_steps: list) -> dict:
    """
    Determines required external APIs based on the user's function description.

    Args:
        description (str): User description of the function to generate.
        intermediate_steps (list): List of intermediate steps.

    Returns:
        dict: A dictionary containing updated intermediate steps and external_apis as a list of dictionaries.
    """
    from litellm import completion
    from pydantic import BaseModel, Field, ValidationError, validator
    from typing import List, Optional, Union
    import json

    # Define Pydantic models
    class Endpoint(BaseModel):
        method: Optional[str]
        url: str
        description: Optional[str] = None

    class APIDetails(BaseModel):
        api_name: str = Field(alias="name")  # Use alias to map 'name' to 'api_name'
        purpose: str
        endpoints: Optional[List[Union[Endpoint, str]]] = Field(default_factory=list)

        @validator("endpoints", pre=True, each_item=True)
        def convert_to_endpoint(cls, v):
            """Convert string URLs into Endpoint objects if necessary."""
            if isinstance(v, str):
                return Endpoint(url=v)  # Create an Endpoint object from a URL string
            return v

    class APIResponse(BaseModel):
        name: str
        purpose: str
        endpoints: List[Endpoint]

    # System prompt
    system_prompt = """
    [Your existing system prompt here]
    """

    prompt_for_apis = f"""You are an assistant analyzing function requirements.

    The user has provided the following function description: {description}.

    Identify if this function will require external APIs (including SDKs or libraries). If so, return a structured JSON with a list of external APIs, their purposes, and any relevant endpoints."""

    try:
        api_response = completion(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_for_apis}
            ],
            response_format=APIDetails
        )
        print(f"[DEBUG] API Response: {api_response}")
    except Exception as e:
        print(f"[ERROR] LLM call for APIResponse failed: {e}")
        intermediate_steps.append({"step": "Error in APIResponse LLM Call", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error during APIResponse analysis."}

    # Step 3.2: Access and parse the API response
    try:
        content = api_response.choices[0].message.content
        print(f"[DEBUG] Raw API Content: {content}")
        api_response_parsed = APIResponse.parse_raw(content)
        print(f"[DEBUG] Parsed APIResponse: {api_response_parsed}")
        intermediate_steps.append({"step": "Identify External API", "content": api_response_parsed.dict()})

        # Ensure external_apis is always a list
        external_apis = [api_response_parsed.dict()]  # Wrap in a list
    except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
        print(f"[ERROR] Parsing APIResponse failed: {e}")
        intermediate_steps.append({"step": "Error Parsing APIResponse", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error parsing APIResponse."}

    return {
        "intermediate_steps": intermediate_steps,
        "external_apis": external_apis  # Now a list of dicts
    }

# From drafts/generate_function.py
def handle_api_documentation(api_name: str, description: str, intermediate_steps: list) -> dict:
    """
    Searches API documentation for a given API and extracts relevant information.

    Args:
        api_name (str): Name of the API to search for.
        description (str): User description of the function to generate.
        intermediate_steps (list): List of intermediate steps.

    Returns:
        dict: A dictionary containing updated intermediate steps and api_contexts.
    """
    from litellm import completion
    from pydantic import BaseModel, Field, ValidationError
    from typing import List
    import json
    from urllib.parse import urlparse

    # Define Pydantic models
    class URLSelection(BaseModel):
        selected_urls: List[str] = Field(default_factory=list)

    # Updated ExtractionInfo model with 'requires_more_info'
    class ExtractionInfo(BaseModel):
        relevant_info: str
        additional_urls: List[str] = Field(default_factory=list)
        requires_more_info: bool

    # System prompt
    system_prompt = """
    You are an AI designed to help developers write Python functions using the functionz framework. Every function you generate must adhere to the following rules:

    Function Registration: All functions must be registered with the functionz framework using the @babyagi.register_function() decorator. Each function can include metadata, dependencies, imports, and key dependencies.

    Basic Function Registration Example:

    def function_name(param1, param2):
        # function logic here
        return result

    Metadata and Dependencies: When writing functions, you may include optional metadata (such as descriptions) and dependencies. Dependencies can be other functions or secrets (API keys, etc.).

    Import Handling: Manage imports by specifying them in the decorator as dictionaries with 'name' and 'lib' keys. Include these imports within the function body.

    Secret Management: When using API keys or authentication secrets, reference the stored key with globals()['key_name'].

    Error Handling: Functions should handle errors gracefully, catching exceptions if necessary.

    General Guidelines: Use simple, clean, and readable code. Follow the structure and syntax of the functionz framework. Ensure proper function documentation via metadata.
    """

    # Function to check if a URL is valid
    def is_valid_url(url: str) -> bool:
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except ValueError:
            return False

    # Function to chunk text
    def chunk_text(text: str, chunk_size: int = 100000, overlap: int = 10000) -> List[str]:
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            if end < len(text):
                # Find the last newline within the overlap
                last_newline = text.rfind('\n', end - overlap, end)
                if last_newline != -1:
                    end = last_newline + 1
            chunks.append(text[start:end])
            start = end - overlap
        return chunks

    search_query = f"{api_name} API documentation python"
    print(f"[DEBUG] Searching for API documentation with query: {search_query}")
    try:
        search_results = serpapi_search_v2(query=search_query)
        print(f"[DEBUG] Search Results for {api_name}: {search_results}")
        intermediate_steps.append({"step": f"Search API Documentation for {api_name}", "content": search_results})
    except Exception as e:
        print(f"[ERROR] serpapi_search_v2 failed for {api_name}: {e}")
        intermediate_steps.append({"step": f"Error Searching API Documentation for {api_name}", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error searching API documentation."}

    link_selection_prompt = f"""You are given the following search results for the query "{search_query}":
    {json.dumps(search_results)}

    Which links seem most relevant for obtaining Python API documentation? Return them as a structured JSON list of URLs."""

    try:
        link_selection_response = completion(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": link_selection_prompt}
            ],
            response_format=URLSelection
        )
        print(f"[DEBUG] Link Selection Response for {api_name}: {link_selection_response}")
    except Exception as e:
        print(f"[ERROR] LLM call for URLSelection failed for {api_name}: {e}")
        intermediate_steps.append({"step": f"Error in URLSelection LLM Call for {api_name}", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error in URLSelection LLM call."}

    # Step 4.2: Access and parse the link selection response
    try:
        content = link_selection_response.choices[0].message.content
        print(f"[DEBUG] Raw Link Selection Content for {api_name}: {content}")
        link_selection_parsed = URLSelection.parse_raw(content)
        print(f"[DEBUG] Parsed URLSelection for {api_name}: {link_selection_parsed}")
        intermediate_steps.append({"step": f"Select Relevant URLs for {api_name}", "content": link_selection_parsed.dict()})
    except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
        print(f"[ERROR] Parsing URLSelection response for {api_name} failed: {e}")
        intermediate_steps.append({"step": f"Error Parsing URLSelection Response for {api_name}", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error parsing URLSelection response."}

    selected_urls = link_selection_parsed.selected_urls or []
    print(f"[DEBUG] Selected URLs for {api_name}: {selected_urls}")
    scraped_urls = set()
    api_scrape_info = {}
    api_contexts = []
    accumulated_info = ""  # To accumulate relevant info

    requires_more_info = True  # Initialize to True to start the loop

    # Step 5: Scrape and recursively explore additional URLs until no more info is needed
    while selected_urls and requires_more_info:
        current_url = selected_urls.pop(0)
        print(f"[DEBUG] Scraping URL: {current_url}")
        if current_url in scraped_urls or not is_valid_url(current_url):
            print(f"[DEBUG] URL already scraped or invalid: {current_url}")
            continue

        try:
            scrape_result = scrape_website(current_url)
            print(f"[DEBUG] Scrape Result for {current_url}: {scrape_result}")
        except Exception as e:
            print(f"[ERROR] scrape_website failed for {current_url}: {e}")
            intermediate_steps.append({"step": f"Error Scraping URL: {current_url}", "content": str(e)})
            continue  # Skip to the next URL

        scraped_urls.add(current_url)
        if not scrape_result.get("error"):
            api_scrape_info[current_url] = scrape_result
            intermediate_steps.append({"step": f"Scrape URL: {current_url}", "content": scrape_result})
        else:
            print(f"[WARN] Error in scrape_result for {current_url}: {scrape_result.get('error')}")
            intermediate_steps.append({"step": f"Scrape Error for URL: {current_url}", "content": scrape_result.get("error")})
            continue  # Skip to the next URL

        # Step 6: Use LLM to extract relevant info and decide if more info is needed
        extraction_prompt = f"""The user wants to create a function described as follows: {description}.
You have accumulated the following relevant API information so far:
{accumulated_info}

You have just scraped the following new API documentation:
{json.dumps(scrape_result)}

Based on the new information, extract any additional relevant API methods, endpoints, and usage patterns needed to implement the user's function. Indicate whether more information is required by setting 'requires_more_info' to true or false. If any other URLs should be scraped for further information, include them in the 'additional_urls' field."""

        # Chunk the extraction prompt if it's too long
        extraction_prompt_chunks = chunk_text(extraction_prompt)
        extraction_results = []

        for chunk in extraction_prompt_chunks:
            try:
                extraction_response = completion(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": chunk}
                    ],
                    response_format=ExtractionInfo
                )
                print(f"[DEBUG] Extraction Response: {extraction_response}")
                extraction_results.append(extraction_response)
            except Exception as e:
                print(f"[ERROR] LLM call for ExtractionInfo failed: {e}")
                intermediate_steps.append({"step": "Error in ExtractionInfo LLM Call", "content": str(e)})
                continue  # Skip to the next chunk

        # Combine extraction results
        combined_extraction = {
            "relevant_info": "",
            "additional_urls": [],
            "requires_more_info": False
        }
        for result in extraction_results:
            try:
                content = result.choices[0].message.content
                parsed_result = ExtractionInfo.parse_raw(content)
                combined_extraction["relevant_info"] += parsed_result.relevant_info + "\n"
                combined_extraction["additional_urls"].extend(parsed_result.additional_urls)
                if parsed_result.requires_more_info:
                    combined_extraction["requires_more_info"] = True
            except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
                print(f"[ERROR] Parsing ExtractionInfo response failed: {e}")
                intermediate_steps.append({"step": "Error Parsing ExtractionInfo Response", "content": str(e)})

        # Update accumulated info
        accumulated_info += combined_extraction["relevant_info"]
        print(f"[DEBUG] Updated Accumulated Info: {accumulated_info}")

        # Include extracted info in API contexts
        api_contexts.append(combined_extraction["relevant_info"])
        print(f"[DEBUG] Updated API Contexts: {api_contexts}")

        # Queue additional URLs for scraping
        new_urls = [url for url in combined_extraction["additional_urls"] if url not in scraped_urls and is_valid_url(url)]
        print(f"[DEBUG] New URLs to Scrape: {new_urls}")
        selected_urls.extend(new_urls)

        # Check if more information is required
        requires_more_info = combined_extraction["requires_more_info"]
        print(f"[DEBUG] Requires More Info: {requires_more_info}")

    return {
        "intermediate_steps": intermediate_steps,
        "api_contexts": api_contexts
    }

# From drafts/generate_function.py
def generate_final_function_code(description: str, reusable_function_code: dict, reference_function_code: dict, api_contexts: list, intermediate_steps: list) -> dict:
    """
    Generates the final function code using all gathered information.

    Args:
        description (str): User description of the function to generate.
        reusable_function_code (dict): Codes of reusable functions.
        reference_function_code (dict): Codes of reference functions.
        api_contexts (list): List of API contexts.
        intermediate_steps (list): List of intermediate steps.

    Returns:
        dict: A dictionary containing updated intermediate steps and the combined final function details.
    """
    from litellm import completion
    from pydantic import BaseModel, Field, ValidationError
    from typing import Dict, Any, List, Optional
    import json

    # Define Pydantic model
    class GeneratedFunction(BaseModel):
        name: str
        code: str
        metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
        imports: Optional[List[Dict[str, str]]] = Field(default_factory=list)
        dependencies: List[str] = Field(default_factory=list)
        key_dependencies: List[str] = Field(default_factory=list)
        triggers: List[str] = Field(default_factory=list)

        class Config:
            extra = "forbid"

    # System prompt
    system_prompt = """
    You are an AI designed to help developers write Python functions using the functionz framework. Every function you generate must adhere to the following rules:

    Function Registration: All functions must be registered with the functionz framework using the @babyagi.register_function() decorator. Each function can include metadata, dependencies, imports, and key dependencies.

    Basic Function Registration Example:

    def function_name(param1, param2):
        # function logic here
        return result

    Metadata and Dependencies: When writing functions, you may include optional metadata (such as descriptions) and dependencies. Dependencies can be other functions or secrets (API keys, etc.).

    Import Handling: Manage imports by specifying them in the decorator as dictionaries with 'name' and 'lib' keys. Include these imports within the function body.

    Secret Management: When using API keys or authentication secrets, reference the stored key with globals()['key_name'].

    Error Handling: Functions should handle errors gracefully, catching exceptions if necessary.

    General Guidelines: Use simple, clean, and readable code. Follow the structure and syntax of the functionz framework. Ensure proper function documentation via metadata.
    """

    # Function to chunk text
    def chunk_text(text: str, chunk_size: int = 100000, overlap: int = 10000) -> List[str]:
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            if end < len(text):
                # Find the last newline within the overlap
                last_newline = text.rfind('\n', end - overlap, end)
                if last_newline != -1:
                    end = last_newline + 1
            chunks.append(text[start:end])
            start = end - overlap
        return chunks

    final_prompt = f"""{system_prompt}

    The user wants to create a function with the following description: {description}.

    You have the following internal reusable functions:
    {json.dumps(reusable_function_code)}

    You have the following internal reference functions:
    {json.dumps(reference_function_code)}

    You have the following context on the necessary external APIs and their usage:
    {json.dumps(api_contexts)}

    Generate a complete function using the functionz framework that adheres to the provided guidelines and utilizes the specified internal and external functions. Ensure the function is registered with the correct metadata, dependencies, and includes all relevant imports.

    Provide the function details in a structured format including:
    1. Function name
    2. Complete function code (do not the @babyagi.register_function decorator)
    3. Metadata (description)
    4. Imports
    5. Dependencies
    6. Key dependencies
    7. Triggers (if any)
    """

    # Chunk the final prompt if it's too long
    final_prompt_chunks = chunk_text(final_prompt)
    final_results = []

    for chunk in final_prompt_chunks:
        try:
            final_response = completion(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": chunk}
                ],
                response_format=GeneratedFunction
            )
            print(f"[DEBUG] Final Response: {final_response}")
            final_results.append(final_response)
        except Exception as e:
            print(f"[ERROR] LLM call for GeneratedFunction failed: {e}")
            intermediate_steps.append({"step": "Error in GeneratedFunction LLM Call", "content": str(e)})
            return {"intermediate_steps": intermediate_steps, "error": "# Error during GeneratedFunction generation."}

    # Combine final results
    combined_final = {
        "name": "",
        "code": "",
        "metadata": {},
        "imports": [],
        "dependencies": [],
        "key_dependencies": [],
        "triggers": []
    }
    for result in final_results:
        try:
            content = result.choices[0].message.content
            parsed_result = GeneratedFunction.parse_raw(content)
            if not combined_final["name"]:
                combined_final["name"] = parsed_result.name
            combined_final["code"] += parsed_result.code + "\n"
            if parsed_result.metadata:
                combined_final["metadata"].update(parsed_result.metadata or {})
            if parsed_result.imports:
                combined_final["imports"].extend(parsed_result.imports)
            if parsed_result.dependencies:
                combined_final["dependencies"].extend(parsed_result.dependencies)
            if parsed_result.key_dependencies:
                combined_final["key_dependencies"].extend(parsed_result.key_dependencies)
            if parsed_result.triggers:
                combined_final["triggers"].extend(parsed_result.triggers)

        except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
            print(f"[ERROR] Parsing GeneratedFunction response failed: {e}")
            intermediate_steps.append({"step": "Error Parsing GeneratedFunction Response", "content": str(e)})
            return {"intermediate_steps": intermediate_steps, "error": "# Error parsing GeneratedFunction response."}

    # Remove duplicates from lists
    combined_final["imports"] = list({json.dumps(imp): imp for imp in combined_final["imports"]}.values())
    combined_final["dependencies"] = list(set(combined_final["dependencies"]))
    combined_final["key_dependencies"] = list(set(combined_final["key_dependencies"]))
    combined_final["triggers"] = list(set(combined_final["triggers"]))

    print(f"[DEBUG] Combined Final GeneratedFunction: {combined_final}")
    intermediate_steps.append({"step": "Generate Final Function", "content": combined_final})

    return {
        "intermediate_steps": intermediate_steps,
        "combined_final": combined_final
    }

# From drafts/generate_function.py
def add_function_to_database(combined_final: dict, intermediate_steps: list) -> dict:
    """
    Adds the generated function to the database.

    Args:
        combined_final (dict): The combined final function details.
        intermediate_steps (list): List of intermediate steps.

    Returns:
        dict: A dictionary containing updated intermediate steps and the success status.
    """
    try:
        success = add_new_function(
            name=combined_final["name"],
            code=combined_final["code"],
            metadata=combined_final["metadata"],
            imports=combined_final["imports"],
            dependencies=combined_final["dependencies"],
            key_dependencies=combined_final["key_dependencies"],
            triggers=combined_final["triggers"]
        )
        intermediate_steps.append({"step": "Add Function to Database", "content": {"success": success}})
    except Exception as e:
        print(f"[ERROR] Failed to add function to database: {e}")
        intermediate_steps.append({"step": "Error Adding Function to Database", "content": str(e)})
        success = False

    return {
        "intermediate_steps": intermediate_steps,
        "success": success
    }

# From drafts/generate_function.py
def generate_function_from_description(description: str) -> dict:
    """
    Main function that generates a Python function based on a user-provided description.

    Args:
        description (str): User description of the function to generate.

    Returns:
        dict: A dictionary containing intermediate steps, the final generated function code, and whether it was successfully added to the database.
    """
    intermediate_steps = []

    # Step 1: Fetch existing functions
    result = fetch_existing_functions(description)
    if "error" in result:
        return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
    existing_functions = result["existing_functions"]
    intermediate_steps.extend(result["intermediate_steps"])

    # Step 2: Analyze internal functions
    result = analyze_internal_functions(description, existing_functions, intermediate_steps)
    if "error" in result:
        return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
    intermediate_steps = result["intermediate_steps"]
    reusable_functions = result["reusable_functions"]
    reference_functions = result["reference_functions"]

    # Step 3: Fetch function codes for reusable and reference functions
    reusable_function_code = {}
    if reusable_functions:
        result = fetch_function_codes(reusable_functions, intermediate_steps)
        if "error" in result:
            return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
        intermediate_steps = result["intermediate_steps"]
        reusable_function_code = result["function_codes"]

    reference_function_code = {}
    if reference_functions:
        result = fetch_function_codes(reference_functions, intermediate_steps)
        if "error" in result:
            return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
        intermediate_steps = result["intermediate_steps"]
        reference_function_code = result["function_codes"]

    # Step 4: Determine required external APIs
    result = determine_required_external_apis(description, intermediate_steps)
    if "error" in result:
        return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
    intermediate_steps = result["intermediate_steps"]
    external_apis_dicts = result["external_apis"]

    # Ensure external_apis_dicts is a list
    if not isinstance(external_apis_dicts, list):
        external_apis_dicts = [external_apis_dicts]

    # Reconstruct APIResponse objects from dicts
    from typing import Optional, List
    from pydantic import BaseModel

    class Endpoint(BaseModel):
        method: Optional[str]
        url: str
        description: Optional[str] = None

    class APIResponse(BaseModel):
        name: str
        purpose: str
        endpoints: List[Endpoint]

    external_apis = []
    for api_dict in external_apis_dicts:
        api_response_parsed = APIResponse(**api_dict)
        external_apis.append(api_response_parsed)

    # Step 5: Handle API documentation and extract contexts
    api_contexts = []
    for api_response_parsed in external_apis:
        api_name = api_response_parsed.name
        result = handle_api_documentation(api_name, description, intermediate_steps)
        if "error" in result:
            return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
        intermediate_steps = result["intermediate_steps"]
        api_contexts.extend(result["api_contexts"])

    # Step 6: Generate final function code
    result = generate_final_function_code(description, reusable_function_code, reference_function_code, api_contexts, intermediate_steps)
    if "error" in result:
        return {"intermediate_steps": result["intermediate_steps"], "final_code": result["error"], "added_to_database": False}
    intermediate_steps = result["intermediate_steps"]
    combined_final = result["combined_final"]

    # Step 7: Add function to database
    result = add_function_to_database(combined_final, intermediate_steps)
    intermediate_steps = result["intermediate_steps"]
    success = result["success"]

    return {
        "intermediate_steps": intermediate_steps,
        "final_code": combined_final["code"],
        "added_to_database": success
    }

# From drafts/generate_function.py
def is_valid_url(url: str) -> bool:
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except ValueError:
            return False

# From drafts/generate_function.py
def chunk_text(text: str, chunk_size: int = 100000, overlap: int = 10000) -> List[str]:
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            if end < len(text):
                # Find the last newline within the overlap
                last_newline = text.rfind('\n', end - overlap, end)
                if last_newline != -1:
                    end = last_newline + 1
            chunks.append(text[start:end])
            start = end - overlap
        return chunks

# From drafts/generate_function.py
def convert_to_endpoint(cls, v):
            """Convert string URLs into Endpoint objects if necessary."""
            if isinstance(v, str):
                return Endpoint(url=v)  # Create an Endpoint object from a URL string
            return v


# From drafts/self_build2.py
class QueryGenerationResponse(BaseModel):
        queries: List[str] = Field(..., description="A list of generated synthetic queries.")

# From drafts/self_build2.py
def generate_and_process_queries(user_description: str, num_queries: int) -> dict:
    """
    Generates X synthetic queries based on the user description and processes each query
    using the choose_or_create_function.

    Args:
        user_description (str): The user's description to base synthetic queries on.
        num_queries (int): The number of synthetic queries to generate.

    Returns:
        dict: A dictionary containing the results of each query execution, intermediate steps, and any relevant information.
    """
    from litellm import completion
    from pydantic import BaseModel, Field, ValidationError
    from typing import List, Dict, Any
    import json

    intermediate_steps = []
    results = []

    # Step 1: Generate synthetic queries based on the user description
    system_prompt = """
You are an AI assistant specialized in generating relevant and distinct queries based on a given description.

Given a user description, generate a specified number of unique and diverse queries that a user might ask an AI assistant.
Ensure that the queries are varied and cover different aspects of the description.

Return your response in the following JSON format:

{
    "queries": [
        "First synthetic query.",
        "Second synthetic query.",
        ...
    ]
}

Provide only the JSON response, without any additional text.
"""

    class QueryGenerationResponse(BaseModel):
        queries: List[str] = Field(..., description="A list of generated synthetic queries.")

    generation_prompt = f"""
User Description:
\"\"\"{user_description}\"\"\"

Number of Queries to Generate:
{num_queries}
"""

    try:
        generation_response = completion(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": generation_prompt}
            ],
            response_format=QueryGenerationResponse
        )
        print(f"[DEBUG] Generation Response: {generation_response}")
    except Exception as e:
        print(f"[ERROR] LLM call for query generation failed: {e}")
        intermediate_steps.append({"step": "Error in Query Generation LLM Call", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error generating synthetic queries."}

    # Parse the response
    try:
        content = generation_response.choices[0].message.content
        print(f"[DEBUG] Raw Generation Content: {content}")
        generation_parsed = QueryGenerationResponse.parse_raw(content)
        print(f"[DEBUG] Parsed Query Generation: {generation_parsed}")
        intermediate_steps.append({"step": "Generate Synthetic Queries", "content": generation_parsed.dict()})
    except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
        print(f"[ERROR] Parsing query generation response failed: {e}")
        intermediate_steps.append({"step": "Error Parsing Query Generation Response", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error parsing synthetic queries."}

    synthetic_queries = generation_parsed.queries

    if not synthetic_queries or len(synthetic_queries) != num_queries:
        print(f"[ERROR] Number of generated queries does not match the requested number.")
        intermediate_steps.append({
            "step": "Query Count Mismatch",
            "content": f"Requested: {num_queries}, Generated: {len(synthetic_queries)}"
        })
        return {
            "intermediate_steps": intermediate_steps,
            "error": "# The number of generated queries does not match the requested number."
        }

    # Step 2: Process each synthetic query using choose_or_create_function
    for idx, query in enumerate(synthetic_queries, start=1):
        intermediate_steps.append({"step": f"Processing Query {idx}", "content": query})
        try:
            # Assuming choose_or_create_function is accessible within the scope
            # If it's in a different module, you might need to import it accordingly
            query_result = choose_or_create_function(query)
            results.append({
                "query": query,
                "result": query_result.get("execution_result"),
                "intermediate_steps": query_result.get("intermediate_steps", [])
            })
            intermediate_steps.append({
                "step": f"Executed Query {idx}",
                "content": query_result.get("execution_result")
            })
        except Exception as e:
            print(f"[ERROR] Processing query {idx} failed: {e}")
            intermediate_steps.append({
                "step": f"Error Processing Query {idx}",
                "content": str(e)
            })
            results.append({
                "query": query,
                "error": f"# Error processing query: {e}"
            })

    return {
        "intermediate_steps": intermediate_steps,
        "results": results
    }


# From drafts/choose_or_create_function.py
class FunctionDecision(BaseModel):
        use_existing_function: bool = Field(..., description="True if an existing function can be used; False if a new function needs to be generated.")
        function_name: Optional[str] = Field(None, description="Name of the existing function to use.")
        function_description: Optional[str] = Field(None, description="Description of the new function to generate.")

# From drafts/choose_or_create_function.py
class FunctionParameters(BaseModel):
            parameters: Dict[str, Any]

            class Config:
                extra = 'forbid'

# From drafts/choose_or_create_function.py
def choose_or_create_function(user_input: str) -> dict:
    """
    Takes user input, compares against existing functions, decides whether to use an existing function or generate a new one, then executes the function with generated parameters.

    Args:
        user_input (str): The user's input or request.

    Returns:
        dict: A dictionary containing the result of the function execution, intermediate steps, and any relevant information.
    """
    from litellm import completion
    from pydantic import BaseModel, Field, ValidationError
    from typing import List, Optional, Dict, Any
    import json

    intermediate_steps = []

    # Step 1: Fetch existing functions
    try:
        existing_functions = display_functions_wrapper()
        print(f"[DEBUG] Existing Functions: {existing_functions}")
        intermediate_steps.append({"step": "Fetch Existing Functions", "content": existing_functions})
    except Exception as e:
        print(f"[ERROR] Failed to fetch existing functions: {e}")
        intermediate_steps.append({"step": "Error Fetching Existing Functions", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error fetching existing functions."}

    # Step 2: Use LLM to decide whether to use an existing function or generate a new one
    system_prompt = """
You are an assistant that helps decide whether an existing function can fulfill a user's request or if a new function needs to be created.

Please analyze the user's input and the list of available functions.

Return your decision in the following JSON format:

{
    "use_existing_function": true or false,
    "function_name": "name of the existing function" (if applicable),
    "function_description": "description of the function to generate" (if applicable)
}

Provide only the JSON response, without any additional text.
"""

    class FunctionDecision(BaseModel):
        use_existing_function: bool = Field(..., description="True if an existing function can be used; False if a new function needs to be generated.")
        function_name: Optional[str] = Field(None, description="Name of the existing function to use.")
        function_description: Optional[str] = Field(None, description="Description of the new function to generate.")

    decision_prompt = f"""
The user has provided the following input:
\"{user_input}\"

Available Functions:
{existing_functions}
"""

    try:
        decision_response = completion(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": decision_prompt}
            ],
            response_format=FunctionDecision
        )
        print(f"[DEBUG] Decision Response: {decision_response}")
    except Exception as e:
        print(f"[ERROR] LLM call for FunctionDecision failed: {e}")
        intermediate_steps.append({"step": "Error in FunctionDecision LLM Call", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error during function decision analysis."}

    # Parse the response
    try:
        content = decision_response.choices[0].message.content
        print(f"[DEBUG] Raw Decision Content: {content}")
        decision_parsed = FunctionDecision.parse_raw(content)
        print(f"[DEBUG] Parsed FunctionDecision: {decision_parsed}")
        intermediate_steps.append({"step": "Function Decision", "content": decision_parsed.dict()})
    except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
        print(f"[ERROR] Parsing FunctionDecision response failed: {e}")
        intermediate_steps.append({"step": "Error Parsing FunctionDecision Response", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error parsing FunctionDecision response."}

    if decision_parsed.use_existing_function and decision_parsed.function_name:
        function_name = decision_parsed.function_name
        print(f"[INFO] Using existing function: {function_name}")
    elif not decision_parsed.use_existing_function and decision_parsed.function_description:
        # Generate the new function
        print(f"[INFO] Generating new function based on description.")
        gen_result = generate_function_from_description(decision_parsed.function_description)
        intermediate_steps.extend(gen_result.get("intermediate_steps", []))
        if not gen_result.get("added_to_database"):
            print(f"[ERROR] Failed to generate and add new function.")
            return {"intermediate_steps": intermediate_steps, "error": "# Error generating new function."}
        # Get the function name from the generated function code
        function_name = gen_result.get("function_name")
        if not function_name:
            # Extract function name from the code
            import re
            match = re.search(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', gen_result.get("final_code", ""))
            if match:
                function_name = match.group(1)
                print(f"[INFO] Extracted function name: {function_name}")
            else:
                print(f"[ERROR] Function name not found in generated code.")
                return {"intermediate_steps": intermediate_steps, "error": "# Function name not found in generated code."}
    else:
        print(f"[ERROR] Invalid decision or missing information.")
        return {"intermediate_steps": intermediate_steps, "error": "# Invalid function decision."}

    # Step 3: Get the function code using get_function_wrapper
    try:
        function_info = get_function_wrapper(function_name)
        if not function_info:
            print(f"[ERROR] Function {function_name} not found.")
            intermediate_steps.append({"step": "Error Fetching Function", "content": f"Function {function_name} not found."})
            return {"intermediate_steps": intermediate_steps, "error": f"# Function {function_name} not found."}
        print(f"[DEBUG] Function Info: {function_info}")
        intermediate_steps.append({"step": "Fetch Function Info", "content": function_info})
    except Exception as e:
        print(f"[ERROR] Fetching function info failed: {e}")
        intermediate_steps.append({"step": "Error Fetching Function Info", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error fetching function info."}

    # Step 4: Use LLM to generate parameters for the function based on user input
    param_prompt = f"""
    The user has provided the following input:
    \"{user_input}\"

    The function to execute is:
    {function_info.get('code', '')}

    Generate a JSON object with a single key "parameters" that contains the parameters required by the function, filled in appropriately based on the user's input.

    Return only the JSON object, with no additional text.
    """

    try:
        # Define a Pydantic model with a fixed field "parameters"
        class FunctionParameters(BaseModel):
            parameters: Dict[str, Any]

            class Config:
                extra = 'forbid'  # This sets 'additionalProperties' to False

        param_response = completion(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an assistant that provides only JSON-formatted data, with no additional text."},
                {"role": "user", "content": param_prompt}
            ],
            response_format=FunctionParameters  # Keep the same parsing format
        )
        print(f"[DEBUG] Parameter Response: {param_response}")
    except Exception as e:
        print(f"[ERROR] LLM call for parameter generation failed: {e}")
        intermediate_steps.append({"step": "Error in Parameter Generation LLM Call", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error generating parameters."}

    # Parse the response using the Pydantic model
    try:
        content = param_response.choices[0].message.content
        print(f"[DEBUG] Raw Parameter Content: {content}")
        function_params_model = FunctionParameters.parse_raw(content)
        function_params = function_params_model.parameters  # Extract the parameters dictionary
        print(f"[DEBUG] Parsed Parameters: {function_params}")
        intermediate_steps.append({"step": "Generate Function Parameters", "content": function_params})
    except (ValidationError, IndexError, AttributeError, json.JSONDecodeError) as e:
        print(f"[ERROR] Parsing parameters failed: {e}")
        intermediate_steps.append({"step": "Error Parsing Parameters", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error parsing function parameters."}

    # Step 5: Execute the function using execute_function_wrapper
    try:
        # Ensure that function_params is a dictionary
        if not isinstance(function_params, dict):
            raise TypeError("function_params must be a dictionary")
        execution_result = execute_function_wrapper(function_name, **function_params)
        print(f"[DEBUG] Execution Result: {execution_result}")
        intermediate_steps.append({"step": "Execute Function", "content": execution_result})
    except Exception as e:
        print(f"[ERROR] Function execution failed: {e}")
        intermediate_steps.append({"step": "Error Executing Function", "content": str(e)})
        return {"intermediate_steps": intermediate_steps, "error": "# Error executing function."}

    return {
        "intermediate_steps": intermediate_steps,
        "execution_result": execution_result
    }


# From drafts/code_writing_functions.py
def check_existing_functions(user_input):
  import json

  while True:
      # Get all functions and their descriptions
      functions = get_all_functions_wrapper()
      function_descriptions = [
          {"name": f['name'], "description": f['metadata'].get('description', '')}
          for f in functions
      ]

      # Prepare the prompt
      prompt = f"""
You are an expert software assistant. The user has provided the following request:

"{user_input}"

Below is a list of available functions with their descriptions:

{function_descriptions}

Determine if any of the existing functions perfectly fulfill the user's request. If so, return the name of the function.

Provide your answer in the following JSON format:
{{
  "function_found": true or false,
  "function_name": "<name of the function if found, else null>"
}}

Examples:

Example 1:
User input: "Calculate the sum of two numbers"
Functions: [{{"name": "add_numbers", "description": "Adds two numbers"}}]
Response:
{{
  "function_found": true,
  "function_name": "add_numbers"
}}

Example 2:
User input: "Translate text to French"
Functions: [{{"name": "add_numbers", "description": "Adds two numbers"}}]
Response:
{{
  "function_found": false,
  "function_name": null
}}

Now, analyze the user's request and provide the JSON response.
"""

      response = gpt_call(prompt)

      # Try to parse the JSON response
      try:
          result = json.loads(response)
          if 'function_found' in result and isinstance(result['function_found'], bool) and \
             ('function_name' in result):
              return result
          else:
              raise ValueError("Invalid JSON structure")
      except Exception as e:
          # If parsing fails, retry
          continue

# From drafts/code_writing_functions.py
def break_down_task(user_input):
  import json
  while True:
      # Prepare the prompt with detailed context
      prompt = f"""
You are an expert software assistant helping to break down a user's request into smaller functions for a microservice-inspired architecture. The system is designed to be modular, with each function being small and designed optimally for potential future reuse.

When breaking down the task, consider the following:

- Each function should be as small as possible and do one thing well.
- Use existing functions where possible. You have access to functions such as 'gpt_call', 'find_similar_function', and others in our function database.
- Functions can depend on each other. Use 'dependencies' to specify which functions a function relies on.
- Functions should include appropriate 'imports' if external libraries are needed.
- Provide the breakdown as a list of functions, where each function includes its 'name', 'description', 'input_parameters', 'output_parameters', 'dependencies', and 'code' (just a placeholder or brief description at this stage).
- Make sure descriptions are detailed so an engineer could build it to spec.
- Every sub function you create should be designed to be reusable by turning things into parameters, vs hardcoding them.

User request:

"{user_input}"

Provide your answer in JSON format as a list of functions. Each function should have the following structure:

{{
  "name": "function_name",
  "description": "Brief description of the function",
  "input_parameters": [{{"name": "param1", "type": "type1"}}, ...],
  "output_parameters": [{{"name": "output", "type": "type"}}, ...],
  "dependencies": ["dependency1", "dependency2", ...],
  "imports": ["import1", "import2", ...],
  "code": "Placeholder or brief description"
}}

Example:

[
  {{
      "name": "process_data",
      "description": "Processes input data",
      "input_parameters": [{{"name": "data", "type": "str"}}],
      "output_parameters": [{{"name": "processed_data", "type": "str"}}],
      "dependencies": [],
      "imports": [],
      "code": "Placeholder for process_data function"
  }},
  {{
      "name": "analyze_data",
      "description": "Analyzes processed data",
      "input_parameters": [{{"name": "processed_data", "type": "str"}}],
      "output_parameters": [{{"name": "analysis_result", "type": "str"}}],
      "dependencies": ["process_data"],
      "imports": [],
      "code": "Placeholder for analyze_data function"
  }}
]

Now, provide the breakdown for the user's request.
"""

      response = gpt_call(prompt)

      # Try to parse the JSON response
      try:
          functions = json.loads(response)
          # Basic validation of the structure
          if isinstance(functions, list) and all('name' in func and 'description' in func for func in functions):
              return functions
          else:
              raise ValueError("Invalid JSON structure")
      except Exception as e:
          # If parsing fails, retry
          continue

# From drafts/code_writing_functions.py
def decide_imports_and_apis(context):
  import json
  while True:
      # Get all available functions and their imports
      all_functions = get_all_functions_wrapper()
      existing_imports = set()
      for func in all_functions:
          existing_imports.update(func.get('imports', []))

      # Prepare the prompt
      prompt = f"""
You are an expert software assistant helping to decide what imports and external APIs are needed for a set of functions based on the context provided.

Context:

{context}

Existing standard Python imports:

{list(existing_imports)}

Determine the libraries (imports) and external APIs needed for these functions. Separate standard Python libraries from external libraries or APIs.

Provide your answer in the following JSON format:

{{
  "standard_imports": ["import1", "import2", ...],
  "external_imports": ["external_import1", "external_import2", ...],
  "external_apis": ["api1", "api2", ...],
  "documentation_needed": [
      {{"name": "external_import1", "type": "import" or "api"}},
      ...
  ]
}}

Note: 'documentation_needed' should include any external imports or APIs for which documentation should be looked up.

Example:

{{
  "standard_imports": ["os", "json"],
  "external_imports": ["requests"],
  "external_apis": ["SerpAPI"],
  "documentation_needed": [
      {{"name": "requests", "type": "import"}},
      {{"name": "SerpAPI", "type": "api"}}
  ]
}}

Now, analyze the context and provide the JSON response.
"""

      response = gpt_call(prompt)

      # Try to parse the JSON response
      try:
          result = json.loads(response)
          # Basic validation of the structure
          if all(key in result for key in ['standard_imports', 'external_imports', 'external_apis', 'documentation_needed']):
              return result
          else:
              raise ValueError("Invalid JSON structure")
      except Exception as e:
          # If parsing fails, retry
          continue

# From drafts/code_writing_functions.py
def get_functions_that_depend_on(function_name):
  all_functions = get_all_functions_wrapper()
  dependent_functions = []
  for function in all_functions:
      if function_name in function.get('dependencies', []):
          dependent_functions.append(function['name'])
  return dependent_functions

# From drafts/code_writing_functions.py
def generate_function_code(function, context):
    while True:

        print("\033[1;32mGenerating code for function: ", function["name"], "\033[0m")
        # Gather dependent functions and their code
        dependencies = function.get('dependencies', [])
        dependency_code = ''
        for dep in dependencies:
            dep_function = get_function_wrapper(dep)
            if dep_function:
                dependency_code += f"\n# Code for dependency function '{dep}':\n{dep_function['code']}\n"

        # Gather functions that depend on the same imports
        imports = function.get('imports', [])
        functions_with_same_imports = []
        all_functions = get_all_functions_wrapper()
        for func_with_imports in all_functions:
            if set(func_with_imports.get('imports', [])) & set(imports):
                functions_with_same_imports.append(func_with_imports)

        similar_imports_functions_code = ''
        for func_with_imports in functions_with_same_imports:
            similar_imports_functions_code += f"\n# Code for function '{func_with_imports['name']}' that uses similar imports:\n{func_with_imports['code']}\n"

        # Prepare the prompt
        prompt = f"""
You are an expert Python programmer. Your task is to write detailed and working code for the following function based on the context provided. Do not provide placeholder code, but rather do your best like you are the best senior engineer in the world and provide the best code possible. DO NOT PROVIDE PLACEHOLDER CODE.

Function details:

Name: {function['name']}
Description: {function['description']}
Input parameters: {function['input_parameters']}
Output parameters: {function['output_parameters']}
Dependencies: {function['dependencies']}
Imports: {function['imports']}

Overall context:

{context}

Dependency code:

{dependency_code}

Code from functions with similar imports:

{similar_imports_functions_code}

Please provide the function details in JSON format, following this structure:

{{
  "function_name": "<function_name>",
  "metadata": {{
    "description": "<function_description>",
    "input_parameters": {function['input_parameters']},
    "output_parameters": {function['output_parameters']}
  }},
  "code": "<function_code_as_string>",
  "imports": {function['imports']},
  "dependencies": {function['dependencies']},
  "key_dependencies": [],
  "triggers": []
}}

**Example JSON Output:**

{{
  "function_name": "example_function",
  "metadata": {{
    "description": "An example function.",
    "input_parameters": [{{"name": "param1", "type": "str"}}],
    "output_parameters": [{{"name": "result", "type": "str"}}]
  }},
  "code": "<complete function code goes here>",
  "imports": ["os"],
  "dependencies": [],
  "key_dependencies": [],
  "triggers": []
}}

Provide the JSON output only, without any additional text. Do not provide placeholder code, but write complete code that is ready to run and provide the expected output.

Now, please provide the JSON output for the function '{function['name']}'.
"""

        response = gpt_call(prompt)

        try:
            # Parse the JSON response
            import json
            function_data = json.loads(response)

            # Return the parsed function data
            return function_data
        except json.JSONDecodeError as e:
            # If parsing fails, retry
            print(f"JSON decoding error: {str(e)}")
            continue
        except Exception as e:
            print(f"Error processing function data: {str(e)}")
            return None

# From drafts/code_writing_functions.py
def create_function(function, context):
    # Decide imports and APIs
    imports_and_apis = decide_imports_and_apis(context)
    function['imports'] = imports_and_apis.get('standard_imports', []) + imports_and_apis.get('external_imports', [])

    # Update context with imports and APIs
    context.update({'imports_and_apis': imports_and_apis})

    # Generate function code
    function_data = generate_function_code(function, context)

    if function_data:
        # Register the function using the parsed JSON data
        add_new_function(
            name=function_data['function_name'],
            code=function_data['code'],
            metadata=function_data['metadata'],
            imports=function_data.get('imports', []),
            dependencies=function_data.get('dependencies', []),
            key_dependencies=function_data.get('key_dependencies', []),
            triggers=function_data.get('triggers', [])
        )

        #print(f"Function '{function_data['function_name']}' registered successfully.")

        return {
            'name': function_data['function_name'],
            'code': function_data['code'],
            'metadata': function_data['metadata'],
            'imports': function_data.get('imports', []),
            'dependencies': function_data.get('dependencies', []),
            'key_dependencies': function_data.get('key_dependencies', []),
            'triggers': function_data.get('triggers', [])
        }
    else:
        print("Failed to generate function code.")
        return None

# From drafts/code_writing_functions.py
def generate_functions(function_breakdown, context):
  for function in function_breakdown:
      function_name = function['name']
      # Find similar functions
      similar_functions = find_similar_function(function['description'])
      function_found = False
      for similar_function_name in similar_functions:
          similar_function = get_function_wrapper(similar_function_name)
          if similar_function and similar_function['metadata'].get('description', '') == function['description']:
              function_found = True
              break
      if not function_found:
          # Combine context for this function
          function_context = context.copy()
          function_context.update({'function': function})
          create_function(function, function_context)

# From drafts/code_writing_functions.py
def run_final_function(function_name, *args, **kwargs):
  result = func.execute_function(function_name, *args, **kwargs)
  return result

# From drafts/code_writing_functions.py
def extract_function_parameters(user_input, function_name):
    import json
    # Get the function code and parameters
    function = get_function_wrapper(function_name)
    if not function:
        print(f"Function '{function_name}' not found.")
        return None

    # Prepare the prompt to convert user input into function parameters
    while True:
        prompt = f"""
You are an expert assistant. The user wants to execute the following function:

Function code:
{function['code']}

Function description:
{function['metadata'].get('description', '')}

Function parameters:
{function['metadata'].get('input_parameters', [])}

The user has provided the following input:
"{user_input}"

Your task is to extract the required parameters from the user's input and provide them in JSON format that matches the function's parameters.

Provide your answer in the following JSON format:
{{
  "parameters": {{
    "param1": value1,
    "param2": value2,
    ...
  }}
}}

Ensure that the parameters match the function's required input parameters.

Examples:

Example 1:

Function code:
def add_numbers(a, b):
    return a + b

Function parameters:
[{{"name": "a", "type": "int"}}, {{"name": "b", "type": "int"}}]

User input: "Add 5 and 3"

Response:
{{
  "parameters": {{
    "a": 5,
    "b": 3
  }}
}}

Example 2:

Function code:
def greet_user(name):
    return f"Hello, {{name}}!"

Function parameters:
[{{"name": "name", "type": "str"}}]

User input: "Say hello to Alice"

Response:
{{
  "parameters": {{
    "name": "Alice"
  }}
}}

Now, using the function provided and the user's input, extract the parameters and provide the JSON response.
"""
        response = gpt_call(prompt)

        # Try to parse the JSON response
        try:
            result = json.loads(response)
            if 'parameters' in result and isinstance(result['parameters'], dict):
                return result['parameters']
            else:
                raise ValueError("Invalid JSON structure")
        except Exception as e:
            # If parsing fails, retry
            continue

# From drafts/code_writing_functions.py
def process_user_input(user_input):
    # First, check if an existing function satisfies the user input
    print("\033[1;95mProcessing user input: ", user_input, "\033[0m")
    result = check_existing_functions(user_input)
    if result['function_found']:
        function_name = result['function_name']
    else:
        # Break down the task into functions
        function_breakdown = break_down_task(user_input)
        # Context to be passed around
        context = {'user_input': user_input, 'function_breakdown': function_breakdown}
        # Generate the required functions
        generate_functions(function_breakdown, context)
        # Assume the main function is the first one in the breakdown
        function_name = function_breakdown[0]['name']

    # Extract parameters from user input for the function
    parameters = extract_function_parameters(user_input, function_name)
    if parameters is None:
        print("Failed to extract parameters from user input.")
        return None

    # Call the function with the parameters
    output = run_final_function(function_name, **parameters)
    return output


# From default/os.py
def get_full_directory_contents_cleaned():
    current_directory = os.getcwd()  # Get current working directory
    directory_structure = {}

    # Walk through the directory and its subdirectories
    for root, dirs, files in os.walk(current_directory):
        # Filter out hidden directories, '__pycache__', and hidden files
        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']
        files = [f for f in files if not f.startswith('.')]

        # Remove the current directory path from the root
        relative_root = root.replace(current_directory, "").lstrip(os.sep)
        directory_structure[relative_root] = {
            "folders": dirs,
            "files": files
        }

    return {"current_directory": current_directory, "directory_structure": directory_structure}

from babyagi.functionz.core.framework import func

# From default/default_functions.py
def execute_function_wrapper(function_name: str, *args, **kwargs):
    # Create an initial log for the wrapper
    wrapper_log_id = func.db.add_log(
        function_name="execute_function_wrapper",
        message="Wrapper execution started.",
        timestamp=datetime.now(),
        params={"function_name": function_name, "args": args, "kwargs": kwargs},
        output=None,
        time_spent=0,
        parent_log_id=None,
        triggered_by_log_id=None,
        log_type='started'
    )
    # Execute the function with the wrapper's log ID as the parent ID
    result = func.execute_function(function_name, *args, parent_log_id=wrapper_log_id, **kwargs)

    # Update the wrapper log after execution
    func.db.update_log(wrapper_log_id, output=result, log_type='success', message="Wrapper execution completed.")
    return result

# From default/default_functions.py
def add_new_function(
    name: str,
    code: str,
    metadata: dict = None,
    imports: list = None,
    dependencies: list = None,
    key_dependencies: list = None,
    triggers: list = None
) -> bool:

    try:
        func.registrar.add_function(
            name=name,
            code=code,
            metadata=metadata,
            imports=imports,
            dependencies=dependencies,
            key_dependencies=key_dependencies,
            triggers=triggers
        )
        #print(f"Function '{name}' added successfully.")
        return True
    except Exception as e:
        print(f"Error adding function '{name}': {str(e)}")
        return False

# From default/default_functions.py
def function_added_or_updated(action=None, triggered_function_name=None):
    """
    Triggered whenever a function is added or updated.
    """
    print(f"Function '{triggered_function_name}' has been {action}.")
    function_data = func.get_function(triggered_function_name) if triggered_function_name else None
    if function_data:
        return triggered_function_name
    else:
        print(f"Function '{triggered_function_name}' not found in the database.")
        return None

# From default/default_functions.py
def add_key_wrapper(key_name: str, key_value: str):
    return func.add_key(key_name, key_value)

# From default/default_functions.py
def get_function_versions_wrapper(name: str):
    return func.get_function_versions(name)

# From default/default_functions.py
def get_function_wrapper(name: str):
    return func.get_function(name)

# From default/default_functions.py
def get_all_functions_wrapper():
    return func.get_all_functions()

# From default/default_functions.py
def activate_function_version_wrapper(name: str, version: int):
    return func.activate_function_version(name, version)

# From default/default_functions.py
def display_functions_wrapper():
    return func.display()

# From default/default_functions.py
def get_logs_wrapper(function_name: str = None, start_date: datetime = None,
                 end_date: datetime = None):
    return func.get_logs(function_name, start_date, end_date)

# From default/default_functions.py
def add_trigger_wrapper(triggered_function_name: str, triggering_function_name: Optional[str] = None):
    return func.add_trigger(triggered_function_name, triggering_function_name)

# From default/default_functions.py
def get_all_imports_wrapper():
    return func.get_all_imports()

import litellm

# From default/function_calling_chat.py
def chat_with_functions(chat_history, available_function_names) -> str:
    def map_python_type_to_json(python_type: str) -> dict:
        """
        Maps Python type annotations to JSON Schema types.

        Args:
            python_type (str): The Python type as a string.

        Returns:
            dict: The corresponding JSON Schema type with additional details if necessary.
        """
        type_mapping = {
            "str": {"type": "string"},
            "int": {"type": "integer"},
            "float": {"type": "number"},
            "bool": {"type": "boolean"},
            "list": {"type": "array", "items": {"type": "string"}},  # Assuming list of strings
            "dict": {"type": "object"},
            "Any": {"type": "string"}  # Default to string for unsupported types
        }
        return type_mapping.get(python_type, {"type": "string"})

    # Enable verbose logging for LiteLLM
    litellm.set_verbose = True

    # Initialize chat context with system message
    chat_context = [
        {"role": "system", "content": "You are a helpful assistant."}
    ]

    # Validate and append chat history
    if not isinstance(chat_history, list):
        raise ValueError("chat_history must be a list of messages.")

    for message in chat_history:
        if not isinstance(message, dict):
            raise ValueError("Each message in chat_history must be a dictionary.")
        role = message.get('role')
        content = message.get('message')
        if role not in ['user', 'assistant', 'system']:
            raise ValueError("Message role must be 'user', 'assistant', or 'system'.")
        if not isinstance(content, str):
            raise ValueError("Message content must be a string.")
        chat_context.append({"role": role, "content": content})

    # Handle available_function_names input
    if isinstance(available_function_names, str):
        # Split the string by commas and strip whitespace
        available_function_names = [name.strip() for name in available_function_names.split(',') if name.strip()]
    elif isinstance(available_function_names, list):
        # Ensure all elements are strings and strip whitespace
        available_function_names = [name.strip() for name in available_function_names if isinstance(name, str) and name.strip()]
    else:
        raise ValueError("available_function_names must be a string or a list of strings.")

    if not available_function_names:
        raise ValueError("No valid function names provided in available_function_names.")

    # Fetch available functions from the database
    tools = []
    for func_name in available_function_names:
        # Retrieve function details using the get_function_wrapper
        function_data = get_function_wrapper(func_name)
        if function_data:
            # Construct the tool definition for LiteLLM
            tool = {
                "type": "function",
                "function": {
                    "name": function_data['name'],
                    "description": function_data['metadata']['description'],
                    "parameters": {
                        "type": "object",
                        "properties": {},
                        "required": []
                    },
                },
            }

            # Map input_parameters to the tool's parameters
            for param in function_data.get('input_parameters', []):
                # Convert Python types to JSON Schema types
                json_schema = map_python_type_to_json(param['type'])
                tool['function']['parameters']['properties'][param['name']] = {
                    **json_schema,
                    "description": param.get('description', '')
                }
                if param.get('required', False):
                    tool['function']['parameters']['required'].append(param['name'])

            tools.append(tool)
        else:
            # Handle the case where the function is not found
            raise ValueError(f"Function '{func_name}' not found in the database.")


    # Call LiteLLM's completion API with the user message and available tools
    response = litellm.completion(
        model="gpt-4-turbo",
        messages=chat_context,
        tools=tools,
        tool_choice="auto"
    )

    # Extract the message from the response
    response_message = response['choices'][0]['message']

    # Check if the model wants to call any functions
    tool_calls = response_message.get('tool_calls', [])

    # If there are function calls, execute them
    if tool_calls:
        # Append the assistant's message to the chat context
        chat_context.append(response_message)

        for tool_call in tool_calls:
            function_name = tool_call['function']['name']
            function_args = json.loads(tool_call['function']['arguments'])
            tool_call_id = tool_call['id']  # Extract the tool_call_id

            # Execute the function using execute_function_wrapper
            try:
                function_response = execute_function_wrapper(function_name, **function_args)
            except Exception as e:
                function_response = f"Error executing function '{function_name}': {str(e)}"

            # Ensure function_response is a string
            if not isinstance(function_response, str):
                function_response = json.dumps(function_response)

            # Append the function response to the chat context
            chat_context.append({
                "tool_call_id": tool_call_id,  # Include the tool_call_id
                "role": "tool",  # Use 'tool' as per LiteLLM's protocol
                "name": function_name,
                "content": function_response
            })

        # Call LiteLLM again with the updated context including function responses
        second_response = litellm.completion(
            model="gpt-4-turbo",
            messages=chat_context
        )

        # Extract and return the assistant's final response
        assistant_response = second_response['choices'][0]['message']['content']
        return assistant_response
    else:
        # If no functions are called, return the assistant's message directly
        assistant_response = response_message.get('content', '')
        return assistant_response

# From default/function_calling_chat.py
def map_python_type_to_json(python_type: str) -> dict:
        """
        Maps Python type annotations to JSON Schema types.

        Args:
            python_type (str): The Python type as a string.

        Returns:
            dict: The corresponding JSON Schema type with additional details if necessary.
        """
        type_mapping = {
            "str": {"type": "string"},
            "int": {"type": "integer"},
            "float": {"type": "number"},
            "bool": {"type": "boolean"},
            "list": {"type": "array", "items": {"type": "string"}},  # Assuming list of strings
            "dict": {"type": "object"},
            "Any": {"type": "string"}  # Default to string for unsupported types
        }
        return type_mapping.get(python_type, {"type": "string"})

from litellm import embedding
from sklearn.metrics.pairwise import cosine_similarity

# From default/ai_functions.py
def gpt_call(prompt: str) -> str:
    from litellm import completion
    messages = [{"role": "user", "content": prompt}]
    response = completion(model="gpt-4o", messages=messages)
    return response['choices'][0]['message']['content']

# From default/ai_functions.py
def description_writer(function_code: str) -> str:
    prompt = (
        f"Provide a concise and clear description for the following Python function:\n\n"
        f"{function_code}\n\n"
        f"Description:"
    )
    description = func.gpt_call(prompt)
    return description

# From default/ai_functions.py
def ai_description_generator(function_name: str) -> None:
    print(f"Generating AI description for function: {function_name}")
    function = func.db.get_function(function_name)
    if not function:
        print(f"Function '{function_name}' not found in the database.")
        return

    description = function.get('metadata', {}).get('description', '').strip()
    function_code = function.get('code', '')

    if not description and function_code.strip():
        #print(f"Generating description for function '{function_name}'.")
        generated_description = func.description_writer(function_code)
        func.update_function(
            name=function_name,
            metadata={"description": generated_description}
        )
        print(f"Description for function '{function_name}' has been generated and updated.")
        return f"Description for function '{function_name}' has been generated and updated."
    elif not function_code.strip():
        print(f"Function '{function_name}' has no code to generate a description.")
        return f"Function '{function_name}' has no code to generate a description."
    else:
        print(f"Function '{function_name}' already has a non-empty description.")
        return f"Function '{function_name}' already has a non-empty description."

# From default/ai_functions.py
def generate_missing_descriptions() -> None:
    all_functions = func.db.get_all_functions()
    missing_description_functions = [
        func_info['name'] for func_info in all_functions
        if not func_info.get('metadata', {}).get('description')
    ]
    if not missing_description_functions:
        print("All functions already have descriptions.")
        return
    print(f"Found {len(missing_description_functions)} function(s) without descriptions. Generating descriptions...")
    for function_name in missing_description_functions:
        func.ai_description_generator(function_name)
    print("Description generation process completed.")

# From default/ai_functions.py
def embed_input(input_text: str, model: str = "text-embedding-ada-002", 
                encoding_format: str = "float", dimensions: int = None, 
                timeout: int = 600) -> list:
    from litellm import embedding
    import os

    # Set OpenAI API Key from environment variables
    os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')

    # Prepare the embedding request with optional parameters
    embedding_params = {
        "model": model,
        "input": [input_text],
        "encoding_format": encoding_format,
        "timeout": timeout
    }

    if dimensions:
        embedding_params["dimensions"] = dimensions

    # Call the LiteLLM embedding function
    response = embedding(**embedding_params)

    # Return the embedding from the response
    return response['data'][0]['embedding']

# From default/ai_functions.py
def embed_function_description(function: str) -> None:
    print(f"Embedding description for function: {function}")
    # Retrieve the function details from the database
    function_data = func.db.get_function(function)
    if not function_data:
        print(f"Function '{function}' not found in the database.")
        return

    description = function_data.get('metadata', {}).get('description', '').strip()
    if description:
        print(f"Embedding description for function '{function}'.")
        embedding = func.embed_input(description)

        # Check if 'function_embeddings.csv' exists, create it if not
        file_path = 'function_embeddings.csv'
        file_exists = os.path.isfile(file_path)

        # Create a list to store CSV data
        rows = []

        if file_exists:
            with open(file_path, mode='r') as file:
                reader = csv.reader(file)
                rows = list(reader)

        # Look for the function in the existing rows
        function_found = False
        for i, row in enumerate(rows):
            if row[0] == function:  # function column is the first column
                rows[i][1] = str(embedding)  # Update the embedding
                function_found = True
                print(f"Updated embedding for function '{function}'.")

        if not function_found:
            # Add a new row if the function is not found
            rows.append([function, str(embedding)])
            print(f"Added new function '{function}' with its embedding.")

        # Write back the data to the CSV file (create or update)
        with open(file_path, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerows(rows)

        print(f"Embedding for function '{function}' has been saved.")
        return embedding
    else:
        print(f"Function '{function}' has no description to embed.")
        return f"Function '{function}' has no description to embed."

# From default/ai_functions.py
def find_similar_function(description: str, top_n: int = 3):
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    # Step 1: Embed the input description
    #print(f"Embedding input description: {description}")
    input_embedding = func.embed_input(description)

    # Step 2: Load stored embeddings and descriptions from CSV
    file_path = 'function_embeddings.csv'
    stored_embeddings = []
    stored_functions = []

    if not os.path.isfile(file_path):
        print(f"No embeddings found in {file_path}.")
        return []

    with open(file_path, mode='r') as file:
        reader = csv.reader(file)
        for row in reader:
            stored_functions.append(row[0])
            stored_embeddings.append(np.fromstring(row[1].strip("[]"), sep=','))

    if not stored_embeddings:
        print("No embeddings stored.")
        return []

    # Step 3: Calculate cosine similarity between the input embedding and stored embeddings
    similarities = cosine_similarity([input_embedding], stored_embeddings)

    # Step 4: Sort stored functions by similarity
    sorted_indices = np.argsort(similarities[0])[::-1]  # Sort in descending order

    # Step 5: Return the top N most similar functions
    similar_functions = [stored_functions[i] for i in sorted_indices[:top_n]]

    print(f"Top {top_n} similar functions: {similar_functions}")
    return similar_functions

# From default/ai_functions.py
def generate_missing_embeddings() -> None:
    # Step 1: Retrieve all functions from the database
    all_functions = func.db.get_all_functions()
    all_function_names = [func_info['name'] for func_info in all_functions]

    # Step 2: Check if 'function_embeddings.csv' exists
    file_path = 'function_embeddings.csv'
    file_exists = os.path.isfile(file_path)

    # Read existing embeddings from CSV if the file exists
    embedded_functions = []
    if file_exists:
        with open(file_path, mode='r') as file:
            reader = csv.reader(file)
            embedded_functions = [row[0] for row in reader]  # First column is the function name

    # Step 3: Find functions without embeddings
    missing_embeddings = [func_name for func_name in all_function_names if func_name not in embedded_functions]

    if not missing_embeddings:
        print("All functions already have embeddings.")
        return

    print(f"Found {len(missing_embeddings)} function(s) without embeddings. Generating embeddings...")

    # Step 4: Embed the functions that are missing
    for function_name in missing_embeddings:
        print(f"Embedding function: {function_name}")
        func.embed_function_description(function_name)

    print("Embedding generation process completed.")

# From default/ai_functions.py
def choose_function(prompt: str) -> str:
    functions = func.get_all_functions()
    prompt = (
        f"Which functions are most relevant to the following input? It could be ones to use or look at as reference to build a new one:\n\n"
        f"{prompt}\n\n"
        f"Functions:{functions}"
    )
    choice = func.gpt_call(prompt)
    return {"functions":functions,"choice":choice}

from firecrawl import FirecrawlApp

# From plugins/firecrawl.py
def crawl_website(url: str, limit: int = 100, formats: list = ["markdown", "html"], poll_interval: int = 30):
    """
    Submits a crawl job for the given URL and returns the crawl job status and job ID.
    """
    from firecrawl import FirecrawlApp
    api_key = globals()['firecrawl_api_key']
    app = FirecrawlApp(api_key=api_key)

    try:
        crawl_status = app.crawl_url(
            url, 
            params={'limit': limit, 'scrapeOptions': {'formats': formats}},
            poll_interval=poll_interval
        )
        return crawl_status
    except Exception as e:
        return {"error": str(e)}

# From plugins/firecrawl.py
def check_crawl_status(crawl_id: str):
    """
    Checks the status of the crawl job and returns the job details including markdown and HTML data.
    """
    from firecrawl import FirecrawlApp
    api_key = globals()['firecrawl_api_key']
    app = FirecrawlApp(api_key=api_key)

    try:
        crawl_status = app.check_crawl_status(crawl_id)
        return crawl_status
    except Exception as e:
        return {"error": str(e)}

# From plugins/firecrawl.py
def scrape_website(url: str, formats: list = ["markdown", "html"]):
    """
    Scrapes the given URL and returns the data (markdown, HTML, and metadata).
    """
    from firecrawl import FirecrawlApp
    api_key = globals()['firecrawl_api_key']
    app = FirecrawlApp(api_key=api_key)

    try:
        scrape_result = app.scrape_url(url, params={'formats': formats})
        return scrape_result
    except Exception as e:
        return {"error": str(e)}


# From plugins/voilanorbert.py
def search_contact_by_name_domain(name, domain):
  """
  Searches for a contact by name and domain using the VoilaNorbert API.

  Args:
      name (str): Full name of the person to search.
      domain (str): Domain of the company the person works for.

  Returns:
      dict: The contact information if found, otherwise an appropriate message.
  """
  api_key = globals().get('voilanorbert_api_key')
  if not api_key:
      return {"error": "API key not found"}

  # Prepare the API request
  search_url = 'https://api.voilanorbert.com/2018-01-08/search/name'
  auth = ('any_string', api_key)
  data = {'name': name, 'domain': domain}

  try:
      # POST request to initiate search
      response = requests.post(search_url, auth=auth, data=data)
      if response.status_code == 402:
          return {"error": "No credits available for this search"}
      elif response.status_code != 200:
          return {"error": f"Failed to search contact: {response.json()}"}

      result = response.json()
      contact_id = result.get('id')

      # Polling to check if the email is found
      contact_url = f'https://api.voilanorbert.com/2018-01-08/contacts/{contact_id}'
      while True:
          contact_response = requests.get(contact_url, auth=auth)
          if contact_response.status_code == 200:
              contact_data = contact_response.json()
              if not contact_data['searching']:
                  if contact_data['email']:
                      return {
                          "email": contact_data['email']['email'],
                          "score": contact_data['email']['score']
                      }
                  return {"message": "Email not found!"}
          time.sleep(10)

  except requests.RequestException as e:
      return {"error": str(e)}

# From plugins/voilanorbert.py
def search_contact_by_domain(domain):
  """
  Searches for contacts by domain using the VoilaNorbert API.

  Args:
      domain (str): The domain of the company to search for contacts.

  Returns:
      list: A list of found contacts with emails if available.
  """
  api_key = globals().get('voilanorbert_api_key')
  if not api_key:
      return {"error": "API key not found"}

  # Prepare the API request
  search_url = 'https://api.voilanorbert.com/2018-01-08/search/domain'
  auth = ('any_string', api_key)
  data = {'domain': domain}

  try:
      # POST request to initiate search
      response = requests.post(search_url, auth=auth, data=data)
      if response.status_code == 402:
          return {"error": "No credits available for this search"}
      elif response.status_code != 200:
          return {"error": f"Failed to search contacts: {response.json()}"}

      result = response.json()
      return result.get('result', [])

  except requests.RequestException as e:
      return {"error": str(e)}


# From plugins/harmonic.py
def harmonic_enrich_company(identifier):
    """
    Enrich a company using its URL, domain, or identifier.
    Returns the full response from Harmonic API.
    """
    api_key = globals()['harmonic_api_key']
    url = "https://api.harmonic.ai/companies"
    headers = {"accept": "application/json", "apikey": api_key}

    # Determine the appropriate parameter based on identifier type
    if identifier.startswith('http'):
        params = {"crunchbase_url": identifier} if 'crunchbase.com' in identifier else {"website_url": identifier}
    elif '.' in identifier and not identifier.startswith('http'):
        params = {"website_domain": identifier}
    else:
        url += f"/{identifier}"
        params = {}

    # Use POST if parameters are present, otherwise GET
    response = requests.post(url, headers=headers, params=params) if params else requests.get(url, headers=headers)
    response.raise_for_status()
    return response.json()

# From plugins/harmonic.py
def harmonic_search_companies(keywords, include_ids_only=False):
    """
    Search for companies using keywords.
    Returns a list of companies and their metadata.
    """
    api_key = globals()['harmonic_api_key']
    url = "https://api.harmonic.ai/search/companies_by_keywords"
    headers = {"accept": "application/json", "apikey": api_key, "Content-Type": "application/json"}

    data = {"keywords": keywords, "include_ids_only": include_ids_only}
    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()
    return response.json()

# From plugins/harmonic.py
def harmonic_enrich_person_by_id(person_id):
    """
    Retrieve detailed information about a person using their Harmonic ID.
    """
    api_key = globals()['harmonic_api_key']
    url = f"https://api.harmonic.ai/persons/{person_id}"
    headers = {"accept": "application/json", "apikey": api_key}

    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.json()


# From plugins/payman.py
def store_payman_api_keys():
    # Store test API key
    func.add_key('payman_test_api_key', os.environ['PAYMAN_TEST_API_KEY'])
    # Store real API key
    func.add_key('payman_api_key', os.environ['PAYMAN_API_KEY'])

# From plugins/payman.py
def create_task(title: str, description: str, payout: int, currency: str = "USD", category: str = "MARKETING", real_money: bool = False):
    if real_money:
        api_key = globals()['payman_api_key']
        base_url = "https://agent.payman.ai/api"
    else:
        api_key = globals()['payman_test_api_key']
        base_url = "https://agent-sandbox.payman.ai/api"

    headers = {
        "x-payman-api-secret": api_key,
        "Content-Type": "application/json",
        "Accept": "application/vnd.payman.v1+json"
    }
    payload = {
        "title": title,
        "description": description,
        "payout": payout,  # Payout in cents (e.g. 5000 for $50)
        "currency": currency,
        "category": category,
        "requiredSubmissions": 1,
        "submissionPolicy": "OPEN_SUBMISSIONS_ONE_PER_USER"
    }
    try:
        response = requests.post(f"{base_url}/tasks", headers=headers, json=payload)
        return response.json()
    except requests.exceptions.HTTPError as e:
        return {"error": str(e)}

# From plugins/payman.py
def get_task_by_id(task_id: str, real_money: bool = False):
    if real_money:
        api_key = globals()['payman_api_key']
        base_url = "https://agent.payman.ai/api"
    else:
        api_key = globals()['payman_test_api_key']
        base_url = "https://agent-sandbox.payman.ai/api"

    headers = {
        "x-payman-api-secret": api_key,
        "Content-Type": "application/json",
        "Accept": "application/vnd.payman.v1+json"
    }
    try:
        response = requests.get(f"{base_url}/tasks/{task_id}", headers=headers)
        return response.json()
    except requests.exceptions.HTTPError as e:
        return {"error": str(e)}

# From plugins/payman.py
def get_all_tasks(page: int = 0, limit: int = 20, real_money: bool = False):
    if real_money:
        api_key = globals()['payman_api_key']
        base_url = "https://agent.payman.ai/api"
    else:
        api_key = globals()['payman_test_api_key']
        base_url = "https://agent-sandbox.payman.ai/api"

    headers = {
        "x-payman-api-secret": api_key,
        "Content-Type": "application/json",
        "Accept": "application/vnd.payman.v1+json"
    }
    params = {
        "page": page,
        "limit": limit
    }
    try:
        response = requests.get(f"{base_url}/tasks", headers=headers, params=params)
        return response.json()
    except requests.exceptions.HTTPError as e:
        return {"error": str(e)}

# From plugins/payman.py
def get_task_submissions(task_id: str, statuses: list = None, page: int = 0, limit: int = 20, real_money: bool = False):
    if real_money:
        api_key = globals()['payman_api_key']
        base_url = "https://agent.payman.ai/api"
    else:
        api_key = globals()['payman_test_api_key']
        base_url = "https://agent-sandbox.payman.ai/api"

    headers = {
        "x-payman-api-secret": api_key,
        "Content-Type": "application/json",
        "Accept": "application/vnd.payman.v1+json"
    }

    params = {
        "page": page,
        "limit": limit
    }

    if statuses:
        params["statuses"] = ",".join(statuses)

    try:
        response = requests.get(f"{base_url}/tasks/{task_id}/submissions", headers=headers, params=params)
        return response.json()
    except requests.exceptions.HTTPError as e:
        return {"error": str(e)}

# From plugins/payman.py
def approve_task_submission(submission_id: str, real_money: bool = False):
    if real_money:
        api_key = globals()['payman_api_key']
        base_url = "https://agent.payman.ai/api"
    else:
        api_key = globals()['payman_test_api_key']
        base_url = "https://agent-sandbox.payman.ai/api"

    headers = {
        "x-payman-api-secret": api_key,
        "Content-Type": "application/json",
        "Accept": "application/vnd.payman.v1+json"
    }
    try:
        response = requests.post(f"{base_url}/tasks/submissions/{submission_id}/approve", headers=headers)
        return response.json()
    except requests.exceptions.HTTPError as e:
        return {"error": str(e)}

# From plugins/payman.py
def reject_task_submission(submission_id: str, rejection_reason: str, real_money: bool = False):
    if real_money:
        api_key = globals()['payman_api_key']
        base_url = "https://agent.payman.ai/api"
    else:
        api_key = globals()['payman_test_api_key']
        base_url = "https://agent-sandbox.payman.ai/api"

    headers = {
        "x-payman-api-secret": api_key,
        "Content-Type": "application/json",
        "Accept": "application/vnd.payman.v1+json"
    }
    try:
        response = requests.post(f"{base_url}/tasks/submissions/{submission_id}/reject", headers=headers, json=rejection_reason)
        return response.json()
    except requests.exceptions.HTTPError as e:
        return {"error": str(e)}


# From plugins/wokelo.py
def get_auth_token():
  """Obtain an authentication token using Wokelo API credentials stored as secrets."""
  import requests
  BASE_URL = 'https://api.wokelo.ai'
  url = BASE_URL + '/auth/token'

  headers = {"Content-Type": "application/x-www-form-urlencoded"}
  data = {
      "client_id": "B5I07FeItrqH5V8ytQKNwHPDHeMQnGBheg7A6FAg",
      "client_secret": "JkVEP6FZhTkolz9vwkFSFAMVKLO0r9CnYU2RlGcRSzxZGZSkdSbSCed30VHg55IWU94F3sh0fTGUy8dTGslQZmcpCGvPhEUs9w3uobWa4ftXvsahriFCReRIxEUdd2f8",
      "grant_type": "password",
      "username": globals()['wokelo_username'],
      "password": globals()['wokelo_password'],
  }

  response = requests.post(url, headers=headers, data=data)

  if response.status_code == 200:
      # Successfully obtained token
      token_info = response.json()
      return token_info.get("access_token")
  else:
      return {"error": f"Failed to obtain token. Status code: {response.status_code}", "details": response.text}

# From plugins/wokelo.py
def create_industry_snapshot(access_token: str, industry_name: str):
  """Initiate a new industry snapshot report."""
  import requests
  BASE_URL = 'https://api.wokelo.ai'
  url = f'{BASE_URL}/api/industry_primer/v3/start/'
  headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
  payload = {"industry": industry_name}

  response = requests.post(url, json=payload, headers=headers)
  if response.status_code == 200:
      return response.json().get('report_id')
  else:
      return {"error": "Failed to create industry snapshot.", "details": response.text}

# From plugins/wokelo.py
def check_report_status(access_token: str, report_id: str):
  """Check the status of a generated report in Wokelo."""
  import requests
  BASE_URL = 'https://api.wokelo.ai'
  url = f'{BASE_URL}/api/assets/get_report_status/?report_id={report_id}'
  headers = {'Authorization': f'Bearer {access_token}'}

  response = requests.get(url, headers=headers)
  if response.status_code == 200:
      return response.json()
  else:
      return {"error": "Failed to check report status.", "details": response.text}

# From plugins/wokelo.py
def download_report(access_token: str, report_id: str, file_type: str = 'pdf'):
  """Download a specific report in a given file format."""
  import requests
  BASE_URL = 'https://api.wokelo.ai'
  url = f'{BASE_URL}/api/assets/download_report/'
  headers = {'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json'}
  payload = {"file_type": file_type, "report_id": report_id}

  response = requests.post(url, json=payload, headers=headers)
  if response.status_code == 200:
      return response.content
  else:
      return {"error": "Failed to download report.", "details": response.text}

# From plugins/wokelo.py
def generate_industry_report(industry_name: str, file_type: str = 'pdf'):
  """
  Complete workflow for generating and downloading an industry report.

  Args:
      industry_name (str): The name of the industry to generate the report for.
      file_type (str): The format of the report file to be downloaded. Default is 'pdf'.

  Returns:
      str: File path of the downloaded report.
  """
  # Step 1: Get an authentication token.
  access_token = get_auth_token()
  if not isinstance(access_token, str):
      return access_token  # If there's an error, return the error message.

  # Step 2: Create an industry snapshot.
  report_id = create_industry_snapshot(access_token, industry_name)
  if not isinstance(report_id, str):
      return report_id  # If there's an error, return the error message.

  # Step 3: Check report status until it's ready.
  print(f"Initiated report creation. Waiting for the report (ID: {report_id}) to be exported.")
  while True:
      status_info = check_report_status(access_token, report_id)
      if 'status' in status_info and status_info['status'] == 'exported':
          print(f"Report is ready for download: {report_id}")
          break
      elif 'status' in status_info:
          print(f"Current report status: {status_info['status']}. Checking again in 30 seconds...")
      else:
          return status_info  # Error occurred.
      time.sleep(30)

  # Step 4: Download the report.
  report_content = download_report(access_token, report_id, file_type)
  if isinstance(report_content, dict) and 'error' in report_content:
      return report_content  # Return the error if download failed.

  # Step 5: Save the report locally.
  report_filename = f'report_{report_id}.{file_type}'
  with open(report_filename, 'wb') as report_file:
      report_file.write(report_content)

  return f"Report downloaded successfully: {report_filename}"

from functionz import func
from pyairtable import Table
from pyairtable.formulas import match

# From plugins/airtable.py
def init_airtable(base_id, table_name):
    """
    Initialize the Airtable Table instance.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :return: Airtable Table instance
    """
    api_token = globals()["airtable_access_token"]
    from pyairtable import Table
    return Table(api_token, base_id, table_name)

# From plugins/airtable.py
def create_record(base_id, table_name, record_data):
    """
    Create a new record in the specified Airtable table.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param record_data: Dictionary containing record fields and values
    :return: The newly created record
    """
    table = init_airtable(base_id, table_name)
    return table.create(record_data)

# From plugins/airtable.py
def get_record_by_id(base_id, table_name, record_id):
    """
    Retrieve a record by its unique ID from the specified Airtable table.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param record_id: Unique record ID in Airtable
    :return: The record data as a dictionary
    """
    table = init_airtable(base_id, table_name)
    return table.get(record_id)

# From plugins/airtable.py
def update_record(base_id, table_name, record_id, updated_fields):
    """
    Update a record in Airtable with new values for specified fields.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param record_id: Unique record ID in Airtable
    :param updated_fields: Dictionary with updated field values
    :return: The updated record data as a dictionary
    """
    table = init_airtable(base_id, table_name)
    return table.update(record_id, updated_fields)

# From plugins/airtable.py
def delete_record(base_id, table_name, record_id):
    """
    Delete a record from Airtable using its unique ID.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param record_id: Unique record ID in Airtable
    :return: Deletion confirmation message
    """
    table = init_airtable(base_id, table_name)
    return table.delete(record_id)

# From plugins/airtable.py
def get_all_records(base_id, table_name, max_records=100, sort_by=None):
    """
    Get all records from the specified table, with optional sorting.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param max_records: Maximum number of records to retrieve
    :param sort_by: Optional list of fields to sort the records by
    :return: List of all records
    """
    table = init_airtable(base_id, table_name)
    return table.all(max_records=max_records, sort=sort_by)

# From plugins/airtable.py
def batch_upsert_records(base_id, table_name, records, key_fields):
    """
    Upsert multiple records into the specified table.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param records: List of records to be upserted
    :param key_fields: List of fields to use as unique keys
    :return: List of created or updated records
    """
    table = init_airtable(base_id, table_name)
    return table.batch_upsert(records, key_fields=key_fields)

# From plugins/airtable.py
def batch_create_records(base_id, table_name, records):
    """
    Create multiple records in the specified table.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param records: List of records to be created
    :return: List of created records
    """
    table = init_airtable(base_id, table_name)
    return table.batch_create(records)

# From plugins/airtable.py
def batch_delete_records(base_id, table_name, record_ids):
    """
    Batch delete records using their unique IDs.
    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param record_ids: List of record IDs to be deleted
    :return: Confirmation messages for deleted records
    """
    table = init_airtable(base_id, table_name)
    return table.batch_delete(record_ids)

# From plugins/airtable.py
def get_dynamic_records(base_id, table_name, max_records=100, search_query=None, sort_by=None, fields=None, view=None, page_size=100):
    """
    Fetch a dynamic number of records from an Airtable table based on a custom query.

    :param base_id: ID of the Airtable base
    :param table_name: Name of the table within the base
    :param max_records: Maximum number of records to retrieve
    :param search_query: Dictionary of field-value pairs to match (e.g., {"Name": "Alice", "Age": 30})
    :param sort_by: List of fields to sort the records by (e.g., ["Name", "-Age"])
    :param fields: List of specific fields to retrieve (e.g., ["Name", "Age"])
    :param view: View ID or name to filter the records by
    :param page_size: Number of records per page request
    :return: List of matching records
    """
    from pyairtable.formulas import match
    table = init_airtable(base_id, table_name)

    # Construct a formula using the match function if search_query is provided
    formula = None
    if search_query:
        from pyairtable.formulas import match
        formula = match(search_query)

    # Use iterate to handle large datasets if max_records is set higher than page_size
    records = table.iterate(
        formula=formula,
        sort=sort_by,
        fields=fields,
        view=view,
        page_size=page_size,
        max_records=max_records
    )

    # Collect results from the generator into a list
    return list(records)

from e2b_code_interpreter import CodeInterpreter

# From plugins/e2b.py
def execute_code_in_sandbox(code: str):
    """
    This function initializes an E2B sandbox and executes AI-generated Python code within it.

    :param code: Python code to be executed.
    :return: Results and logs from the code execution.
    """
    from e2b_code_interpreter import CodeInterpreter
  
    with CodeInterpreter() as sandbox:
        # Execute the code in the sandbox
        execution = sandbox.notebook.exec_cell(code)

        # Handle execution errors
        if execution.error:
            return {"error": execution.error.name, "message": execution.error.value, "traceback": execution.error.traceback}

        # Gather results
        results = [{"text": result.text, "formats": result.formats()} for result in execution.results]
        logs = {"stdout": execution.logs.stdout, "stderr": execution.logs.stderr}

        return {"results": results, "logs": logs}

# From plugins/e2b.py
def chat_with_llm_and_execute(user_message: str):
    """
    This function calls the OpenAI API (via litellm) to generate Python code based on the user's message,
    then executes that code in an E2B sandbox.

    :param user_message: The message to prompt the LLM with.
    :return: Results from the executed code and logs.
    """
    from litellm import completion
  
    # Load OpenAI API key from environment
    api_key = globals()['openai_api_key']

    # Define the message for the LLM
    messages = [{"role": "user", "content": user_message}]

    # Call the LLM using litellm completion method
    response = completion(model="gpt-3.5-turbo", messages=messages)
    llm_generated_code = response['choices'][0]['message']['content']

    # Execute the generated code in the E2B sandbox
    return execute_code_in_sandbox(llm_generated_code)

# From plugins/e2b.py
def save_chart(base64_png: str, filename: str = "chart.png"):
    """
    Saves a base64-encoded PNG chart to a file.

    :param base64_png: Base64-encoded PNG data.
    :param filename: The name of the file to save the chart.
    :return: The path to the saved chart.
    """
    png_data = base64.b64decode(base64_png)

    # Save the decoded PNG to a file
    with open(filename, "wb") as file:
        file.write(png_data)

    return f"Chart saved to {filename}"

# From plugins/e2b.py
def e2b_llm_to_chart(user_message: str):
    """
    The main workflow function: sends a message to the LLM, executes the generated code, and saves any charts.

    :param user_message: The user's input prompt for the LLM.
    :return: Final results and path to saved chart if applicable.
    """
    # Get code execution results and logs
    execution_results = chat_with_llm_and_execute(user_message)

    # Check if any chart (PNG) was generated
    if execution_results["results"]:
        for result in execution_results["results"]:
            if "png" in result["formats"]:
                # Save the chart if PNG format is present
                chart_filename = save_chart(result["formats"]["png"])
                return {"execution_results": execution_results, "chart_saved_to": chart_filename}

    return {"execution_results": execution_results}


# From plugins/serpapi.py
def serpapi_search_v2(query: str, engine: str = "google", location: str = "United States", language: str = "en", country: str = "us", safe_search: bool = False, num_results: int = 10, start: int = 0, async_request: bool = False, output_format: str = "json"):
  """
  Perform a search using the SerpApi service with a flexible set of parameters.

  Args:
      query (str): The search query.
      engine (str): The search engine to use (e.g., 'google', 'bing'). Default is 'google'.
      location (str): The location to target the search. Default is 'United States'.
      language (str): UI language for the search. Default is 'en'.
      country (str): Country code for the search. Default is 'us'.
      safe_search (bool): Flag for SafeSearch filtering. Default is False.
      num_results (int): Number of search results to retrieve. Default is 10.
      start (int): Pagination offset. Default is 0.
      async_request (bool): Whether to make an asynchronous request. Default is False.
      output_format (str): Format of the output ('json' or 'html'). Default is 'json'.

  Returns:
      dict or str: The search results in the specified format.
  """
  # Import necessary modules and classes within function scope.

  # Get the API key from the global variables.
  api_key = globals().get("serpapi_api_key", "")
  if not api_key:
      raise ValueError("API key is missing. Please provide a valid SerpApi key.")

  # Initialize the SerpApi client.
  client = serpapi.Client(api_key=api_key)

  # Define the search parameters.
  params = {
      "q": query,
      "engine": engine,
      "location": location,
      "hl": language,
      "gl": country,
      "safe": "active" if safe_search else "off",
      "num": num_results,
      "start": start,
      "async": async_request,
      "output": output_format,
  }

  try:
      # Perform the search and get the results.
      search_results = client.search(**params)

      # Return the results in the specified format.
      if output_format == "json":
          return search_results.as_dict()
      elif output_format == "html":
          return search_results.get("raw_html", "No HTML content found.")
      else:
          raise ValueError("Invalid output format specified. Choose either 'json' or 'html'.")

  except requests.exceptions.HTTPError as e:
      # Handle potential SerpApi errors and HTTP errors.
      return {"error": str(e)}


# From plugins/augie.py
def generate_augie_params(user_input, voice_id="29vD33N1CtxCmqQRPOHJ"):
  """
  This function generates JSON parameters for creating an Augie video.
  It uses GPT to structure the user input into the required format, keeping the default voice_id.

  Parameters:
  - user_input: The basic user input text.
  - voice_id: Default voice ID (not generated by GPT).

  Returns: A dictionary with the necessary parameters for creating the Augie video.
  """
  prompt = (
      "You are creating parameters for a video generation API request. "
      "The user has provided input text for the video content. Structure the input into the following JSON format:\n"
      "{\n"
      "  'name': '<brief title>',\n"
      "  'text': '<full video content text>',\n"
      "  'orientation': 'landscape' or 'portrait',\n"
      "  'make_video': true or false\n"
      "}\n"
      "Do not generate a voice ID, use the one provided by the API system."
  )

  gpt_output = gpt_call({"prompt": prompt, "user_input": user_input})

  # Parse GPT output and construct parameters
  params = gpt_output['text']  # Assuming gpt_call returns a structured response.
  params['voice_id'] = voice_id  # Set the default voice ID.

  return params

# From plugins/augie.py
def create_augie(params):
  """Function to create a video on Augie platform with parameters."""
  API_KEY = globals()['augie_api_key']
  BASE_URL = 'https://beta.api.augie.studio/v1'

  headers = {
      'x-api-key': API_KEY,
      'Content-Type': 'application/json'
  }

  import requests
  response = requests.post(f'{BASE_URL}/augies', json=params, headers=headers)

  if response.status_code == 201:
      return response.json()  # Returns the creation response
  else:
      raise Exception(f"Failed to create Augie: {response.text}")

# From plugins/augie.py
def get_augie_status(augie_id):
  """Function to check the status of an Augie video creation."""
  API_KEY = globals()['augie_api_key']
  BASE_URL = 'https://beta.api.augie.studio/v1'

  headers = {
      'x-api-key': API_KEY
  }

  import requests
  response = requests.get(f'{BASE_URL}/augies/{augie_id}/status', headers=headers)

  if response.status_code == 200:
      status_data = response.json()
      if status_data.get('status') == 'succeeded' and 'output' in status_data and 'video' in status_data['output']:
          return {"status": "completed", "video_url": status_data['output']['video']}
      else:
          return {"status": "processing"}
  else:
      raise Exception(f"Failed to get Augie status: {response.text}")

# From plugins/augie.py
def create_and_wait_for_video(user_input, timeout=300, interval=10):
  """
  Wrapper function to create a video from user input and wait for it to be available.
  - user_input: Basic input from the user, processed by GPT.
  - timeout: The max time to wait (in seconds).
  - interval: Time between status checks (in seconds).
  """
  import time

  # Generate parameters using GPT
  params = generate_augie_params(user_input)

  # Create the video
  creation_response = create_augie(params)
  augie_id = creation_response.get('id')

  if not augie_id:
      raise Exception("Failed to retrieve Augie ID after creation.")

  # Wait and check the status periodically
  start_time = time.time()
  while time.time() - start_time < timeout:
      status_response = get_augie_status(augie_id)
      if status_response['status'] == 'completed':
          return status_response  # Return video URL if available
      time.sleep(interval)

  # Timeout reached, return failure
  raise TimeoutError(f"Video creation timed out after {timeout} seconds.")

from langchain_community.llms import Ollama
from langchain_openai import OpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import HarmBlockThreshold
from langchain_google_genai import HarmCategory

# From agent-zero/models.py
def get_api_key(service):
    return os.getenv(f"API_KEY_{service.upper()}")

# From agent-zero/models.py
def get_anthropic_haiku(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("anthropic")
    return ChatAnthropic(model_name="claude-3-haiku-20240307", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_anthropic_sonnet_35(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("anthropic")
    return ChatAnthropic(model_name="claude-3-5-sonnet-20240620", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_anthropic_sonnet(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("anthropic")
    return ChatAnthropic(model_name="claude-3-sonnet-20240229", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_anthropic_opus(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("anthropic")
    return ChatAnthropic(model_name="claude-3-opus-20240229", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_openai_gpt35(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return ChatOpenAI(model_name="gpt-3.5-turbo", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_openai_chat(api_key=None, model="gpt-4o-mini", temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return ChatOpenAI(model_name=model, temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_openai_gpt35_instruct(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return OpenAI(model_name="gpt-3.5-turbo-instruct", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_openai_gpt4(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return ChatOpenAI(model_name="gpt-4-0125-preview", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_openai_gpt4o(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return ChatOpenAI(model_name="gpt-4o", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_groq_mixtral7b(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("groq")
    return ChatGroq(model_name="mixtral-8x7b-32768", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_groq_llama70b(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("groq")
    return ChatGroq(model_name="llama3-70b-8192", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_groq_llama70b_json(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("groq")
    return ChatGroq(model_name="llama3-70b-8192", temperature=temperature, api_key=api_key, model_kwargs={"response_format": {"type": "json_object"}})

# From agent-zero/models.py
def get_groq_llama8b(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("groq")
    return ChatGroq(model_name="Llama3-8b-8192", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_ollama(model_name, temperature=DEFAULT_TEMPERATURE):
    return Ollama(model=model_name,temperature=temperature)

# From agent-zero/models.py
def get_groq_gemma(api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("groq")
    return ChatGroq(model_name="gemma-7b-it", temperature=temperature, api_key=api_key)

# From agent-zero/models.py
def get_ollama_dolphin(temperature=DEFAULT_TEMPERATURE):
    return Ollama(model="dolphin-llama3:8b-256k-v2.9-fp16", temperature=temperature)

# From agent-zero/models.py
def get_ollama_phi(temperature=DEFAULT_TEMPERATURE):
    return Ollama(model="phi3:3.8b-mini-instruct-4k-fp16",temperature=temperature)

# From agent-zero/models.py
def get_google_chat(model_name="gemini-1.5-flash-latest", api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("google")
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, google_api_key=api_key, 
                                  safety_settings={HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE })

# From agent-zero/models.py
def get_embedding_hf(model_name="sentence-transformers/all-MiniLM-L6-v2"):
    return HuggingFaceEmbeddings(model_name=model_name)

# From agent-zero/models.py
def get_embedding_openai(api_key=None):
    api_key = api_key or get_api_key("openai")
    return OpenAIEmbeddings(api_key=api_key)

import models
from ansio import application_keypad
from ansio import mouse_input
from ansio import raw_input
from ansio.input import InputEvent
from ansio.input import get_input_event
from agent import AgentConfig
from python.helpers.print_style import PrintStyle
from python.helpers.files import read_file
from python.helpers import files
import python.helpers.timed_input
import readline

# From agent-zero/main.py
def initialize():
    
    # main chat model used by agents (smarter, more accurate)

    # chat_llm = models.get_groq_llama70b(temperature=0.2)
    # chat_llm = models.get_groq_llama70b_json(temperature=0.2)
    # chat_llm = models.get_groq_llama8b(temperature=0.2)
    # chat_llm = models.get_openai_gpt35(temperature=0)
    # chat_llm = models.get_openai_gpt4o(temperature=0)
    chat_llm = models.get_openai_chat(temperature=0)
    # chat_llm = models.get_anthropic_opus(temperature=0)
    # chat_llm = models.get_anthropic_sonnet(temperature=0)
    # chat_llm = models.get_anthropic_sonnet_35(temperature=0)
    # chat_llm = models.get_anthropic_haiku(temperature=0)
    # chat_llm = models.get_ollama_dolphin()
    # chat_llm = models.get_ollama(model_name="gemma2:27b")
    # chat_llm = models.get_ollama(model_name="llama3:8b-text-fp16")
    # chat_llm = models.get_ollama(model_name="gemma2:latest")
    # chat_llm = models.get_ollama(model_name="qwen:14b")
    # chat_llm = models.get_google_chat()


    # utility model used for helper functions (cheaper, faster)
    utility_llm = models.get_openai_chat(temperature=0)
    
    # embedding model used for memory
    embedding_llm = models.get_embedding_openai()
    # embedding_llm = models.get_embedding_hf()

    # agent configuration
    config = AgentConfig(
        chat_model = chat_llm,
        utility_model = utility_llm,
        embeddings_model = embedding_llm,
        # memory_subdir = "",
        auto_memory_count = 0,
        # auto_memory_skip = 2,
        # rate_limit_seconds = 60,
        # rate_limit_requests = 30,
        # rate_limit_input_tokens = 0,
        # rate_limit_output_tokens = 0,
        # msgs_keep_max = 25,
        # msgs_keep_start = 5,
        # msgs_keep_end = 10,
        # max_tool_response_length = 3000,
        # response_timeout_seconds = 60,
        code_exec_docker_enabled = True,
        # code_exec_docker_name = "agent-zero-exe",
        # code_exec_docker_image = "frdel/agent-zero-exe:latest",
        # code_exec_docker_ports = { "22/tcp": 50022 }
        # code_exec_docker_volumes = { files.get_abs_path("work_dir"): {"bind": "/root", "mode": "rw"} }
        code_exec_ssh_enabled = True,
        # code_exec_ssh_addr = "localhost",
        # code_exec_ssh_port = 50022,
        # code_exec_ssh_user = "root",
        # code_exec_ssh_pass = "toor",
        # additional = {},
    )
    
    # create the first agent
    agent0 = Agent( number = 0, config = config )

    # start the chat loop
    chat(agent0)

# From agent-zero/main.py
def chat(agent:Agent):
    
    # start the conversation loop  
    while True:
        # ask user for message
        with input_lock:
            timeout = agent.get_data("timeout") # how long the agent is willing to wait
            if not timeout: # if agent wants to wait for user input forever
                PrintStyle(background_color="#6C3483", font_color="white", bold=True, padding=True).print(f"User message ('e' to leave):")        
                import readline # this fixes arrow keys in terminal
                user_input = input("> ")
                PrintStyle(font_color="white", padding=False, log_only=True).print(f"> {user_input}") 
                
            else: # otherwise wait for user input with a timeout
                PrintStyle(background_color="#6C3483", font_color="white", bold=True, padding=True).print(f"User message ({timeout}s timeout, 'w' to wait, 'e' to leave):")        
                import readline # this fixes arrow keys in terminal
                # user_input = timed_input("> ", timeout=timeout)
                user_input = timeout_input("> ", timeout=timeout)
                                    
                if not user_input:
                    user_input = read_file("prompts/fw.msg_timeout.md")
                    PrintStyle(font_color="white", padding=False).stream(f"{user_input}")        
                else:
                    user_input = user_input.strip()
                    if user_input.lower()=="w": # the user needs more time
                        user_input = input("> ").strip()
                    PrintStyle(font_color="white", padding=False, log_only=True).print(f"> {user_input}")        
                    
                    

        # exit the conversation when the user types 'exit'
        if user_input.lower() == 'e': break

        # send message to agent0, 
        assistant_response = agent.message_loop(user_input)
        
        # print agent0 response
        PrintStyle(font_color="white",background_color="#1D8348", bold=True, padding=True).print(f"{agent.agent_name}: reponse:")        
        PrintStyle(font_color="white").print(f"{assistant_response}")

# From agent-zero/main.py
def intervention():
    if Agent.streaming_agent and not Agent.paused:
        Agent.paused = True # stop agent streaming
        PrintStyle(background_color="#6C3483", font_color="white", bold=True, padding=True).print(f"User intervention ('e' to leave, empty to continue):")        

        import readline # this fixes arrow keys in terminal
        user_input = input("> ").strip()
        PrintStyle(font_color="white", padding=False, log_only=True).print(f"> {user_input}")        
        
        if user_input.lower() == 'e': os._exit(0) # exit the conversation when the user types 'exit'
        if user_input: Agent.streaming_agent.intervention_message = user_input # set intervention message if non-empty
        Agent.paused = False

# From agent-zero/main.py
def capture_keys():
        global input_lock
        intervent=False            
        while True:
            if intervent: intervention()
            intervent = False
            time.sleep(0.1)
            
            if Agent.streaming_agent:
                # with raw_input, application_keypad, mouse_input:
                with input_lock, raw_input, application_keypad:
                    event: InputEvent | None = get_input_event(timeout=0.1)
                    if event and (event.shortcut.isalpha() or event.shortcut.isspace()):
                        intervent=True
                        continue

# From agent-zero/main.py
def timeout_input(prompt, timeout=10):
    return timed_input.timeout_input(prompt=prompt, timeout=timeout)

from python.helpers import perplexity_search
from python.helpers.tool import Tool
from python.helpers.tool import Response

# From tools/online_knowledge_tool.py
class OnlineKnowledge(Tool):
    def execute(self,**kwargs):
        return Response(
            message=process_question(self.args["question"]),
            break_loop=False,
        )

# From tools/online_knowledge_tool.py
def process_question(question):
    return str(perplexity_search.perplexity_search(question))

from  import online_knowledge_tool
from python.helpers import duckduckgo_search
from  import memory_tool
import concurrent.futures

# From tools/knowledge_tool.py
class Knowledge(Tool):
    def execute(self, question="", **kwargs):
        with concurrent.futures.ThreadPoolExecutor() as executor:
            # Schedule the two functions to be run in parallel

            # perplexity search, if API provided
            if os.getenv("API_KEY_PERPLEXITY"):
                perplexity = executor.submit(perplexity_search.perplexity_search, question)
            else: perplexity = None

            # duckduckgo search
            duckduckgo = executor.submit(duckduckgo_search.search, question)

            # memory search
            future_memory = executor.submit(memory_tool.search, self.agent, question)

            # Wait for both functions to complete
            perplexity_result = (perplexity.result() if perplexity else "") or ""
            duckduckgo_result = duckduckgo.result()
            memory_result = future_memory.result()

        msg = files.read_file("prompts/tool.knowledge.response.md", 
                              online_sources = perplexity_result + "\n\n" + str(duckduckgo_result),
                              memory = memory_result )

        if self.agent.handle_intervention(msg): pass # wait for intervention and handle it, if paused

        return Response(message=msg, break_loop=False)


# From tools/unknown.py
class Unknown(Tool):
    def execute(self, **kwargs):
        return Response(
                message=files.read_file("prompts/fw.tool_not_found.md",
                                        tool_name=self.name,
                                        tools_prompt=files.read_file("prompts/agent.tools.md")), 
                break_loop=False)


# From tools/call_subordinate.py
class Delegation(Tool):

    def execute(self, message="", reset="", **kwargs):
        # create subordinate agent using the data object on this agent and set superior agent to his data object
        if self.agent.get_data("subordinate") is None or str(reset).lower().strip() == "true":
            subordinate = Agent(self.agent.number+1, self.agent.config)
            subordinate.set_data("superior", self.agent)
            self.agent.set_data("subordinate", subordinate) 
        # run subordinate agent message loop
        return Response( message=self.agent.get_data("subordinate").message_loop(message), break_loop=False)

from langchain.storage import InMemoryByteStore
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
from langchain_chroma import Chroma
from  import files
from langchain_core.documents import Document

# From helpers/vector_db.py
class VectorDB:

    def __init__(self, embeddings_model, in_memory=False, cache_dir="./cache"):
        print("Initializing VectorDB...")
        self.embeddings_model = embeddings_model

        em_cache = files.get_abs_path(cache_dir,"embeddings")
        db_cache = files.get_abs_path(cache_dir,"database")
        
        if in_memory:
            self.store = InMemoryByteStore()
        else:
            self.store = LocalFileStore(em_cache)


        #here we setup the embeddings model with the chosen cache storage
        self.embedder = CacheBackedEmbeddings.from_bytes_store(
            embeddings_model, 
            self.store, 
            namespace=getattr(embeddings_model, 'model', getattr(embeddings_model, 'model_name', "default")) )


        self.db = Chroma(embedding_function=self.embedder,persist_directory=db_cache)
        
        
    def search_similarity(self, query, results=3):
        return self.db.similarity_search(query,results)
    
    def search_similarity_threshold(self, query, results=3, threshold=0.5):
        return self.db.search(query, search_type="similarity_score_threshold", k=results, score_threshold=threshold)

    def search_max_rel(self, query, results=3):
        return self.db.max_marginal_relevance_search(query,results)

    def delete_documents_by_query(self, query:str, threshold=0.1):
        k = 100
        tot = 0
        while True:
            # Perform similarity search with score
            docs = self.search_similarity_threshold(query, results=k, threshold=threshold)

            # Extract document IDs and filter based on score
            # document_ids = [result[0].metadata["id"] for result in docs if result[1] < score_limit]
            document_ids = [result.metadata["id"] for result in docs]
            

            # Delete documents with IDs over the threshold score
            if document_ids:
                # fnd = self.db.get(where={"id": {"$in": document_ids}})
                # if fnd["ids"]: self.db.delete(ids=fnd["ids"])
                # tot += len(fnd["ids"])
                self.db.delete(ids=document_ids)
                tot += len(document_ids)
                                    
            # If fewer than K document IDs, break the loop
            if len(document_ids) < k:
                break
        
        return tot

    def delete_documents_by_ids(self, ids:list[str]):
        # pre = self.db.get(ids=ids)["ids"]
        self.db.delete(ids=ids)
        # post = self.db.get(ids=ids)["ids"]
        #TODO? compare pre and post
        return len(ids)
        
    def insert_document(self, data):
        id = str(uuid.uuid4())
        self.db.add_documents(documents=[ Document(data, metadata={"id": id}) ], ids=[id])
        
        return id

# From helpers/vector_db.py
def search_similarity(self, query, results=3):
        return self.db.similarity_search(query,results)

# From helpers/vector_db.py
def search_similarity_threshold(self, query, results=3, threshold=0.5):
        return self.db.search(query, search_type="similarity_score_threshold", k=results, score_threshold=threshold)

# From helpers/vector_db.py
def search_max_rel(self, query, results=3):
        return self.db.max_marginal_relevance_search(query,results)

# From helpers/vector_db.py
def delete_documents_by_query(self, query:str, threshold=0.1):
        k = 100
        tot = 0
        while True:
            # Perform similarity search with score
            docs = self.search_similarity_threshold(query, results=k, threshold=threshold)

            # Extract document IDs and filter based on score
            # document_ids = [result[0].metadata["id"] for result in docs if result[1] < score_limit]
            document_ids = [result.metadata["id"] for result in docs]
            

            # Delete documents with IDs over the threshold score
            if document_ids:
                # fnd = self.db.get(where={"id": {"$in": document_ids}})
                # if fnd["ids"]: self.db.delete(ids=fnd["ids"])
                # tot += len(fnd["ids"])
                self.db.delete(ids=document_ids)
                tot += len(document_ids)
                                    
            # If fewer than K document IDs, break the loop
            if len(document_ids) < k:
                break
        
        return tot

# From helpers/vector_db.py
def delete_documents_by_ids(self, ids:list[str]):
        # pre = self.db.get(ids=ids)["ids"]
        self.db.delete(ids=ids)
        # post = self.db.get(ids=ids)["ids"]
        #TODO? compare pre and post
        return len(ids)

# From helpers/vector_db.py
def insert_document(self, data):
        id = str(uuid.uuid4())
        self.db.add_documents(documents=[ Document(data, metadata={"id": id}) ], ids=[id])
        
        return id

import webcolors

# From helpers/print_style.py
class PrintStyle:
    last_endline = True
    log_file_path = None

    def __init__(self, bold=False, italic=False, underline=False, font_color="default", background_color="default", padding=False, log_only=False):
        self.bold = bold
        self.italic = italic
        self.underline = underline
        self.font_color = font_color
        self.background_color = background_color
        self.padding = padding
        self.padding_added = False  # Flag to track if padding was added
        self.log_only = log_only

        if PrintStyle.log_file_path is None:
            logs_dir = files.get_abs_path("logs")
            os.makedirs(logs_dir, exist_ok=True)
            log_filename = datetime.now().strftime("log_%Y%m%d_%H%M%S.html")
            PrintStyle.log_file_path = os.path.join(logs_dir, log_filename)
            with open(PrintStyle.log_file_path, "w") as f:
                f.write("<html><body style='background-color:black;font-family: Arial, Helvetica, sans-serif;'><pre>\n")

    def _get_rgb_color_code(self, color, is_background=False):
        try:
            if color.startswith("#") and len(color) == 7:
                r = int(color[1:3], 16)
                g = int(color[3:5], 16)
                b = int(color[5:7], 16)
            else:
                rgb_color = webcolors.name_to_rgb(color)
                r, g, b = rgb_color.red, rgb_color.green, rgb_color.blue
            
            if is_background:
                return f"\033[48;2;{r};{g};{b}m", f"background-color: rgb({r}, {g}, {b});"
            else:
                return f"\033[38;2;{r};{g};{b}m", f"color: rgb({r}, {g}, {b});"
        except ValueError:
            return "", ""

    def _get_styled_text(self, text):
        start = ""
        end = "\033[0m"  # Reset ANSI code
        if self.bold:
            start += "\033[1m"
        if self.italic:
            start += "\033[3m"
        if self.underline:
            start += "\033[4m"
        font_color_code, _ = self._get_rgb_color_code(self.font_color)
        background_color_code, _ = self._get_rgb_color_code(self.background_color, True)
        start += font_color_code
        start += background_color_code
        return start + text + end

    def _get_html_styled_text(self, text):
        styles = []
        if self.bold:
            styles.append("font-weight: bold;")
        if self.italic:
            styles.append("font-style: italic;")
        if self.underline:
            styles.append("text-decoration: underline;")
        _, font_color_code = self._get_rgb_color_code(self.font_color)
        _, background_color_code = self._get_rgb_color_code(self.background_color, True)
        styles.append(font_color_code)
        styles.append(background_color_code)
        style_attr = " ".join(styles)
        escaped_text = html.escape(text).replace("\n", "<br>")  # Escape HTML special characters
        return f'<span style="{style_attr}">{escaped_text}</span>'

    def _add_padding_if_needed(self):
        if self.padding and not self.padding_added:
            if not self.log_only:
                print()  # Print an empty line for padding
            self._log_html("<br>")
            self.padding_added = True

    def _log_html(self, html):
        with open(PrintStyle.log_file_path, "a") as f: # type: ignore
            f.write(html)

    @staticmethod
    def _close_html_log():
        if PrintStyle.log_file_path:
            with open(PrintStyle.log_file_path, "a") as f:
                f.write("</pre></body></html>")            

    def get(self, *args, sep=' ', **kwargs):
        text = sep.join(map(str, args))
        return text, self._get_styled_text(text), self._get_html_styled_text(text)
        
    def print(self, *args, sep=' ', **kwargs):
        self._add_padding_if_needed()
        if not PrintStyle.last_endline: 
            print()
            self._log_html("<br>")
        plain_text, styled_text, html_text = self.get(*args, sep=sep, **kwargs)
        if not self.log_only:
            print(styled_text, end='\n', flush=True)
        self._log_html(html_text+"<br>\n")
        PrintStyle.last_endline = True

    def stream(self, *args, sep=' ', **kwargs):
        self._add_padding_if_needed()
        plain_text, styled_text, html_text = self.get(*args, sep=sep, **kwargs)
        if not self.log_only:
            print(styled_text, end='', flush=True)
        self._log_html(html_text)
        PrintStyle.last_endline = False

    def is_last_line_empty(self):
        lines = sys.stdin.readlines()
        return bool(lines) and not lines[-1].strip()

# From helpers/print_style.py
def print(self, *args, sep=' ', **kwargs):
        self._add_padding_if_needed()
        if not PrintStyle.last_endline: 
            print()
            self._log_html("<br>")
        plain_text, styled_text, html_text = self.get(*args, sep=sep, **kwargs)
        if not self.log_only:
            print(styled_text, end='\n', flush=True)
        self._log_html(html_text+"<br>\n")
        PrintStyle.last_endline = True

# From helpers/print_style.py
def stream(self, *args, sep=' ', **kwargs):
        self._add_padding_if_needed()
        plain_text, styled_text, html_text = self.get(*args, sep=sep, **kwargs)
        if not self.log_only:
            print(styled_text, end='', flush=True)
        self._log_html(html_text)
        PrintStyle.last_endline = False

# From helpers/print_style.py
def is_last_line_empty(self):
        lines = sys.stdin.readlines()
        return bool(lines) and not lines[-1].strip()

from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.outputs.llm_result import LLMResult

# From helpers/perplexity_search.py
class PerplexityCrewLLM(BaseLLM):
    api_key: str
    model_name: str

    def call_perplexity_ai(self, prompt: str) -> LLMResult:
        url = "https://api.perplexity.ai/chat/completions"

        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "system",
                    "content": "Be precise and concise."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "accept": "application/json",
            "content-type": "application/json"
        }

        response = requests.post(url, json=payload, headers=headers)

        # Convert the response JSON to dictionary
        json_response = response.json()

        return json_response

    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None,
                  run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> LLMResult:
        generations = []
        for prompt in prompts:
            generations.append([self._call(prompt, stop=stop, **kwargs)])
        return LLMResult.construct(generations=generations)

    def _call(self, prompt: str, stop: Optional[List[str]] = None, max_tokens: Optional[int] = None) -> LLMResult:
        response_data = self.call_perplexity_ai(prompt)
        model = LLMResult.construct(text=response_data['choices'][0]["message"]["content"]) # type: ignore

        return model

    @property
    def _llm_type(self) -> str:
        return "PerplexityAI"

# From helpers/perplexity_search.py
def PerplexitySearchLLM(api_key,model_name="sonar-medium-online",base_url="https://api.perplexity.ai"):    
    client = OpenAI(api_key=api_key_from_env, base_url=base_url)
        
    def call_model(query:str):
        messages = [
        #It is recommended to use only single-turn conversations and avoid system prompts for the online LLMs (sonar-small-online and sonar-medium-online).
        
        # {
        #     "role": "system",
        #     "content": (
        #         "You are an artificial intelligence assistant and you need to "
        #         "engage in a helpful, detailed, polite conversation with a user."
        #     ),
        # },
        {
            "role": "user",
            "content": (
                query
            ),
        },
        ]
        
        response = client.chat.completions.create(
            model=model_name,
            messages=messages, # type: ignore
        )
        result = response.choices[0].message.content #only the text is returned
        return result
    
    return call_model

# From helpers/perplexity_search.py
def perplexity_search(search_query: str):
    return call_llm(search_query)

# From helpers/perplexity_search.py
def call_perplexity_ai(self, prompt: str) -> LLMResult:
        url = "https://api.perplexity.ai/chat/completions"

        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "system",
                    "content": "Be precise and concise."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "accept": "application/json",
            "content-type": "application/json"
        }

        response = requests.post(url, json=payload, headers=headers)

        # Convert the response JSON to dictionary
        json_response = response.json()

        return json_response

# From helpers/perplexity_search.py
def call_model(query:str):
        messages = [
        #It is recommended to use only single-turn conversations and avoid system prompts for the online LLMs (sonar-small-online and sonar-medium-online).
        
        # {
        #     "role": "system",
        #     "content": (
        #         "You are an artificial intelligence assistant and you need to "
        #         "engage in a helpful, detailed, polite conversation with a user."
        #     ),
        # },
        {
            "role": "user",
            "content": (
                query
            ),
        },
        ]
        
        response = client.chat.completions.create(
            model=model_name,
            messages=messages, # type: ignore
        )
        result = response.choices[0].message.content #only the text is returned
        return result

import select

# From helpers/shell_local.py
class LocalInteractiveSession:
    def __init__(self):
        self.process = None
        self.full_output = ''

    def connect(self):
        # Start a new subprocess with the appropriate shell for the OS
        if sys.platform.startswith('win'):
            # Windows
            self.process = subprocess.Popen(
                ['cmd.exe'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
        else:
            # macOS and Linux
            self.process = subprocess.Popen(
                ['/bin/bash'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )

    def close(self):
        if self.process:
            self.process.terminate()
            self.process.wait()

    def send_command(self, command: str):
        if not self.process:
            raise Exception("Shell not connected")
        self.full_output = ""
        self.process.stdin.write(command + '\n') # type: ignore
        self.process.stdin.flush() # type: ignore
 
    def read_output(self) -> Tuple[str, Optional[str]]:
        if not self.process:
            raise Exception("Shell not connected")

        partial_output = ''
        while True:
            rlist, _, _ = select.select([self.process.stdout], [], [], 0.1)
            if rlist:
                line = self.process.stdout.readline()  # type: ignore
                if line:
                    partial_output += line
                    self.full_output += line
                    time.sleep(0.1)
                else:
                    break  # No more output
            else:
                break  # No data available

        if not partial_output:
            return self.full_output, None
        
        return self.full_output, partial_output

# From helpers/shell_local.py
def connect(self):
        # Start a new subprocess with the appropriate shell for the OS
        if sys.platform.startswith('win'):
            # Windows
            self.process = subprocess.Popen(
                ['cmd.exe'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
        else:
            # macOS and Linux
            self.process = subprocess.Popen(
                ['/bin/bash'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )

# From helpers/shell_local.py
def close(self):
        if self.process:
            self.process.terminate()
            self.process.wait()

# From helpers/shell_local.py
def send_command(self, command: str):
        if not self.process:
            raise Exception("Shell not connected")
        self.full_output = ""
        self.process.stdin.write(command + '\n') # type: ignore
        self.process.stdin.flush()

# From helpers/shell_local.py
def read_output(self) -> Tuple[str, Optional[str]]:
        if not self.process:
            raise Exception("Shell not connected")

        partial_output = ''
        while True:
            rlist, _, _ = select.select([self.process.stdout], [], [], 0.1)
            if rlist:
                line = self.process.stdout.readline()  # type: ignore
                if line:
                    partial_output += line
                    self.full_output += line
                    time.sleep(0.1)
                else:
                    break  # No more output
            else:
                break  # No data available

        if not partial_output:
            return self.full_output, None
        
        return self.full_output, partial_output

import paramiko

# From helpers/shell_ssh.py
class SSHInteractiveSession:

    end_comment = "# @@==>> SSHInteractiveSession End-of-Command  <<==@@"

    ps1_label = "SSHInteractiveSession CLI>"
    
    def __init__(self, hostname: str, port: int, username: str, password: str):
        self.hostname = hostname
        self.port = port
        self.username = username
        self.password = password
        self.client = paramiko.SSHClient()
        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.shell = None
        self.full_output = b''

    def connect(self):
        # try 3 times with wait and then except
        errors = 0
        while True:
            try:
                self.client.connect(self.hostname, self.port, self.username, self.password)
                self.shell = self.client.invoke_shell(width=160,height=48)
                # self.shell.send(f'PS1="{SSHInteractiveSession.ps1_label}"'.encode())
                return
                # while True: # wait for end of initial output
                #     full, part = self.read_output()
                #     if full and not part: return
                #     time.sleep(0.1)
            except Exception as e:
                errors += 1
                if errors < 3:
                    print(f"SSH Connection attempt {errors}...")
                    time.sleep(5)
                else:
                    raise e

    def close(self):
        if self.shell:
            self.shell.close()
        if self.client:
            self.client.close()

    def send_command(self, command: str):
        if not self.shell:
            raise Exception("Shell not connected")
        self.full_output = b""
        self.shell.send((command + " \\\n" +SSHInteractiveSession.end_comment + "\n").encode())

    def read_output(self) -> Tuple[str, str]:
        if not self.shell:
            raise Exception("Shell not connected")

        partial_output = b''
        while self.shell.recv_ready():
            data = self.shell.recv(1024)
            partial_output += data
            self.full_output += data
            time.sleep(0.1)  # Prevent busy waiting

        # Decode once at the end
        decoded_partial_output = partial_output.decode('utf-8', errors='replace')
        decoded_full_output = self.full_output.decode('utf-8', errors='replace')
        
        decoded_partial_output = self.clean_string(decoded_partial_output)
        decoded_full_output = self.clean_string(decoded_full_output)

        # Split output at end_comment
        if SSHInteractiveSession.end_comment in decoded_full_output:
            decoded_full_output = decoded_full_output.split(SSHInteractiveSession.end_comment)[-1].lstrip("\r\n")
            decoded_partial_output = decoded_partial_output.split(SSHInteractiveSession.end_comment)[-1].lstrip("\r\n")
        
        return decoded_full_output, decoded_partial_output


    def clean_string(self, input_string):
        # Remove ANSI escape codes
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        cleaned = ansi_escape.sub('', input_string)
        
        # Replace '\r\n' with '\n'
        cleaned = cleaned.replace('\r\n', '\n')
        
        return cleaned

# From helpers/shell_ssh.py
def clean_string(self, input_string):
        # Remove ANSI escape codes
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        cleaned = ansi_escape.sub('', input_string)
        
        # Replace '\r\n' with '\n'
        cleaned = cleaned.replace('\r\n', '\n')
        
        return cleaned


# From helpers/messages.py
def truncate_text(output, threshold=1000):
    if len(output) <= threshold:
        return output

    # Adjust the file path as needed
    placeholder = files.read_file("./prompts/fw.msg_truncated.md", removed_chars=(len(output) - threshold))

    start_len = (threshold - len(placeholder)) // 2
    end_len = threshold - len(placeholder) - start_len

    truncated_output = output[:start_len] + placeholder + output[-end_len:]
    return truncated_output


# From helpers/dirty_json.py
class DirtyJson:
    def __init__(self):
        self._reset()

    def _reset(self):
        self.json_string = ""
        self.index = 0
        self.current_char = None
        self.result = None
        self.stack = []

    @staticmethod
    def parse_string(json_string):
        parser = DirtyJson()
        return parser.parse(json_string)
    
    def parse(self, json_string):
        self._reset()
        self.json_string = json_string
        self.current_char = self.json_string[0]
        self._parse()
        return self.result
        
    def feed(self, chunk):
        self.json_string += chunk
        if not self.current_char and self.json_string:
            self.current_char = self.json_string[0]
        self._parse()
        return self.result

    def _advance(self, count=1):
        self.index += count
        if self.index < len(self.json_string):
            self.current_char = self.json_string[self.index]
        else:
            self.current_char = None

    def _skip_whitespace(self):
        while self.current_char is not None and self.current_char.isspace():
            self._advance()

    def _parse(self):
        if self.result is None:
            self.result = self._parse_value()
        else:
            self._continue_parsing()

    def _continue_parsing(self):
        while self.current_char is not None:
            if isinstance(self.result, dict):
                self._parse_object_content()
            elif isinstance(self.result, list):
                self._parse_array_content()
            elif isinstance(self.result, str):
                self.result = self._parse_string()
            else:
                break

    def _parse_value(self):
        self._skip_whitespace()
        if self.current_char == '{':
            if self._peek(1) == '{':  # Handle {{
                self._advance(2)
            return self._parse_object()
        elif self.current_char == '[':
            return self._parse_array()
        elif self.current_char in ['"', "'", "`"]:
            if self._peek(2) == self.current_char * 2:  # type: ignore
                return self._parse_multiline_string()
            return self._parse_string()
        elif self.current_char and (self.current_char.isdigit() or self.current_char in ['-', '+']):
            return self._parse_number()
        elif self._match("true"):
            return True
        elif self._match('false'):
            return False
        elif self._match('null') or self._match("undefined"):
            return None
        elif self.current_char:
            return self._parse_unquoted_string()
        return None

    def _match(self, text: str) -> bool:
        cnt = len(text)
        if self._peek(cnt).lower() == text.lower():
            self._advance(cnt)
            return True
        return False
    
    def _parse_object(self):
        obj = {}
        self._advance()  # Skip opening brace
        self.stack.append(obj)
        self._parse_object_content()
        return obj

    def _parse_object_content(self):
        while self.current_char is not None:
            self._skip_whitespace()
            if self.current_char == '}':
                if self._peek(1) == '}':  # Handle }}
                    self._advance(2)
                else:
                    self._advance()
                self.stack.pop()
                return
            if self.current_char is None:
                self.stack.pop()
                return  # End of input reached while parsing object
            
            key = self._parse_key()
            value = None
            self._skip_whitespace()
            
            if self.current_char == ':':
                self._advance()
                value = self._parse_value()
            elif self.current_char is None:
                value = None  # End of input reached after key
            else:
                value = self._parse_value()
                
            self.stack[-1][key] = value
            
            self._skip_whitespace()
            if self.current_char == ',':
                self._advance()
                continue
            elif self.current_char != '}':
                if self.current_char is None:
                    self.stack.pop()
                    return  # End of input reached after value
                continue

    def _parse_key(self):
        self._skip_whitespace()
        if self.current_char in ['"', "'"]:
            return self._parse_string()
        else:
            return self._parse_unquoted_key()

    def _parse_unquoted_key(self):
        result = ""
        while self.current_char is not None and not self.current_char.isspace() and self.current_char not in [':', ',', '}', ']']:
            result += self.current_char
            self._advance()
        return result

    def _parse_array(self):
        arr = []
        self._advance()  # Skip opening bracket
        self.stack.append(arr)
        self._parse_array_content()
        return arr

    def _parse_array_content(self):
        while self.current_char is not None:
            self._skip_whitespace()
            if self.current_char == ']':
                self._advance()
                self.stack.pop()
                return
            value = self._parse_value()
            self.stack[-1].append(value)
            self._skip_whitespace()
            if self.current_char == ',':
                self._advance()
            elif self.current_char != ']':
                self.stack.pop()
                return

    def _parse_string(self):
        result = ""
        quote_char = self.current_char
        self._advance()  # Skip opening quote
        while self.current_char is not None and self.current_char != quote_char:
            if self.current_char == '\\':
                self._advance()
                if self.current_char in ['"', "'", '\\', '/', 'b', 'f', 'n', 'r', 't']:
                    result += {'b': '\b', 'f': '\f', 'n': '\n', 'r': '\r', 't': '\t'}.get(self.current_char, self.current_char)
                elif self.current_char == 'u':
                    unicode_char = ""
                    for _ in range(4):
                        if self.current_char is None:
                            return result
                        unicode_char += self.current_char
                        self._advance()
                    result += chr(int(unicode_char, 16))
                    continue
            else:
                result += self.current_char
            self._advance()
        if self.current_char == quote_char:
            self._advance()  # Skip closing quote
        return result

    def _parse_multiline_string(self):
        result = ""
        quote_char = self.current_char
        self._advance(3)  # Skip first quote
        while self.current_char is not None:
            if self.current_char == quote_char and self._peek(2) == quote_char * 2: # type: ignore
                self._advance(3)  # Skip first quote
                break
            result += self.current_char
            self._advance()
        return result.strip()

    def _parse_number(self):
        number_str = ""
        while self.current_char is not None and (self.current_char.isdigit() or self.current_char in ['-', '+', '.', 'e', 'E']):
            number_str += self.current_char
            self._advance()
        try:
            return int(number_str)
        except ValueError:
            return float(number_str)

    def _parse_true(self):
        self._advance()
        for char in 'rue':
            if self.current_char != char:
                return None
            self._advance()
        return True

    def _parse_false(self):
        self._advance()
        for char in 'alse':
            if self.current_char != char:
                return None
            self._advance()
        return False

    def _parse_null(self):
        self._advance()
        for char in 'ull':
            if self.current_char != char:
                return None
            self._advance()
        return None

    def _parse_unquoted_string(self):
        result = ""
        while self.current_char is not None and self.current_char not in [':', ',', '}', ']']:
            result += self.current_char
            self._advance()
        self._advance()
        return result.strip()

    def _peek(self, n):
        peek_index = self.index
        result = ''
        for _ in range(n):
            if peek_index < len(self.json_string):
                result += self.json_string[peek_index]
                peek_index += 1
            else:
                break
        return result

# From helpers/dirty_json.py
def parse_string(json_string):
        parser = DirtyJson()
        return parser.parse(json_string)

# From helpers/dirty_json.py
def feed(self, chunk):
        self.json_string += chunk
        if not self.current_char and self.json_string:
            self.current_char = self.json_string[0]
        self._parse()
        return self.result

from duckduckgo_search import DDGS

from print_style import PrintStyle

# From helpers/rate_limiter.py
class CallRecord:
    timestamp: float
    input_tokens: int
    output_tokens: int = 0

# From helpers/rate_limiter.py
class RateLimiter:
    def __init__(self, max_calls: int, max_input_tokens: int, max_output_tokens: int, window_seconds: int = 60):
        self.max_calls = max_calls
        self.max_input_tokens = max_input_tokens
        self.max_output_tokens = max_output_tokens
        self.window_seconds = window_seconds
        self.call_records: deque = deque()

    def _clean_old_records(self, current_time: float):
        while self.call_records and current_time - self.call_records[0].timestamp > self.window_seconds:
            self.call_records.popleft()

    def _get_counts(self) -> Tuple[int, int, int]:
        calls = len(self.call_records)
        input_tokens = sum(record.input_tokens for record in self.call_records)
        output_tokens = sum(record.output_tokens for record in self.call_records)
        return calls, input_tokens, output_tokens

    def _wait_if_needed(self, current_time: float, new_input_tokens: int):
        while True:
            self._clean_old_records(current_time)
            calls, input_tokens, output_tokens = self._get_counts()
            
            wait_reasons = []
            if self.max_calls > 0 and calls >= self.max_calls:
                wait_reasons.append("max calls")
            if self.max_input_tokens > 0 and input_tokens + new_input_tokens > self.max_input_tokens:
                wait_reasons.append("max input tokens")
            if self.max_output_tokens > 0 and output_tokens >= self.max_output_tokens:
                wait_reasons.append("max output tokens")
            
            if not wait_reasons:
                break
            
            oldest_record = self.call_records[0]
            wait_time = oldest_record.timestamp + self.window_seconds - current_time
            if wait_time > 0:
                PrintStyle(font_color="yellow", padding=True).print(f"Rate limit exceeded. Waiting for {wait_time:.2f} seconds due to: {', '.join(wait_reasons)}")
                time.sleep(wait_time)
            current_time = time.time()

    def limit_call_and_input(self, input_token_count: int) -> CallRecord:
        current_time = time.time()
        self._wait_if_needed(current_time, input_token_count)
        new_record = CallRecord(current_time, input_token_count)
        self.call_records.append(new_record)
        return new_record

    def set_output_tokens(self, output_token_count: int):
        if self.call_records:
            self.call_records[-1].output_tokens += output_token_count
        return self

# From helpers/rate_limiter.py
def rate_limited_function(input_token_count: int, output_token_count: int):
    # First, limit the call and input tokens (this may wait)
    rate_limiter.limit_call_and_input(input_token_count)
    
    # Your function logic here
    print(f"Function called with {input_token_count} input tokens")
    
    # After processing, set the output tokens (this doesn't wait)
    rate_limiter.set_output_tokens(output_token_count)
    print(f"Function completed with {output_token_count} output tokens")

# From helpers/rate_limiter.py
def limit_call_and_input(self, input_token_count: int) -> CallRecord:
        current_time = time.time()
        self._wait_if_needed(current_time, input_token_count)
        new_record = CallRecord(current_time, input_token_count)
        self.call_records.append(new_record)
        return new_record

# From helpers/rate_limiter.py
def set_output_tokens(self, output_token_count: int):
        if self.call_records:
            self.call_records[-1].output_tokens += output_token_count
        return self

from langchain_core.embeddings import Embeddings

# From helpers/vdb.py
def insert(self, data:str):
        
        id = str(uuid.uuid4())
        emb = self.embeddings_model.embed_documents([data])[0]

        self.collection.add(
            ids=[id],
            embeddings=[emb],
            documents=[data],
            )

        return id


# From helpers/files.py
def remove_code_fences(text):
    return re.sub(r'~~~\w*\n|~~~', '', text)

# From helpers/files.py
def get_abs_path(*relative_paths):
    return os.path.join(get_base_dir(), *relative_paths)

# From helpers/files.py
def exists(*relative_paths):
    path = get_abs_path(*relative_paths)
    return os.path.exists(path)

# From helpers/files.py
def get_base_dir():
    # Get the base directory from the current file path
    base_dir = os.path.dirname(os.path.abspath(os.path.join(__file__,"../../")))
    return base_dir


# From helpers/errors.py
def format_error(e: Exception, max_entries=2):
    traceback_text = traceback.format_exc()
    # Split the traceback into lines
    lines = traceback_text.split('\n')
    
    # Find all "File" lines
    file_indices = [i for i, line in enumerate(lines) if line.strip().startswith("File ")]
    
    # If we found at least one "File" line, keep up to max_entries
    if file_indices:
        start_index = max(0, len(file_indices) - max_entries)
        trimmed_lines = lines[file_indices[start_index]:]
    else:
        # If no "File" lines found, just return the original traceback
        return traceback_text
    
    # Find the error message at the end
    error_message = ""
    for line in reversed(trimmed_lines):
        if re.match(r'\w+Error:', line):
            error_message = line
            break
    
    # Combine the trimmed traceback with the error message
    result = "Traceback (most recent call last):\n" + '\n'.join(trimmed_lines)
    if error_message:
        result += f"\n\n{error_message}"
    
    return result

from python.helpers.files import get_abs_path

# From helpers/docker.py
class DockerContainerManager:
    def __init__(self, image: str, name: str, ports: Optional[Dict[str, int]] = None, volumes: Optional[Dict[str, Dict[str, str]]] = None):
        self.client = docker.from_env()
        self.image = image
        self.name = name
        self.ports = ports
        self.volumes = volumes
        self.container = None

    def cleanup_container(self) -> None:
        if self.container:
            try:
                self.container.stop()
                self.container.remove()
                print(f"Stopped and removed the container: {self.container.id}")
            except Exception as e:
                print(f"Failed to stop and remove the container: {e}")

    def start_container(self) -> None:
        existing_container = None
        for container in self.client.containers.list(all=True):
            if container.name == self.name:
                existing_container = container
                break

        if existing_container:
            if existing_container.status != 'running':
                print(f"Starting existing container: {self.name} for safe code execution...")
                existing_container.start()
                self.container = existing_container
                time.sleep(2) # this helps to get SSH ready
                
            else:
                self.container = existing_container
                # print(f"Container with name '{self.name}' is already running with ID: {existing_container.id}")
        else:
            print(f"Initializing docker container {self.name} for safe code execution...")
            self.container = self.client.containers.run(
                self.image,
                detach=True,
                ports=self.ports,
                name=self.name,
                volumes=self.volumes,
            )
            atexit.register(self.cleanup_container)
            print(f"Started container with ID: {self.container.id}")
            time.sleep(5)

# From helpers/docker.py
def cleanup_container(self) -> None:
        if self.container:
            try:
                self.container.stop()
                self.container.remove()
                print(f"Stopped and removed the container: {self.container.id}")
            except Exception as e:
                print(f"Failed to stop and remove the container: {e}")

# From helpers/docker.py
def start_container(self) -> None:
        existing_container = None
        for container in self.client.containers.list(all=True):
            if container.name == self.name:
                existing_container = container
                break

        if existing_container:
            if existing_container.status != 'running':
                print(f"Starting existing container: {self.name} for safe code execution...")
                existing_container.start()
                self.container = existing_container
                time.sleep(2) # this helps to get SSH ready
                
            else:
                self.container = existing_container
                # print(f"Container with name '{self.name}' is already running with ID: {existing_container.id}")
        else:
            print(f"Initializing docker container {self.name} for safe code execution...")
            self.container = self.client.containers.run(
                self.image,
                detach=True,
                ports=self.ports,
                name=self.name,
                volumes=self.volumes,
            )
            atexit.register(self.cleanup_container)
            print(f"Started container with ID: {self.container.id}")
            time.sleep(5)

from inputimeout import inputimeout
from inputimeout import TimeoutOccurred


from ta.utils import dropna
from ta.momentum import RSIIndicator
from ta.momentum import StochasticOscillator
from ta.momentum import TSIIndicator
from ta.trend import SMAIndicator
from ta.trend import EMAIndicator
from ta.trend import WMAIndicator
from ta.trend import MACD
from ta.trend import ADXIndicator
from ta.volume import AccDistIndexIndicator
from ta.volume import MFIIndicator

# From auto_gpt_metatrader/indicators.py
class Indicators():
    def fetch(symbol, timeframe):
        symbol = symbol.replace('/', '')
        symbol = symbol.upper()
        timeframe_map = {
            "1 minute": "1m",
            "1 min": "1m",
            "1min": "1m",
            "M1": "1m",
            "5 minutes": "5m",
            "5 min": "5m",
            "5min": "5m",
            "M5": "1m",
            "15 minutes": "15m",
            "15 min": "15m",
            "15min": "15m",
            "M15": "15m",
            "30 minutes": "30m",
            "30 min": "30m",
            "30min": "30m",
            "M30": "30m",
            "1 hour": "1h",
            "1hour": "1h",
            "1hr": "1h",
            "H1": "1h",
            "4 hours": "4h",
            "4hours": "4h",
            "4 hrs": "4h",
            "H4": "4h",
            "1 day": "1d",
            "1day": "1d",
            "D1": "1d",
            "1 week": "1w",
            "1week": "1w",
            "W1": "1w",
            "1 month": "1mn",
            "1month": "1mn",
            "M1": "1mn"

        }
        # Check if the user input matches any of the keys in the dictionary
        if timeframe in timeframe_map:
            timeframe = timeframe_map[timeframe]
        else:
            # Assume that the user input is already in the correct format
            pass

        url = f"https://mt-market-data-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/historical-market-data/symbols/{symbol}/timeframes/{timeframe}/candles?limit=100"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)
        if response:
            candlesticks = response.json()
            return candlesticks
        else:
            return 'Failed to get candlesticks.'

    def money_flow_index(candlesticks, period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close', 'tickVolume'])
            # Clean NaN values
            df = dropna(df)
            
            mfi = MFIIndicator(df['high'], df['low'], df['close'], volume=df['tickVolume'], window=int(period))
            return f'Current MFI Value: {mfi.money_flow_index().iloc()[-1]}'
        else:
            return f'Failed to get candlesticks'

    def volume(candlesticks):
        if candlesticks:
            volumes = [float(candlestick['tickVolume']) for candlestick in candlesticks]
            return np.sum(volumes[-14:])
        else:
            return f'Failed to get candlesticks'

    def rsi(candlesticks, period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            # Clean NaN values
            df = dropna(df)
            rsi_indicator = RSIIndicator(df['close'], window=float(period))
            current_rsi = rsi_indicator.rsi().iloc[-1]
            return f'Current RSI Value: {current_rsi}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    # Simple Moving Average (SMA)

    def sma(candlesticks, period):
        # Get the candlesticks data
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            df = dropna(df)
            sma = SMAIndicator(df['close'], window=float(period))
            return f'Current Simple Moving Average Value: {sma.sma_indicator().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    # Exponential Moving Average (EMA)
    def ema(candlesticks, period):
        # Get the candlesticks data
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            df = dropna(df)
            ema = EMAIndicator(df['close'], window=float(period))
            return f'Current Exponential Moving Average Value: {ema.ema_indicator().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    # Weighted Moving Average (WMA)
    def wma(candlesticks, period):
        # Get the candlesticks data
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            df = dropna(df)
            wma = WMAIndicator(df['close'], window=float(period))
            return f'Current Weighted Moving Average Value: {wma.wma_indicator().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    # Moving Average Convergence Divergence (MACD)
    def macd(candlesticks, fast_period, slow_period, signal_period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
    # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            macd = MACD(df['close'], window_slow=int(
                        12), window_fast=int(26), window_sign=int(9))
            signal_line = macd.macd_signal().iloc()[-1]
            macd_line = macd.macd().iloc()[-1]
            macd_diff = macd.macd_diff().iloc()[-1]
            macd_values = {"macd_line": macd_line, 'macd_diff': macd_diff, 'signal_line': signal_line}
            return f'Current MACD Values: {macd_values}'
        
        if not candlesticks:
            return f'Failed to get candlesticks'

    # Average Directional Movement Index (ADX)
    def adx(candlesticks, period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close'])
            # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            adx = ADXIndicator(df['high'], df['low'], df['close'], window=int(period))
            return f' Current Average Directional Movement Index Value: {adx.adx().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    # Accumulation/Distribution Index (ADI)
    def adi(candlesticks):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close', 'tickVolume'])
            # Clean NaN values
            df = dropna(df)
            adi = AccDistIndexIndicator(df['high'], df['low'], df['close'], volume=df['tickVolume'])
            return f'Current Accumulation/Distribution Index: {adi.acc_dist_index().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    def fib_retracements(candlesticks, high, low):
        high = float(high)
        low = float(low)
        levels = [0.236, 0.382, 0.5, 0.618, 0.786]
        diff = high - low
        retracements = []
        for level in levels:
            retracements.append(high - level * diff)
        return f'Current Fibonacci Retracements are: {retracements}'

    # Stochastic Oscillator
    def stochastic_oscillator(candlesticks, period, smooth_period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close'])
            # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            stotch = StochasticOscillator(
                df['high'], df['low'], df['close'], window=int(period), smooth_window=int(smooth_period))
            return f'Current Stochastic Oscillator value is: {stotch.stoch().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

    # True Strength Index
    def tsi(candlesticks, slow_period, fast_period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            tsi = TSIIndicator(df['close'], window_slow=int(slow_period), window_fast=int(fast_period))
            return f'Current True Strength Index value is: {tsi.tsi().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def fetch(symbol, timeframe):
        symbol = symbol.replace('/', '')
        symbol = symbol.upper()
        timeframe_map = {
            "1 minute": "1m",
            "1 min": "1m",
            "1min": "1m",
            "M1": "1m",
            "5 minutes": "5m",
            "5 min": "5m",
            "5min": "5m",
            "M5": "1m",
            "15 minutes": "15m",
            "15 min": "15m",
            "15min": "15m",
            "M15": "15m",
            "30 minutes": "30m",
            "30 min": "30m",
            "30min": "30m",
            "M30": "30m",
            "1 hour": "1h",
            "1hour": "1h",
            "1hr": "1h",
            "H1": "1h",
            "4 hours": "4h",
            "4hours": "4h",
            "4 hrs": "4h",
            "H4": "4h",
            "1 day": "1d",
            "1day": "1d",
            "D1": "1d",
            "1 week": "1w",
            "1week": "1w",
            "W1": "1w",
            "1 month": "1mn",
            "1month": "1mn",
            "M1": "1mn"

        }
        # Check if the user input matches any of the keys in the dictionary
        if timeframe in timeframe_map:
            timeframe = timeframe_map[timeframe]
        else:
            # Assume that the user input is already in the correct format
            pass

        url = f"https://mt-market-data-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/historical-market-data/symbols/{symbol}/timeframes/{timeframe}/candles?limit=100"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)
        if response:
            candlesticks = response.json()
            return candlesticks
        else:
            return 'Failed to get candlesticks.'

# From auto_gpt_metatrader/indicators.py
def money_flow_index(candlesticks, period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close', 'tickVolume'])
            # Clean NaN values
            df = dropna(df)
            
            mfi = MFIIndicator(df['high'], df['low'], df['close'], volume=df['tickVolume'], window=int(period))
            return f'Current MFI Value: {mfi.money_flow_index().iloc()[-1]}'
        else:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def volume(candlesticks):
        if candlesticks:
            volumes = [float(candlestick['tickVolume']) for candlestick in candlesticks]
            return np.sum(volumes[-14:])
        else:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def rsi(candlesticks, period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            # Clean NaN values
            df = dropna(df)
            rsi_indicator = RSIIndicator(df['close'], window=float(period))
            current_rsi = rsi_indicator.rsi().iloc[-1]
            return f'Current RSI Value: {current_rsi}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def sma(candlesticks, period):
        # Get the candlesticks data
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            df = dropna(df)
            sma = SMAIndicator(df['close'], window=float(period))
            return f'Current Simple Moving Average Value: {sma.sma_indicator().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def ema(candlesticks, period):
        # Get the candlesticks data
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            df = dropna(df)
            ema = EMAIndicator(df['close'], window=float(period))
            return f'Current Exponential Moving Average Value: {ema.ema_indicator().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def wma(candlesticks, period):
        # Get the candlesticks data
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            df = dropna(df)
            wma = WMAIndicator(df['close'], window=float(period))
            return f'Current Weighted Moving Average Value: {wma.wma_indicator().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def macd(candlesticks, fast_period, slow_period, signal_period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
    # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            macd = MACD(df['close'], window_slow=int(
                        12), window_fast=int(26), window_sign=int(9))
            signal_line = macd.macd_signal().iloc()[-1]
            macd_line = macd.macd().iloc()[-1]
            macd_diff = macd.macd_diff().iloc()[-1]
            macd_values = {"macd_line": macd_line, 'macd_diff': macd_diff, 'signal_line': signal_line}
            return f'Current MACD Values: {macd_values}'
        
        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def adx(candlesticks, period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close'])
            # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            adx = ADXIndicator(df['high'], df['low'], df['close'], window=int(period))
            return f' Current Average Directional Movement Index Value: {adx.adx().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def adi(candlesticks):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close', 'tickVolume'])
            # Clean NaN values
            df = dropna(df)
            adi = AccDistIndexIndicator(df['high'], df['low'], df['close'], volume=df['tickVolume'])
            return f'Current Accumulation/Distribution Index: {adi.acc_dist_index().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def fib_retracements(candlesticks, high, low):
        high = float(high)
        low = float(low)
        levels = [0.236, 0.382, 0.5, 0.618, 0.786]
        diff = high - low
        retracements = []
        for level in levels:
            retracements.append(high - level * diff)
        return f'Current Fibonacci Retracements are: {retracements}'

# From auto_gpt_metatrader/indicators.py
def stochastic_oscillator(candlesticks, period, smooth_period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['high', 'low', 'close'])
            # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            stotch = StochasticOscillator(
                df['high'], df['low'], df['close'], window=int(period), smooth_window=int(smooth_period))
            return f'Current Stochastic Oscillator value is: {stotch.stoch().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'

# From auto_gpt_metatrader/indicators.py
def tsi(candlesticks, slow_period, fast_period):
        if candlesticks:
            df = pd.DataFrame(candlesticks, columns=['close'])
            # Clean NaN values
            df = dropna(df)
            # Calculate the RSI values
            tsi = TSIIndicator(df['close'], window_slow=int(slow_period), window_fast=int(fast_period))
            return f'Current True Strength Index value is: {tsi.tsi().iloc()[-1]}'

        if not candlesticks:
            return f'Failed to get candlesticks'


# From auto_gpt_metatrader/fcs.py
class Fcs():
    def get_important_forex_news() -> str:
            url = 'https://fcsapi.com/api-v3/forex/economy_cal'
            params = {
                'access_key': fcs_api
            }

            try:
                response = requests.get(url, params=params)
                response.raise_for_status()  # Raise an exception if the response status is not OK (2xx)

                json_data = response.json()
                important_events = []
                for item in json_data['response']:
                    if item['importance'] == '2':
                        important_events.append(item)
                return important_events

            except requests.exceptions.RequestException as e:
                
                return f'Error fetching data from FCS API:{e}'

# From auto_gpt_metatrader/fcs.py
def get_important_forex_news() -> str:
            url = 'https://fcsapi.com/api-v3/forex/economy_cal'
            params = {
                'access_key': fcs_api
            }

            try:
                response = requests.get(url, params=params)
                response.raise_for_status()  # Raise an exception if the response status is not OK (2xx)

                json_data = response.json()
                important_events = []
                for item in json_data['response']:
                    if item['importance'] == '2':
                        important_events.append(item)
                return important_events

            except requests.exceptions.RequestException as e:
                
                return f'Error fetching data from FCS API:{e}'


# From auto_gpt_metatrader/lunarcrush.py
class LunarCrush():
    def get_stock_of_the_day() -> float:
        url = "https://lunarcrush.com/api3/stockoftheday"
        headers = {
            'Authorization': f'Bearer {lunarcrush_api}'
        }

        response = requests.request("GET", url, headers=headers)

        if response.status_code == 200:
            return response.text.encode('utf8')
        else:
            raise Exception(
                f"Failed to get Stock of the day from LunarCrush; status code {response.status_code}")

# From auto_gpt_metatrader/lunarcrush.py
def get_stock_of_the_day() -> float:
        url = "https://lunarcrush.com/api3/stockoftheday"
        headers = {
            'Authorization': f'Bearer {lunarcrush_api}'
        }

        response = requests.request("GET", url, headers=headers)

        if response.status_code == 200:
            return response.text.encode('utf8')
        else:
            raise Exception(
                f"Failed to get Stock of the day from LunarCrush; status code {response.status_code}")


# From auto_gpt_metatrader/trading.py
class Trading():
    def fetch_candlesticks(symbol, timeframe):
        symbol = symbol.replace('/', '')
        symbol = symbol.upper()
        timeframe_map = {
            "1 minute": "1m",
            "1 min": "1m",
            "1min": "1m",
            "M1": "1m",
            "5 minutes": "5m",
            "5 min": "5m",
            "5min": "5m",
            "M5": "1m",
            "15 minutes": "15m",
            "15 min": "15m",
            "15min": "15m",
            "M15": "15m",
            "30 minutes": "30m",
            "30 min": "30m",
            "30min": "30m",
            "M30": "30m",
            "1 hour": "1h",
            "1hour": "1h",
            "1hr": "1h",
            "H1": "1h",
            "4 hours": "4h",
            "4hours": "4h",
            "4 hrs": "4h",
            "H4": "4h",
            "1 day": "1d",
            "1day": "1d",
            "D1": "1d",
            "1 week": "1w",
            "1week": "1w",
            "W1": "1w",
            "1 month": "1mn",
            "1month": "1mn",
            "MN": "1mn"
        }
        # Check if the user input matches any of the keys in the dictionary
        if timeframe in timeframe_map:
            timeframe = timeframe_map[timeframe]
        else:
            # Assume that the user input is already in the correct format
            pass

        url = f"https://mt-market-data-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/historical-market-data/symbols/{symbol}/timeframes/{timeframe}/candles?limit=15"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)
        if response:
            candlesticks = response.json()
            return candlesticks
        else:
            return 'Failed to get candlesticks.'

    

    def close_trade(position_id) -> None:
        trade_data = {
            'actionType': 'POSITION_CLOSE_ID',
            'positionId': position_id
        }
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
        response = requests.post(
            url, headers=headers, json=trade_data)
        response = response.json()
        return response

    def close_all_trades():
        url2 = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        positions = requests.get(url2, headers=headers)
        positions = positions.json()
        responses = []
        for position in positions:
            trade_data = {
                'actionType': 'POSITION_CLOSE_ID',
                'positionId': position['id']
            }
            url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
            response = requests.post(
                url, headers=headers, json=trade_data)
            if response:
                print(f"Successfully closed trade for {position['symbol']}")
            else:
                print(f"Failed to close trade for {position['symbol']}")
            if response:
                responses.append(f"Successfully closed trade for {position['symbol']}")
            else:
                responses.append(f"Failed to close trade for {position['symbol']}")

        if responses:
            return responses
        else:
            return f'No trades to close.'

    def get_positions():
        url2 = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        positions = requests.get(url2, headers=headers)
        if positions:
            positions = positions.json()
            return positions
        else:
            return f'Failed to get positions'

    def get_account_information():
        url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/account-information"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)
        if response:
            response = response.json()
            return response
        else:
            return f'Failed to get account information'

    def place_trade(symbol, volume, signal) -> None:
        signal = signal.upper()
        symbol = symbol.upper()
        symbol = symbol.replace('/', '')
        # Place the new trade
        if signal == 'BUY':
            trade_data = {
                'symbol': symbol,
                'actionType': 'ORDER_TYPE_BUY',
                'volume': float(volume),
                'comment': 'Auto-GPT MetaTrader Plugin'
            }
            headers = {
                "auth-token": token,
                "Content-Type": "application/json"
            }
            url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
            response = requests.post(url, headers=headers, json=trade_data)

            response = response.json()
            return response

        elif signal == 'SELL':
            trade_data = {
                'symbol': symbol,
                'actionType': 'ORDER_TYPE_SELL',
                'volume': float(volume),
                'comment': 'Auto-GPT MetaTrader Plugin'
            }
            headers = {
                "auth-token": token,
                "Content-Type": "application/json"
            }
            url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
            response = requests.post(url, headers=headers, json=trade_data)

            response = response.json()
            return response

# From auto_gpt_metatrader/trading.py
def fetch_candlesticks(symbol, timeframe):
        symbol = symbol.replace('/', '')
        symbol = symbol.upper()
        timeframe_map = {
            "1 minute": "1m",
            "1 min": "1m",
            "1min": "1m",
            "M1": "1m",
            "5 minutes": "5m",
            "5 min": "5m",
            "5min": "5m",
            "M5": "1m",
            "15 minutes": "15m",
            "15 min": "15m",
            "15min": "15m",
            "M15": "15m",
            "30 minutes": "30m",
            "30 min": "30m",
            "30min": "30m",
            "M30": "30m",
            "1 hour": "1h",
            "1hour": "1h",
            "1hr": "1h",
            "H1": "1h",
            "4 hours": "4h",
            "4hours": "4h",
            "4 hrs": "4h",
            "H4": "4h",
            "1 day": "1d",
            "1day": "1d",
            "D1": "1d",
            "1 week": "1w",
            "1week": "1w",
            "W1": "1w",
            "1 month": "1mn",
            "1month": "1mn",
            "MN": "1mn"
        }
        # Check if the user input matches any of the keys in the dictionary
        if timeframe in timeframe_map:
            timeframe = timeframe_map[timeframe]
        else:
            # Assume that the user input is already in the correct format
            pass

        url = f"https://mt-market-data-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/historical-market-data/symbols/{symbol}/timeframes/{timeframe}/candles?limit=15"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)
        if response:
            candlesticks = response.json()
            return candlesticks
        else:
            return 'Failed to get candlesticks.'

# From auto_gpt_metatrader/trading.py
def close_trade(position_id) -> None:
        trade_data = {
            'actionType': 'POSITION_CLOSE_ID',
            'positionId': position_id
        }
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
        response = requests.post(
            url, headers=headers, json=trade_data)
        response = response.json()
        return response

# From auto_gpt_metatrader/trading.py
def close_all_trades():
        url2 = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        positions = requests.get(url2, headers=headers)
        positions = positions.json()
        responses = []
        for position in positions:
            trade_data = {
                'actionType': 'POSITION_CLOSE_ID',
                'positionId': position['id']
            }
            url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
            response = requests.post(
                url, headers=headers, json=trade_data)
            if response:
                print(f"Successfully closed trade for {position['symbol']}")
            else:
                print(f"Failed to close trade for {position['symbol']}")
            if response:
                responses.append(f"Successfully closed trade for {position['symbol']}")
            else:
                responses.append(f"Failed to close trade for {position['symbol']}")

        if responses:
            return responses
        else:
            return f'No trades to close.'

# From auto_gpt_metatrader/trading.py
def get_positions():
        url2 = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/positions"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        positions = requests.get(url2, headers=headers)
        if positions:
            positions = positions.json()
            return positions
        else:
            return f'Failed to get positions'

# From auto_gpt_metatrader/trading.py
def get_account_information():
        url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/account-information"
        headers = {
            "auth-token": token,
            "Content-Type": "application/json"
        }
        response = requests.get(url, headers=headers)
        if response:
            response = response.json()
            return response
        else:
            return f'Failed to get account information'

# From auto_gpt_metatrader/trading.py
def place_trade(symbol, volume, signal) -> None:
        signal = signal.upper()
        symbol = symbol.upper()
        symbol = symbol.replace('/', '')
        # Place the new trade
        if signal == 'BUY':
            trade_data = {
                'symbol': symbol,
                'actionType': 'ORDER_TYPE_BUY',
                'volume': float(volume),
                'comment': 'Auto-GPT MetaTrader Plugin'
            }
            headers = {
                "auth-token": token,
                "Content-Type": "application/json"
            }
            url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
            response = requests.post(url, headers=headers, json=trade_data)

            response = response.json()
            return response

        elif signal == 'SELL':
            trade_data = {
                'symbol': symbol,
                'actionType': 'ORDER_TYPE_SELL',
                'volume': float(volume),
                'comment': 'Auto-GPT MetaTrader Plugin'
            }
            headers = {
                "auth-token": token,
                "Content-Type": "application/json"
            }
            url = f"https://mt-client-api-v1.{region}.agiliumtrade.ai/users/current/accounts/{account_id}/trade"
            response = requests.post(url, headers=headers, json=trade_data)

            response = response.json()
            return response

